# Emopathy: A Centralized Hub for Emotion and Empathy Recognition

CKJ: **Graphics Placeholder**

> **"I do not ask the wounded person how he feels, I myself become the wounded person."**  
> â€“ Walt Whitman, *Song of Myself*

---

Emotion and empathy are at the heart of human interaction, shaping how we understand and respond to one another. While emotion recognition has made significant strides in NLP and AI, driven by benchmark datasets like IEMOCAP, empathy remains underexplored due to the lack of standardized datasets and benchmarks. This paper provides an integrated summary of available resources for emotion and empathy-related tasks, including datasets, labeling conventions, and SOTA models. By synthesizing these elements, we aim to bridge existing gaps, introduce SOTA AI/ML models, and discuss implications and future directions to advance AI-driven understanding of human emotions and empathy.

Emopathy is a companion repository to this review paper, designed as a centralized hub for researchers. It consolidates the datasets, models, and insights discussed in the paper, offering a practical and insightful resource for fostering innovation and exploration in emotion and empathy recognition.

---

## Our Approach

To curate the content, we conducted an exhaustive review of **emotion** and **empathy** datasets and related AI/ML research. Our methodology included the following steps:

1. **Database Search**  
   We leveraged reputable sources such as **Scopus**, **Web of Science**, **ProQuest**, **ACM**, **Google Scholar**, and **ACL** to gather a comprehensive collection of publications.  

2. **Boolean Filtering**  
   Advanced search queries were employed to identify relevant works published after 2014. Scripts for this process are available:  
   - [Boolean Search Scripts](https://github.com/ninackjeong/emopathy-dataset-review/tree/main/boolean-search)  
   - [ACL Filtering Script](https://github.com/ninackjeong/emopathy-dataset-review/blob/main/filtering.py)  
   - [Filtering Script for Other Databases](https://github.com/ninackjeong/emopathy-dataset-review/blob/main/filtering.py)  

3. **(Semi-)Manual Screening**  
   The results were refined by (semi-)manual screening to ensure the datasets and papers met our quality and relevance criteria.


### Findings Overview
CKJ: Update numbers after finishing final screening!

- **Emotion Datasets**: Identified **FFF** datasets spanning text, audio, and video.  
- **Empathy Datasets**: Discovered **GGG** datasets, underscoring the need for further development.  
- **AI/ML Models**:  
  - Emotion recognition: **HHH** papers reviewed; top **XXX** models ranked by F1 scores.  
  - Empathy recognition: **III** papers identified; all models included due to limited research.  

---

## Trending Emotion/Empathy-related Tasks
CKJ: Forshadowing our future works!

We are currently analyzing emerging trends in emotion and empathy research. Expect updates on:  
- **Multimodal Emotion Recognition**: Integrating text, speech, and visual cues for enhanced context understanding.  
- **Empathy Prediction in Conversational AI**: Understanding human-like empathetic responses in dialogue systems.  
- **Applications in Healthcare**: Leveraging emotion and empathy recognition in mental health support, therapy, and patient care.  

---

## Table of Contents
- [Benchmark Datasets](#benchmark-datasets)  
  - [Emotion](#emotion)  
  - [Empathy](#empathy)  
- [SOTA ML Models](#sota-ml-models)  
  - [Emotion Recognition](#emotion-recognition)  
  - [Empathy Recognition](#empathy-recognition)  

---

## Benchmark Datasets
CKJ: We are working on this on [a separate sheet](https://docs.google.com/spreadsheets/d/1704Q1WFzSVgyDUeczfqA7h7QPOQgYPyXtyOO2MJmFHk/edit?gid=1071129490#gid=1071129490)

### Emotion
| Dataset | Author | Year	| Conversation Setting | Corpus Setting	| Modality | Source	| Labels | Annotation	| Statistics | Dataset Link | Paper Link |
| ------- | ------ | ---- | -------------------- | -------------- | -------- | ------ | ------ | ---------- | ---------- | ------------ | ---------- |
| IEMOCAP | Busso et al. | 2008 | Dyadic | Laboratory | Text/audio/video (facial/hand movements) | The use of plays (scripted sessions), and improvisation based hypothetical scenarios (spontaneous sessions) | <ul><li>Categorical: 8 emotions (Ekman's 7 emotions + neutral)</li><li>Continuous: activation, valency, etc.</li></ul> | Subjects after recording (self-assessment) & 6 human evaluators | <ul><li>ten actors (female 5, male 5) were recorded in dyadic sessions (5 sessions with 2 subjects each)</li><li>12 hours</li><li>10039 (scripted session: 5255 turns; spontaneous sessions: 4784 turns) with an average duration of 4.5 seconds. The average value of words per turn was 11.4.</li></ul> | [By request](https://sail.usc.edu/iemocap/index.html) | [IEMOCAP: Interactive emotion dyadic motion capture database](https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf) | <!-- rows 2- 33 go here> 
| BoLD | Yu Luo, et al. | 2018 | Emotional expression and body movement | Capture spontaneous bodily expressions in naturalistic settings | Visual (videos) | Diverse video sources to ensure a wide range of spontaneous bodily expressions | <ul><li>categorical</li><li>emotion categories</li></ul> | <ul><li>Annotations include emotional labels assigned to each video clip, focusing on perceived emotions based on body movements</li></ul> | <ul><li>Contains 9,876 video clips featuring 13,239 human characters</li></ul> | [Body Language Dataset](https://paperswithcode.com/dataset/bold) | [ARBEE: Towards Automated Recognition of Bodily Expression of Emotion In the Wild](https://arxiv.org/abs/1808.09568)|
| CMU-MultiPIE | Ralph Gross, et al. | 2008 | Facial images | Controlled lab environment, subjects were imaged under various conditions including different poses, illuminations and expressions | Visual (image data) | Collected at Carnegie Mellon University | <ul><li>Categorical</li><li>images were labeled based on subject idenitty, pose, illumination condition, and facial expression</li></ul> | <ul><li>Each image is annotated with metadata specifying the subject ID, camera viewpoint, illumination condition, and the type of facial expression displayed</li></ul> | <ul><li>Comprises over 750,000 images of 337 subjects, captured across up to 4 sessions over 5 months</li><li>337 subjects were captured under 15 viewpoints and 19 illumination conditions</li></ul> | [Database](https://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html) | [Multi-PIE](https://ieeexplore.ieee.org/document/4813399) |
| RAF-DB | Shan Li, Weihong Deng, and Jun Ping Du | 2017 | Static facial images | Real-world, unconstrained environments, images were collected from internet | Visual (static facial images) | Internet, encompassing diverse subjects in terms of age, gender, ethnicity, head poses, lighting conditions, and occlusions | <ul><li>categorical</li><li>based expressions or compound expression</li></ul> | <ul><li>Anger, disgust, fear, happiness, sadness, surprise, neutral</li><li>Each image was annotated by 40 independent taggers to ensure reliability</li></ul> | <ul><li>Contains 29,672 facial images with 15,339 images labeled with basic expressions and 14,33 images labeled with compound</li></ul> | [dataset](https://www.whdeng.cn/RAF/model1.html) | [Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild](https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Reliable_Crowdsourcing_and_CVPR_2017_paper.pdf) |
 | iCV-MEFED | Jianfeg Guo, et al. | 2018 | Static facial images | Controlled lab environment, images captured under uniform lighting conditions with consistent background  | Visual (static facial images) | 125 subjects that are a balanced representation of genders and diverse ethnic backgrounds, aged between 18 and 37 years | <ul><li>categorical</li><li>each image is labeled with one of 50 compound emotion categories</li></ul> | <ul><li>Each image is annotated with a specific compound emotion label, combining a dominant and a complementary emotion</li><li>Labels were assessed and validated by psychologists to ensure accuracy</li></ul> | <ul><li>Contains 31,250 images with each of the 125 subjects contributing 250 images</li><li>5 samples for each of the 50 emotion categories</li></ul> |[dataset](https://www.researchgate.net/figure/Sample-of-different-emotion-categories-in-the-iCV-MEFED-dataset_fig2_370605669)|[Emotion Recognition Based on Facial Expressions Using Convolutional Neural Network (CNN)](https://ieeexplore.ieee.org/document/9302866)|
| AFEW | Abhinav Dhall, et al. | 2012 | Group (Acted facial expressions) | Movies | Audio-visual | 54 movies, encompassing a wide range of genres and scenarios to ensure diversity in expressions and contexts | <ul><li>categorical</li><li>one of 7 emotions (anger, disgust, fear, happiness, netural, sadness, and surprise)</li></ul> | <ul><li>Each clip is annotated with emotion lable corresponding to predominant facial expression</li><li>Each clip annotated with metadata including in the movie source, scene details, and temporal information</li></ul> | <ul><li>1,809 video clips</li><li>54 movies with each clip lasting between 0.5 to 4 seconds</li></ul> |[Acted Facial Expressions in the Wild](https://paperswithcode.com/dataset/acted-facial-expressions-in-the-wild-afew)|[AFEW-VA Database for Valence and Arousal estimation In-The-Wild](https://www.researchgate.net/publication/313685463_AFEW-VA_Database_for_Valence_and_Arousal_estimation_In-The-Wild)|
| KDEF | Daniel Lundqvist and Manuel Calvo |  1998 | Static facial images | Controlled lab environment | Visual (static facial images) | Images of 7- individuals (Half female and half male) | <ul><li>categorical</li><li>each image is labeled with one of the seven basic emotion categories</li></ul> | <ul><li>Emotion lable corresponding to the displayed facial expression</li><li>Metadata including the model's ID, gender, and the angle of the photograph</li></ul> | <ul><li>4900 images</li><li>70 models</li></ul> |[KDEF](https://www.kdef.se/)|[Facial expressions of emotion (KDEF): Identification under different display-duration conditions](https://link.springer.com/article/10.3758/BRM.40.1.109)|
| JAFFE | Lyons et al. | 1998 | Monadic | Controlled. Each expresser took pictures of herself while looking through a semi-reflective plastic sheet towards the camera. Tungsten lights were positioned to illuminate the face evenly. A box enclosed the region between the camera and plastic sheet to reduce back-reflection | Visual (images) | Images of female participants 8-bit grayscale, .tiff format, no compression | categorical: Ekman's six (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and neutral | <ul><li>Models code facial expression images using a multi-orientation, multiresolution set of Gabor filters that are topographically ordered and approximately aligned with the face</li><li>60 Japanese participants also label the facial expressions </li></ul> | <ul><li> 219 images </li><li> 10 female participants posed 3 or 4 examples of each of the six basic facial expressions (happiness, sadness, surprise, anger, disgust, fear) and a neutral face </li></ul> | [JAFFE](https://zenodo.org/records/14974867)| [Coding Facial Expressions with Gabor Wavelets (IVC Special Issue)](https://arxiv.org/pdf/2009.05938) |
| MMI | Pantic et al. | 2005 | Monadic |Controlled ~1/4 of samples had natural lighting and variable backgrounds were used. ~ 3/4 of samples used a blue screen background and two high-intensity lamps with reflective umbrellas | visual (image and videos) | Recordings of the full temporal pattern of a facial expressions, from Neutral, through a series of onset, apex, and offset phases and back again to a neutral face. Recordings of naturalistic expressions were later added. Each session is a recording of induced laughter (subject watched comedy clips), and lasts about 20 minutes | <ul><li>categorical: Ekman's six (Anger, Disgust, Fear, Happiness, Sadness, Surprise)</li><li>FACS Action Unit (AU) activated indicating for each frame whether an AU is in the neutral, onset, apex or offset phase</li></ul> | Participants were instructed by an expert (a FACS coder) on how to display the required facial expressions, and they were asked to include a short neutral state at the beginning and at the end of each expression | <ul><li>Over 2900 images and videos of 75 subjects</li><li>Session 2401-2894 contain images</li><li>1395 were AU coded, 197 were categorized with Ekman's six emotions</li><li>Session 2895-2903 contain videos</li></ul> |[link may not be secure](https://service.tib.eu/ldmservice/dataset/mmi-database)|[Web-based Database for Facial Expression Analysis](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1521424)|
| BU-4DFE | Yin et al. | 2008 | Monadic | Controlled. Recording setup using a dynamic face capturing system | Dynamic 3D models created from a 3D video sequences. Resolution of approx. 35,000 vertices | 3D recordings of participants who acted out the emotions | categorical: Ekman's six (Anger, Disgust, Fear, Happiness, Sadness, Surprise)	With the guidance of a psychologist, each subject was requested to perform the 6 emotions | With the guidance of a psychologist, each subject was requested to perform the 6 emotions| <ul><li>606 3D facial expression sequences from each of the 101 subjects</li><li>61,206 total sequences</li><li>58 female and 43 male subjects, with a variety of ethnic/racial ancestries, including Asian, Black, Latino, and White</li></ul> |[Analyzing Facial Expressions and Emotions in Three Dimensional Space with Multimodal Sensing](https://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)|[A High-Resolution 3D Dynamic Facial Expression Database](https://www.researchgate.net/publication/224401018_A_high-resolution_3D_dynamic_facial_expression_database)|
| BU-EEG | Li et al. | 2020 | Monadic | Controlled Recording setup with 128 sensors around the participant's head to record EEG signals | EEG signals and videos | EEG signals and face videos of both posed facial actions and spontaneous expressions recorded for this database | <ul><li>Ekman's six emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise)</li><li>10 facial action units</li></ul> | Each subject was requested to perform the specific emotion/facial action unit | <ul><li>29 participants with different ages, genders, ethnic backgrounds</li><li>2,320 recorded experiment trials</li></ul> |[Analyzing Facial Expressions and Emotions in Three Dimensional Space with Multimodal Sensing](https://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)|[	An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis](https://ieeexplore.ieee.org/document/9320173)|
| BP4D++ | Li et al. | 2023 | Monadic | Controlled recording setup | video and physiological measurements. 3D sequence, 2D RGB sequence, thermal sequence, and the sequences of physiological data (e.g., heart rate, blood pressure, skin conductance (EDA), and respiration rate), and meta-data (facial features and partially coded FACS) | Recordings of participants who acted out the emotions | 10 emotions	| Annotated by three expert Facial Action Coding System (FACS) coders for Action Unit (AU) coding | <ul><li> 233 participants (132 females and 101 males ages 18 to 70 years old)</li><li>Each participant acted out 10 emotions</li><li>94,000 frames</li></ul> |[Analyzing Facial Expressions and Emotions in Three Dimensional Space with Multimodal Sensing](https://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)|[Disagreement Matters: Exploring Internal Diversification for Redundant Attention in Generic Facial Action Analysis](https://ieeexplore.ieee.org/document/10154146)|
| CASME II | Yan et al. | 2014 | Monadic | Controlled laboratory environment | Visual (videos) | Recordings of spontaneous and dynamic micro-expressions | categorical: 5 micro-expressions - happiness, disgust, surprise, repression, other | Two coders annotated the micro-expressions. Aftewards, the coders discussed and arbitrated the disagreements | <ul><li>247 micro-expressions</li><li>The recordings have high temporal resolution (200 fps) and relatively higher face resolution at 280Ã—340 pixels</li><li>35 participants (mean age 22.03 years old)</li></ul> |[CASME](http://casme.psych.ac.cn/casme/e2)|[CASME II: An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation](https://pmc.ncbi.nlm.nih.gov/articles/PMC3903513/pdf/pone.0086041.pdf)|
| SFEW | Dhall et al. | 2011 | Monadic, dyadic, and group | Movie set | Visual and audio (videos) | Selected frames from the movie clips used in the AFEW database | categorical: Ekman's six (Angry, Disgust, Fear, Happy, Sad, Surprise) and neutral | Two independent annotators | <ul><li>700 images from 37 movies</li><li>95 subjects</li></ul> |[AFEW Database](https://users.cecs.anu.edu.au/~few_group/AFEW.html)|[Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark](https://ieeexplore.ieee.org/document/6130508)|
| DFEW | Jiang et al. | 2020 |  Monadic, dyadic, and group | Movie set | Visual and audio (videos) | Clips from thousands of movies | categorical: Ekman's six (Angry, Disgust, Fear, Happy, Sad, Surprise) and neutral | <ul><li>Used crowdsourcing to hire 12 expert annotators</li><li>Each clip was independently annotated 10 times</li></ul> | <ul><li>16,372 movie clips from thousands of movies</li><li>At most 20 clips from each movie</li></ul> |[DFEW Database](https://dfew-dataset.github.io/)|[DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild](https://arxiv.org/pdf/2008.05924)|
| GEMEP | BÃ¤nziger et al. | 2012 |  Monadic | Laboratory | Visual and audio (videos) | Recordings of 10 participants who acted out the specific emotions | categorical: 18 emotions <ul><li>joy (elation), amusement, pride, pleasure, relief, interest, admiration, tenderness, surprise, cold anger (irritation), hot anger (rage), panic fear, anxiety (worry), despair, sadness (depression), disgust, contempt, shame</li></ul> | Each subject was requested to perform the specific emotion | <ul><li>1,260 recordings</li><li>10 professional theater actors (5 males, 5 females)</li><li>Each actor had multiple scenarios for each of the emotion categories that the actor had to enact</li></ul> |[](https://www.unige.ch/cisa/gemep)|[Introducing the Geneva Multimodal Expression Corpus for Experimental Research on Emotion Perception](https://psycnet.apa.org/fulltext/2011-25876-001.pdf?auth_token=269eaa9798fda2057ade2ae4870c9723b1da0afe)|
| SEED-VII | Jiang et al. | 2024 | Monadic, dyadic, and group | Laboratory | EEG signals and eye movement | Recordings of participants reacting to movie clips | categorical: Ekman's six (Anger, Disgust, Fear, Happy, Sad, Surprise) and neutral | Participant self-assessment | Each participant underwent 4 sessions where they viewed 20 clips per session <ul><li>12 video clips for each of the 6 emotions, 8 video clips for neutral</li><li>Each clip was 2-5 minutes</li></ul> |[SEED-VII](https://bcmi.sjtu.edu.cn/home/seed/seed-vii.html)|[SEED-VII: A Multimodal Dataset of Six Basic Emotions with Continuous Labels for Emotion Recognition](https://ieeexplore.ieee.org/document/10731546)|
| AMIGOS | Miranda-Correa et al. | 2021 | Monadic and group | Laboratory | EEG, ECG, and GSR signals | Recordings of participants reacting to videos | <ul><li> valence, arousal, control, familiarity, like/dislike</li><li>categorical: Ekman's six (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and neutral</li></ul> | <ul><li>Participant self-assessment</li><li>External assessment by 3 annotators</li></ul> | The data is collected in two experimental settings <ul><li>In the first one, 40 participants watched 16 short emotional videos while they were alone</li><li>In the second one, the same participants watched 4 long videos, some of them alone and the rest in groups</li></ul> |[AMIGOS dataset](https://www.eecs.qmul.ac.uk/mmv/datasets/amigos/)|[AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups](https://www.eecs.qmul.ac.uk/mmv/datasets/amigos/doc/Paper_TAC.pdf)|
| CreativeIT | Metallinou et al. | 2015 | Dyadic | Laboratory. Each actor wore a special body suit and 45 markers were placed across his/her body | Video and motion | Recordings of improvised conversations of actors | Dimensional: activation, valence, and dominance	| Annotated by the Feeltrace software and by psychology students | <ul><li>16 actors</li><li>2 to 10-minute interactions</li><li>8 full sessions, each of which contains approximately 1 hour of audiovisual data</li><li>Recorded 33 two-sentence exercises and 17 paraphrases</li></ul> |[CreativeIT Database, Not Secure link?](https://www.slrb.net/USC-CreativeIT.html)|[The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations](https://link.springer.com/article/10.1007/s10579-015-9300-0)|
| DECAF | Abadi et al. | 2015 | Monadic | Laboratory | Magnetoencephalogram (MEG) signals, near-infra-red (NIR) facial videos, horizontal Electrooculogram (hEOG), Electrocardiogram (ECG), and trapezius-Electromyogram (tEMG) peripheral physiological responses | Magnetoencephalogram (MEG)signals, near-infra-red (NIR) facial videos, horizontal Electrooculogram (hEOG), Electrocardiogram (ECG), and trapezius-Electromyogram (tEMG) peripheral physiological responses | Dimensional: activation, valence, and dominance	| Participant self-assessment 30 university graduate students reacted to 40 one-minute music video segments and 36 movie clips | <ul><li>All stimuli were shown at 1024 Ã— 768 pixel resolution and a screen refresh rate of 60 Hz</li></ul>|[decaf dataset](https://decaf-dataset.github.io/)|[DECAF: MEG-based Multimodal Database for Decoding Affective Physiological Responses](https://decaf-dataset.github.io/DECAF/DECAF_TAC.pdf)|


### Empathy
| Dataset | Author | Year	| Conversation Setting | Corpus Setting	| Modality | Source	| Labels | Annotation	| Statistics | Dataset Link | Paper Link |
| ------- | ------ | ---- | -------------------- | -------------- | -------- | ------ | ------ | ---------- | ---------- | ------------ | ---------- |
| Five Factor Model | Lewis Goldberg | 1993 | N.A | Psycholexical analysis of personality descriptors from dictionaries and linguistic sources. | Textual â€“ Analysis of written language descriptors. | Dictionaries and linguistic corpora containing personality descriptors. | Categorical â€“ The study identified five primary personality factors: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. | Not applicable (N/A) â€“ Since the study did not involve participant data, there were no annotations in the traditional sense. | The research employed factor analysis to identify the five-factor structure, demonstrating that these factors accounted for a significant portion of the variance in personality descriptors. | N/A - This paper does not have a dataset |[An alternative "description of personality": the big-five factor structure](https://pubmed.ncbi.nlm.nih.gov/2283588/)|
| Empathetic Dialogues (Facebook AI) 25k | Aravind Sesagiri Raamkumar and Yinping Yang | 2022 | Dyadic | The conversations were collected via Amazon Mechanical Turk, where crowd-workers were paired to engage in dialogues based on specific emotional situations. | Textual â€“ The dataset contains text-based dialogues without accompanying audio or visual data. | comprises one million emotional conversations from movie subtitles | Categorical â€“ Each conversation is associated with one of 32 emotion labels, representing the emotional context of the dialogue. | Each dialogue includes annotations specifying the emotion category | The dataset contains 24,850 conversations, each grounded in emotional situations, facilitating the training and evaluation of dialogue systems in generating empathetic responses. |[Empathetic Dialogues (Facebook AI) 25k](https://www.kaggle.com/datasets/atharvjairath/empathetic-dialogues-facebook-ai)|[Empathetic Conversational Systems: A Review of Current Advances, Gaps, and Opportunities](https://arxiv.org/abs/2206.05017)|
| MEDIC | Zhou'an Zhu, Xin Li, et al. | 2023 | Face-to-face psychological counseling sessions between counselors and clients. | Multimodal â€“ Incorporates textual (transcripts), visual (video frames), and audio (speech) data. | Video recordings of actual counseling sessions | Collected from real-world counseling sessions | Categorical â€“ Each video clip is annotated with three labels: <ul><li>Expression of Experience (EE): Indicates whether the client has expressed experiences that can trigger empathy.</li><li>Emotional Reaction (ER): Reflects the counselor's affective response to the client's experience.</li><li>Cognitive Reaction (CR): Represents the counselor's understanding and perspective-taking regarding the client's experience.</li></ul> | Each clip is manually annotated for EE, ER, and CR using a three-level scale:<ul><li>0: No expression/reaction.</li><li>1: Weak expression/reaction.</li><li>2: Strong expression/reaction.</li></ul> | The dataset comprises 771 video clips with the following characteristics:<ul><li>Average Number of Speaking Turns per Clip: 4.29</li><li>Average Number of Words per Clip: 129.45</li><li>Average Duration per Clip: Approximately 52.76 seconds</li></ul> |[MEDIC Dataset](https://crisisnlp.qcri.org/medic/)|[MEDIC: A Multimodal Empathy Dataset in Counseling](https://arxiv.org/pdf/2305.02842)|
| OMG-Empathy | Pablo Barros, Nikhil Churamani, Angelica Lim, and Stefan Wermterâ€‹ | 2019 | Dyadic | Video recordings of interactions between speakers and listeners, each lasting approximately 5 minutes, with speakers conveying one of eight different stories designed to elicit varying emotional responses. | Multimodal â€“ Incorporates visual (video recordings), auditory (audio recordings), and textual (transcripts) data. | Each participant held 2 dialogues with each speaker, each of them based on a different storyline. Each story detailed a specific fictional situation and it demanded gradual changes in affective behavior from the speaker. | Continuous â€“ Listeners provided continuous self-assessed valence annotations (ranging from -1 to +1) indicating their emotional state throughout the storytelling session. | Listeners used a joystick to continuously rate their valence (emotional state) in real-time while watching the recorded interaction, capturing the dynamic nature of their empathic responses. | The dataset comprises 80 video recordings, totaling approximately 7 hours of interaction data, with each video averaging around 5 minutes and 12 seconds in length. |[OMG-Emotion (One-Minute Gradual-Emotional Behavior)](https://paperswithcode.com/dataset/omg-emotion)|[The OMG-Empathy Dataset: Evaluating the Impact of Affective Behavior in Storytelling](https://arxiv.org/abs/1908.11706)|
| OMG-Empathy: Affective Faces | Scott Geng, Revant Teotia, Purva Tendulkar, Sachit Menon, and Carl Vondrick | 2023 | Dyadic | Video recordings | Multimodal â€“ Includes visual (video recordings), auditory (audio recordings), and textual (automatic speech recognition transcripts) data. | sourced from "The Skin Deep," a public YouTube channel | Not explicitly labeled; the dataset focuses on natural interactions without predefined categorical labels. | The dataset includes pre-computed Automatic Speech Recognition (ASR) transcripts using Whisper, visual embeddings from various pretrained face models, and active speaker annotations using TalkNet. | Comprises of 692 videos, each averaging slightly over 10 minutes, resulting in a total of approximately 115 hours of video content. |[OMG-Emotion Dataset](https://paperswithcode.com/dataset/omg-emotion)|[Affective Faces for Goal-Driven Dyadic Communication](https://arxiv.org/abs/2301.10939)|
| Empathic Conversations: A Multi-level Dataset of Contextualized Conversations | Damilola Omitaomu, Shabnam Tafreshi, Tingting Liu, Sven Buechel, Chris Callison-Burch, Johannes Eichstaedt, Lyle Ungar, and JoÃ£o Sedoc | 2022 | Dyadic | Written information about pairs of participants engaging in discussions about provided news articles designed to elicit empathy and personal distress. | Textual | Collected from conversations | Continuous and Categorical â€“ The dataset includes various annotations: <ul><li>Self-Reported Empathy and Distress: Participants' self-assessments after reading news articles.</li><li>Counterpart Other-Report: Participants' ratings of their conversational partner's empathy.</li><li>Third-Party Annotations: Turn-by-turn assessments of self-disclosure, emotion, and empathy levels.</li></ul> | 500 conversations, each averaging 30 minutes in length (minimum 12 minutes, maximum 65 minutes)|[must contact them for dataset](https://paperswithcode.com/paper/empathic-conversations-a-multi-level-dataset")|[Affective Faces for Goal-Driven Dyadic Communication](https://arxiv.org/pdf/2301.10939)|
| LLM-GEm | Md Rakibul Hasan, Md Zakir Hossain, Tom Gedeon, Shafin Rahman | 2024 | N/A - includes only essays | College age Participants read newspaper articles and wrote essays reflecting their thoughts and feelings in response to the content. | Textual â€“ The dataset comprises written essays. | Collected from individuals' written reactions to newspaper articles. | Continuous â€“ Each essay is annotated with an empathy score, typically on a scale from 1 to 7, indicating the level of empathy expressed by the participant. | Annotations include self-reported empathy scores provided by participants after writing their essays. Additionally, demographic information such as age, gender, and education level is collected to analyze potential correlations between these factors and empathy levels. | To evaluate peopleâ€™s empathy towards newspaper articles, we experiment with three datasets, consisting of written essays in English, demographic data and ground truth empathy score. The NewsEmpathy v2 training dataset consists of whole NewsEmpathy v1 data samples, while the v2 validation and test sets consist of new samples. The v3 dataset |[LLM-GEm dataset](https://github.com/hasan-rakibul/LLM-GEm)|[LLM-GEm: Large Language Model-Guided Prediction of Peopleâ€™s Empathy Levels towards Newspaper Article](https://aclanthology.org/2024.findings-eacl.147.pdf)|
| Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: Task Formulations and Machine Learning Methods | Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, Shrikanth S. Narayanan | 2008 | N/A does not introduce a new dataset | Recorded in a controlled environment with professional actors engaging in emotional dialogues. | Multimodal â€“ Includes video, audio, and motion capture data. | Reviews IEMOCAP, SEMAINE, DEAP, and MELD | N/A | N/A | Provides a comprehensive overview of various approaches and methodologies for detecting empathy across multiple modalities. | N/A - This paper does not have a dataset |[Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: Task Formulations and Machine Learning Methods](https://arxiv.org/html/2311.00721v2)|
| Modeling Empathy and Distress in Reaction to News Stories | Sven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, and JoÃ£o Sedoc | 2018 | Individual written reactions (messages) to news articles, not conversations. | Participants read news articles and wrote short messages reflecting their reactions, which were then collected to form the dataset. | Textual | Collected textual reactions | Continuous â€“ Each message is annotated with two separate scores: <ul><li>Empathic Concern: Reflects the participant's other-oriented feelings of sympathy and compassion.</li><li>Personal Distress: Captures self-oriented feelings of discomfort and anxiety.</li></ul> | Participants provided self-assessments of their empathic concern and personal distress using multi-item scales after writing their reactions to the news articles. | comprises 1,860 messages written in response to 418 news articles, with each message averaging between 300 to 800 characters in length. |[empathetic reactions](https://github.com/wwbp/empathic_reactions)|[Modeling Empathy and Distress in Reaction to News Stories]( https://aclanthology.org/D18-1507/)|
| Empathy-Mental-Health | Ashish Sharma, Adam S. Miner, David C. Atkins, and Tim Althoffâ€‹ | 2020 | Dyadic | Collected from online mental health platforms, specifically TalkLife and mental health-related subreddits, focusing on peer-to-peer support conversations. | Textual â€“ The dataset comprises text-based posts and responses without accompanying audio or visual data. | <ul><li>TalkLife: A peer support network where individuals discuss mental health challenges.</li><li>Reddit: Specifically, subreddits related to mental health support.</li></ul> | Categorical â€“ Each response is annotated for empathy using a framework that includes three components: <ul><li>Emotional Reactions: Expressions of the responder's own emotions.</li><li>Interpretations: Demonstrations of understanding the seeker's feelings or experiences.</li><li>Explorations: Attempts to explore or inquire further into the seeker's situation.</li></ul> | Each (post, response) pair is annotated by trained freelancers for the three empathy components mentioned above. Annotations also include supporting evidence (rationales) highlighting specific parts of the text that justify the assigned labels | The dataset consists of 10,000 (post, response) pairs: <ul><li>TalkLife: 7,000 pairs.</li><li>Reddit: 3,000 pairs.</li></ul> Each response is annotated for empathy components, providing a rich resource for understanding empathy in text-based mental health support. |[Ask for access](https://github.com/behavioral-data/Empathy-Mental-Health)|[Empathy in Text-based Mental Health Support](https://github.com/behavioral-data/Empathy-Mental-Health?tab=readme-ov-file)|
| BAUM-1 | S. Zhalehpour, O. Onder, Z. Akhtar, and C. Eroglu Erdem | 2016 | Not applicable â€“ The dataset consists of individual spontaneous reactions to visual stimuli, not conversations. | Participants were shown a sequence of images and short video clips designed to elicit specific emotions and mental states. They then expressed their feelings and thoughts about the stimuli in their own words, without using predetermined script | Multimodal â€“ The dataset includes audio-visual recordings capturing both facial expressions and speech. | Collected at BahÃ§eÅŸehir University, Turkey, specifically for research in affective computing and mental state recognition | Categorical â€“ The dataset encompasses 13 distinct emotional and mental states: happiness, anger, sadness, disgust, fear, surprise, boredom, contempt, confusion, neutral, thinking, concentrating, and bothered. | Each video clip is labeled with the corresponding emotional or mental state based on the stimuli presented and the participant's spontaneous reaction. | The dataset comprises of 1,184 multimodal facial video clips collected from 31 Turkish subjects. These clips capture spontaneous facial expressions and speech corresponding to the 13 specified emotional and mental states. |[BAUM-1: A Spontaneous Audio-Visual Face Database of Affective and Mental States](https://www.semanticscholar.org/paper/BAUM-1%3A-A-Spontaneous-Audio-Visual-Face-Database-of-Zhalehpour-Onder/0629bc2b12245195af989e21573369329b7ef2b7)|[BAUM-1: A Spontaneous Audio-Visual Face Database of Affective and Mental States](https://ieeexplore.ieee.org/document/7451244)|
| 404 YouTube vloggers (194 M, 210 F)/ YouTube personality dataset | Javier B. Biel and Daniel Gatica-Perez | 2013 | Monologue â€“ Vloggers speaking directly to the camera, sharing personal thoughts, opinions, or experiences. | Collected from publicly available YouTube vlogs where individuals explicitly show themselves in front of a webcam, discussing a variety of topics including personal issues, politics, movies, and books. | Multimodal â€“ Includes manual transcriptions of speech (text), audio features, and visual features. | Publicly available YouTube vlogs | Continuous â€“ Personality impression scores based on the Big Five personality traits: Extraversion, Agreeableness, Conscientiousness, Neuroticism (Emotional Stability), and Openness to Experience. | â€‹Each vlog was annotated by multiple independent annotators using Amazon's Mechanical Turk platform. Annotators watched one-minute slices of each vlog and rated their impressions using a personality questionnaire | The dataset comprises of 404 vlogs from 404 unique vloggers (194 males and 210 females).Each vlog has corresponding manual transcriptions and personality impression scores.	|[BAUM-1](https://archive.ics.uci.edu/dataset/473/baum+1)|[Computational Personality Recognition in Social Media](https://core.ac.uk/download/pdf/55893069.pdf)|
| 47 (27 M 20 F)/YouTube dataset | Javier B. Biel and Daniel Gatica-Perez | 2012 | Monologue â€“ Vloggers speaking directly to the camera, sharing personal thoughts, opinions, or experiences. | Collected from publicly available YouTube vlogs where individuals explicitly show themselves in front of a webcam, discussing a variety of topics including personal issues, politics, movies, and books. | Multimodal â€“ Includes manual transcriptions of speech (text), audio features, and visual features. | Publicly available YouTube vlogs | Continuous â€“ Personality impression scores based on the Big Five personality traits: Extraversion, Agreeableness, Conscientiousness, Neuroticism (Emotional Stability), and Openness to Experience. | Each vlog was annotated by multiple independent annotators using Amazon's Mechanical Turk platform. Annotators watched one-minute slices of each vlog and rated their impressions using a personality questionnaire. | The dataset comprises of 47 vlogs from 47 unique vloggers (27 males and 20 females). Each vlog has corresponding manual transcriptions and personality impression scores. | Not available |[The YouTube Lens: Crowdsourced Personality Impressions and Audiovisual Analysis of Vlogs](https://www.idiap.ch/~gatica/publications/BielGatica-tmm12.pdf)|
| ISEAR | Klaus R. Scherer and Harald Wallbott | 1994 | Not applicable â€“ The dataset consists of individual self-reported experiences, not conversations | Participants from 37 countries provided written descriptions of situations in which they experienced specific emotions.â€‹ | Textual â€“ Written descriptions of emotional experiences. | Collected through surveys conducted across various countries. | Categorical â€“ Seven major emotions: joy, fear, anger, sadness, disgust, shame, and guilt. | Each entry includes the self-reported emotion category and a textual description of the situation that elicited the emotion | The dataset comprises of 7,666 samples, with each emotion category represented by approximately 1,000 instances |[Research Material](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)|[Text-Based Intelligent Learning Emotion System](https://www.scirp.org/journal/paperinformation?paperid=73931)|
| 47 (27 M and 20 F)/YouTube dataset SenticNet | Louis-Philippe Morency, Rada Mihalcea, and Payal Joshi | 2011 | Monologue â€“ Vloggers speaking directly to the camera, sharing personal opinions and reviews. | Collected from publicly available YouTube opinion videos where individuals express their sentiments on various topics. | Multimodal â€“ Includes manually transcribed text, audio features, and visual features. | Publicly available YouTube opinion videos. | Categorical â€“ Each utterance is annotated with sentiment polarity: positive, neutral, or negative. | Each video was segmented into utterances, and each utterance was annotated by multiple independent annotators for sentiment polarity. The annotations were then averaged to obtain the final label. | The dataset comprises of 47 videos from 47 unique vloggers (27 males and 20 females). Each video contains an average of 6 utterances, with each utterance approximately 5 seconds long, resulting in a total of 298 utterances. | N/A |[Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis](https://aclanthology.org/2023.findings-acl.821.pdf)|


## SOTA ML Models

### Emotion Recognition
The SOTA models for emotion recognition, ranked by F1 score:
- Categories: Transformer-based models, graph models, etc.  

### Empathy Recognition
A detailed list of empathy recognition models:
- Mostly regression tasks.



database,title,authors,year,journal,volume,issue,doi,abstract,keywords,url,type,category,is_dataset,is_ml
scopus,Emotion embedding framework with emotional self-attention mechanism for speaker recognition,"Li, D.; Yang, Z.; Liu, J.; Yang, H.; Wang, Z.",2024,,238,,10.1016/j.eswa.2023.122244,"The emotional states of speech have a great impact on the efficiency of speaker recognition (SR) system. Many researchers focus on how to map speech with different emotions to an emotion invariant embedding, which reduces the diversity of data. This paper proposes a new emotion embedding framework with self-attention mechanism for speaker recognition. First, several deep neural networks (DNNs) are trained to classify speakers in different emotional states as emotion embedding extractors during development phase. Then at enrollment stage, these pre-trained models are used to extend emotion embeddings from neutral speech. In order to make the final speaker embedding more representative, the classification model is trained with self-attention mechanism in emotion dimension, so that the framework can automatically annotate the weights of the emotion embeddings. Experiments were carried out on both Mandarin Affective Speech Corpus (MASC) and Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D). The results show the proposed method achieves the best of Identification Rate (IR) and Equal Error Rate (EER) which are 59.14%, 15.79% on MASC and 75.98%, 8.14% on CREMA-D compared with state-of-the-art methods. In addition, the cross-database experiments also further demonstrate the practicability of the method in real scenes. © 2023 Elsevier Ltd",Attention mechanisms; Emotion Recognition; Affective speech; Deep neural networks; Embeddings; Emotion embedding; Emotional self-attention; Emotional state; Emotional states; Multi-modal; Speaker recognition; Speaker recognition system; Speech corpora; Speech recognition,,,emotion,No,No
scopus,Enhancing Multimodal Emotion Recognition through Attention Mechanisms in BERT and CNN Architectures,"Makhmudov, F.; Kultimuratov, A.; Cho, Y.-I.",2024,,14,,10.3390/app14104199,"Emotion detection holds significant importance in facilitating human–computer interaction, enhancing the depth of engagement. By integrating this capability, we pave the way for forthcoming AI technologies to possess a blend of cognitive and emotional understanding, bridging the divide between machine functionality and human emotional complexity. This progress has the potential to reshape how machines perceive and respond to human emotions, ushering in an era of empathetic and intuitive artificial systems. The primary research challenge involves developing models that can accurately interpret and analyze emotions from both auditory and textual data, whereby auditory data require optimizing CNNs to detect subtle and intense emotional fluctuations in speech, and textual data necessitate access to large, diverse datasets to effectively capture nuanced emotional cues in written language. This paper introduces a novel approach to multimodal emotion recognition, seamlessly integrating speech and text modalities to accurately infer emotional states. Employing CNNs, we meticulously analyze speech using Mel spectrograms, while a BERT-based model processes the textual component, leveraging its bidirectional layers to enable profound semantic comprehension. The outputs from both modalities are combined using an attention-based fusion mechanism that optimally weighs their contributions. The proposed method here undergoes meticulous testing on two distinct datasets: Carnegie Mellon University’s Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset and the Multimodal Emotion Lines Dataset (MELD). The results demonstrate superior efficacy compared to existing frameworks, achieving an accuracy of 88.4% and an F1-score of 87.9% on the CMU-MOSEI dataset, and a notable weighted accuracy (WA) of 67.81% and a weighted F1 (WF1) score of 66.32% on the MELD dataset. This comprehensive system offers precise emotion detection and introduces several significant advancements in the field. © 2024 by the authors.",attention mechanism; BERT; CNN; cross-modal emotion recognition; deep learning; emotion recognition,,,emotion,No,Yes
scopus,A multi-modal driver emotion dataset and study: Including facial expressions and synchronized physiological signals,"Xiang, G.; Yao, S.; Deng, H.; Wu, X.; Wang, X.; Xu, Q.; Yu, T.; Wang, K.; Peng, Y.",2024,,130,,10.1016/j.engappai.2023.107772,"To address the limitations of databases in the field of emotion recognition and to cater to the trend of integrating data from multiple sources, we have established a multi-modal emotional dataset based on spontaneous expression of drivers. By selecting emotional induction materials and inducing emotions before each driving task, facial expression videos and synchronous physiological signals of the drivers during driving were collected. The dataset includes records of 64 participants under five different emotions (neutral, happy, angry, sad, and fear), and the emotional valence, arousal, and peak time of all participants in each driving task were recorded. To analyze the dataset, spatio-temporal convolutional neural networks were designed to analyze the different modalities of data with varying durations in the dataset, aiming to investigate their performance in emotion recognition. The results demonstrate that the fusion of multi-modal data significantly improves the accuracy of driver's emotion recognition, with accuracy increases of 11.28% and 6.83% compared to using only facial video signals or physiological signals, respectively. Therefore, the publication and analysis of multi-modal emotional data for driving scenarios is crucial to support further research in the fields of multimodal perception and intelligent transportation engineering. © 2023 Elsevier Ltd",Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Driver emotion recognition; Driving tasks; Emotion recognition; Emotional valences; Face recognition; Facial expression; Facial Expressions; Modal analysis; Multi-modal information; Multiple source; Physiological signal; Physiological signals; Physiology; Smart vehicle; Smart vehicles,,,emotion,No,Yes
scopus,A Three-stage multimodal emotion recognition network based on text low-rank fusion,"Zhao, L.; Yang, Y.; Ning, T.",2024,,30,,10.1007/s00530-024-01345-5,"Multimodal emotion recognition has achieved good results in emotion recognition tasks by fusing multimodal information such as audio, text, and visual. How to use multimodal interaction and fusion to transform sparse unimodal into compact multimodal has become a vital research hotspot in multimodal emotion recognition. However, in multimodality, the extracted unimodal information needs to be representative. The multimodal fusion will cause the loss of feature information, which creates a particular challenge for multimodal emotion recognition. To address these problems, this paper proposes a three-stage multimodal emotion recognition network based on text low-rank fusion by extracting unimodal features, combining bimodal features, and fusing multimodal features. Specifically, we introduce a Residual-based Attention Mechanism for the first feature extraction stage, which can filter out redundant information and extract valuable unimodal information. Then, we use the Cross-modal Transformer to complete the inter-modal interaction. Finally, we introduce a Text-based Low-rank Fusion Module that enhances multimodal fusion by leveraging the complementarity between different modalities, ensuring comprehensive fused features. The accuracy of the proposed model on CMU-MOSEI, CMU-MOSI, and IEMOCAP datasets is 82.1%, 80.8%, and 83.0%, respectively. Meanwhile, many ablation experiments are conducted in this paper to verify the effectiveness and generalization of the model. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Character recognition; Fusion modules; Low-rank fusion; Multi-modal fusion; Multimodal emotion recognition; Network-based; Residual-based attention mechanism; Residual-based Attention Mechanism; Text-based low-rank fusion module; Text-based Low-rank Fusion Module; Unimodal,,,emotion,No,Yes
scopus,Part of speech weighted multi-modal emotion analysis model with dynamic adjustment of semantic representation,"Hua, Q.; Chen, Z.; Zhang, F.; Dong, C.",2024,,41,,10.3724/SP.J.1249.2024.03283,"In order to better utilize of the semantic information contained in the part of speech of words and the contextual information of unnatural language accompanying the appearance of words, a part of speech weighted multimodal sentiment analysis model with dynamic adjustment of semantic representation (PM-DS) is proposed. The PM-DS model takes natural language as the main body, and uses bidirectional encoder representation from transformer model, generalized autoregressive pre-training model for language understanding (XLNet) and a robustly optimized BERT pretraining approach (RoBERTa) to embed words into text patterns, respectively. A dynamic semantic adjustment module is created to effectively combine natural language and unnatural language information. The part of speech weighting module is designed to extract the part of speech of words and assigned weights to optimize sentiment discrimination. Comparative experimental results with the current advanced models such as tensor fusion network and low-rank multimodal fusion show that the average absolute errors of PW-DS model on public data sets CMU-MOSI and CMU-MOSEI are 0. 607 and 0. 510, respectively, and the binary classification accuracies are 89. 02% and 86. 93%, respectively, which is better than the models in the comparative experiments. The effects of different modules on the model are also analyzed through ablation experiments. The experimental results demonstrate that the proposed model is effective to deal with the problem of multi-modal emotion analysis. © 2024 Editorial Office of Journal of Shenzhen University. All rights reserved.",artificial intelligence; dynamical adjustment semantic; multimodal sentiment analysis; part of speech weighting; visualization of multimodal vector position; visualization of part of speech weights,,,emotion,No,Yes
scopus,Token-disentangling Mutual Transformer for multimodal emotion recognition,"Yin, G.; Liu, Y.; Liu, T.; Zhang, H.; Fang, F.; Tang, C.; Jiang, L.",2024,,133,,10.1016/j.engappai.2024.108348,"Multimodal emotion recognition presents a complex challenge, as it involves the identification of human emotions using various modalities such as video, text, and audio. Existing methods focus mainly on the fusion information from multimodal data, but ignore the interaction of the modality-specific heterogeneity features that contribute differently to emotions, leading to sub-optimal results. To tackle this challenge, we propose a novel Token-disentangling Mutual Transformer (TMT) for robust multimodal emotion recognition, by effectively disentangling and interacting inter-modality emotion consistency features and intra-modality emotion heterogeneity features. Specifically, the TMT consists of two main modules: multimodal emotion Token disentanglement and Token mutual Transformer. In the multimodal emotion Token disentanglement, we introduce a Token separation encoder with an elaborated Token disentanglement regularization, which effectively disentangle the inter-modality emotion consistency feature Token from each intra-modality emotion heterogeneity feature Token; consequently, the emotion-related consistency and heterogeneity information can be performed independently and comprehensively. Furthermore, we devise the Token mutual Transformer with two cross-modal encoders to interact and fuse the disentangled feature Tokens by using bi-directional query learning, which delivers more comprehensive and complementary multimodal emotion representations for multimodal emotion recognition. We evaluate our model on three popular three-modality emotion datasets, namely CMU-MOSI, CMU-MOSEI, and CH-SIMS, and the experimental results affirm the superior performance of our model compared to state-of-the-art methods, achieving state-of-the-art recognition performance. Evaluation Codes and models are released at https://github.com/cug-ygh/TMT. © 2024 Elsevier Ltd",Emotion Recognition; Performance; Multi-modal; Speech recognition; Character recognition; Multimodal emotion recognition; Bi-directional; Bi-directional query learning; Intermodality; Learning systems; Multimodal emotion token disentanglement; Multimodal emotion Token disentanglement; Query learning; Signal encoding; Token mutual transformer; Token mutual Transformer; Token separation learning,,,emotion,No,Yes
scopus,Adaptive Graph Learning for Multimodal Conversational Emotion Detection,"Tu, G.; Xie, T.; Liang, B.; Wang, H.; Xu, R.",2024,,38,,10.1609/aaai.v38i17.29876,"Multimodal Emotion Recognition in Conversations (ERC) aims to identify the emotions conveyed by each utterance in a conversational video. Current efforts encounter challenges in balancing intra- and inter-speaker context dependencies when tackling intra-modal interactions. This balance is vital as it encompasses modeling self-dependency (emotional inertia) where speakers’ own emotions affect them and modeling interpersonal dependencies (empathy) where counterparts’ emotions influence a speaker. Furthermore, challenges arise in addressing cross-modal interactions that involve content with conflicting emotions across different modalities. To address this issue, we introduce an adaptive interactive graph network (IGN) called AdaIGN that employs the Gumbel Softmax trick to adaptively select nodes and edges, enhancing intra- and cross-modal interactions. Unlike undirected graphs, we use a directed IGN to prevent future utterances from impacting the current one. Next, we propose Node- and Edge-level Selection Policies (NESP) to guide node and edge selection, along with a Graph-Level Selection Policy (GSP) to integrate the utterance representation from original IGN and NESP-enhanced IGN. Moreover, we design a task-specific loss function that prioritizes text modality and intra-speaker context selection. To reduce computational complexity, we use pre-defined pseudo labels through self-supervised methods to mask unnecessary utterance nodes for selection. Experimental results show that AdaIGN outperforms state-of-the-art methods on two popular datasets. Our code will be available at https://github.com/TuGengs/AdaIGN. © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Emotion Recognition; Multi-modal; Multimodal emotion recognition; 'current; Artificial intelligence; Context dependency; Cross-modal interaction; Directed graphs; Emotion detection; Graph networks; Level selection; Modal interactions; Selection policies; Undirected graphs,,,emotion,No,No
scopus,Using transformers for multimodal emotion recognition: Taxonomies and state of the art review,"Hazmoune, S.; Bougamouza, F.",2024,,133,,10.1016/j.engappai.2024.108339,"Emotion recognition is an aspect of human-computer interaction, affective computing, and social robotics. Conventional unimodal approaches for emotion recognition, depending on single data sources such as facial expressions or speech signals often fall short in capturing the complexity and context-dependent nature of emotions. Multimodal Emotion Recognition (MER), which integrates information from multiple modalities, has emerged as a promising solution to overcome these limitations. In recent years, Transformers-based approaches have gathered significant attention in the fields of natural language processing and computer vision, highlighting their ability to capture long-range dependencies and semantic representations. These models have rapidly achieved the MER state-of-the-art. However, current survey papers that cover MER lack a specific focus on Transformer-based techniques. To bridge this research gap, this review paper provides a comprehensive investigation of Transformers-based approaches for MER. It explores various Transformer architectures and proposes several scenarios for using Transformers at different stages of MER process. In addition, it examines datasets suitable for MER, discusses fusion mechanisms, and introduces novel taxonomies in both MER and Transformer technologies. The review also addresses challenges and future research directions. Through this review, we aim to provide researchers with an inclusive understanding of the current state-of-the-art in Transformers-based approaches for MER, paving the way for further advancements in this rapidly developing field. © 2024 Elsevier Ltd",Semantics; Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; 'current; Affective Computing; Cross-attention; Human computer interaction; Human robot interaction; Modality fusion; Modality Fusion; Social robotics; State of the art; State-of-the art reviews; Survey; Taxonomies; Taxonomy; Transformers,,,emotion,No,No
scopus,LMR-CBT: learning modality-fused representations with CB-Transformer for multimodal emotion recognition from unaligned multimodal sequences,"Fu, Z.; Liu, F.; Xu, Q.; Fu, X.; Qi, J.",2024,,18,,10.1007/s11704-023-2444-y,"Learning modality-fused representations and processing unaligned multimodal sequences are meaningful and challenging in multimodal emotion recognition. Existing approaches use directional pairwise attention or a message hub to fuse language, visual, and audio modalities. However, these fusion methods are often quadratic in complexity with respect to the modal sequence length, bring redundant information and are not efficient. In this paper, we propose an efficient neural network to learn modality-fused representations with CB-Transformer (LMR-CBT) for multimodal emotion recognition from unaligned multi-modal sequences. Specifically, we first perform feature extraction for the three modalities respectively to obtain the local structure of the sequences. Then, we design an innovative asymmetric transformer with cross-modal blocks (CB-Transformer) that enables complementary learning of different modalities, mainly divided into local temporal learning, cross-modal feature fusion and global self-attention representations. In addition, we splice the fused features with the original features to classify the emotions of the sequences. Finally, we conduct word-aligned and unaligned experiments on three challenging datasets, IEMOCAP, CMU-MOSI, and CMU-MOSEI. The experimental results show the superiority and efficiency of our proposed method in both settings. Compared with the mainstream methods, our approach reaches the state-of-the-art with a minimum number of parameters. © 2024, Higher Education Press.",Emotion Recognition; Multi-modal; Speech recognition; Multimodal emotion recognition; Classification (of information); computational affection; Computational affection; Cross model; Cross-model block; cross-model blocks; Learn+; Learning modalities; Modality-fused representation; modality-fused representations; Modeling blocks; multimodal emotion recognition; Unaligned multimodal sequence; unaligned multimodal sequences; Visual languages,,,emotion,No,No
scopus,Improvement of Multimodal Emotion Recognition Based on Temporal-Aware Bi-Direction Multi-Scale Network and Multi-Head Attention Mechanisms,"Wu, Y.; Zhang, S.; Li, P.",2024,,14,,10.3390/app14083276,"Emotion recognition is a crucial research area in natural language processing (NLP), aiming to identify emotional states such as happiness, anger, and sadness from various sources like speech, text, and facial expressions. In this paper, we propose an improved MMER (multimodal emotion recognition) method using TIM-Net (Temporal-Aware Bi-Direction Multi-Scale Network) and attention mechanisms. Firstly, we introduce the methods for extracting and fusing the multimodal features. Then, we present the TIM-Net and attention mechanisms, which are utilized to enhance the MMER algorithm. We evaluate our approach on the IEMOCAP and MELD datasets, and compared to existing methods, our approach demonstrates superior performance. The weighted accuracy recall (WAR) on the IEMOCAP dataset is 83.9%, and the weighted accuracy recall rate on the MELD dataset is 62.7%. Finally, the impact of the TIM-Net model and the attention mechanism on the emotion recognition performance is further investigated through ablation experiments. © 2024 by the authors.",attention mechanism; deep learning; emotion recognition; multimodal; TIM-Net,,,emotion,No,Yes
scopus,MMD-MII Model: A Multilayered Analysis and Multimodal Integration Interaction Approach Revolutionizing Music Emotion Classification,"Wang, J.; Sharifi, A.; Gadekallu, T.R.; Shankar, A.",2024,,17,,10.1007/s44196-024-00489-6,"Music plays a vital role in human culture and society, serving as a universal form of expression. However, accurately classifying music emotions remains challenging due to the intricate nature of emotional expressions in music and the integration of diverse data sources. To address these challenges, we propose the Multilayered Music Decomposition and Multimodal Integration Interaction (MMD-MII) model. This model employs cross-processing to facilitate interaction between audio and lyrics, ensuring coherence in emotional representation. Additionally, we introduce a hierarchical framework based on the music theory, focusing on the main and chorus sections, with the chorus processed separately to extract precise emotional representations. Experimental results on the DEAM and FMA datasets demonstrate the effectiveness of the MMD-MII model, achieving accuracies of 49.68% and 49.54% respectively. Compared with the existing methods, our model outperforms in accuracy and F1 scores, offering promising implications for music recommendation systems, healthcare, psychology, and advertising, where accurate emotional analysis is essential. © The Author(s) 2024.",Emotion Recognition; Multi-modal; Modal analysis; Deep learning; Classification (of information); Emotion analysis; Features extraction; Integration; Multi-layered; Multimodal; Multimodal integration; Music; Music decompositions; Music emotion classification; Music emotion classifications; Music feature extraction; Music structure analysis,,,emotion,No,Yes
scopus,Cross-modal credibility modelling for EEG-based multimodal emotion recognition,"Zhang, Y.; Liu, H.; Wang, D.; Zhang, D.; Lou, T.; Zheng, Q.; Quek, C.",2024,,21,,10.1088/1741-2552/ad3987,"Objective. The study of emotion recognition through electroencephalography (EEG) has garnered significant attention recently. Integrating EEG with other peripheral physiological signals may greatly enhance performance in emotion recognition. Nonetheless, existing approaches still suffer from two predominant challenges: modality heterogeneity, stemming from the diverse mechanisms across modalities, and fusion credibility, which arises when one or multiple modalities fail to provide highly credible signals. Approach. In this paper, we introduce a novel multimodal physiological signal fusion model that incorporates both intra-inter modality reconstruction and sequential pattern consistency, thereby ensuring a computable and credible EEG-based multimodal emotion recognition. For the modality heterogeneity issue, we first implement a local self-attention transformer to obtain intra-modal features for each respective modality. Subsequently, we devise a pairwise cross-attention transformer to reveal the inter-modal correlations among different modalities, thereby rendering different modalities compatible and diminishing the heterogeneity concern. For the fusion credibility issue, we introduce the concept of sequential pattern consistency to measure whether different modalities evolve in a consistent way. Specifically, we propose to measure the varying trends of different modalities, and compute the inter-modality consistency scores to ascertain fusion credibility. Main results. We conduct extensive experiments on two benchmarked datasets (DEAP and MAHNOB-HCI) with the subject-dependent paradigm. For the DEAP dataset, our method improves the accuracy by 4.58%, and the F1 score by 0.63%, compared to the state-of-the-art baseline. Similarly, for the MAHNOB-HCI dataset, our method improves the accuracy by 3.97%, and the F1 score by 4.21%. In addition, we gain much insight into the proposed framework through significance test, ablation experiments, confusion matrices and hyperparameter analysis. Consequently, we demonstrate the effectiveness of the proposed credibility modelling through statistical analysis and carefully designed experiments. Significance. All experimental results demonstrate the effectiveness of our proposed architecture and indicate that credibility modelling is essential for multimodal emotion recognition. © 2024 IOP Publishing Ltd.","Emotions; emotion; Emotion Recognition; Performance; Speech recognition; Emotion recognition; Physiological signals; human; Multimodal emotion recognition; Intermodality; multimodal emotion recognition; Article; controlled study; recognition; Recognition, Psychology; electroencephalography; Electroencephalography; attention; benchmarking; Benchmarking; Biomedical signal processing; body temperature; comprehension; correlation analysis; Credibility models; Cross-modal; dynamic time warping; EEG; Electric Power Supplies; electroencephalogram; electrooculography; Electrophysiology; eye movement; F1 scores; heart function; human experiment; methodology; natural language processing; peripheral nervous system; Physiological models; plant seed; power supply; self concept; sequential pattern consistency; Sequential pattern consistency; Sequential patterns",,,emotion,No,Yes
scopus,Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations,"Shou, Y.; Meng, T.; Ai, W.; Zhang, F.; Yin, N.; Li, K.",2024,,112,,10.1016/j.inffus.2024.102590,"With the rapid development of social media and human–computer interaction, multimodal emotion recognition in conversations (MERC) tasks have begun to receive widespread research attention. The MERC task is to extract and fuse complementary semantic information from different modalities to classify the speaker's emotion. However, the existing feature fusion methods usually directly map the features of other modalities into the same feature space for information fusion, which cannot eliminate the heterogeneity between different modalities and make the subsequent emotion class boundary learning more difficult. In addition, existing graph contrastive learning methods obtain consistent feature representations by maximizing mutual information between multiple views, which may lead to overfitting of the model. To tackle the above problem, we propose a novel Adversarial Alignment and Graph Fusion via Information Bottleneck for Multimodal Emotion Recognition in Conversations (AGF-IB) method. Firstly, we input video, audio, and text features into a multi-layer perceptron (MLP) to map them into separate feature spaces. Secondly, we build a generator and a discriminator for the three modal features, respectively, through adversarial representation to achieve information interaction between modalities and eliminate the heterogeneity among modalities. Thirdly, we introduce graph contrastive representation learning to capture intra-modal and inter-modal complementary semantic information and learn intra-class and inter-class boundary information of emotion categories. Furthermore, instead of maximizing the mutual information (MI) between multiple views, we use information bottleneck theory to minimize the MI between views. Specifically, we construct a graph structure for the three modal features respectively and perform contrastive representation learning on nodes with different emotions in the same modality and nodes with the same emotion in different modalities, to improve the feature representation ability of nodes. Finally, we use MLP to complete the emotional classification of the speaker. Extensive experiments show that AGF-IB can improve emotion recognition accuracy on IEMOCAP and MELD datasets. Furthermore, since AGF-IB is a general multimodal fusion and contrastive learning method, it can be applied to other multimodal tasks in a plug-and-play manner, e.g., humor detection. © 2024 Elsevier B.V.",Semantics; Emotion Recognition; Speech recognition; Multimodal emotion recognition; Learning systems; Human computer interaction; Classification (of information); Adversarial representation learning; Class boundary; Feature fusion; Feature space; Features fusions; Graph contrastive representation learning; Graphic methods; Information bottleneck; Multimodal emotion recognition in conversation; Multimodal emotion recognition in conversations; Mutual informations; Semantics Information,,,emotion,No,Yes
scopus,A high speed inference architecture for multimodal emotion recognition based on sparse cross modal encoder,"Cui, L.; Zhang, Y.; Cui, Y.; Wang, B.; Sun, X.",2024,,36,,10.1016/j.jksuci.2024.102092,"In recent years, multimodal emotion recognition models are using pre-trained networks and attention mechanisms to pursue higher accuracy, which increases the training burden and slows down the training and inference speed. In order to strike a balance between speed and accuracy, this paper proposes a speed-optimized multimodal emotion recognition architecture for speech and text emotion recognition. In the feature extraction part, a lightweight residual graph convolutional network (ResGCN) is selected as the speech feature extractor, and an efficient RoBERTa pre-trained network is used as the text feature extractor. Then, an algorithm complexity-optimized sparse cross-modal encoder (SCME) is proposed and used to fuse these two types of features. Finally, a new gated fusion module (GF) is used to weight multiple results and input them into a fully connected layer (FC) for classification. The proposed method is tested on the IEMOCAP dataset and the MELD dataset, achieving weighted accuracies (WA) of 82.4% and 65.0%, respectively. This method achieves higher accuracy than the listed methods while having an acceptable training and inference speed. © 2024",Emotion recognition; Deep learning; Attention mechanism; Cross-modal encoder; Pre-trained network,,,emotion,No,Yes
scopus,Cross-Cultural Emotion Recognition With EEG and Eye Movement Signals Based on Multiple Stacked Broad Learning System,"Gong, X.; Chen, C.L.P.; Zhang, T.",2024,,11,,10.1109/TCSS.2023.3298324,"With increasing social globalization, interaction between people from different cultures has become more frequent. However, there are significant differences in the expression and comprehension of emotions across cultures. Therefore, developing computational models that can accurately identify emotions among different cultures has become a significant research problem. This study aims to investigate the similarities and differences in emotion cognition processes in different cultural groups by employing a fusion of electroencephalography (EEG) and eye movement (EM) signals. Specifically, an effective adaptive region selection method is proposed to investigate the most emotion-related activated brain regions in different groups. By selecting these commonly activated regions, we can eliminate redundant features and facilitate the development of portable acquisition devices. Subsequently, the multiple stacked broad learning system (MSBLS) is designed to explore the complementary information of EEG and EM features and the effective emotional information still contained in the residual value. The intracultural subject-dependent (ICSD), intracultural subject-independent, and cross-cultural subject-independent (CCSI) experiments have been conducted on the SEED-CHN, SEED-GER, and SEED-FRA datasets. Extensive experiments manifest that MSBLS achieves superior performance compared with current state-of-the-art methods. Moreover, we discover that some brain regions (the anterior frontal, temporal, and middle parieto-occipital lobes) and Gamma frequency bands show greater activation during emotion cognition in diverse cultural groups. © 2014 IEEE.",Cognition; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Learning systems; Affective Computing; Features extraction; Electroencephalography; Electrophysiology; Affective computing; Brain; Brain modeling; Broad learning system; broad learning system (BLS); Computation theory; Computational modelling; cross-cultural emotion recognition; Cross-cultural emotion recognition; Cultural difference; Electroencephalogram; electroencephalogram (EEG); Eye movement; eye movement (EM); Eye movements; multimodal fusion,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition via Convolutional Neural Networks: Comparison of different strategies on two multimodal datasets,"Bilotti, U.; Bisogni, C.; De Marsico, M.; Tramonte, S.",2024,,130,,10.1016/j.engappai.2023.107708,"The aim of this paper is to investigate emotion recognition using a multimodal approach that exploits convolutional neural networks (CNNs) with multiple input. Multimodal approaches allow different modalities to cooperate in order to achieve generally better performances because different features are extracted from different pieces of information. In this work, the facial frames, the optical flow computed from consecutive facial frames, and the Mel Spectrograms (from the word melody) are extracted from videos and combined together in different ways to understand which modality combination works better. Several experiments are run on the models by first considering one modality at a time so that good accuracy results are found on each modality. Afterward, the models are concatenated to create a final model that allows multiple inputs. For the experiments the datasets used are BAUM-1 ((Bahçeşehir University Multimodal Affective Database - 1) and RAVDESS (Ryerson Audio–Visual Database of Emotional Speech and Song), which both collect two distinguished sets of videos based on the different intensity of the expression, that is acted/strong or spontaneous/normal, providing the representations of the following emotional states that will be taken into consideration: angry, disgust, fearful, happy and sad. The performances of the proposed models are shown through accuracy results and some confusion matrices, demonstrating better accuracy than the compared proposals in the literature. The best accuracy achieved on BAUM-1 dataset is about 95%, while on RAVDESS it is about 95.5%. © 2023 The Authors",Emotion Recognition; Performance; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Convolutional neural networks; Biometrics; Convolution; Convolutional neural network; Input models; Multi-input model; Multi-modal approach; Multiinput; Multiple inputs,,,emotion,No,Yes
scopus,"Enhancing emotion recognition using multimodal fusion of physiological, environmental, personal data","Kim, H.; Hong, T.",2024,,249,,10.1016/j.eswa.2024.123723,"Human emotion recognition, crucial for interpersonal relations and human-building interaction, identifies emotions from various behavioral signals to improve user interactions. To enhance the performance of emotion recognition, this study proposed a novel model that fuses physiological, environmental, and personal data. A unique dataset was created via experiments conducted in an environmental chamber, and an emotion recognition model was subsequently developed using a multimodal fusion approach. The model transforms physiological data into 2D images to capture time series and spatial features and uniquely incorporates metadata, including environmental and personal data. The model's generalizability was validated using a leave-one-sample-out approach. The result showed 31.6% reduction of error with a predicted area when physiological, environmental, and personal data were fused in the emotion recognition model, suggesting that incorporating various contextual factors beyond physiological changes, such as the surrounding environment and inherent or acquired individual traits, can significantly enhance the model's understanding of emotions. Furthermore, the model was to be robust to individual differences, offering consistent emotion recognition across different subjects. These findings suggest that the proposed model can serve as a potent tool for emotion recognition in built environmental applications. © 2024 Elsevier Ltd",Emotion Recognition; Performance; Speech recognition; Behavioral research; Emotion recognition; Physiology; Multi-modal fusion; Affective Computing; Human computer interaction; Physiological models; Affective computing; Human emotion recognition; Human-computer interaction; Interpersonal relations; Model transforms; Multimodal fusion; Recognition models; Swin transformer; User interaction,,,emotion,No,Yes
scopus,Modality-collaborative Transformer with Hybrid Feature Reconstruction for Robust Emotion Recognition,"Chen, C.; Zhang, P.",2024,,20,,10.1145/3640343,"As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community. Despite recent progress, this field still confronts two major challenges in real-world applications: (1) improving the efficiency of constructing joint representations from unaligned multimodal features and (2) relieving the performance decline caused by random modality feature missing. In this article, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT is a novel attention-based encoder that concurrently extracts and dynamically balances the intra- and inter-modality relations for all associated modalities. With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity. To improve the robustness of MCT, we further introduce HFR, which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA). During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise-complete and -incomplete representations. Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Semantics; Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Additional key word and phrasesmultimodal emotion recognition; Additional Key Words and PhrasesMultimodal emotion recognition; Data missing; Feature reconstruction; hybrid feature reconstruction; Hybrid feature reconstruction; Hybrid features; Key words; Sequential data; sequential data missing; Sequential data missing; transformer,,,emotion,No,Yes
scopus,DAST: A Domain-Adaptive Learning Combining Spatio-Temporal Dynamic Attention for Electroencephalography Emotion Recognition,"Jin, H.; Gao, Y.; Wang, T.; Gao, P.",2024,,28,,10.1109/JBHI.2023.3307606,"Multimodal emotion recognition with EEG-based have become mainstream in affective computing. However, previous studies mainly focus on perceived emotions (including posture, speech or face expression et al.) of different subjects, while the lack of research on induced emotions (including video or music et al.) limited the development of two-ways emotions. To solve this problem, we propose a multimodal domain adaptive method based on EEG and music called the DAST, which uses spatio-temporal adaptive attention (STA-attention) to globally model the EEG and maps all embeddings dynamically into high-dimensionally space by adaptive space encoder (ASE). Then, adversarial training is performed with domain discriminator and ASE to learn invariant emotion representations. Furthermore, we conduct extensive experiments on the DEAP dataset, and the results show that our method can further explore the relationship between induced and perceived emotions, and provide a reliable reference for exploring the potential correlation between EEG and music stimulation.  © 2013 IEEE.","music; Machine Learning; Humans; Emotions; Semantics; emotion; Emotion Recognition; Speech recognition; emotion recognition; Behavioral research; Emotion recognition; human; physiology; Multimodal emotion recognition; Affective Computing; Correlation; Music; Article; autoencoder; machine learning; electroencephalography; Electroencephalography; procedures; attention; correlation analysis; EEG; Electrophysiology; Adaptive learning; adaptive space encoder; adversarial attack (machine learning); adversarial training; algorithm; Algorithms; classifier; domain adaptation; Domain adaptation; domain adaptive learning; domain adversarial learning; domain discriminator; emotion classifier; emotionality; Face expressions; global embedding; Kernel; learning algorithm; locally linear embedding; nerve cell network; neuroimaging; semantics; signal processing; Signal Processing, Computer-Assisted; spatial embedding; spatio temporal adaptive attention mechanism block; Spatio-temporal dynamics; spatiotemporal analysis; temporal embedding; training; Two ways",,,emotion,No,Yes
scopus,An Intra- and Inter-Emotion Transformer-Based Fusion Model with Homogeneous and Diverse Constraints Using Multi-Emotional Audiovisual Features for Depression Detection,"Teng, S.; Liu, J.; Huang, Y.; Chai, S.; Tateyama, T.; Huang, X.; Lin, L.; Chen, Y.-W.",2024,,E107.D,,10.1587/transinf.2023HCP0006,"Depression is a prevalent mental disorder affecting a significant portion of the global population, leading to considerable disability and contributing to the overall burden of disease. Consequently, designing efficient and robust automated methods for depression detection has become imperative. Recently, deep learning methods, especially multimodal fusion methods, have been increasingly used in computer-aided depression detection. Importantly, individuals with depression and those without respond differently to various emotional stimuli, providing valuable information for detecting depression. Building on these observations, we propose an intra- and inter-emotional stimulus transformer-based fusion model to effectively extract depression-related features. The intra-emotional stimulus fusion framework aims to prioritize different modalities, capitalizing on their diversity and complementarity for depression detection. The inter-emotional stimulus model maps each emotional stimulus onto both invariant and specific subspaces using individual invariant and specific encoders. The emotional stimulus-invariant subspace facilitates efficient information sharing and integration across different emotional stimulus categories, while the emotional stimulus specific subspace seeks to enhance diversity and capture the distinct characteristics of individual emotional stimulus categories. Our proposed intra- and inter-emotional stimulus fusion model effectively integrates multimodal data under various emotional stimulus categories, providing a comprehensive representation that allows accurate task predictions in the context of depression detection. We evaluate the proposed model on the Chinese Soochow University students dataset, and the results outperform state-of-the-art models in terms of concordance correlation coefficient (CCC), root mean squared error (RMSE) and accuracy. Copyright © 2024 The Institute of Electronics, Information and Communication Engineers.",Transformer; deep learning; Deep learning; Learning systems; transformer; depression detection; Depression detection; Audio-visual features; audiovisual; Automated methods; Computer aided instruction; emotion fusion; Emotion fusion; Fusion model; Global population; Mean square error; Mental disorders; stimuli; Stimulus,,,emotion,No,No
scopus,Emotion detection using convolutional neural network and long short-term memory: a deep multimodal framework,"Tahir, M.; Halim, Z.; Waqas, M.; Sukhia, K.N.; Tu, S.",2024,,83,,10.1007/s11042-023-17653-3,"Emotion detection systems play a crucial role in enhancing human-computer interaction. Existing systems predominantly rely on machine learning techniques. This study introduces a novel emotion detection method that employs deep learning techniques to identify five basic human emotions and the pleasure dimensions (valence) associated with these emotions, using text and keystroke dynamics. To facilitate this, we develop a non-acted dataset, DEKT-345 × 2, which includes text and keystroke features. The dataset is created by inducing emotions in participants under controlled conditions. Deep learning models are subsequently employed to predict a person’s affective state using textual content. Semantic analysis of the text data is achieved by employing the global vector (Glove) representation of words. For both text and keystroke-based analysis, one-dimensional convolutional neural network (Conv1D), long short-term memory (LSTM), sandwich Conv1D, and sandwich LSTM models are employed. The robustness of our proposed method is assessed using the DEKT-345 × 2 dataset, which collects text and keystroke information from 69 participants. Through parameter tuning on training and validation data, we establish models that demonstrate superior performance compared to five related approaches and three machine learning classifiers. Our proposed framework achieves an accuracy of 88.57% using the LSTM model, 80% using the sandwich LSTM model, 71.42% using the Conv1D model, and 51.48% using the sandwich Conv1D model on text data across the five emotion classes. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",Long short-term memory; Semantics; Text analysis; Emotion Recognition; Emotion recognition; Deep learning; Character recognition; Learning systems; Data mining; Emotion detection; Human computer interaction; Classification (of information); Convolutional neural networks; Brain; Convolution; Convolutional neural network; Affective dataset; Learning algorithms; Learning system; Memory modeling; Sentiment classification; Short text analyse; Short text analysis; Short texts,,,emotion,No,Yes
scopus,On Using Physiological Sensors and AI to Monitor Emotions in a Bug-Hunting Game,"Silvis-Cividjian, N.; Kenyon, J.; Nazarian, E.; Sluis, S.; Gevonden, M.",2024,,1,,10.1145/3649217.3653611,"Although software testing is key to a safe society, the process itself is often perceived by students as boring and stressful. Therefore, only few consider a career in testing. The adverse effect is sub-optimally tested code, with dangerous bugs left undetected. A better understanding of what testers ""feel""when learning the skill in class can remedy this situation, by means of personalized, motivating bio-feedback. In order to test our hypothesis, we propose an innovative approach that uses physiological wearable sensors (cardiac activity, respiration, and skin conductance) to monitor in real-time the affective state of testers engaged in a bug-hunting game. This is a work in progress. We present the envisioned methodology and the results of two feasibility experiments. The first experiment created a training dataset, by recording bio-signals and self-reports from eleven participants involved in a mood-induction session followed by a bug-hunting task. The second experiment showed that it is possible to use deep-learning to recognize emotions from a large set of labelled multimodal (ECG, EDA and ICG) physiological data. The classification accuracy using a binary (positive-negative) emotions model was 85%, higher than the accuracy obtained using a four-emotions (anxious, down, enthusiastic and relaxed) model (57%). Future work includes optimizing the sensory system, improving the accuracy of automated emotions recognition, increasing the validity of ground-truth emotions labelling, and investigating ways to provide individualized and formative (instead of summative) bio-feedback. The proposed approach can contribute to a more sentiment-aware education, and a more objective evaluation of the effect of teaching interventions. © 2024 Owner/Author.",Emotion Recognition; Emotion recognition; Physiology; Deep learning; Learning systems; Sentiment analysis; Biomedical signal processing; sentiment analysis; automated emotion recognition; Automated emotion recognition; Biometric ECG signal; biometric ECG signals; Bug hunting; bug-hunting gamification; Bug-hunting gamification; deep-learning; Deep-learning; ECG signals; Electrocardiography; Feedback; Gamification; Software testing; software testing education; Software testing education; Software testings,,,emotion,Yes,Yes
scopus,"Multimodal Emotion Recognition with Deep Learning: Advancements, challenges, and future directions","Geetha, A.V.; Mala, T.; Priyanka, D.; Uma, E.",2024,,105,,10.1016/j.inffus.2023.102218,"In recent years, affective computing has become a topic of considerable interest, driven by its ability to enhance several domains, such as mental health monitoring, human–computer interaction, and personalized advertising. The progress of affective computing has been extensively supported by the emergence of sub-domains such as sentiment analysis and emotion recognition. Furthermore, Deep Learning (DL) techniques have made significant advancements in the realm of emotion recognition, resulting in the emergence of Multimodal Emotion Recognition (MER) systems that are capable of effectively processing data from various sources, such as audio, video, and text. However, despite the considerable progress made, there are still several challenges that persist in MER systems. Moreover, existing surveys often lack a specific focus on MER and the associated DL architectures. To address these research gaps, this study provides an in-depth systematic review of DL-based MER systems. This review encompasses the recent state-of-the-art models, foundational theories, DL architectures, mechanisms for fusing multimodal information, relevant datasets, performance evaluation, and practical applications. Additionally, the study identifies key challenges and limitations in MER systems and suggests future research opportunities. The main objective of this review is to provide a thorough comprehension of the present cutting-edge MER, thus enabling researchers in both academia and industry to stay up to date with the most recent developments in this rapidly evolving domain. © 2023 Elsevier B.V.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Deep Learning; Character recognition; Multimodal emotion recognition; Affective Computing; Human computer interaction; Affective computing; Computation theory; Audio systems; Data handling; Emotion models; Multi-modality; Multimodal affective computing; Multimodal Emotion Recognition; Multimodality; Recognition systems,,,emotion,No,Yes
scopus,Self-supervised Multimodal Emotion Recognition Combining Temporal Attention Mechanism and Unimodal Label Automatic Generation Strategy,"Sun, Q.; Wang, S.",2024,,46,,10.11999/JEIT231107,"Most multimodal emotion recognition methods aim to find an effective fusion mechanism to construct the features from heterogeneous modalities, so as to learn the feature representation with semantic consistency. However, these methods usually ignore the emotionally semantic differences between modalities. To solve this problem, one multi-task learning framework is proposed. By training one multimodal task and three unimodal tasks jointly, the emotionally semantic consistency information among multimodal features and the emotionally semantic difference information contained in each modality are respectively learned. Firstly, in order to learn the emotionally semantic consistency information, one Temporal Attention Mechanism (TAM) based on a multilayer recurrent neural network is proposed. The contribution degree of emotional features is described by assigning different weights to time series feature vectors. Then, for multimodal fusion, the fine-grained feature fusion per semantic dimension is carried out in the semantic space. Secondly, one self-supervised Unimodal Label Automatic Generation (ULAG) strategy based on the inter-modal feature vector similarity is proposed in order to effectively learn the difference information of emotional semantics in each modality. A large number of experimental results on three datasets CMU-MOSI, CMU-MOSEI, CH-SIMS, confirm that the proposed TAM-ULAG model has strong competitiveness, and has improved the classification indices (Acc2, F1) and regression index (MAE, Corr) compared with the current benchmark models. For binary classification, the recognition rate is 87.2% and 85.8% on the CMU-MOSEI and CMU-MOSEI datasets, and 81.47% on the CH-SIMS dataset. The results show that simultaneously learning the emotionally semantic consistency information and the emotionally semantic difference information for each modality is helpful in improving the performance of self-supervised multimodal emotion recognition method. © 2024 Science Press. All rights reserved.",Semantics; Attention mechanisms; Emotion Recognition; Speech recognition; Multi-modal fusion; Multimodal emotion recognition; Unimodal; Learning systems; Classification (of information); Learn+; Multimodal fusion; Multitask learning; Automatic Generation; Large datasets; Multi-task learning; Multilayer neural networks; Recurrent neural networks; Self-supervised label generation; Semantic consistency; Temporal attention mechanism; Temporal Attention mechanism,,,emotion,Yes,Yes
scopus,Enhanced multimodal emotion recognition in healthcare analytics: A deep learning based model-level fusion approach,"Islam, M.M.; Nooruddin, S.; Karray, F.; Muhammad, G.",2024,,94,,10.1016/j.bspc.2024.106241,"Deep learning techniques have drawn considerable interest in emotion recognition due to recent technological developments in healthcare analytics. Automatic patient emotion recognition can assist healthcare analytics by providing feedback to the stakeholders of competent healthcare about the conditions of the patients and their satisfaction levels. In this paper, we propose a novel model-level fusion technique based on deep learning for enhanced emotion recognition from multimodal signals to monitor patients in connected healthcare. The representative visual features from the video signals are extracted through the Depthwise Separable Convolution Neural Network, and the optimized temporal attributes are derived from the multiple physiological data utilizing Bi-directional Long Short-Term Memory. A soft attention method fused the high multimodal features obtained from the two data modalities to retrieve the most significant features by focusing on emotionally salient parts of the features. We exploited two face detection methods, Histogram of Oriented Gradients and Convolutional Neural Network-based face detector (ResNet-34), to observe the effects of facial features on emotion recognition. Lastly, extensive experimental evaluations have been conducted using the widely used Bio Vid Emo DB multimodal dataset to verify the performance of the proposed architecture. Experimental results show that the developed fusion architecture improved the accuracy of emotion recognition from multimodal signals and outperformed the performance of both state-of-the-art techniques and baseline methods. © 2024 Elsevier Ltd",Long short-term memory; convolutional neural network; emotion; Health care; Emotion Recognition; Multi-modal; Speech recognition; deep learning; Emotion recognition; Face recognition; diagnosis; female; human; male; videorecording; Multimodal emotion recognition; Bi-directional; Learning systems; controlled study; Convolutional neural networks; attention; Brain; Convolution; Convolutional neural network; article; Bi-directional long short-term memory; Depthwise separable convolutional neural network; Depthwise separable convolutional neural networks; facies; Healthcare analytic; Healthcare analytics; histogram; Level fusion; Memory architecture; Network architecture; residual neural network; short term memory; Soft attention,,,emotion,No,Yes
scopus,Novel multimodal emotion detection method using Electroencephalogram and Electrocardiogram signals,"Saha, P.; Ansaruddin Kunju, A.K.; Majid, M.E.; Bin Abul Kashem, S.; Nashbat, M.; Ashraf, A.; Hasan, M.; Khandakar, A.; Shafayet Hossain, M.; Alqahtani, A.; Chowdhury, M.E.H.",2024,,92,,10.1016/j.bspc.2024.106002,"Emotion Recognition Systems (ERS) play a pivotal role in facilitating naturalistic Human-Machine Interactions (HMI). The research has utilized a dataset with diverse physiological signals, including Electroencephalogram (EEG), Photoplethysmography (PPG), and Electrocardiogram (ECG), to detect emotions evoked by video stimuli. The study has addressed challenges with EEG data, particularly prefrontal channels contaminated by eye blink artifacts. To tackle this, a novel 1D deep learning model, MultiResUNet3p, effectively generated clean EEG signals. Extensive features have been extracted from each modality (TD, FD, TFD), and the study identified that combining 112 features from EEG and ECG achieved the highest accuracy. The emotion classification task encompassed six emotions, and the model demonstrates outstanding performance with 96.12% accuracy in binary classification (Positive vs. Negative) and 94.25% accuracy in a complex multiclass classification of six emotions (Happy, Anger, Disgust, Fear, Neutral, and Sad). This research underscores the potential of integrating multiple physiological signals and advanced techniques to significantly improve emotion recognition accuracy, particularly in real-world scenarios involving naturalistic Human-Machine Interactions (HMI). © 2024 Elsevier Ltd",Machine Learning; emotion; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Physiology; Deep learning; diagnosis; human; videorecording; Learning systems; Classification (of information); machine learning; electroencephalography; Electroencephalography; Biomedical signal processing; electroencephalogram; human experiment; Electroencephalogram; Machine-learning; Recognition systems; article; 1d segmentation model; 1D segmentation model; anger; binary classification; disgust; electric potential; electrocardiogram; Electrocardiogram; Electrocardiogram (ECG); Electrocardiograms; electrocardiography; Electroencephalogram (EEG); Emotion recognition system; Emotion recognition system (ERS); eyelid reflex; fear; Finite difference method; Frequency domain analysis; Frequency domains; Frequency-domain; Frequency-domain (FD); multiclass classification; normal human; photoelectric plethysmography; Photoplethysmography; Photoplethysmography (PPG); Segmentation models; Time domain; Time Domain (TD); Time domain analysis; Time frequency domain; Time-frequency domain; Time-frequency domain (TFD),,,emotion,No,Yes
scopus,Interpretable multimodal emotion recognition using hybrid fusion of speech and image data,"Kumar, P.; Malik, S.; Raman, B.",2024,,83,,10.1007/s11042-023-16443-1,"This paper proposes a multimodal emotion recognition system based on hybrid fusion that classifies the emotions depicted by speech utterances and corresponding images into discrete classes. A new interpretability technique has been developed to identify the important speech and image features leading to the prediction of particular emotion classes. The proposed system’s architecture has been determined through intensive ablation studies. It fuses the speech & image features and then combines speech, image, and intermediate fusion outputs. The proposed interpretability technique incorporates the divide and conquer approach to compute shapely values denoting each speech and image feature’s importance. We have also constructed a large-scale dataset, IIT-R SIER dataset, consisting of speech utterances, corresponding images, and class labels, i.e., ‘anger,’ ‘happy,’ ‘hate,’ and ‘sad.’ The proposed system has achieved 83.29% accuracy for emotion recognition. The enhanced performance of the proposed system advocates the importance of utilizing complementary information from multiple modalities for emotion recognition. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",Image processing; Emotion Recognition; Speech recognition; Modal analysis; Multimodal emotion recognition; Affective Computing; Affective computing; Information fusion; Hybrid fusions; Image features; Image fusion; Images processing; Interpretability; Interpretable AI; Large dataset; Multimodal analysis; Speech and image processing; Speech features; Speech utterance,,,emotion,Yes,Yes
scopus,Learning emotional prompt features with multiple views for visual emotion analysis,"Xu, Q.; Wei, Y.; Yuan, S.; Wu, J.; Wang, L.; Wu, C.",2024,,108,,10.1016/j.inffus.2024.102366,"Visual emotion analysis(VEA) aiming to detect the emotions behind images, has gained increasing attention with the development of online social media. Recent studies in prompt learning have significantly advanced visual emotion classification. However, these methods usually utilize random vectors or non-emotional texts as the initialization for prompt optimization. This restricts the emotional semantic representation of prompts and hinders the performance of the model. To tackle this problem, we leverage emotional prompts with multiple views to enhance the semantic emotional information. We first translate the image to caption as context prompt(COP) from the view of background information for the image. Additionally, we introduce hybrid emotion prompt(HEP) from the view of the interaction between the emotional visual and textual information, where different modalities are integrated with a novel Emotion Joint Congruity Learning module. Furthermore, we also provide label prompt(LP) to enhance the emotional association with labels, enabling better emotional information fusion. Extensive experiments conducted on five publicly visual emotion classification datasets, i.e. EmoSet, FI, have demonstrated the superiority of our MVP model over cutting-edge methods. © 2024 Elsevier B.V.",Semantics; Emotion Recognition; Multi-modal fusion; Social networking (online); Classification (of information); Emotion analysis; Multimodal fusion; Association reactions; Emotion classification; Emotional information; Multiple views; Online social medias; Optimisations; Prompt learning; Random vectors; Visual emotion analyse; Visual emotion analysis,,,emotion,No,Yes
scopus,Self-supervised utterance order prediction for emotion recognition in conversations,"Jiang, D.; Liu, H.; Tu, G.; Wei, R.; Cambria, E.",2024,,577,,10.1016/j.neucom.2024.127370,"As the order of the utterances in a conversation changes, the meaning of the utterance also changes, and sometimes, this will cause different semantics or emotions. However, the existing representation learning models do not pay close attention to capturing the internal semantic differences of utterance caused by the change of utterance order. Based on this, we build a self-supervised utterance order prediction approach to learn the logical order of utterance, which helps understand the deep semantic relationship between adjacent utterances. Specially, the utterance binary composed of two adjacent utterances, which are ordered or disordered, is fed to the self-supervised model so that the self-supervised model can obtain firm representation learning ability for the semantic differences of the adjacent sentences. The self-supervised method is applied to the downstream conversation emotion recognition task to test the value of the approach. The features extracted from the self-supervised model are fused with the multimodal features to obtain a richer utterance representation. After that, emotion recognition models are applied to two different datasets. The experiment results show that our proposed approach outperforms the current state of the art on ERC benchmark datasets. © 2024 Elsevier B.V.",Semantics; emotion; Emotion Recognition; Speech recognition; Emotion recognition; adult; female; human; male; Learning systems; Learn+; learning; recognition; benchmarking; Learning models; semantics; article; conversation; ERC; feature learning (machine learning); Forecasting; Learning abilities; prediction; Self-supervised learning; Semantic difference; Semantic relationships; Supervised learning; Supervised methods; Utterance order prediction,,,emotion,No,No
scopus,Exploring the Algorithm for Diagnosing Burnout of Counselors in Higher Vocational Colleges in Guangdong Province Based on Emotion Recognition,"Dang, R.; Samad, N.A.",2024,,5,,10.61707/ybcg6t04,"The main objective of the research is to deal with the drawbacks of conventional self-report tests by designing and validating a novel Emotion Recognition (ER) algorithm for assessing Counsellor Burnout (CB) in Higher Vocational Colleges (HVC) in Guangdong Province. Applying the Maslach Burnout Inventory (MBI) Survey Questionnaires, voice and facial expression files, and text analysis of counselling sessions, researchers collected an enormous dataset from 97 counsellors. The study employed a multimodal fusion layer to integrate multiple neural network architectures, namely RNNs for audio content, CNNs for video content, and a BERT+LSTM hybrid for textual analysis. This enabl ed a thorough examination of each ER. By dramatically exhibiting an accuracy of more than 92% for identifying the symptoms of burnout, this method is better than the conventional methods that have been used in previous years. The technique, which successfully recognizes complex, real-time emotions that are frequently ignored when individuals communicate emotions in person, can be used to obtain a more accurate and expert evaluation of burnout because of its capacity to recognize these reactions accurately. Different sources of data and contemporary Machine Learning (ML) methods have enhanced CB prediction and observation. This approach enhances counsellors' emotional stability and assists in developing pedagogical mental health interventions. Findings demonstrate that ML techniques may enhance mental assessment and counselling in demanding employment. © 2024, Transnational Press London Ltd. All rights reserved.",Machine Learning; Emotion Recognition; CNN; Accuracy; Maslach Burnout Inventory; Mental Health Diagnostics,,,emotion,No,Yes
scopus,Multimodal Knowledge-enhanced Interactive Network with Mixed Contrastive Learning for Emotion Recognition in Conversation,"Shen, X.; Huang, X.; Zou, S.; Gan, X.",2024,,582,,10.1016/j.neucom.2024.127550,"Emotion Recognition in Conversations (ERC) aims to accurately identify the emotional labels of each utterance in a conversation, holding significant application value in human–computer interaction. Existing research suggests introducing commonsense knowledge (CSK) and multimodal information enhances model performance in ERC tasks. However, several challenges persist: (1) the neglect of complex psychological influences between utterances; (2) noise issues within modal information; (3) prediction challenges for emotion labels with few samples in different categories that exhibit semantic similarity but distinct emotional categories. To address the above problems, we propose a Multimodal Knowledge-enhanced Interactive Network with Mixed Contrastive Learning (MKIN-MCL). Firstly, we establish a knowledge aggregation graph to capture the dependencies of commonsense knowledge (CSK) between utterances during a conversation. We actively aggregate relevant knowledge information to enhance text features. Simultaneously, we apply feature filters for acoustic and visual modalities to eliminate noise and enhance feature quality. Furthermore, we implement an interactive attention module by stacking designed Cross-modal Interactive Transformers (CITs) to continuously explore the relevance between the interacting parties in their respective semantic spaces, thus improving the effectiveness of modality interaction while reducing noise generated during the interaction. Lastly, we employ the Mixed Contrastive Learning (MCL) strategy to enhance the model's ability to handle few-shot labels. This strategy utilizes unsupervised contrastive learning to improve the representation capability of the multimodal fusion features and supervised contrastive learning to extract information from few-shot labels. Extensive experiments on two benchmark datasets, IEMOCAP and MELD, validate the effectiveness and superiority of the proposed model. © 2024 Elsevier B.V.",Semantics; emotion; Transformer; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multi-modal information; Human computer interaction; Article; information processing; learning; benchmarking; semantics; Modeling performance; conversation; accuracy; acoustics; Commonsense knowledge; Contrastive learning; correlational study; Emotion recognition in conversation; knowledge; Knowledge information; model; Multimodal interaction; Multimodal Interaction; noise reduction; process optimization; unsupervised machine learning,,,emotion,No,Yes
scopus,Joint low-rank tensor fusion and cross-modal attention for multimodal physiological signals based emotion recognition,"Wan, X.; Wang, Y.; Wang, Z.; Tang, Y.; Liu, B.",2024,,45,,10.1088/1361-6579/ad5bbc,"Objective. Physiological signals based emotion recognition is a prominent research domain in the field of human-computer interaction. Previous studies predominantly focused on unimodal data, giving limited attention to the interplay among multiple modalities. Within the scope of multimodal emotion recognition, integrating the information from diverse modalities and leveraging the complementary information are the two essential issues to obtain the robust representations. Approach. Thus, we propose a intermediate fusion strategy for combining low-rank tensor fusion with the cross-modal attention to enhance the fusion of electroencephalogram, electrooculogram, electromyography, and galvanic skin response. Firstly, handcrafted features from distinct modalities are individually fed to corresponding feature extractors to obtain latent features. Subsequently, low-rank tensor is fused to integrate the information by the modality interaction representation. Finally, a cross-modal attention module is employed to explore the potential relationships between the distinct latent features and modality interaction representation, and recalibrate the weights of different modalities. And the resultant representation is adopted for emotion recognition. Main results. Furthermore, to validate the effectiveness of the proposed method, we execute subject-independent experiments within the DEAP dataset. The proposed method has achieved the accuracies of 73.82% and 74.55% for valence and arousal classification. Significance. The results of extensive experiments verify the outstanding performance of the proposed method. © 2024 Institute of Physics and Engineering in Medicine.","Humans; Attention; Emotions; emotion; Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; human; physiology; Multi-modal fusion; Unimodal; Human computer interaction; electroencephalography; Electroencephalography; attention; Biomedical signal processing; Cross-modal; electrooculography; Electrophysiology; multimodal fusion; signal processing; Signal Processing, Computer-Assisted; deep neural network; electrodermal response; electromyography; Electromyography; Electrooculography; Galvanic Skin Response; Limited attentions; Multiple modalities; physiological signals; Research domains; Tensor fusion; Tensors",,,emotion,No,Yes
scopus,"Exploring contactless techniques in multimodal emotion recognition: insights into diverse applications, challenges, solutions, and prospects","Khan, U.A.; Xu, Q.; Liu, Y.; Lagstedt, A.; Alamäki, A.; Kauttonen, J.",2024,,30,,10.1007/s00530-024-01302-2,"In recent years, emotion recognition has received significant attention, presenting a plethora of opportunities for application in diverse fields such as human–computer interaction, psychology, and neuroscience, to name a few. Although unimodal emotion recognition methods offer certain benefits, they have limited ability to encompass the full spectrum of human emotional expression. In contrast, Multimodal Emotion Recognition (MER) delivers a more holistic and detailed insight into an individual's emotional state. However, existing multimodal data collection approaches utilizing contact-based devices hinder the effective deployment of this technology. We address this issue by examining the potential of contactless data collection techniques for MER. In our tertiary review study, we highlight the unaddressed gaps in the existing body of literature on MER. Through our rigorous analysis of MER studies, we identify the modalities, specific cues, open datasets with contactless cues, and unique modality combinations. This further leads us to the formulation of a comparative schema for mapping the MER requirements of a given scenario to a specific modality combination. Subsequently, we discuss the implementation of Contactless Multimodal Emotion Recognition (CMER) systems in diverse use cases with the help of the comparative schema which serves as an evaluation blueprint. Furthermore, this paper also explores ethical and privacy considerations concerning the employment of contactless MER and proposes the key principles for addressing ethical and privacy concerns. The paper further investigates the current challenges and future prospects in the field, offering recommendations for future research and development in CMER. Our study serves as a resource for researchers and practitioners in the field of emotion recognition, as well as those intrigued by the broader outcomes of this rapidly progressing technology. © The Author(s) 2024.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; Human-computer interaction; Contact less; Contactless data collection; Contactless multimodal emotion recognition; Contactless technique; Data acquisition; Data collection; Data privacy; Ethical and privacy consideration; Ethical and privacy considerations; Ethical technology; Real-world implementation,,,emotion,No,No
scopus,A multimodal shared network with a cross-modal distribution constraint for continuous emotion recognition,"Li, C.; Xie, L.; Shao, X.; Pan, H.; Wang, Z.",2024,,133,,10.1016/j.engappai.2024.108413,"Continuous emotion recognition has been a compelling topic in affective computing because it can interpret human emotions subtly and continuously. Existing studies have achieved advanced emotion recognition performance using multimodal knowledge. However, these studies generally ignore the circumstances where some particular modalities are missing in the inference phase and thus become sensitive to the absence of modalities. To resolve this issue, we propose a novel multimodal shared network with a cross-modal distribution constraint, i.e. the DS-Net, which aims to improve the robustness of the model to missing modalities. The training process of the proposed network generally includes two components: multimodal shared space modeling and a cross-modal distribution matching constraint. The former utilizes the local and temporal information of multimodal signals for multimodal shared space modeling, while the latter further enhances the multimodal shared space via a loose constraint method. Coupled with the latter, the former can effectively exploit the complementarity between videos and peripheral physiological signals (PPSs), thus enhancing the discriminative capability of the shared space. Based on the shared space, the DS-Net works during the inference phase with only one modality input and can leverage multimodal knowledge to improve emotion recognition accuracy. Comprehensive experiments were conducted on two public datasets. Results demonstrate that the proposed method is competitive or superior to the current state-of-the-art methods. Further experiments indicate that the proposed method can be extended to handle other modalities and to deal with partially missing modalities, demonstrating its potential in real-world applications. © 2024 Elsevier Ltd",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Physiological signals; Physiology; Multi-modality; Multimodality; Continuous emotion recognition; Cross modality; Cross-modality; Facial video; Facial videos; Knowledge management; Missing modality; Peripheral physiological signal; Peripheral physiological signals; Shared spaces,,,emotion,No,Yes
scopus,Analyzing audiovisual data for understanding user's emotion in human−computer interaction environment,"Yang, J.; Li, Z.; Du, X.",2024,,58,,10.1108/DTA-08-2023-0414,"Purpose: Although numerous signal modalities are available for emotion recognition, audio and visual modalities are the most common and predominant forms for human beings to express their emotional states in daily communication. Therefore, how to achieve automatic and accurate audiovisual emotion recognition is significantly important for developing engaging and empathetic human–computer interaction environment. However, two major challenges exist in the field of audiovisual emotion recognition: (1) how to effectively capture representations of each single modality and eliminate redundant features and (2) how to efficiently integrate information from these two modalities to generate discriminative representations. Design/methodology/approach: A novel key-frame extraction-based attention fusion network (KE-AFN) is proposed for audiovisual emotion recognition. KE-AFN attempts to integrate key-frame extraction with multimodal interaction and fusion to enhance audiovisual representations and reduce redundant computation, filling the research gaps of existing approaches. Specifically, the local maximum–based content analysis is designed to extract key-frames from videos for the purpose of eliminating data redundancy. Two modules, including “Multi-head Attention-based Intra-modality Interaction Module” and “Multi-head Attention-based Cross-modality Interaction Module”, are proposed to mine and capture intra- and cross-modality interactions for further reducing data redundancy and producing more powerful multimodal representations. Findings: Extensive experiments on two benchmark datasets (i.e. RAVDESS and CMU-MOSEI) demonstrate the effectiveness and rationality of KE-AFN. Specifically, (1) KE-AFN is superior to state-of-the-art baselines for audiovisual emotion recognition. (2) Exploring the supplementary and complementary information of different modalities can provide more emotional clues for better emotion recognition. (3) The proposed key-frame extraction strategy can enhance the performance by more than 2.79 per cent on accuracy. (4) Both exploring intra- and cross-modality interactions and employing attention-based audiovisual fusion can lead to better prediction performance. Originality/value: The proposed KE-AFN can support the development of engaging and empathetic human–computer interaction environment. © 2023, Emerald Publishing Limited.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Data mining; Human computer interaction; Benchmarking; Attention mechanism; Cross modality; Audio-visual fusion; Audio-visual interactions; Audiovisual emotion recognition; Audiovisual fusion; Audiovisual interaction and fusion; Cross-modality interaction; Data reduction; Extraction; Intra-modality interaction; Key-frame extraction; Redundancy,,,emotion,No,Yes
scopus,Spanish MEACorpus 2023: A multimodal speech–text corpus for emotion analysis in Spanish from natural environments,"Pan, R.; García-Díaz, J.A.; Rodríguez-García, M.Á.; Valencia-García, R.",2024,,90,,10.1016/j.csi.2024.103856,"In human–computer interaction, emotion recognition provides a deeper understanding of the user's emotions, enabling empathetic and effective responses based on the user's emotional state. While deep learning models have improved emotion recognition solutions, it is still an active area of research. One important limitation is that most emotion recognition systems use only text as input, ignoring features such as voice intonation. Another limitation is the limited number of datasets available for multimodal emotion recognition. In addition, most published datasets contain emotions that are simulated by professionals and produce limited results in real-world scenarios. In other languages, such as Spanish, hardly any datasets are available. Therefore, our contributions to emotion recognition are as follows. First, we compile and annotate a new corpus for multimodal emotion recognition in Spanish (Spanish MEACorpus 2023), which contains 13.16 h of speech divided into 5129 segments labeled by considering Ekman's six basic emotions. The dataset is extracted from YouTube videos in natural environments. Second, we explore several deep learning models for emotion recognition using text- and audio-based features. Third, we evaluate different multimodal techniques to build a multimodal recognition system that improves the results of unimodal models, achieving a Macro F1-score of 87.745%, using late fusion with concatenation strategy approach. © 2024 The Authors",Natural language processing; Transformer; Emotion Recognition; Multi-modal; Speech recognition; Modal analysis; Deep learning; Character recognition; Learning systems; Human computer interaction; Transformers; Classification (of information); Emotion analysis; Deep-learning; Linguistics; Language processing; Multimodal emotion analyse; Multimodal emotion analysis; Natural languages; Speech emotion analyse; Speech emotion analysis; Speech emotions; Text classification,,,emotion,Yes,Yes
scopus,FedCMD: A Federated Cross-modal Knowledge Distillation for Drivers' Emotion Recognition,"Bano, S.; Tonellotto, N.; Cassarà, P.; Gotta, A.",2024,,15,,10.1145/3650040,"Emotion recognition has attracted a lot of interest in recent years in various application areas such as healthcare and autonomous driving. Existing approaches to emotion recognition are based on visual, speech, or psychophysiological signals. However, recent studies are looking at multimodal techniques that combine different modalities for emotion recognition. In this work, we address the problem of recognizing the user's emotion as a driver from unlabeled videos using multimodal techniques. We propose a collaborative training method based on cross-modal distillation, i.e., ""FedCMD""(Federated Cross-Modal Distillation). Federated Learning (FL) is an emerging collaborative decentralized learning technique that allows each participant to train their model locally to build a better generalized global model without sharing their data. The main advantage of FL is that only local data is used for training, thus maintaining privacy and providing a secure and efficient emotion recognition system. The local model in FL is trained for each vehicle device with unlabeled video data by using sensor data as a proxy. Specifically, for each local model, we show how driver emotional annotations can be transferred from the sensor domain to the visual domain by using cross-modal distillation. The key idea is based on the observation that a driver's emotional state indicated by a sensor correlates with facial expressions shown in videos. The proposed ""FedCMD""approach is tested on the multimodal dataset ""BioVid Emo DB""and achieves state-of-the-art performance. Experimental results show that our approach is robust to non-identically distributed data, achieving 96.67% and 90.83% accuracy in classifying five different emotions with IID (independently and identically distributed) and non-IID data, respectively. Moreover, our model is much more robust to overfitting, resulting in better generalization than the other existing methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Emotion Recognition; Speech recognition; Emotion recognition; Learning systems; Cross-modal; Application area; cross-modal distillation; Cross-modal distillation; Distillation; Distributed data; federated learning; Federated learning; Independently and identically distributed; Local model; Multi-modal techniques; transfer learning; Transfer learning,,,emotion,No,Yes
scopus,Deep CNN with late fusion for real time multimodal emotion recognition,"Dixit, C.; Satapathy, S.M.",2024,,240,,10.1016/j.eswa.2023.122579,"Emotion recognition is a fundamental aspect of human communication and plays a crucial role in various domains. This project aims at developing an efficient model for real-time multimodal emotion recognition in videos of human oration (opinion videos), where the speakers express their opinions about various topics. Four separate datasets contributing 20,000 samples for text, 1,440 for audio, 35,889 for images, and 3,879 videos for multimodal analysis respectively are used. One model is trained for each of the modalities: fastText for text analysis because of its efficiency, robustness to noise, and pre-trained embeddings; customized 1-D CNN for audio analysis using its translation invariance, hierarchical feature extraction, scalability, and generalization; custom 2-D CNN for image analysis because of its ability to capture local features and handle variations in image content. They are tested and combined on the CMU-MOSEI dataset using both bagging and stacking to find the most effective architecture. They are then used for real-time analysis of speeches. Each of the models is trained on 80% of the datasets, the remaining 20% is used for testing individual and combined accuracies in CMU-MOSEI. The emotions finally predicted by the architecture correspond to the six classes in the CMU-MOSEI dataset. This cross-dataset training and testing of the models makes them robust and efficient for general use, removes reliance on a specific domain or dataset, and adds more data points for model training. The proposed architecture was able to achieve an accuracy of 85.85% and an F1-score of 83 on the CMU-MOSEI dataset. © 2023 Elsevier Ltd",Emotion Recognition; Speech recognition; CNN; Emotion recognition; Modal analysis; Deep learning; Multimodal emotion recognition; Multimodal analysis; Stackings; Statistical tests; Cross dataset; Ensemble learning; Fasttext; FastText; Human communications; Image analysis; Late fusion; Real- time; Stacking,,,emotion,No,Yes
scopus,A Resilient Overlay for Human Emotion Recognition Using Mixed Frameworks in Machine-Human Interactions,"Fayaz, F.A.; Malik, A.; Batra, I.; Ansarullah, S.I.",2024,,5,,10.1007/s42979-024-02762-z,"Human sentiments are irrational responses to things or situations associated with various physiological, behavioural, and cognitive processes. Understanding human sentiments is pivotal in diverse fields such as human–computer interaction, virtual reality, self-driving technology, and health monitoring. The scientific community is becoming more and more interested in emotion recognition. Emotion recognition, mainly through brain-generated electroencephalogram (EEG) waves, has gained significant attention in developing brain-computer interfaces. This article addresses critical research gaps in existing cross-subject emotion detection methods based on EEG signals and multi-modal techniques that combine EEG data with other modalities. We proposed an effective hybrid technique for cross-subject sentiment identification using EEG and facial movements. The suggested method combines a spectrum of directed variations and local binary pattern characteristics with spectral and statistical features from the facial dataset. The next step is classifying emotions using support vector machines, k-nearest neighbour, and ensembles. Notably, our method tackles the challenge of category imbalance using an up-sampling approach. Our results are promising after evaluating the efficiency of our proposed technique on a sentiment analysis dataset with physiological data and employing tenfold cross-validation. Notably, the accuracy levels for valence and arousal reach impressive highs at 96.25% and 97.10%, respectively. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024.",Machine learning; Artificial intelligence; Multimodal; EEG; Human–computer interaction,,,emotion,No,Yes
scopus,Semantic-wise guidance for efficient multimodal emotion recognition with missing modalities,"Liu, S.; Wang, Y.; Wang, K.; Li, B.; Yang, F.; Yang, S.",2024,,30,,10.1007/s00530-024-01310-2,"Emotions play an important role in human–computer interaction. Multimodal emotion recognition combines feature information from different modalities to recognize emotional states. However, in real application scenarios, data from all modalities may not always be available. Thus, in multimodal emotion recognition a big challenge is how to utilize the semantic information from available modalities to predict missing modality data. To address this issue, this study proposes a Semantic-Wise Guidance for Missing Modality Imagination Network (SWG-MMIN) consisting of three main modules, that is, the Comprehensive Modality Feature Enrichment (CMFE) module, the Semantic-Wise Fusion (SWF) module, and the Semantic-Wise Feature Guided Imagination (SWGI) module. The CMFE module addresses the issue of semantic loss in the process of integrating multimodal features by enhancing the semantic information. The SWF module performs an adaptive fusion of invariant and specific features of multimodal data. The SWGI module facilitates the missing modality data generation and enhances the robustness of joint multimodal representation. Extensive experiments are conducted on two benchmark datasets, IEMOCAP and MSP-IMPROV. The experimental results demonstrate that the SWG-MMIN model surpasses all baseline models under full modalities and uncertain missing modalities, significantly improving emotion recognition performance. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Semantics; Emotion Recognition; Emotional state; Speech recognition; Emotion recognition; Fusion modules; Multimodal emotion recognition; Human computer interaction; Semantics Information; Enrichment modules; Feature information; Missing modality imagination; Modality feature enrichment; Semantic-wise fusion,,,emotion,No,Yes
scopus,Facial video-based non-contact emotion recognition: A multi-view features expression and fusion method,"Tao, X.; Su, L.; Rao, Z.; Li, Y.; Wu, D.; Ji, X.; Liu, J.",2024,,96,,10.1016/j.bspc.2024.106608,"Emotion recognition finds broad applications across psychology, computer science, and artificial intelligence. However, the intricacies of emotional states pose challenges, rendering single modality-based emotion recognition approaches less robust. In this study, we introduce a multi-view features fusion algorithm leveraging convolutional neural networks (CNNs) for enhanced emotion detection. Initially, imaging photoplethysmography (IPPG) signals are extracted from face videos. Subsequently, we employ heart rate variability (HRV) for feature extraction and deploy branch convolutional neural networks to achieve multi-view representation of emotional attributes within IPPG and facial video signals. Our methodology was validated using the DEAP public dataset, demonstrating that our approach attained accuracies of 72.37% and 70.82% for arousal and valence dimensions, respectively. Notably, our multi-view strategy enhances emotion recognition accuracy by 7.23% and 5.31% for arousal and valence, respectively, when contrasted with methodologies relying solely on facial expressions. This advancement underscores our method's capability to capture multimodal emotional expressions through facial videos without the necessity for additional sensors, thus significantly elevating the precision and robustness of emotion recognition endeavors. © 2024 The Author(s)",convolutional neural network; Facial expressions; facial expression; Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; female; human; male; videorecording; Article; controlled study; Convolutional neural networks; Features fusions; Convolution; Convolutional neural network; algorithm; photoelectric plethysmography; Photoplethysmography; arousal; cohort analysis; facial recognition; Feature expression; feature extraction; Feature extraction; heart rate variability; Imaging photoplethysmography; Multi-view feature fusion; Multi-view features fusion; Multi-views; Non-contact; Non-contact emotion recognition; valence (emotion),,,emotion,No,Yes
scopus,Emotion-Aware In-Car Feedback: A Comparative Study,"Mwaita, K.F.; Bhaumik, R.; Ahmed, A.; Sharma, A.; De Angeli, A.; Haller, M.",2024,,8,,10.3390/mti8070054,"We investigate personalised feedback mechanisms to help drivers regulate their emotions, aiming to improve road safety. We systematically evaluate driver-preferred feedback modalities and their impact on emotional states. Using unobtrusive vision-based emotion detection and self-labeling, we captured the emotional states and feedback preferences of 21 participants in a simulated driving environment. Results show that in-car feedback systems effectively influence drivers’ emotional states, with participants reporting positive experiences and varying preferences based on their emotions. We also developed a machine learning classification system using facial marker data to demonstrate the feasibility of our approach for classifying emotional states. Our contributions include design guidelines for tailored feedback systems, a systematic analysis of user reactions across three feedback channels with variations, an emotion classification system, and a dataset with labeled face landmark annotations for future research. © 2024 by the authors.",facial expression; driver wellness; multimodal sensing; real-time emotion detection; sensor fusion,,,emotion,Yes,No
scopus,Deep Feature Extraction and Attention Fusion for Multimodal Emotion Recognition,"Yang, Z.; Li, D.; Hou, F.; Song, Y.; Gao, Q.",2024,,71,,10.1109/TCSII.2023.3318814,"Recently, electroencephalogram (EEG)-based multimodal emotion recognition has emerged as one of the research hotspots in affective computing. However, the existing methods tend to ignore the interaction information between the EEG and other modal features. In this brief, we propose a novel model termed EEANet (EEG and eye movement Attention Network) to find the modal correlation at feature level. The DE feature and 31 eye movement features were extracted from the pre-processed EEG and eye movement signals, and then two feedforward encoders were used to capture the deep features, respectively. The interactive attention layer is applied to learn multi-modal complementary information and semantic-level context information. Finally, the multi-head self-attention mechanism allows the model to focus on the discriminative features for emotion classification. The model was verified on the SEED-IV dataset, and the results showed that the accuracy of emotion recognition was significantly improved with the EEANet, and the average accuracy of the four classifications was 92.26%.  © 2004-2012 IEEE.",Semantics; Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Multimodal emotion recognition; Correlation; Classification (of information); Features extraction; Electroencephalography; Biomedical signal processing; EEG; Electrophysiology; eye movement; Brain modeling; Eye movements; Extraction; Feature extraction; Feedforward systems; Hotspots; interactive attention; Interactive attention; self-attention; Self-attention,,,emotion,No,Yes
scopus,MRSLN: A Multimodal Residual Speaker-LSTM Network to alleviate the over-smoothing issue for Emotion Recognition in Conversation,"Lu, N.; Tan, Z.; Qian, J.",2024,,580,,10.1016/j.neucom.2024.127467,"Multimodal Emotion Recognition in Conversation (ERC) plays a significant role in the field of human–computer intelligent interaction since it enables computers to perceive and infer the emotions expressed by the individuals, thereby intelligently responding to them. Most of current ERC methods pay more attention to modeling the complex interaction between different modalities. However, the features extracted by their unimodal networks are over-smoothed and may contain insufficient intra-speaker contextual information, which results in suboptimal results. In this paper, we focus on the unimodal learning and propose a simple late fusion framework named Multimodal Residual Speaker-LSTM Network (MRSLN), which uses speaker information to directly model inter-speaker and intra-speaker dependency, rather than fuse it into the learned features. MRSLN uses the speaker-LSTM consisting of the inter-speaker LSTM, intra-speaker LSTM, and the residual network between the input and output of the inter-speaker LSTM. Our proposed method can alleviate the issue of over-smoothing in deep Long Short Term Memory (LSTM) network and also incorporate additional intra-speaker contextual information. Extensive experiments conducted on IEMOCAP and MELD datasets demonstrate that MRSLN effectively captures inter-speaker and intra-speaker information and outperforms currently complex state-of-the-art (SOTA) models in efficiency and classification performance. © 2024 Elsevier B.V.",Long short-term memory; emotion; facial expression; Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; human; Multimodal emotion recognition; Unimodal; Human computer interaction; Classification (of information); Article; recognition; Multimodal emotion recognition in conversation; conversation; Complex networks; Inter-speaker dependency; Intra-speaker dependency; language; long short term memory network; mathematical computing; Memory network; Over-smoothing; Residual network; voice,,,emotion,No,Yes
scopus,Fusing facial and speech cues for enhanced multimodal emotion recognition,"Tomar, P.S.; Mathur, K.; Suman, U.",2024,,16,,10.1007/s41870-023-01697-7,"Emotion recognition is a technology that enables computers to recognize and interpret human emotions by analyzing facial expressions, voice, text or physiological signals. It finds applications in human–computer interaction, mental health assessment, and personalized content recommendation, offering insights into user sentiment and engagement. In this paper, we introduced an innovative approach to emotion recognition which combines facial expressions and speech cues within a multimodal system. This fusion of two distinct modalities is achieved through two specific methods: feature-level fusion and decision-level fusion. To evaluate the effectiveness of our approach, we conduct experiments using the eNTERFACE'05 dataset. Our comparative analysis reveals that the integration of fusion-based techniques can substantially enhance the performance of emotion recognition systems. Additionally, our findings highlight the superiority of feature-level fusion over decision-level fusion in terms of overall performance. © The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management 2024.",Speech emotion recognition; Multimodal emotion recognition; Affective computing; Facial emotion recognition; Human Computer Interaction (HCI),,,emotion,No,Yes
scopus,Exploring the impact of computer-mediated emotional interactions on human facial and physiological responses,"Saffaryazdi, N.; Kirkcaldy, N.; Lee, G.; Loveys, K.; Broadbent, E.; Billinghurst, M.",2024,,14,,10.1016/j.teler.2024.100131,"Remote communication has become pervasive, yet its impact on human emotions, empathy, and physiological responses remains unclear. This study addresses this gap by investigating how emotional video-mediated conversations differ from face-to-face interactions, focusing on behavioral and physiological responses. We create a multimodal dataset of Electrodermal Activity (EDA) signals, Photoplethysmography (PPG) signals, and facial videos from two people conversing about emotional topics in face-to-face and remote video-mediated conditions. We use a series of repeated measures ANOVA with aligned rank transform to compare face-to-face to remote conversation in terms of heart rate activity, electrodermal activity, and facial action units. We also explore how subjective empathy between people varies in these two conditions. Our findings reveal significant differences in physiological responses between face-to-face and remote conversation and variations in perceived empathy based on interaction setting, highlighting the nuanced influence of communication channels. We also show that we can recognize emotions more accurately when we pre-train a random forest classifier with one condition's data (an increase of 20% to 45% for various modalities). Finally, we discuss the research findings and limitations and offer insights for optimizing human–computer interaction and understanding human emotional responses in an increasingly tech-mediated world. © 2024 The Author(s)",Empathy; Facial expressions; Emotion recognition; Physiological signals; Remote communication,,,emotion,No,No
scopus,Emotion recognition based on multimodal physiological signals using spiking feed-forward neural networks,"Yang, X.; Yan, H.; Zhang, A.; Xu, P.; Pan, S.H.; Vai, M.I.; Gao, Y.",2024,,91,,10.1016/j.bspc.2023.105921,"Real-time emotion recognition via wearable devices is a pivotal component of health monitoring and human–computer interaction. To realize this objective, a spiking feed-forward neural networks (SFNNs) model was developed, which leverages six physiological signals from the psychophysiology of positive and negative emotions (POPANE) dataset to construct feature vectors. By converting well-trained artificial neural networks (ANNs) to spiking neural networks (SNNs) and employing weight normalization techniques, the SFNNs with data-based normalization achieved a maximum classification accuracy of 88.17% at a maximum input firing rate of 1000 Hz. In comparison to existing models, the SFNNs model integrates multimodal physiological signals to classify six discrete emotions, demonstrating high classification performance and rapid convergence speed, rendering it ideal for real-time emotion recognition. This work has potential applications in psychological diagnosis and medical rehabilitation through the use of wearable wristbands. © 2023",emotion; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; artificial neural network; human; physiology; Human computer interaction; Features extraction; Article; electroencephalography; Biomedical signal processing; Physiological models; electrocardiography; deep neural network; electromyography; Wearable technology; feature extraction; Feature extraction; heart rate; Diagnosis; diastolic blood pressure; Feed forward neural net works; feed forward neural network; Feedforward neural networks; heart output; mathematics; membrane potential; membrane resistance; Multimodal physiological signal; Multimodal physiological signals; R wave; Real-time emotion recognition; Spiking feed-forward neural network; Spiking feed-forward neural networks; spiking neural network; systolic blood pressure; Time series; time series analysis; Times series; vascular resistance; Wearable wristband; Wearable wristbands,,,emotion,No,Yes
scopus,Multimodal individual emotion recognition with joint labeling based on integrated learning and clustering,"Ke, S.; Nie, C.; Wang, Y.; He, B.",2024,,6,,10.11959/j.issn.2096-6652.202401,"To address the low recognition accuracy of generic emotion recognition models when faced with different individuals, a multimodal individual emotion recognition technique based on joint labelling with integrated learning and clustering was proposed. The method first trained a generic emotion recognition model based on a public dataset, then anallysed the distributional differences between the data in the public dataset and the unlabelled data of individuals, and established a cross-domain model for predicting and labelling pseudo-labels of individual data. At the same time, the individual data were weighted clustered and labelled with cluster labels, and the cluster labels were used to jointly label with pseudo-labels, and high confidence samples were screened to further train the generic model to obtain a personalized emotion recognition model. Using this method to annotate these data with the experimentally collected data of 3 emotions from 3 subjects, the final optimized personalized model achieved an average recognition accuracy of more than 80% for the 3 emotions, which was at least a 35% improvement compared to the original generic model. © 2024 Beijing Xintong Media Co., Ltd.. All rights reserved.",domain adaptation; clustering; individual emotion recognition; integrated learning; joint annotation,,,emotion,Yes,Yes
scopus,Emotion-aware hierarchical interaction network for multimodal image aesthetics assessment,"Zhu, T.; Li, L.; Chen, P.; Wu, J.; Yang, Y.; Li, Y.",2024,,154,,10.1016/j.patcog.2024.110584,"Image aesthetics assessment (IAA) has attracted increasing attention recently but is still challenging due to its high abstraction and complexity. Intuitively, image emotion and aesthetics are both human subjective feelings evoked by visual content. Previous researches have demonstrated that image aesthetics has intrinsic relationships with emotion. In fact, human emotional experience has potential impact on the perception of image aesthetics. Therefore, emotion information can be exploited to enhance aesthetic representation learning. Inspired by this, this paper presents an Emotion-Aware Hierarchical Interaction NETwork (EAHI-NET) for multimodal image aesthetics assessment, which explores both intra-modal and inter-modal interactions between aesthetics and emotions hierarchically. Specifically, we first propose the intra-modal emotion-aesthetics interaction module to obtain emotion-enhanced visual and textual aesthetic representations respectively. Then we propose the inter-modal feature enhancement to obtain the cross-modal aesthetic and emotion features. Finally, we design the inter-modal emotion-aesthetics interaction module to further investigate the cross-modal interplay between aesthetics and emotion, based on which hierarchical feature representations are achieved for multimodal IAA. The extensive experiments show that the proposed method can outperform the state-of-the-arts on multimodal AVA and Photo.net datasets. © 2024 Elsevier Ltd",Emotion Recognition; Behavioral research; Emotion analysis; Cross-modal; Arts computing; Hierarchical interactions; Image Aesthetics; Image aesthetics assessment; Image emotion analyse; Image emotion analysis; Image esthetic assessment; Image representation; Interaction modules; Interaction networks; Multi-modal learning; Multimodal images; Multimodal learning,,,emotion,No,No
scopus,Fusing pairwise modalities for emotion recognition in conversations,"Fan, C.; Lin, J.; Mao, R.; Cambria, E.",2024,,106,,10.1016/j.inffus.2024.102306,"Multimodal fusion has the potential to significantly enhance model performance in the domain of Emotion Recognition in Conversations (ERC) by efficiently integrating information from diverse modalities. However, existing methods face challenges as they directly integrate information from different modalities, making it difficult to assess the individual impact of each modality during training and to capture nuanced fusion. To deal with it, we propose a novel framework named Fusing Pairwise Modalities for ERC. In this proposed method, the pairwise fusion technique is incorporated into multimodal fusion to enhance model performance, which enables each modality to contribute unique information, thereby facilitating a more comprehensive understanding of the emotional context. Additionally, a designed density loss is applied to characterise fused feature density, with a specific focus on mitigating redundancy in pairwise fusion methods. The density loss penalises feature density during training, contributing to a more efficient and effective fusion process. To validate the proposed framework, we conduct comprehensive experiments on two benchmark datasets, namely IEMOCAP and MELD. The results demonstrate the superior performance of our approach compared to state-of-the-art methods, indicating its effectiveness in addressing challenges related to multimodal fusion in the context of ERC. © 2024 Elsevier B.V.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal; Feature fusion; Features fusions; Modeling performance; Emotion recognition in conversation; Density loss; Emotion recognition in conversations; Feature density; Graph convolution network; Integrating information,,,emotion,No,Yes
scopus,A multimodal teacher speech emotion recognition method in the smart classroom,"Zhao, G.; Zhang, Y.; Chu, J.",2024,,25,,10.1016/j.iot.2024.101069,"As a collection of various IoT devices, smart classroom can record various forms of teaching data and provide rich data for recognizing teachers' emotions. Recognizing and analyzing teachers' emotions can promote teachers' professional development. Nowadays, most of the automatic emotion recognition methods for teachers in smart classroom are based on facial expressions. However, since teachers usually keep smiling to mobilize the classroom atmosphere, the recognition results may not reflect the real mental state of teachers. By observing teaching videos, it is found that the prosody and text in the teachers' speech can reflect the implicit emotion of the teacher. Therefore, a multimodal teacher emotion dataset (MTED) was built based on teaching videos recorded by IoT cameras and microphones in smart classroom. A neural network combining multiple prosodic features and text content for teacher speech emotion recognition is proposed. The proposed method fills the gap in teacher speech emotion recognition, our proposed method has higher accuracy. Experimental results show that ProsodyBERT achieves 78.6 % UA4 and 66.2 % UA6 on IEMOCAP and MELD, respectively, surpassing the existing methods. The proposed method reached 82.1% UA6 on MTED self-built dataset, which is 9.6 %-21.4 % higher than that of unimodal method in teacher emotion recognition. An ablation experiment is designed and implemented on MTED dataset to explore the influence of each module in ProsodyBERT on teacher speech emotion recognition task. The experimental results in the smart classroom record show that ProsodyBERT has higher accuracy and stronger robustness than unimodal methods. © 2024 Elsevier B.V.",Multimodal emotion recognition; Prosodic feature; Smart classroom; Teacher emotion,,,emotion,No,Yes
scopus,GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition,"Lian, Z.; Sun, L.; Sun, H.; Chen, K.; Wen, Z.; Gu, H.; Liu, B.; Tao, J.",2024,,108,,10.1016/j.inffus.2024.102367,"Recently, GPT-4 with Vision (GPT-4V) has demonstrated remarkable visual capabilities across various tasks, but its performance in emotion recognition has not been fully evaluated. To bridge this gap, we present the quantitative evaluation results of GPT-4V on 21 benchmark datasets covering 6 tasks: visual sentiment analysis, tweet sentiment analysis, micro-expression recognition, facial emotion recognition, dynamic facial emotion recognition, and multimodal emotion recognition. This paper collectively refers to these tasks as “Generalized Emotion Recognition (GER)”. Through experimental analysis, we observe that GPT-4V exhibits strong visual understanding capabilities in GER tasks. Meanwhile, GPT-4V shows the ability to integrate multimodal clues and exploit temporal information, which is also critical for emotion recognition. However, it is worth noting that GPT-4V is primarily designed for general domains and cannot recognize micro-expressions that require specialized knowledge. To the best of our knowledge, this paper provides the first quantitative assessment of GPT-4V for GER tasks. We have open-sourced the code and encourage subsequent researchers to broaden the evaluation scope by including more tasks and datasets. Our code and evaluation results are available at: https://github.com/zeroQiaoba/gpt4v-emotion. © 2024 Elsevier B.V.",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Modal analysis; Multi-modal fusion; Sentiment analysis; Multimodal fusion; Codes (symbols); Evaluation results; Facial emotions; Generalized emotion recognition; Generalized Emotion Recognition (GER); GPT-4 with vision; GPT-4 with Vision (GPT-4V); Micro-expressions; Temporal modeling; Temporal models; Zero-shot benchmark; Zero-shot learning,,,emotion,No,Yes
scopus,"Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects","Zhang, S.; Yang, Y.; Chen, C.; Zhang, X.; Leng, Q.; Zhao, X.",2024,,237,,10.1016/j.eswa.2023.121692,"Emotion recognition has recently attracted extensive interest due to its significant applications to human–computer interaction. The expression of human emotion depends on various verbal and non-verbal languages like audio, visual, text, etc. Emotion recognition is thus well suited as a multimodal rather than single-modal learning problem. Owing to the powerful feature learning capability, extensive deep learning methods have been recently leveraged to capture high-level emotional feature representations for multimodal emotion recognition (MER). Therefore, this paper makes the first effort in comprehensively summarize recent advances in deep learning-based multimodal emotion recognition (DL-MER) involved in audio, visual, and text modalities. We focus on: (1) MER milestones are given to summarize the development tendency of MER, and conventional multimodal emotional datasets are provided; (2) The core principles of typical deep learning models and its recent advancements are overviewed; (3) A systematic survey and taxonomy is provided to cover the state-of-the-art methods related to two key steps in a MER system, including feature extraction and multimodal information fusion; (4) The research challenges and open issues in this field are discussed, and promising future directions are given. © 2023 Elsevier Ltd","Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Character recognition; Multimodal emotion recognition; Learning systems; Human computer interaction; Visual languages; Features extraction; Information fusion; Extraction; Feature extraction; Audio-visual; Future prospects; Multimodal information fusion; Multimodal information fusion, review; Systematic Review",,,emotion,No,No
scopus,Generating and encouraging: An effective framework for solving class imbalance in multimodal emotion recognition conversation,"Li, Q.; Huang, P.; Xu, Y.; Chen, J.; Deng, Y.; Yin, S.",2024,,133,,10.1016/j.engappai.2024.108523,"In recent years, Intelligent Personal Assistants (IPAs) have emerged as important tools in human–computer interaction, with a wide range of applications such as voice assistant, virtual customer service, and navigation. Capturing and understanding the prominent emotional needs of users is important for improving the quality of service of IPAs. Multimodal emotion recognition in conversation (MMERC) aimed at automatically identifying and tracking the emotional states of speakers during the dialogue process has become a crucial component for building emotional IPAs and attracted increasing attention. Current research in this field is based on graph simulation for cross-modal and single-modal interactions. However, these methods ignore the highly imbalanced class problem inherent in MMERC, leading to a decrease in the generalization ability of the model and an inability to effectively recognize minority emotion classes. Data mining methods use oversampling to solve the imbalanced classification, but they are unsuitable for MMERC as they disrupt the conversational coherence and modality alignment characteristics of multimodal emotion recognition datasets. To overcome these problems, this paper proposes an IMBA-MMERC, which is an effective framework to address the pervasive issue of class imbalance in MMERC. Within this framework, sample generation for multimodal conversation tackles the application challenges that exist in multimodal conversational emotion recognition datasets, and well-classified encouraging loss mitigates the performance degradation of the model on certain majority classes due to decision boundary deviations. On two English benchmark datasets and one Chinese public dataset, we used two performance indicators to demonstrate the effectiveness and superiority of the proposed IMBA-MMERC. Ablation experiment, case study, and histograms visualization further verify the well performance of the proposed framework. © 2024 Elsevier Ltd",Emotion Recognition; Speech recognition; Multimodal emotion recognition; Data mining; Human computer interaction; Classification (of information); Benchmarking; Multimodal emotion recognition in conversation; Class imbalance; Classifieds; Convolutional networks; Customer-service; Graph convolutional network; Quality of service; Quality-of-service; Sample generation; Sample generations; Virtual reality; Well-classified encouraging loss,,,emotion,No,Yes
scopus,A Review of Key Technologies for Emotion Analysis Using Multimodal Information,"Zhu, X.; Guo, C.; Feng, H.; Huang, Y.; Feng, Y.; Wang, X.; Wang, R.",2024,,16,,10.1007/s12559-024-10287-z,"Emotion analysis, an integral aspect of human–machine interactions, has witnessed significant advancements in recent years. With the rise of multimodal data sources such as speech, text, and images, there is a profound need for a comprehensive review of pivotal elements within this domain. Our paper delves deep into the realm of emotion analysis, examining multimodal data sources encompassing speech, text, images, and physiological signals. We provide a curated overview of relevant literature, academic forums, and competitions. Emphasis is laid on dissecting unimodal processing methods, including preprocessing, feature extraction, and tools across speech, text, images, and physiological signals. We further discuss the nuances of multimodal data fusion techniques, spotlighting early, late, model, and hybrid fusion strategies. Key findings indicate the essentiality of analyzing emotions across multiple modalities. Detailed discussions on emotion elicitation, expression, and representation models are presented. Moreover, we uncover challenges such as dataset creation, modality synchronization, model efficiency, limited data scenarios, cross-domain applicability, and the handling of missing modalities. Practical solutions and suggestions are provided to address these challenges. The realm of multimodal emotion analysis is vast, with numerous applications ranging from driver sentiment detection to medical evaluations. Our comprehensive review serves as a valuable resource for both scholars and industry professionals. It not only sheds light on the current state of research but also highlights potential directions for future innovations. The insights garnered from this paper are expected to pave the way for subsequent advancements in deep multimodal emotion analysis tailored for real-world deployments. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Image processing; Emotion Recognition; Modal analysis; Multi-modal information; Physiological signals; Physiology; Multi-modal data; Multi-modal fusion; Emotion analysis; Multimodal fusion; Data fusion; Data-source; Emotional analysis; Image signal; Multimodal information; Speech signals; Text images,,,emotion,No,No
scopus,Emotion Fusion-Sense (Emo Fu-Sense) – A novel multimodal emotion classification technique,"Umair, M.; Rashid, N.; Shahbaz Khan, U.; Hamza, A.; Iqbal, J.",2024,,94,,10.1016/j.bspc.2024.106224,"Human emotions play a vital role in overall well-being. With the advent of advance technologies growing interest has been observed in developing a multimodal emotion classification system that can accurately interpret human emotions. The article presents a comprehensive overview of a multimodal emotion classification system (Emo Fu-Sense) designed to capture the rich and nuanced nature of human emotions. Objective of Emo Fu-Sense is to integrates information from Electrocardiogram (ECG), Galvanic Skin Response (GSR), Electroencephalograph (EEG), respiration amplitude and body temperature to achieve holistic understanding of emotional states. To effectively extract information from multimodal data, designed system employs conventional methods with sophisticated machine learning algorithms including Long Short-Term Memory (LSTM), a variety of Recurrent Neural Network (RNN). Recommended solution extracts column wise features independently from different modalities based on the windowing operation. Finally, feature fusion and modality biasing were used to combine the information from different modalities. The proposed method has not only highlighted the limitations of unimodal system but has achieved a classification accuracy of 92.62 %, with an average F1-Score of 93 % and 9.2 % of Mean Absolute Error (MAE). Obtained results are better than existing state-of-the-art approaches. Evaluation of the multimodal emotion classification system was conducted on MAHNOB-HCI dataset, which encompasses a wide range of emotional expressions across various contexts and individuals. The integration of multiple modalities and advanced machine learning techniques enables a more comprehensive understanding of emotional states and highlights the significance of research and development in the field of affective computing. © 2024 Elsevier Ltd",Long short-term memory; emotion; Multi-modal; Classification (of information); Article; data accuracy; electroencephalography; Electroencephalography; body temperature; EEG; Electrophysiology; natural language processing; Feature fusion; Features fusions; LSTM; signal processing; Learning algorithms; electrocardiogram; Electrocardiograms; Emotion classification; electrodermal response; feature extraction; heart rate variability; long short term memory network; medical information; Bayesian learning; Body temperature; breathing; classification algorithm; data integration; ECG; Electroencephalograph; emo fu sense; energy; entropy; evaluation study; Fourier transform; frequency; Galvanic skin response; GSR; Human emotion; intermethod comparison; k nearest neighbor; logistic regression analysis; mean absolute error; Modality biasing; Multimodal emotion classification; R wave amplitude; recurrent neural network; Respiration amplitude; signal decomposition; support vector machine; waveform,,,emotion,No,Yes
scopus,TER-CA-WGNN: Trimodel Emotion Recognition Using Cumulative Attribute-Weighted Graph Neural Network,"Al-Saadawi, H.F.T.; Das, R.",2024,,14,,10.3390/app14062252,"Affective computing is a multidisciplinary field encompassing artificial intelligence, natural language processing, linguistics, computer science, and social sciences. This field aims to deepen our comprehension and capabilities by deploying inventive algorithms. This article presents a groundbreaking approach, the Cumulative Attribute-Weighted Graph Neural Network, which is innovatively designed to integrate trimodal textual, audio, and visual data from the two multimodal datasets. This method exemplifies its effectiveness in performing comprehensive multimodal sentiment analysis. Our methodology employs vocal inputs to generate speaker embeddings trimodal analysis. Using a weighted graph structure, our model facilitates the efficient integration of these diverse modalities. This approach underscores the interrelated aspects of various emotional indicators. The paper’s significant contribution is underscored by its experimental results. Our novel algorithm achieved impressive performance metrics on the CMU-MOSI dataset, with an accuracy of 94% and precision, recall, and F1-scores above 92% for Negative, Neutral, and Positive emotion categories. Similarly, on the IEMOCAP dataset, the algorithm demonstrated its robustness with an overall accuracy of 93%, where exceptionally high precision and recall were noted in the Neutral and Positive categories. These results mark a notable advancement over existing state-of-the-art models, illustrating the potential of our approach in enhancing Sentiment Recognition through the synergistic use of trimodal data. This study’s comprehensive analysis and significant results demonstrate the proposed algorithm’s effectiveness in nuanced emotional state recognition and pave the way for future advancements in affective computing, emphasizing the value of integrating multimodal data for improved accuracy and robustness. © 2024 by the authors.",CNN; sentiment analysis; cumulative attribute-weighted graph neural network; RNN; trimodal emotion analysis,,,emotion,No,Yes
scopus,Emotion aided multi-task framework for video embedded misinformation detection,"Kumari, R.; Gupta, V.; Ashok, N.; Ghosal, T.; Ekbal, A.",2024,,83,,10.1007/s11042-023-17208-6,"Online news consumption via social media platforms has accelerated the growth of digital journalism. Adverse to traditional media, digital media has lower entry barriers and allows everyone as a content creator, resulting in numerous fake news productions to attract public attention. As multimedia content is more convenient for users than expressing their feelings through text, images and video-embedded fake news is being circulated rapidly on social media nowadays. Emotional appeal in fake news is also a driving factor in its rapid dissemination. Although prior studies have made a remarkable effort toward fake news detection, they give less emphasis on exploring video modality and emotional appeal in fake news. To bridge this gap, this paper presents the following two contributions: i) It first develops a video-based multimodal fake news detection dataset named FakeClips and ii) It introduces a deep multitask framework dedicated to video-embedded multimodal fake news detection in which fake news detection is the main task and emotion recognition is the auxiliary task. The results reveal that investigating emotion and fake news together in a multitasking framework achieves 9.04% and 5.27% gains in terms of accuracy and f-score, respectively over the state-of-the-art model i.e. Fake Video Detection Model. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",Emotion Recognition; Multi-modal; Deep learning; Social networking (online); Multi tasks; Fake detection; Multimodal emotion; Digital storage; Multimodal fake news detection; Online news; Social media platforms; Supervised contrastive learning; Video embedded fake news,,,emotion,No,Yes
scopus,Speaker-aware cognitive network with cross-modal attention for multimodal emotion recognition in conversation,"Guo, L.; Song, Y.; Ding, S.",2024,,296,,10.1016/j.knosys.2024.111969,"Emotion recognition in conversation (ERC) has gained considerable attention owing to its extensive applications in the field of human-computer interaction. However, previous models have had certain limitations in exploring the potential emotional relationships within the conversation due to their inability to fully leverage speaker information. Additionally, information from various modalities such as text, audio, and video can synergistically enhance and supplement the analysis of emotional context within the conversation. Nonetheless, effectively fusing multimodal features to understand the detailed contextual information in the conversation is challenging. This paper proposes a Speaker-Aware Cognitive network with Cross-Modal Attention (SACCMA) for multimodal ERC to effectively leverage multimodal information and speaker information. Our proposed model primarily consists of the modality encoder and the cognitive module. The modality encoder is employed to fuse multimodal feature information from speech, text, and vision using a cross-modal attention mechanism. Subsequently, the fused features and speaker information are separately fed into the cognitive module to enhance the perception of emotions within the dialogue. Compared to seven common baseline methods, our model increased the Accuracy score by 2.71 % and 1.70 % on the IEMOCAP and MELD datasets, respectively. Additionally, the F1 score improved by 2.92 % and 0.70 % for each dataset. Various experiments also demonstrate the effectiveness of our method. © 2024 Elsevier B.V.",Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Multimodal emotion recognition; Signal encoding; Human computer interaction; Cross-modal; Emotion recognition in conversation; Multimodal features; Audio and video; Cognitive network; Contextual information; Cross-modal attention; Speaker-aware,,,emotion,No,Yes
scopus,Bi-stream graph learning based multimodal fusion for emotion recognition in conversation,"Lu, N.; Han, Z.; Han, M.; Qian, J.",2024,,106,,10.1016/j.inffus.2024.102272,"Emotion Recognition in Conversation (ERC) is the process of automatically detecting and understanding emotions expressed in a conversation, which plays an important role in human–computer interaction. A conversation generates different modality data including words, tone of voice and facial expression. Multimodal ERC can fuse the information from multiple views to comprehensively model emotion dynamics in a conversation. Graph Neural Networks (GNNs) are employed by multimodal ERC to learn intra-modal long-range contextual information and inter-modal interaction. However, fusing different modalities within a graph may generate the conflict of multimodal information and suffer from data heterogeneity issue. In the paper, we propose a novel Bi-stream Graph Learning based Multimodal Fusion (BiGMF) approach for ERC. It consists of a unimodal stream graph learning for modeling the intra-modal long-range context information and a cross-modal stream graph learning for modeling the inter-modal interactions, which uses GNNs to learn the intra- and inter-modal information in parallel. The separation learning scheme can successfully alleviate the conflict and heterogeneity in multimodal data fusion, and promote the explicitly modeling of cross-modal relations. The experimental results on two public datasets further verify that the superiority of the proposed approach compared to the SOTA approaches. © 2024 Elsevier B.V.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Modal interactions; Learn+; Multimodal fusion; Emotion recognition in conversation; Data fusion; Contextual information; Graph neural networks; Inter-modal interaction; Stream graphs,,,emotion,No,No
scopus,MSER: Multimodal speech emotion recognition using cross-attention with deep fusion,"Khan, M.; Gueaieb, W.; El Saddik, A.; Kwon, S.",2024,,245,,10.1016/j.eswa.2023.122946,"In human–computer interaction (HCI) and especially speech signal processing, emotion recognition is one of the most important and challenging tasks due to multi-modality and limited data availability. Nowadays, an intelligent system is required for real-world applications to efficiently process and understand the speaker's emotional state and to enhance the analytical abilities to assist communication by a human-machine interface (HMI). Designing a reliable and robust Multimodal Speech Emotion Recognition (MSER) to efficiently recognize emotions through multi-modality such as speech and text is necessary. This paper propose a novel MSER model with a deep feature fusion technique using a multi-headed cross-attention mechanism. The proposed model utilize audio and text cues to predict the emotion label accordingly. Our proposed model process the raw speech signal and text by CNN and feeds to corresponding encoders for discriminative and semantic feature extractions. The cross-attention mechanism is applied to both features to enhance the interaction between text and audio cues by crossway to extract the most relevant information for emotion recognition. Finally, combining the region-wise weights from both encoders enables interaction among different layers and paths by the proposed deep feature fusion scheme. The authors evaluate the proposed system using the IEMOCAP and MELD datasets and conduct extensive experiments that obtain state-of-the-art (SOTA) results and show a 4.5% improved recognition rate, respectively. Our model secured a significant improvement over SOTA methods, which shows the robustness and effectiveness of the proposed MSER model. © 2023 Elsevier Ltd",Semantics; Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Character recognition; Signal encoding; Affective Computing; Human computer interaction; Auto encoders; Affective computing; Multi-modality; Audio signal processing; Auto-encoders; Deep fusion; Intelligent systems; Interface states; Multimodal speech emotion recognition; Speech and text processing; Speech communication; Text-processing,,,emotion,Yes,No
scopus,A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition,"Meng, T.; Shou, Y.; Ai, W.; Du, J.; Liu, H.; Li, K.",2024,,569,,10.1016/j.neucom.2023.127109,"As an important development direction of natural language processing, emotion recognition in conversation (ERC) remains a challenge in sentiment analysis. Given the large-scale dialogue datasets and their wide application in the fields of recommendation systems and human–machine dialogue systems, researchers have begun to pay more attention to the issue of ERC. In recent research, the task of ERC has been largely based on the graph structure to model the speaker level. However, most existing studies simply splice multimodal features, and the heterogeneity of multimodal features tends to be overlooked. Hence, this paper proposes a multivariate messaging framework to embed heterogeneous information into multimodal relational graphs. In the process of aggregating graph node information, we take into account the homogeneity of nodes and assign different weights to different nodes so as to better aggregate semantic information. In order to improve the robustness of the model, we utilize the mechanism of sharing weights among neighbors to reduce the number of network parameters and improve the generalization ability of the model. In so doing, the node information is aggregated through the constructed graph network, and the final semantic vector representation is obtained. Experiments over two benchmark datasets for ERC show that our proposed model achieves improved performance in accuracy and F1 value. © 2023 Elsevier B.V.",Semantics; emotion; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; diagnosis; human; benchmarking; Benchmarking; natural language processing; Graphic methods; sentiment analysis; nerve cell network; article; Large dataset; conversation; Emotion recognition in conversation; Multimodal features; Graph neural networks; Graph theory; Heterogeneous graph; Heterogeneous graph neural network; Heterogeneous Graph Neural Network; Message passing; Message-passing; Multi-messaging; Self attention mechanism,,,emotion,No,Yes
scopus,Transformer-Based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild,"Zhang, X.; Li, M.; Lin, S.; Xu, H.; Xiao, G.",2024,,34,,10.1109/TCSVT.2023.3312858,"Dynamic expression recognition in the wild is a challenging task due to various obstacles, including low light condition, non-positive face, and face occlusion. Purely vision-based approaches may not suffice to accurately capture the complexity of human emotions. To address this issue, we propose a Transformer-based Multimodal Emotional Perception (T-MEP) framework capable of effectively extracting multimodal information and achieving significant augmentation. Specifically, we design three transformer-based encoders to extract modality-specific features from audio, image, and text sequences, respectively. Each encoder is carefully designed to maximize its adaptation to the corresponding modality. In addition, we design a transformer-based multimodal information fusion module to model cross-modal representation among these modalities. The unique combination of self-attention and cross-attention in this module enhances the robustness of output-integrated features in encoding emotion. By mapping the information from audio and textual features to the latent space of visual features, this module aligns the semantics of the three modalities for cross-modal information augmentation. Finally, we evaluate our method on three popular datasets (MAFW, DFEW, and AFEW) through extensive experiments, which demonstrate its state-of-the-art performance. This research offers a promising direction for future studies to improve emotion recognition accuracy by exploiting the power of multimodal features.  © 1991-2012 IEEE.",Semantics; Transformer; Emotion Recognition; Speech recognition; deep learning; Behavioral research; Emotion recognition; Face recognition; Deep learning; Signal encoding; Data mining; Features extraction; Convolutional neural network; Facial expression recognition; Information fusion; Feature extraction; Multimodal information fusion; Dynamic facial expression; Dynamic facial expression recognition; multimodal information fusion; Neural networks; semantic alignment; Semantic alignments,,,emotion,No,Yes
scopus,Real-time Multi-CNN-based Emotion Recognition System for Evaluating Museum Visitors’ Satisfaction,"Kwon, D.H.; Yu, J.M.",2024,,17,,10.1145/3631123,"Conventional studies on the satisfaction of museum visitors focus on collecting information through surveys to provide a one-way service to visitors, and thus it is impossible to obtain feedback on the real-time satisfaction of visitors who are experiencing the museum exhibition program. In addition, museum practitioners lack research on automated ways to evaluate a produced content program’s lifecycle and its appropriateness. To overcome these problems, we propose a novel multiconvolutional neural network, called VimoNet, which is able to recognize visitors emotions automatically in real-time based on their facial expressions and body gestures. Furthermore, we design a user preference model of content and a framework to obtain feedback on content improvement for providing personalized digital cultural heritage content to visitors. Specifically, we define seven emotions of visitors and build a dataset of visitor facial expressions and gestures with respect to the emotions. Using the dataset, we proceed with feature fusion of face and gesture images trained on the DenseNet-201 and VGG-16 models for generating a combined emotion recognition model. From the results of the experiment, VimoNet achieved a classification accuracy of 84.10%, providing 7.60% and 14.31% improvement, respectively, over a single face and body gesture-based method of emotion classification performance. It is thus possible to automatically capture the emotions of museum visitors via VimoNet, and we confirm its feasibility through a case study with respect to digital content of cultural heritage. © 2024 Copyright held by the owner/author(s).",Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Deep learning; Multimodal emotion recognition; Real- time; Digital contents; E-learning; Emotion understanding; intelligence emotion understanding; Intelligence emotion understanding; Life cycle; Museum visitor; museum visitors; Museum-digital content; museum-digital contents; Museums; visitor study; Visitor study,,,emotion,No,Yes
scopus,Zero-shot multitask intent and emotion prediction from multimodal data: A benchmark study,"Singh, G.V.; Firdaus, M.; Chauhan, D.S.; Ekbal, A.; Bhattacharyya, P.",2024,,569,,10.1016/j.neucom.2023.127128,"Empathy involves comprehending and sharing the emotions of another person. In the realm of conversational AI, empathy pertains to the AI's capacity to understand and respond suitably to the user's emotions and needs. Conversational AI with empathetic capabilities can heighten the user experience by making interactions more personalized and natural. At present, machine learning algorithms are commonly utilized in existing conversational AI systems to recognize emotions and corresponding empathetic intents from annotated data. Nonetheless, this approach is not without limitations, being expensive and time-consuming. Our present work takes a holistic approach to empathy in conversational AI, where we propose a novel zero-shot multitask framework, the Zero-shot Intent Emotion Detection (ZIED) network, identifies both emotions and intents in a multimodal setting. We developed an end-to-end model that concurrently captures textual, audio, and visual representations and integrates the different modalities using cross-attention mechanisms. Our experimental results, based on the EmoInt-MD dataset, show that incorporating all three modalities results in the best performance for both emotion and empathetic intent detection. We observed a noteworthy improvement of over 6% and 4% for intent and emotion, respectively, for various ratios of seen and unseen classes. © 2023 Elsevier B.V.",Natural language processing; emotion; empathy; adult; diagnosis; human; Multi-modal data; Emotion detection; controlled study; machine learning; benchmarking; natural language processing; learning algorithm; Learning algorithms; article; prediction; Language processing; Natural languages; Natural language processing systems; Zero-shot learning; Benchmark study; Emotion predictions; Intent detection; Multimodal data; Multitask framework; Zero-shot setting,,,emotion,Yes,Yes
scopus,Incongruity-aware multimodal physiology signals fusion for emotion recognition,"Li, J.; Chen, N.; Zhu, H.; Li, G.; Xu, Z.; Chen, D.",2024,,105,,10.1016/j.inffus.2023.102220,"Various physiological signals can reflect the human's emotional states objectively. How to take advantage of the common as well as complementary properties of different physiological signals in representing the emotional states is an interesting problem. Although various models have been constructed to fuse multimodal physiological signals for emotion recognition, the possible incongruity existing among different physiological signals in representing the emotional states and the redundancy resulted from the fusion, which may affect the performance of the fusion schemes seriously, were seldom considered. To this end, a fusion model, which can eliminate the incongruity among different physiological signals and reduce the information redundancy to some extent, is proposed. First, one physiological signal is chosen as the primary modality due to its prominent performance in emotion recognition, and the remaining physiological signals are viewed as the auxiliary modalities. Secondly, the Cross Modal Transformer (CMT) is adopted to optimize the features of the auxiliary modalities by eliminating the incongruity among them, and then Low Rank Fusion (LRF) is performed to eliminate information redundancy caused by fusion. Thirdly, the modified CMT (MCMT) is constructed to enhance the feature of the primary modality by that of each optimized auxiliary modality feature. Fourthly, Self-Attention Transformer (SAT) is performed on the concatenation result of all the enhanced primary modality features to take full advantage of the common as well complementary properties among them in representing the emotional states. Finally, the enhanced primary modality feature and the optimized auxiliary features are fused by concatenation for emotion recognition. Extensive experimental results on DEAP and WESAD datasets demonstrate that (i) The incongruity does exist among different physiological signals, and the CMT-based auxiliary modality feature optimization strategy can eliminate the incongruity prominently; (ii) The emotion prediction accuracy of the primary modality can be enhanced by the auxiliary modality; (iii) All the key modules in the proposed model, CMT, LRF, and MCMT, contribute to the performance enhancement of the proposed model; iv) The proposed model outperforms State-Of-The-Art (SOTA) models in emotion recognition task. © 2023 Elsevier B.V.",Emotion Recognition; Emotional state; Speech recognition; Emotion recognition; Physiological signals; Physiology; Multi-modal fusion; Biomedical signal processing; Cross-modal; Physiological models; Redundancy; Cross-modal transformer; Cross-Modal Transformer (CMT); Low rank fusion; Low Rank Fusion (LRF); Physiological signal incongruity; Primary modality; Self-attention transformer; Self-Attention Transformer (SAT),,,emotion,No,Yes
scopus,"An emotion-driven, transformer-based network for multimodal fake news detection","Yadav, A.; Gupta, A.",2024,,13,,10.1007/s13735-023-00315-3,"Social media is filled with multimedia data in the form of news and is heavily impacting the daily lives of the people. However, the rise of fake news is causing distress and becoming a major source of concern. Several attempts have been made in the past to detect fake news, but it still remains a challenging problem. In this study, we propose an emotion-driven framework that extracts emotions from the multimodal data to identify fake news. We use the vision transformer, which removes the irrelevant data from the images and enhances the overall classification accuracy. To the best of our knowledge, this is the first work that incorporates multimodal emotions to detect fake news in the multimodal data, comprising of images and text. We conducted several experiments on five datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset, Risdal Fake News Dataset, and Fakeddit Multimodal Dataset, and evaluated the performance of the network by using Precision, Recall, F1 scores, Accuracy, and ROC curves. We also conducted an ablation study to verify the effectiveness of different components involved in the proposed architecture. The experimental results show that the proposed architecture outperforms the state-of-the-art and other baseline methods on all the evaluation metrics. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Emotions; Transformer; Deep learning; Multimodal; Fake news,,,emotion,No,Yes
scopus,Deep Learning-Based Automated Emotion Recognition Using Multi modal Physiological Signals and Time-Frequency Methods,"Sriram Kumar, P.; Govarthan, P.K.; Gadda, A.A.S.; Ganapathy, N.; Ronickom, J.F.A.",2024,,,,10.1109/TIM.2024.3420349,"Accurate prediction and recognition of human emotions are crucial for effective human-computer interfaces. An Automated Emotion Recognition (AER) method is highly desirable, and multimodal approaches have gained scientific attention due to their ability to leverage different modalities for improved accuracy and reliability. Our study has attempted to classify multiple emotional states using multimodal physiological signals, namely Electrodermal Activity (EDA), Electrocardiogram (ECG), and various deep learning networks. The signals were obtained from the publicly available two datasets such as Continuously Annotated Signals of Emotion and Wearable Stress and Affect Detection. The EDA signals were decomposed into tonic and phasic components using the convex-optimization-based EDA method. Further, the ECG and phasic components of EDA were subjected to different Time-Frequency Representations (TFR), namely Short-Time Fourier Transform, Continuous Wavelet Transform (CWT), and Mel-Frequency Cepstrum. These TFRs were fed as inputs to the AlexNet, configurable Convolutional Neural Network (cCNN), and pre-trained VGG16 architectures in unimodal and multimodal settings to extract robust features for the effective classification. Our results demonstrate that multimodal signals outperform unimodal settings, with ECG proving more effective than EDA in emotion classification. The pipeline (EDA+ECG, CWT, and VGG16) yielded the highest accuracies of 86.66% and 83.96% for four-class and three-class emotion classification, respectively. In conclusion, our results suggest that network performance was improved with multimodal physiological signals compared to unimodal signals in accurate AER. Consequently, the proposed method holds promise as a valuable tool for assessing emotional states in automated decision-making. IEEE",Emotion Recognition; Speech recognition; Emotion recognition; Physiology; Deep learning; Classification (of information); Electroencephalography; Biomedical signal processing; Automated emotion recognition; Electrocardiography; Electrocardiogram; Electrocardiograms; Accuracy; Automation; Neural networks; Anxiety disorder; Anxiety disorders; Continuous Wavelet Transform; Continuous wavelet transforms; Convex optimization; Electrodermal activity; Electrodes; Time-frequency analysis; Time-frequency Analysis; Time-frequency representations; Wavelet transforms,,,emotion,No,Yes
scopus,"Emotion Detection Using Machine Learning, ECG Signals and Facial Features","Dua, A.",2024,,,,10.1109/IDCIoT59759.2024.10467415,"This research study introduces a machine learning classifier for distinguishing 3 distinct emotional states using electrocardiogram (ECG) and facial features. A data pre-processing phase is performed by com bining the emotional data from the DREAMER (2015) dataset and the relatively newer DERCFF (2023) (Database for Emotion Recognition through Cardiac-Facial Features). Subsequently, the study leverages the power of the Discrete Cosine Transform to extract essential features, optimizing the model for enhanced accuracy. The DERCFF dataset plays a pi votal role in the study's approach as it provides a unique set of features extracted from facial and cardiac signals. Convolutional Neural Networks (CNNs) are employed to train upon these extracted facial images, facilitating a comprehensive analysis. The objective of this study is to discern the authenticity of the displayed emotions, ultimately enhancing the overall performance of the model. The target emotions consist of happiness, tension (anger and sadness) and calmness, with LightGBM, Bagged Decision Trees ensemble, Linear S VM and Gaussian Naive Bayes methods serving as the classifiers. This study's combined models (ECG and Facial Features) achieve a notable accuracy rate of 93.80/0, underscoring its efficacy in accurately identifying emotions from multimodal cardiac signals. This research contributes to the field of affective computing, with potential applications in mental health monitoring, human-computer interaction, and emotion-Aware technologies.  © 2024 IEEE.",Machine Learning; Emotion Recognition; Emotional state; Deep learning; Deep Learning; Learning systems; Emotion detection; Human computer interaction; Convolutional neural networks; Biomedical signal processing; Machine-learning; Data handling; Electrocardiograms; Biomedical engineering; Biomedical Engineering; Cardiac signals; Decision trees; Discrete cosine transforms; Electrocardiogram signal; Facial feature; Internet of things; Internet of Things; Learning classifiers; Research studies; Signal features,,,emotion,No,Yes
scopus,GCFN: A Novel Network for Integrating Contextual and Speaker State Information in Multimodal Emotion Recognition,"Xie, K.; Xie, Z.",2024,,,,10.1109/AINIT61980.2024.10581706,"Multimodal machine learning is a crucial technology in the realm of emotion recognition and has been extensively researched. Its studies primarily concentrate on single-modal feature extraction and cross-modal feature fusion. Traditional single-modal feature extraction methods, such as LSTM, GRU, and other sequential neural networks, often fail to fully leverage the effective information in audio and video modalities. In contrast, text modalities seem to be more thoroughly exploited for information extraction. To tackle this issue, this paper introduces an innovative approach: the GRU-Cubic Fusion Network (GCFN). This approach utilizes GRU blocks to deeply model the contextual and character status information of the three modalities, thereby enhancing the extraction of each unimodal feature. Additionally, for cross-modal feature fusion, the GCFN network integrates a novel architecture, the CCNN module. This module accomplishes efficient feature fusion through pure convolution operations, eliminating the need for the attention mechanism. This not only improves fusion efficiency but also enhances computational efficiency. Experimental results on the CMU-MOSI dataset demonstrate that GCFN achieves outstanding accuracy in emotion classification tasks. © 2024 IEEE.",Long short-term memory; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Unimodal; Classification (of information); Features extraction; Features fusions; Feature extraction; Contextual information; component; Component; Computational efficiency; Multi dimensional; Multi-dimensional feature fusion; Multi-dimensional Feature Fusion; Multimodal Fusion; Unimodal feature extraction,,,emotion,No,Yes
scopus,"A Systematic Review on Multimodal Emotion Recognition: Building Blocks, Current State, Applications, and Challenges","Kalateh, S.; Estrada-Jimenez, L.A.; Hojjati, S.N.; Barata, J.",2024,,,,10.1109/ACCESS.2024.3430850,"Emotion recognition involves accurately interpreting human emotions from various sources and modalities, including questionnaires, verbal, and physiological signals. With its broad applications in affective computing, computational creativity, human-robot interactions, and market research, the field has seen a surge in interest in recent years. This paper presents a systematic review of multimodal emotion recognition (mer) techniques developed from 2014 to 2024, encompassing verbal, physiological signals, facial, body gesture, and speech as well as emerging methods like sketches emotion recognition. The review explores various emotion models, distinguishing between emotions, feelings, sentiments, and moods, along with human emotional expression, categorized in both artistic and non-verbal ways. It also discusses the background of automated emotion recognition systems and introduces seven criteria for evaluating modalities alongside a current state analysis of mer, drawn from the human-centric perspective of this field. By selecting the PRISMA guidelines and carefully analyzing 45 selected articles, this review provides comprehensive perspectives into existing studies, datasets, technical approaches, identified gaps, and future directions in MER. It also highlights existing challenges and potential applications of the MER. Authors",Artificial Intelligence; Machine learning; Emotion Recognition; Speech recognition; Emotion recognition; Physiology; Deep learning; Multimodal emotion recognition; Learning systems; Affective Computing; Human computer interaction; Human robot interaction; Features extraction; Cultural difference; Machine-learning; Multimodal Emotion Recognition; Feature extraction; Cultural differences; Emotion expression; Emotion Expression; Guideline; Guidelines; Market Research; Mood; Reviews,,,emotion,No,No
scopus,Deep Representation Learning for Multimodal Emotion Recognition Using Physiological Signals,"Zubair, M.; Woo, S.; Lim, S.; Yoon, C.",2024,,,,10.1109/ACCESS.2024.3436556,"Physiological signal analysis has gained a lot of interest in recent years and has been used in a variety of fields including emotion recognition, activity recognition, and health monitoring. However, emotion recognition based on physiological signals is not yet explored entirely using deep learning, and there are still some exciting challenges to be handled. For example, deep representation learning for spatio-temporal feature extraction, the discrimination between adjacent emotions with entangled features, and the imbalanced distribution of data are the most prominent issues in emotion recognition. This work focuses on deep multimodal representation learning of physiological signals to alleviate the aforementioned challenges. We introduce a novel deep learning architecture for emotion classification that effectively extracts spatio-temporal information from physiological signals. We proposed a mutual attention mechanism to extract emotion-specific features for improved classification. To handle the issue of adjacent emotions and imbalanced data, we introduce a dense max-margin loss function based on Gaussian similarity measure. Our experiments on different datasets reveal that the proposed emotion classification methodology effectively learns a balanced deep representation of physiological signals, significantly maximizes the inter-class margin, and reduces intra-class variance to discriminate between different classes of emotions. Authors",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Physiological signals; Physiology; Deep learning; Classification (of information); Features extraction; Biomedical signal processing; EEG; Attention mechanism; Brain modeling; Electrocardiography; Electrocardiograms; Extraction; Feature extraction; ECG; Biomedical monitoring; Imbalanced data,,,emotion,No,No
scopus,Multi-level attention fusion network assisted by relative entropy alignment for multimodal speech emotion recognition,"Lei, J.; Wang, J.; Wang, Y.",2024,,,,10.1007/s10489-024-05630-8,"Multimodal speech emotion recognition can utilize features from different modalities simultaneously to improve the modeling capabilities in affective computing. However, the rough feature combining method may not effectively promote interaction and facilitate learning between different modalities. This paper proposes a novel multimodal Speech Emotion Recognition (SER) framework that maps two feature vectors from different modalities to the same feature space by a Relative Entropy Alignment (REA) mechanism, which facilitates the modalities to complement mutually in learning emotional representations. Specifically, we employ KLD (Kullback-Leibler Divergence) to perform the feature alignment, capturing the temporal correlation between audio and text while ensuring that the features of both modalities tend to align in the feature space, thereby alleviating modality conflicts. Meanwhile, we construct a Multi-level Attention Fusion (MAF) mechanism to capture the emotion representations from different modalities, facilitating information exchange between different modalities while mitigating some redundant features. Furthermore, we extract multi-level acoustic information by wavelet packet transform to enrich the audio modality’s emotional features further. Experimental results on some multimodal emotion datasets, such as Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Multi-modal Emotion Lines Dataset (MELD), demonstrate that our proposed method outperforms the state-of-the-art model. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Affective Computing; Multimodal; Cross-modal; Feature space; Audio acoustics; Alignment; Cross-modal fusion; Entropy; Feature alignment; Modelling capabilities; Multilevels; Relative entropy; Vector spaces,,,emotion,No,No
scopus,TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation,"Yun, T.; Lim, H.; Lee, J.; Song, M.",2024,,1,,,"Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME1 incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the nonverbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional experiments. © 2024 Association for Computational Linguistics.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Speech processing; Cross-modal; Distillation; Dialogue systems; Audio-visual; Computational linguistics; Language model; Nonverbals; Teachers'; Transfer information,,,emotion,No,Yes
scopus,"Multimodal Emotion Recognition Using Attention-Based Model with Language, Audio, and Video Modalities","Sharma, D.; Jayabalan, M.; Sultanova, N.; Mustafina, J.; Yao, D.N.L.",2024,,191,,,"Multimodal emotion identification is becoming increasingly important in human–computer interaction due to the amount of emotional information in human communication. Multimodal emotion recognition is the technique of simultaneously considering several modalities to boost accuracy and robustness. As emotion identification studies become more vital to human–computer interactions, automatic emotion detection systems become increasingly necessary. However, a lack of data presents a problem for multimodal emotion identification. To address this issue, we suggest employing transfer learning, which uses pretrained models such as RoBERTa and attention-based mechanisms such as self-attention to extract relevant features from multiple modalities and multi-head attention to fuse data across modalities. The aim of this paper is to provide a strategy for reliably forecasting emotions in audio, visual, and text by merging and complementing aspects traditionally handled by humans with those typically handled by deep learning. During the study, three popular multimodal emotion recognition datasets, IEMOCAP, CMU-MOSI, and CMU-MOSEI, are analyzed and ranked based on their quality. This study will help in constructing the network with the right amount of focus placed on each feature modality by creating an architecture that efficiently combines textual characteristics retrieved from RoBERTa with other modality-based features. A model better than BERT is introduced as part of this work that helps to improve the performance. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Emotion Recognition; Multi-modal; Speech recognition; Deep learning; Multimodal emotion recognition; Emotion detection; Human computer interaction; Emotional information; Human communications; Self-attention; Audio and video; Detection system; Emotion identification; Emotion identifications; Multi-head attention,,,emotion,No,Yes
scopus,Residual multimodal Transformer for expression-EEG fusion continuous emotion recognition,"Jin, X.; Xiao, J.; Jin, L.; Zhang, X.",2024,,,,10.1049/cit2.12346,"Continuous emotion recognition is to predict emotion states through affective information and more focus on the continuous variation of emotion. Fusion of electroencephalography (EEG) and facial expressions videos has been used in this field, while there are with some limitations in current researches, such as hand-engineered features, simple approaches to integration. Hence, a new continuous emotion recognition model is proposed based on the fusion of EEG and facial expressions videos named residual multimodal Transformer (RMMT). Firstly, the Resnet50 and temporal convolutional network (TCN) are utilised to extract spatiotemporal features from videos, and the TCN is also applied to process the computed EEG frequency power to acquire spatiotemporal features of EEG. Then, a multimodal Transformer is used to fuse the spatiotemporal features from the two modalities. Furthermore, a residual connection is introduced to fuse shallow features with deep features which is verified to be effective for continuous emotion recognition through experiments. Inspired by knowledge distillation, the authors incorporate feature-level loss into the loss function to further enhance the network performance. Experimental results show that the RMMT reaches a superior performance over other methods for the MAHNOB-HCI dataset. Ablation studies on the residual connection and loss function in the RMMT demonstrate that both of them is functional. © 2024 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.",facial expression recognition; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; physiology; 'current; Electroencephalography; Electrophysiology; Facial expression recognition; Distillation; Convolutional networks; Human machine interaction; human-machine interaction; information fusion; Loss functions; regression analysis; Regression analysis; Simple approach; Spatiotemporal feature,,,emotion,No,Yes
scopus,VEDANet: A dense blocked network for visual emotion analysis in multimedia retrieval,"Sharma, K.; Nandal, R.; Kumar, S.; Joshi, K.",2024,,,,10.1007/s11042-024-19646-2,"This research focuses on addressing the challenges of visual emotion analysis and emphasizes the need for improved comprehension and categorization of emotions by machines. The objective is to develop an efficient architecture for emotion classification in real-world scenarios encountered in multimedia retrieval tasks, considering factors like illumination, occlusion, pose variations, small face sizes, multimodal detection, and big data issues. The proposed Dense Blocked Network-based VEDANet architecture surpasses the state-of-the-art on benchmark datasets by leveraging pre-trained CNN architectures to recognize facial features and extract metadata. The exploration of 128 descriptors from a deep residual network further enhances the operations. Using the VEDANet framework, emotions are classified into seven categories with superior accuracy compared to conventional approaches. The study investigates the efficacy of an over-the-top optimization (OTO) layer to enhance emotion classification. The proposed model achieves impressive accuracy scores on diverse datasets, including AffectNet, Google FEC, Yale Face DB, and FER2013, with percentages of 87.30%, 92.75%, 95.07%, and 90.53% respectively. Real-time performance is achieved with exceptional accuracy of 93.46% on live frames, while minimizing turn-around time by optimizing network size and parameters. This research contributes to the advancement of visual emotion analysis, providing a comprehensive and efficient solution for AI-based emotion detection in various applications. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Emotion Recognition; Emotion recognition; Face recognition; Deep learning; Emotion detection; Sentiment analysis; Emotion analysis; Network architecture; Facial emotion recognition; Sentiment Analysis; Facial emotions; Deep learning architecture; Deep Learning Architectures; Emotion Detection; Facial Emotion Recognition; FER; Learning architectures; Multimedia Retrieval,,,emotion,No,Yes
scopus,Cross-modal Guiding Neural Network for Multimodal Emotion Recognition from EEG and Eye Movement Signals,"Fu, B.; Chu, W.; Gu, C.; Liu, Y.",2024,,,,10.1109/JBHI.2024.3419043,"Multimodal emotion recognition research is gaining attention because of the emerging trend of integrating information from different sensory modalities to improve performance. Electroencephalogram (EEG) signals are considered objective indicators of emotions and provide precise insights despite their complex data collection. In contrast, eye movement signals are more susceptible to environmental and individual differences but offer convenient data collection. Conventional emotion recognition methods typically use separate models for different modalities, potentially overlooking their inherent connections. This study introduces a cross-modal guiding neural network designed to fully leverage the strengths of both modalities. The network includes a dual-branch feature extraction module that simultaneously extracts features from EEG and eye movement signals. In addition, the network includes a feature guidance module that uses EEG features to direct eye movement feature extraction, reducing the impact of subjective factors. This study also introduces a feature reweighting module to explore emotion-related features within eye movement signals, thereby improving emotion classification accuracy. The empirical findings from both the SEED-IV dataset and our collected dataset substantiate the commendable performance of the model, thereby confirming its efficacy. IEEE",convolutional neural network; emotion; Emotion Recognition; Speech recognition; Emotion recognition; diagnosis; female; human; videorecording; Multimodal emotion recognition; Features extraction; recognition; electroencephalography; Electroencephalography; attention; Biomedical signal processing; Cross-modal; electroencephalogram; Electrophysiology; eye movement; human experiment; Brain modeling; Electroencephalogram; electroencephalogram (EEG); Eye movements; Convolution; Convolutional neural network; nerve cell network; article; normal human; Accuracy; Features selection; Data acquisition; Extraction; Video; feature extraction; Feature extraction; Neural networks; convolutional neural network (CNN); cross-modal guidance; Cross-modal guidance; feature selection; Videos,,,emotion,No,Yes
scopus,Cultural-Aware AI Model for Emotion Recognition,"Baradaran, M.; Zohari, P.; Mahyar, A.; Motamednia, H.; Rahmati, D.; Gorgin, S.",2024,,,,10.1109/MVIP62238.2024.10491176,"Emotion AI is a research domain that aims to understand human emotions from visual or textual data. However, existing methods often ignore the influence of cultural diversity on emotional interpretation. In this paper, we propose a multimodal deep learning model that integrates cultural awareness into emotion recognition. Our model uses images as the primary data source and comments from individuals across different regions as the secondary data source. Our results show that our model achieves robust performance across various scenarios. Our contribution is to introduce a novel fusion approach that bridges cultural gaps and fosters a more nuanced understanding of emotions. Due to the best of our knowledge, few works are using this approach, for Emotion AI, combining different types of data sources and models. We evaluate our model on the ArtELingo dataset, which contains image-comment pairs with Chinese, Arabic, and English annotations. The experimental results in the evaluation phase demonstrate an impressive 80% recognition accuracy for the model that merges image-text features.  © 2024 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Character recognition; Research domains; Image representation; Human emotion; Cultural diversity; Deepfeature extraction; DeepFeature Extraction; Image Representation; Image representations; Text representation; Text Representation; Textual data; Visual data,,,emotion,No,Yes
scopus,CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition,"Farhat, N.; Bohi, A.; Letaifa, L.B.; Slama, R.",2024,,13072,,10.1117/12.3023377,"The field of affective computing has seen significant advancements in exploring the relationship between emotions and emerging technologies. This paper presents a novel and valuable contribution to this field with the introduction of a comprehensive French multimodal dataset designed specifically for emotion recognition. The dataset encompasses three primary modalities: facial expressions, speech, and gestures, providing a holistic perspective on emotions. Moreover, the dataset has the potential to incorporate additional modalities, such as Natural Language Processing (NLP) to expand the scope of emotion recognition research. The dataset was curated through engaging participants in card game sessions, where they were prompted to express a range of emotions while responding to diverse questions. The study included 10 sessions with 20 participants (9 females and 11 males). The dataset serves as a valuable resource for furthering research in emotion recognition and provides an avenue for exploring the intricate connections between human emotions and digital technologies. © 2024 SPIE. All rights reserved.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal data; Artificial intelligence; Affective Computing; Affective computing; Computer games; Multi-modal dataset; Multimodal data; Primary modality; Card games; Emerging technologies; Facial gestures; Game-Based; Spontaneous dataset,,,emotion,No,No
scopus,"Hybrid Quantum-Classical Neural Network for Multimodal Multitask Sarcasm, Emotion, and Sentiment Analysis","Phukan, A.; Pal, S.; Ekbal, A.",2024,,,,10.1109/TCSS.2024.3388016,"Sarcasm detection in unimodal or multimodal setting is a very complex task. Sarcasm, emotion, and sentiment are related to each other, and hence any multitask model could be an effective way to leverage the interdependence among these tasks. In order to better represent these clandestine associations, we avoid solely relying on traditional machine learning methods to encode the relationships between the modalities. In this article, we propose a hybrid quantum model that banks upon the low computational complexity and robust representational power of a variational quantum circuit (VQC) and the tried and tested dense neural network to tackle sentiment, emotion, and sarcasm classification simultaneously. We empirically establish that the quantum properties like superposition, entanglement, and interference will better capture and replicate not only the cross-modal interactions between text, acoustics, and visuals but also the correlations between the three responses. We consider the extended MUStARD dataset to evaluate our proposed hybrid model. The results show that our proposed hybrid quantum framework yields more promising results for the primary task of sarcasm detection with the help of the two secondary classification tasks, <italic>viz.</italic> sentiment and emotion. IEEE",Machine learning; Modal analysis; Multi-modal data; multimodal data; Job analysis; Sentiment analysis; Task analysis; Machine-learning; Complex networks; Classical neural networks; Hybrid quantum-classical neural network; Interference; Quantum computing; Quantum Computing; Quantum entanglement; quantum machine learning; Quantum machine learning; Quantum machines; Quantum optics; Quantum state; Quantum system; Quantum-classical; Vectors,,,emotion,No,No
scopus,A multimodal fusion-based deep learning framework combined with local-global contextual TCNs for continuous emotion recognition from videos,"Shi, C.; Zhang, Y.; Liu, B.",2024,,54,,10.1007/s10489-024-05329-w,"Continuous emotion recognition plays a crucial role in developing friendly and natural human-computer interaction applications. However, there exist two significant challenges unresolved in this field: how to effectively fuse complementary information from multiple modalities and capture long-range contextual dependencies during emotional evolution. In this paper, a novel multimodal continuous emotion recognition framework was proposed to address the above challenges. For the multimodal fusion challenge, the Multimodal Attention Fusion (MAF) method is proposed to fully utilize complementarity and redundancy between multiple modalities. To tackle temporal context dependencies, the Local Contextual Temporal Convolutional Network (LC-TCN) and the Global Contextual Temporal Convolutional Network (GC-TCN) were presented. These networks have the ability to progressively integrate multi-scale temporal contextual information from input streams of different modalities. Comprehensive experiments are conducted on the RECOLA and SEWA datasets to assess the effectiveness of our proposed framework. The experimental results demonstrate superior recognition performance compared to state-of-the-art approaches, achieving 0.834 and 0.671 on RECOLA, 0.573 and 0.533 on SEWA in terms of arousal and valence, respectively. These findings indicate a novel direction for continuous emotion recognition by exploring temporal multi-scale information. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Multi-modal fusion; Human computer interaction; Convolution; Multiple modalities; Continuous emotion recognition; Convolutional networks; Local/global contextual temporal convolutional network; Multi-modal attention fusion; Multiscale information; Temporal multi-scale information,,,emotion,No,Yes
scopus,Emotional speaker identification using PCAFCM-deepforest with fuzzy logic,"Nassif, A.B.; Shahin, I.; Nemmour, N.",2024,,,,10.1007/s00521-024-10154-w,"Voice is perceived as a form of biometrics which communicates valuable and rich information pertinent to an individual, such as his or her identity, gender, accent, age and emotion. Speaker identification denotes the task of identifying speakers based on their intrinsic voice characteristics. This study proposes a text-independent speaker identification system based on principal component analysis (PCA), fuzzy C-means (FCM) along with deepForest called PCAFCM-deepForest. The proposed approach is evaluated under neutral and adverse talking environments. Given this approach, we assessed our proposed model architecture on five benchmark corpora, namely private Arabic Emirati-accented speech dataset, public English dataset; Crowd-sourced emotional multimodal actors dataset (CREMA), public German database; Berlin database of emotional speech (EmoDB), public Chinese and English; emotional speech database (ESD), and public French dataset; public Canadian French emotional (CaFE) speech dataset. Our analysis shows that the performance of speaker identification has been immensely increased (greatly improved) when fuzzy logic and PCA are both applied to the extracted mel-frequency cepstral coefficients (MFCC). Speaker identification performance achieved by the proposed PCAFCM-deepForest is superior to that obtained by deepForest alone, FCM-deepForest as well as convolutional neural network (CNN). Besides, it surpasses the following conventional models: Random forest and support vector machine (SVM). Our findings demonstrate that the attained average speaker identification accuracy is equivalent to 98.20% using the Emirati database; an average performance which outperforms the existing frameworks. Moreover, PCAFCM-deepForest is fine-tuned using the grid search algorithm, and the achieved complexity is much less than that of CNN. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",Speech recognition; Support vector machines; C-means; Computer circuits; Database systems; Deepforest; DeepForest; Emirati; Emotional speech; Fuzzy C-mean; Fuzzy C-means; Fuzzy logic; Fuzzy neural networks; Fuzzy systems; Fuzzy-Logic; Loudspeakers; Principal component analysis; Principal-component analysis; Principle component analysis; Principle components analysis; Speaker identification,,,emotion,No,No
scopus,"Empowering Facial Analytics: A Unified Approach for Emotion, Age, Gender and Object Identification","Senthil, B.; Sundaram, V.; Chauhan, A.S.; Vekkot, S.",2024,,,,10.1109/SPIN60856.2024.10512093,"This research revolutionizes real-Time video analysis by seamlessly integrating emotion, age, gender, and object detection in each frame of a live video feed. The framework leverages a unified deep model that combines pre-Trained feature extraction models with CNN architecture, each meticulously trained on specialized datasets tailored to its recognition task. The proposed model achieves commendable accuracy of 85.28% in emotion recognition using FER-2013 dataset, 84% in age estimation, and 88.67% in gender classification using UTK-Face dataset, and 90.81% in object recognition leveraging the Fruits and Vegetables dataset. Moreover, the emotion, gender and object models achieved an average precision of 0.865, recall of 0.866 and F-score of 0.864, while age model achieved a mean squared error of 0.1136, quantitatively showcasing the model performance.  © 2024 IEEE.",Emotion Recognition; CNN; Face recognition; Classification (of information); Mean square error; Emotion classification; Extraction; Feature extraction; Object recognition; Emotion identifications; Age and Gender Prediction; Age predictions; Computer vision; Facial feature extraction; Facial features extractions; Gender identification; Gender predictions; Multimodal recognition; Multimodal Recognition; Object detection; Object identification; Objects detection; Unified approach,,,emotion,No,Yes
scopus,CiABL: Completeness-induced Adaptative Broad Learning for Cross-Subject Emotion Recognition with EEG and Eye Movement Signals,"Gong, X.; Chen, C.L.P.; Hu, B.; Zhang, T.",2024,,,,10.1109/TAFFC.2024.3392791,"Although multimodal physiological data from the central and peripheral nervous systems can objectively respond to human emotional states, the individual differences caused by non-stationary and low signal-to-noise properties bring several challenges to cross-subject emotion recognition tasks. Many previous studies usually focused on learning high correlation information between different modalities, which easily leads to incomplete descriptions of different physiological signals and difficulties in aligning critical emotional information. To tackle these challenges, this paper proposes a novel multimodal emotion recognition model for improving the generalization performance to unseen target domain subjects, termed Completeness-induced Adaptative Broad Learning (CiABL). The proposed CiABL can gradely explore the completeness modality representation that encompasses both modality-relevant and modality-independent information, avoiding the loss of performance due to spurious correlations from different modalities. Subsequently, a well-designed weighted representation distribution alignment mechanism of CiABL can appropriately align the marginal and conditional distributions to reduce the influences of individual differences greatly. Extensive experiments on the SEED and SEED-FRA datasets demonstrate the effectiveness and generalization of the proposed CiABL, which outperforms current state-of-the-art methods. In addition, CiABL can precisely quantify the importance of global features to properly explain the modality contribution and averaged activation patterns of the brain under cross-subject emotion recognition tasks. IEEE",Emotion recognition; Physiology; Task analysis; Electroencephalography; EEG; Affective computing; Brain modeling; Eye movement; Adaptation models; Broad learning system (BLS); Multimodal physiological data fusion,,,emotion,No,Yes
scopus,COLD Fusion: Calibrated and Ordinal Latent Distribution Fusion for Uncertainty-Aware Multimodal Emotion Recognition,"Tellamekala, M.K.; Amiriparian, S.; Schuller, B.W.; Andre, E.; Giesbrecht, T.; Valstar, M.",2024,,46,,10.1109/TPAMI.2023.3325770,"Automatically recognising apparent emotions from face and voice is hard, in part because of various sources of uncertainty, including in the input data and the labels used in a machine learning framework. This paper introduces an uncertainty-aware multimodal fusion approach that quantifies modality-wise aleatoric or data uncertainty towards emotion prediction. We propose a novel fusion framework, in which latent distributions over unimodal temporal context are learned by constraining their variance. These variance constraints, Calibration and Ordinal Ranking, are designed such that the variance estimated for a modality can represent how informative the temporal context of that modality is w.r.t. emotion recognition. When well-calibrated, modality-wise uncertainty scores indicate how much their corresponding predictions are likely to differ from the ground truth labels. Well-ranked uncertainty scores allow the ordinal ranking of different frames across different modalities. To jointly impose both these constraints, we propose a softmax distributional matching loss. Our evaluation on AVEC 2019 CES, CMU-MOSEI, and IEMOCAP datasets shows that the proposed multimodal fusion method not only improves the generalisation performance of emotion recognition models and their predictive uncertainty estimates, but also makes the models robust to novel noise patterns encountered at test time.  © 1979-2012 IEEE.",emotion; Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; human; Multi-modal fusion; Learning systems; Job analysis; Task analysis; human experiment; multimodal fusion; article; prediction; Uncertainty; Predictive models; quantitative analysis; Affect recognition; calibration; categorical emotion recognition; Categorical emotion recognition; cold stress; Context models; Data visualization; Dimensional affect recognition; noise; uncertainty; Uncertainty analysis; uncertainty modeling; Uncertainty models,,,emotion,Yes,Yes
scopus,EASUM: Enhancing Affective State Understanding through Joint Sentiment and Emotion Modeling for Multimodal Tasks,"Hwang, Y.; Kim, J.-H.",2024,,,,10.1109/WACV57701.2024.00557,"Multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER) tasks have gained a surge of attention in recent years. Although both tasks share common ground in many ways, they are often treated as a separate task. In this work, we propose, EASUM, a new training scheme for bridging the MSA and MER tasks. EASUM aims to bring mutual benefits to both tasks based on the premise that the sentiment and emotion are closely related; hence each information should provide deeper insight into one's affective state to complement the other. We exploit this premise to further improve the performance of each task by 1) first training a domain general model using four benchmark datasets from the MSA and MER tasks: CMU-MOSI, CMU-MOSEI, MELD, and IEMOCAP. Depending on the dataset, the domain general model learns to predict sentiment or emotion values based on the domain invariant features. 2) Then these values are later used as auxiliary pseudo labels when training a domain specific model for each task. Our premise as well as new training scheme are validated through extensive experiments on the four benchmark datasets. The results also demonstrate that the proposed method outperforms the state-of-the-art on the CMU-MOSI, CMU-MOSEI, and MELD datasets, and performs comparable to the state-of-the-art on the IEMOCAP dataset while using approximately 40% fewer parameters. © 2024 IEEE.",Emotion Recognition; Multi-modal; Multimodal emotion recognition; Sentiment analysis; Benchmarking; Algorithms; Affective state; Applications; Benchmark datasets; Cognitive science; General model; Modeling languages; Psychology and cognitive science; Training schemes; Vision + language and/or other modalities; Vision + language and/or other modality,,,emotion,Yes,Yes
scopus,"Multimodal modelling of human emotion using sound, image and text fusion","Hosseini, S.S.; Yamaghani, M.R.; Poorzaker Arabani, S.",2024,,18,,10.1007/s11760-023-02707-8,"Multimodal emotion recognition and analysis are considered as an evolving field of research. The improvement of the multimodal fusion mechanism plays an important role in the more detailed recognition of the recognised emotion. The performance of the emotion recognition system was optimised, and a model for multimodal emotion recognition from audio, text and video data was proposed. First, the data were fused as a combination of video and audio, then as a combination of audio and text as binary, and finally the results were fused together. The final output included audio, text and video data, taking into account common features. The convolutional neural network and long-term and short-term memory (CNN-LSTM) were then used to extract audio. Next, the Inception-Res Net-v2 network was used to extract facial expressions from the video. The output fused data were used by LSTM as the input of the Softmax classifier to recognise the emotion of audio and video features fusion. In addition, the CNN-LSTM was combined in the form of a binary channel for learning audio emotion features. Meanwhile, a Bi-LSTM network was used to extract the text features and Softmax was used to classify the fused features. Finally, the generated results were fused together for the final classification, and the logistic regression model was used for fusion and classification. As the results showed, the recognition accuracy of the proposed method on the IEMOCAP dataset was 82.9%. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023.",Long short-term memory; Machine learning; Emotion Recognition; Multi-modal; Speech recognition; CNN; Audio data; Character recognition; Multimodal emotion recognition; Classification (of information); Convolutional neural networks; LSTM; Video recording; Machine-learning; Multimodal emotion; RNN; Text-processing; Regression analysis; Audio processing; Fusion; Text processing; Video processing; Video signal processing,,,emotion,No,Yes
scopus,SIA-Net: Sparse Interactive Attention Network for Multimodal Emotion Recognition,"Li, S.; Zhang, T.; Chen, C.L.P.",2024,,,,10.1109/TCSS.2024.3409715,"Multimodal emotion recognition (MER) integrates multiple modalities to identify the user’s emotional state, which is the core technology of natural and friendly human&#x2013;computer interaction systems. Currently, many researchers have explored comprehensive multimodal information for MER, but few consider that comprehensive multimodal features may contain noisy, useless, or redundant information, which interferes with emotional feature representation. To tackle this challenge, this article proposes a sparse interactive attention network (SIA-Net) for MER. In SIA-Net, the sparse interactive attention (SIA) module mainly consists of intramodal sparsity and intermodal sparsity. The intramodal sparsity provides sparse but effective unimodal features for multimodal fusion. The intermodal sparsity adaptively sparses intramodal and intermodal interactive relations and encodes them into sparse interactive attention. The sparse interactive attention with a small number of nonzero weights then act on multimodal features to highlight a few but important features and suppress numerous redundant features. Furthermore, the intramodal sparsity and intermodal sparsity are deep sparse representations that make unimodal features and multimodal interactions sparse without complicated optimization. The extensive experimental results show that SIA-Net achieves superior performance on three widely used datasets. IEEE",Emotion Recognition; Emotional state; Speech recognition; Multimodal emotion recognition; Unimodal; Human computer interaction; Multiple modalities; Multimodal features; Core technology; Deep sparse representation; multimodal emotion recognition (MER); Sparse interactive attention; sparse interactive attention (SIA); Sparse representation,,,emotion,No,Yes
scopus,Sequence Modeling and Feature Fusion for Multimodal Emotion Recognition,"Li, X.; Feng, R.; Yue, H.; Zang, H.; Yang, Q.",2024,,,,10.1109/ICCECT60629.2024.10546216,"Multimodal emotion recognition (MER) refers to the recognition and understanding of emotional states from multiple types of data (e.g., audio, and video, etc.). MER in applications such as human-computer interaction, virtual assistants, and social media, an accurate understanding of user emotions can lead to smarter and more personalized services. For example, virtual assistants can better understand the emotional needs of users and provide responses that are more in line with user expectations. However, the existing multimodal emotion recognition research based on LSTM method has poor modeling ability for speaker context dependence, interlocutor temporal order, and dialogue context. This paper presents a novel approach for detecting multi-modal emotion features using Transformers, specifically designed for tracking emotions in dialogue. Furthermore, the multi-attention mechanism is used to emphasize the effective information, and the context global information of the data is fully paid attention to form the semantic visual related features of multi-level attention mechanism fusion. Finally, the three modalities are effectively fused at the decision level to achieve emotion recognition with high accuracy and strong generalization ability, and improve the model's ability to understand complex scenes. On the IEMOCAP dataset, our proposed method achieves the best emotion recognition results among the comparison methods. © 2024 IEEE.",Long short-term memory; Semantics; Transformer; Attention mechanisms; Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; LSTM; transformer; Virtual reality; Multi-head attention; Modeling abilities; multi-head attention; Sequence features; Sequence models; Virtual assistants,,,emotion,No,Yes
scopus,Learning to compose diversified prompts for image emotion classification,"Deng, S.; Wu, L.; Shi, G.; Xing, L.; Jian, M.; Xiang, Y.; Dong, R.",2024,,,,10.1007/s41095-023-0389-6,"Image emotion classification (IEC) aims to extract the abstract emotions evoked in images. Recently, language-supervised methods such as contrastive language-image pretraining (CLIP) have demonstrated superior performance in image understanding. However, the underexplored task of IEC presents three major challenges: a tremendous training objective gap between pretraining and IEC, shared suboptimal prompts, and invariant prompts for all instances. In this study, we propose a general framework that effectively exploits the language-supervised CLIP method for the IEC task. First, a prompt-tuning method that mimics the pretraining objective of CLIP is introduced, to exploit the rich image and text semantics associated with CLIP. Subsequently, instance-specific prompts are automatically composed, conditioning them on the categories and image content of instances, diversifying the prompts, and thus avoiding suboptimal problems. Evaluations on six widely used affective datasets show that the proposed method significantly outperforms state-of-the-art methods (up to 9.29% accuracy gain on the EmotionROI dataset) on IEC tasks with only a few trained parameters. The code is publicly available at https://github.com/dsn0w/PT-DPC/for research purposes. (Figure presented.) © The Author(s) 2024.",Semantics; Emotion Recognition; Performance; Learning systems; Emotion analysis; Emotion classification; Supervised methods; Image emotion analyse; Multi-modal learning; multimodal learning; Image classification; Abstracting; Classification tasks; image emotion analysis; Pre-training; pretraining model; Pretraining model; prompt tuning; Prompt tuning,,,emotion,No,Yes
scopus,MD-EPN: An Efficient Multimodal Emotion Recognition Method Based on Multi-Dimensional Feature Fusion,"Xie, K.; Xie, Z.",2024,,,,10.1109/NNICE61279.2024.10498191,"Multimodal emotion recognition is extensively applied in diverse areas including driving monitoring, online education, telemedicine, and customer service, marking it as a significant technology in contemporary research. Traditional emotion recognition methods predominantly concentrate on unimodal data processing, which can lead to low accuracy and substantial information loss. In response to these issues, this paper introduces an innovative Multi-Dimensional Emotion Perception Network (MD-EPN) that leverages advanced technologies to integrate textual, auditory, and visual data, thereby aiming to enhance the overall comprehensiveness and precision of emotion recognition. By focusing optimizations on crucial dimensions, the proposed architecture not only bolsters the model's performance but also strikes an effective balance between computational efficiency and accuracy retention, exhibiting exemplary emotion classification accuracy on the CMU-MOSI dataset. © 2024 IEEE.",Artificial Intelligence; Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Classification (of information); Features fusions; Data handling; E-learning; Computational efficiency; Multi dimensional; Multi-dimensional feature fusion; Multi-dimensional Feature Fusion; Multimodal Fusion; component: Emotion Recognition; Component: emotion recognition; Integration strategy; Modality Integration Strategies; Modality integration strategy; Recognition methods,,,emotion,No,Yes
scopus,Transforming Psychological Treatment: A Novel Emotion Evaluation Algorithm,"Padmanabhan, S.; Riyat, S.; Singh, V.; Kapoor, P.; Vashisht, N.; Ganapathy, K.",2024,,20,,10.62441/nano-ntp.v20iS2.79,"Emotions play a crucial part in communication because they enable information about an individual's emotional state to be expressed and inferred. This study develops a novel method for psychological treatment by creating a Modified Tunicate Swarm Fused Spatial Deep Convolutional Neural Network (MTSF-SDCNN) for emotion assessment. The study focuses on the creation and deployment of this specific neural network architecture to assess and understand emotional states, using a curated Emotion Evaluation Dataset as a framework. The dataset is pre-processed using Min-Max Normalization to assure reliability and maximize the neural network's learning process. Following that, Term Frequency Inverse Document Frequency (TF-IDF) is used in feature extraction to identify significant trends in emotional data. The MTSF-SDCNN emotion assessment system is designed to handle multimodal data like emotions, voice patterns, and physiological markers, providing a sophisticated and context-aware understanding of emotional states compared to typical emotion detection techniques. Several tests are being undertaken to confirm the suggested algorithm's outcomes, comparing findings with standard models such as Random Forest (RF), Decision Tree (DT), and Logistic Regression (LR). The evaluation criteria include Accuracy (82%), Precision (87%), Recall (93%), and F1-score (91%), which provide an extensive evaluation of the algorithm's capabilities across various dimensions. These findings have implications to extend emotion analysis and provide realistic opportunities for using traditional technology in psychological treatment approaches. © 2024, Collegium Basilea. All rights reserved.",Emotion Evaluation; Modified Tunicate Swarm Fused Spatial Deep Convolutional Neural Network (MTSF-SDCNN); Psychological Treatment,,,emotion,No,Yes
scopus,An Intelligent Emotion Recognition System based on Speech Terminologies using Artificial Intelligence Assisted Learning Scheme,"Kapileswar, N.; Simon, J.; Devi, K.K.; Polasi, P.K.; Vinod, D.N.; Harish, C.",2024,,,,10.1109/ICONSTEM60960.2024.10568813,"From HCI to mental health monitoring, emotion identification from voice signals is an important and vital task. We provide a novel method for reliable emotion identification from audio data called Artificial Intelligence Assisted Learning Scheme (AIALS). AIALS combines the architectures of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). The CREMA-D dataset, which stands for Crowd-sourced Emotional Multimodal Actors Dataset, is used in our work to train and assess the suggested system. The dataset is rich in authentic emotional speech examples. Feature extraction and emotion classification are the two primary components of the AIALS framework that has been suggested. During the feature extraction stage, pertinent acoustic information is retrieved from raw speech signals by computing Mel-frequency Cepstral coefficients (MFCCs). Subsequently, spectrograms are generated to extract spatial features using the CNN architecture. In the emotion classification stage, the extracted features are fed into the hybrid LSTM-CNN model, which effectively learns temporal dependencies and spatial patterns from the input data. Our experimental results demonstrate the effectiveness of the proposed AIALS framework in accurately recognizing emotions from speech signals. By leveraging the complementary strengths of LSTM and CNN architectures, our model achieves an impressive accuracy of 98% on the CREMA-D dataset, outperforming existing state-of-the-art approaches. Furthermore, the proposed system exhibits robustness across different emotional categories, showcasing its generalization capability.  © 2024 IEEE.",Long short-term memory; Emotion Recognition; Speech recognition; Deep learning; Deep Learning; Learning systems; Classification (of information); Features extraction; Convolutional neural networks; Brain; Convolution; Convolutional neural network; Memory architecture; Network architecture; Emotion classification; Extraction; Feature extraction; Speech communication; Emotion identifications; Learning schemes; Long short-term memory  and convolutional neural network; Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN); Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficient; Mel-frequency cepstral coefficients; Mel-frequency Cepstral coefficients (MFCCs),,,emotion,No,Yes
scopus,PSPN: Pseudo-Siamese Pyramid Network for multimodal emotion analysis,"Yin, Y.; Kong, W.; Tang, J.; Li, J.; Babiloni, F.",2024,,,,10.1007/s11571-024-10123-y,"Emotion recognition plays an important role in human life and healthcare. The EEG has been extensively researched as an objective indicator of intense emotions. However, current existing methods lack sufficient analysis of shallow and deep EEG features. In addition, human emotions are complex and variable, making it difficult to comprehensively represent emotions using a single-modal signal. As a signal associated with gaze tracking and eye movement detection, Eye-related signals provide various forms of supplementary information for multimodal emotion analysis. Therefore, we propose a Pseudo-Siamese Pyramid Network (PSPN) for multimodal emotion analysis. The PSPN model employs a Depthwise Separable Convolutional Pyramid (DSCP) to extract and integrate intrinsic emotional features at various levels and scales from EEG signals. Simultaneously, we utilize a fully connected subnetwork to extract the external emotional features from eye-related signals. Finally, we introduce a Pseudo-Siamese network that integrates a flexible cross-modal dual-branch subnetwork to collaboratively utilize EEG emotional features and eye-related behavioral features, achieving consistency and complementarity in multimodal emotion recognition. For evaluation, we conducted experiments on the DEAP and SEED-IV public datasets. The experimental results demonstrate that multimodal fusion significantly improves the accuracy of emotion recognition compared to single-modal approaches. Our PSPN model achieved the best accuracy of 96.02% and 96.45% on the valence and arousal dimensions of the DEAP dataset, and 77.81% on the SEED-IV dataset, respectively. Our code link is: https://github.com/Yinyanyan003/PSPN.git. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.",Emotion recognition; Multimodal; EEG; Depthwise Separable Convolution; Pseudo-Siamese network; Pyramid network,,,emotion,No,Yes
scopus,FINE-GRAINED DISENTANGLED REPRESENTATION LEARNING FOR MULTIMODAL EMOTION RECOGNITION,"Sun, H.; Zhao, S.; Wang, X.; Zeng, W.; Chen, Y.; Qin, Y.",2024,,,,10.1109/ICASSP48485.2024.10447667,"Multimodal emotion recognition (MMER) is an active research field that aims to accurately recognize human emotions by fusing multiple perceptual modalities. However, inherent heterogeneity across modalities introduces distribution gaps and information redundancy, posing significant challenges for MMER. In this paper, we propose a novel fine-grained disentangled representation learning (FDRL) framework to address these challenges. Specifically, we design modality-shared and modality-private encoders to project each modality into modality-shared and modality-private subspaces, respectively. In the shared subspace, we introduce a fine-grained alignment component to learn modality-shared representations, thus capturing modal consistency. Subsequently, we tailor a fine-grained disparity component to constrain the private subspaces, thereby learning modality-private representations and enhancing their diversity. Lastly, we introduce a fine-grained predictor component to ensure that the labels of the output representations from the encoders remain unchanged. Experimental results on the IEMOCAP dataset show that FDRL outperforms the state-of-the-art methods, achieving 78.34% and 79.44% on WAR and UAR, respectively. © 2024 IEEE.",Multimodal emotion recognition; disentangled representation learning; fine-grained alignment; fine-grained disparity; fine-grained predictor,,,emotion,Yes,No
scopus,Hybrid Approaches to Emotion Recognition: A Comprehensive Survey of Audio-Textual Methods and Their Application,"Wewelwala, S.H.; Sumanathilaka, T.G.D.K.",2024,,,,10.1109/ICARC61713.2024.10499740,"This survey study provides a comprehensive analysis of emotion recognition, documenting its progression from conventional techniques to innovative hybrid algorithms that effectively combine textual and audio information. This work distinguishes itself with its thorough examination of the innovative combination of deep learning-based textual sentiment analysis and sophisticated aural signal processing approaches. This unique combination greatly improves the precision of emotion recognition from intricate sources like social media and consumer feedback. The study addresses the difficulties of processing real-time data and reducing bias in varied datasets. It provides new perspectives on the powers and limitations of neural networks and machine learning in this specific scenario. Our research represents a notable advancement in the utilization of these technologies for human-computer interaction, facilitating the creation of digital interfaces that are more empathic and attuned to human emotions. The final statements underscore the pressing requirement for sophisticated multimodal methodologies and the incorporation of developing technology, underscoring our distinctive contribution to the progress of emotion identification systems. This report offers a thorough analysis of the present condition of the subject and outlines a direction for future advancements, highlighting the crucial significance of emotion identification in improving human-computer interaction.  © 2024 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Modal analysis; Deep learning; Deep Learning; Human computer interaction; Multimodal analysis; Audio; Emotion identifications; Comprehensive analysis; Conventional techniques; Emotional recognition; Emotional Recognition; Hybrid Algorithm; Hybrid algorithms; Hybrid approach; Multimodal Analysis; Signal processing,,,emotion,No,No
scopus,Design of an Emotion Care System for the Elderly Based on Precisely Detecting Emotion States,"Dai, W.; Chen, H.; Zhu, L.; Chen, Y.; Chen, M.; Zhang, Y.",2024,,14726 LNCS,,10.1007/978-3-031-61546-7_21,"With the global fertility rate in decline, the issue of an aging population grows ever more pressing, especially in developing nations where many elderly live alone, often deprived of familial emotional support and timely care. Addressing this, our study centers on emotional well-being and introduces a system capable of detecting and regulating the emotions of the elderly promptly. Utilizing sensors to capture facial imagery and vocal information, we employ a multimodal emotion recognition model for real-time analysis, enabling multi-channel emotional regulation. We validate the necessity for an elderly-specific dataset and confirm the reliability of our model’s emotional recognition accuracy. Additionally, we demonstrate the effectiveness of our regulation strategies through empirical research. This study aims to enhance the quality of life for the elderly in their twilight years. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Multimodal recognition; Aging population; Developing nations; emotion regulation; Emotion regulations; Emotional supports; Fertility rates; Population statistics; Pressung; the elderly; The elderly; Well being,,,emotion,No,Yes
scopus,Comparison and Performance Evaluation of Fusion Mechanism for Audio–Video Based Multimodal Emotion Recognition,"Kumar, H.; Martin, A.",2024,,864,,10.1007/978-981-99-8628-6_19,"Artificial emotional intelligence is an emerging research area in artificial intelligence. In artificial intelligence, machine learning and deep learning techniques have provided more efficient and precise results in unimodal emotion recognition, but still, it is lacking with the limited information for feature extraction and does not consider the contextual behaviors and factors during the emotion classification. To overcome these shortcomings, in our proposed work, we focus on two crucial fusion mechanisms: Tensor fusion and low rank fusion that influence the accuracy and computational complexity of multimodal emotion recognition. In this multimodal approach, we are using audio and video modality to evaluate the performance of tensor fusion and low rank fusion mechanism. These fusion mechanisms are an emerging challenge in various applications such as multimodal emotion recognition, sentiment analysis, and computer vision. In our work, we are comparing and evaluating the performance of two fusion mechanisms, the tensor fusion network and low rank fusion on audio–video on IEMOCAP and CMU-MOSI datasets. We evaluate the performance on the following matrix: Mean Absolute Error (MAE), Pearson correlation (Corr), F1 score, and accuracy. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Audio videos; Deep learning; Multimodal emotion recognition; Learning systems; Classification (of information); Multimodal; Tensor fusion; Tensors; Low rank fusion; Artificial emotional intelligence; Correlation methods; Emotional intelligence; Fusion mechanism,,,emotion,No,Yes
scopus,Enhancing Multimodal Sentiment Analysis with Deep Learning Techniques to Foster Emotional Intelligence,"Kousika, N.; Nanda Kumar, N.; Baskaran, E.; Abbenaya, M.S.; Arthika, G.; Ashwanth, M.",2024,,,,10.1109/ICCSP60870.2024.10544097,"Sentiment analysis (SA) is a critical process in understanding the emotions associated with data on social networking sites. Building upon this foundational concept, this paper introduces a sophisticated web application aimed at enabling users to submit diverse forms of content, including text, images, videos, and audio, for hate content assessment. This platform employs BERT CNN and VGG16 CNN algorithms for performing a thorough analysis of provided information in various formats. Emphasizing the identification of emotions in text and the detection of hate speech, the methodology extends to incorporate facial emotion detection for images. Through meticulous preprocessing, feature extraction, and ensemble learning, the proposed solution provides a robust framework for accurately identifying and mitigating hate content across various modalities. Experimental tests employing varied datasets validated the usefulness of the strategy, contributing to the advancement of hate content detection systems and fostering a safer online environment.  © 2024 IEEE.",Natural language processing; Multi-modal; Deep learning; Learning systems; Emotion detection; Sentiment analysis; Language processing; Natural languages; Deep learning for sentiment analyse; Deep learning for sentiment analysis; Hate content assessment; Learning techniques; Online systems,,,emotion,No,No
scopus,Attention Information Fusion Emotion Analysis Model Based on BiLSTM-BERT,"Cheng, Z.-W.; Li, S.-X.",2024,,,,10.1109/ICCECE61317.2024.10504175,"Aiming at the problem that most sentiment analysis methods do not focus on the weight fusion of local context and global context information and the important information in fusion features is not prominent, AIFM-BB (Attention information fusion emotion analysis model based on BiLSTM-BERT) is proposed. The model uses a pre-trained BERT model to process text vectors and a collaborative speech analysis algorithm to process audio vectors, and introduces BiLSTM and Attention mechanisms. The weight of the feature vector is dynamically adjusted by combining the global information and local information of text and audio and the attention information of text and audio modes. The emotion analysis results are obtained through the Softmax layer after the fusion features are self-focused. The experimental results show that the accuracy of the proposed AIFM-BB model on the CMU-MOSI dataset is 85.3%, which is 2.8 percentage points higher than the baseline. In the multi-category emotion classification task, the accuracy of the model is 44.7%, an improvement of 1.1 percentage points compared with baseline. The accuracy is 83.6% on the CMU-MOSEI dataset, an improvement of 1.4 percentage points compared to baseline.  © 2024 IEEE.",attention mechanism; Attention mechanisms; Multi-modal; deep learning; Deep learning; multimodal sentiment analysis; Sentiment analysis; Emotion analysis; Features extraction; Multimodal sentiment analyse; Information fusion; feature extraction; Analysis models; component; Component; Percentage points,,,emotion,No,Yes
scopus,Intelligent Classification of Music Emotions Based on Multi-feature Fusion Algorithm of Feedforward Neural Network,"Guo, M.",2024,,,,10.1109/EDPEE61724.2024.00147,"The defining trait of emotional music is its power to evoke feelings, and this study aims to explore this facet. Following a comparison of diverse musical elements, the research identifies the melody trend as the foremost indicator of emotion. Subsequently, it introduces the concept of harnessing a feedforward neural network for recognizing musical emotions. Additionally, the paper outlines an analysis of the results attained through this methodology. Experimental findings reveal that the feedforward neural network stands out in recognizing musical emotions, exhibiting notable precision. Typically, in these network frameworks, the Chebyshev orthogonal polynomial cluster functions as the activation mechanism for each neuron in the hidden layer, guaranteeing their distinctiveness. The research utilizes a gradient descent learning algorithm to facilitate supervised training for network parameters. Moreover, the study integrates data from multiple sources, such as audio and lyrics, to construct a multimodal dataset. This holistic approach enhances the training process of the music emotion classification model. The outcomes are encouraging: the algorithm attains an average accuracy of 78.37% in categorizing musical emotions, attesting to its effectiveness and viability. © 2024 IEEE.",Classification (of information); Music; Feedforward neural networks; Neural-networks; Emotion intelligence; Forward neural network; Fusion algorithms; Gradient methods; Intelligent classification; Multi-feature fusion; Multi-feature fusion algorithm; Music emotion intelligence classification; Music Emotion Intelligence Classification; Music emotions; Musical emotion; Orthogonal functions; Polynomials,,,emotion,No,Yes
scopus,Automated Assessment of Encouragement and Warmth in Classrooms Leveraging Multimodal Emotional Features and ChatGPT,"Hou, R.; Fütterer, T.; Bühler, B.; Bozkir, E.; Gerjets, P.; Trautwein, U.; Kasneci, E.",2024,,14829 LNAI,,10.1007/978-3-031-64302-6_5,"Classroom observation protocols standardize the assessment of teaching effectiveness and facilitate comprehension of classroom interactions. Whereas these protocols offer teachers specific feedback on their teaching practices, the manual coding by human raters is resource-intensive and often unreliable. This has sparked interest in developing AI-driven, cost-effective methods for automating such holistic coding. Our work explores a multimodal approach to automatically estimating encouragement and warmth in classrooms, a key component of the Global Teaching Insights (GTI) study’s observation protocol. To this end, we employed facial and speech emotion recognition with sentiment analysis to extract interpretable features from video, audio, and transcript data. The prediction task involved both classification and regression methods. Additionally, in light of recent large language models’ remarkable text annotation capabilities, we evaluated ChatGPT’s zero-shot performance on this scoring task based on transcripts. We demonstrated our approach on the GTI dataset, comprising 367 16-min video segments from 92 authentic lesson recordings. The inferences of GPT-4 and the best-trained model yielded correlations of r=.341 and r=.441 with human ratings, respectively. Combining estimates from both models through averaging, an ensemble approach achieved a correlation of r=.513, comparable to human inter-rater reliability. Our model explanation analysis indicated that text sentiment features were the primary contributors to the trained model’s decisions. Moreover, GPT-4 could deliver logical and concrete reasoning as potential teacher guidelines. Our findings provide insights into using multimodal techniques for automated classroom observation, aiming to foster teacher training through frequent and valuable feedback.  © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Semantics; Emotion Recognition; Multi-modal; Speech recognition; Learning systems; Machine-learning; Multimodal machine learning; Zero-shot learning; Computational linguistics; Teachers'; Cost effectiveness; Regression analysis; AI in education; AI in Education; Automated assessment; ChatGPT zero-shot annotation; Classroom observation; Observation protocol; Personnel training; Teaching effectiveness,,,emotion,Yes,Yes
scopus,Intelligent emotion recognition in product design using multimodal physiological signals and machine learning,"Zhang, L.; Hu, F.; Liu, X.; Wang, Y.; Zhang, H.; Liu, Z.; Yu, C.",2024,,,,10.1080/09544828.2024.2362589,"Identifying emotional responses in products is essential for product design and user research. Traditional methods, such as interviews and surveys, for gathering product experience data are time-consuming and resource-intensive, and often fail to capture users’ genuine emotional intentions. This article introduces an intelligent method for accurately identifying user-product emotions using multimodal physiological signals and machine learning techniques. The study involves designing experiments with 63 representative product images, and collecting various physiological signals (eye movement, electrodermal activity, and pulse). Using the K-means algorithm, we establish efficacy-arousal assessment labels to create a product emotion dataset. Preprocessing methods, including time–frequency analysis and wavelet transformation, ensure high-quality signal analysis data. By extracting temporal, frequency, and time–frequency features from multimodal physiological signals, the Relief algorithm determines the optimal feature combination. Finally, the study evaluates the performance of five machine learning classifiers, with results indicating an 81.31% recognition accuracy for the proposed product emotion recognition framework. This framework proves effective for objectively analyzing user emotions in product experiences. © 2024 Informa UK Limited, trading as Taylor & Francis Group.",Machine learning; User research; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; machine learning; Metadata; Eye movements; Machine-learning; Multimodal physiological signal; Design research; Emotional response; K-means clustering; multimodal physiological signals; Product design; Product emotion; Product experience; Quality control,,,emotion,Yes,Yes
scopus,"Multimodal emotion recognition based on the fusion of vision, EEG, ECG, and EMG signals","Bhatlawande, S.; Shilaskar, S.; Pramanik, S.; Sole, S.",2024,,15,,10.32985/ijeces.15.1.5,"– This paper presents a novel approach for emotion recognition (ER) based on Electroencephalogram (EEG), Electromyogram (EMG), Electrocardiogram (ECG), and computer vision. The proposed system includes two different models for physiological signals and facial expressions deployed in a real-time embedded system. A custom dataset for EEG, ECG, EMG, and facial expression was collected from 10 participants using an Affective Video Response System. Time, frequency, and wavelet domain-specific features were extracted and optimized, based on their Visualizations from Exploratory Data Analysis (EDA) and Principal Component Analysis (PCA). Local Binary Patterns (LBP), Local Ternary Patterns (LTP), Histogram of Oriented Gradients (HOG), and Gabor descriptors were used for differentiating facial emotions. Classification models, namely decision tree, random forest, and optimized variants thereof, were trained using these features. The optimized Random Forest model achieved an accuracy of 84%, while the optimized Decision Tree achieved 76% for the physiological signal-based model. The facial emotion recognition (FER) model attained an accuracy of 84.6%, 74.3%, 67%, and 64.5% using K-Nearest Neighbors (KNN), Random Forest, Decision Tree, and XGBoost, respectively. Performance metrics, including Area Under Curve (AUC), F1 score, and Receiver Operating Characteristic Curve (ROC), were computed to evaluate the models. The outcome of both results, i.e., the fusion of bio-signals and facial emotion analysis, is given to a voting classifier to get the final emotion. A comprehensive report is generated using the Generative Pretrained Transformer (GPT) language model based on the resultant emotion, achieving an accuracy of 87.5%. The model was implemented and deployed on a Jetson Nano. The results show its relevance to ER. It has applications in enhancing prosthetic systems and other medical fields such as psychological therapy, rehabilitation, assisting individuals with neurological disorders, mental health monitoring, and biometric security. © 2024, J.J. Strossmayer University of Osijek, Faculty of Electrical Engineering, Computer Science and Information Technology. All rights reserved.",Feature Fusion; Analysis of Mental Health; Computer Vision; Emotion Recognition (ER); Machine Learning (ML); Physiological Signals,,,emotion,No,Yes
scopus,Emotion State Detection Using EEG Signals—A Machine Learning Perspective,"Naidu, P.G.; Adhitya, C.M.J.; Harshita, S.; Bashpika, T.; Manikumar, V.S.S.S.R.; Muthulakshmi, M.",2024,,947 LNNS,,10.1007/978-981-97-1326-4_38,"Feelings are the foundation of the human experience which influences choices, interpersonal interactions, and overall well-being. Emotional intelligence is necessary for mental health, emotional growth, and effective communication. In the modern world, the study of human emotions is becoming more and more popular. Because the signals produced by the brain are unstable, developing electronic models to identify emotional states from EEG data is challenging. In this study, we propose a deep learning framework-based efficient technique for EEG data analysis developed and collected from the DEAP dataset. We also use feature selection approaches to identify characteristics relevant to the cognitive search task, based on discoveries from neuroscientific research. The multimodal DEAP dataset consists of audiovisual recordings, physiological signals, and self-reported emotional assessments obtained from exposure to a range of multimedia stimuli includes a huge quantity of data. The data set was written by the scientific community and includes the EEG recordings of 32 participants. Each subject's data was obtained from 32 EEG sensors, while the remaining 8 sensors were obtained from pupils and electromyography. The suggested study takes into account “Support Vector Machines (SVMs), Random Forests, Convolutional Neural Networks (CNNs), and Long Short-Term Memory (LSTM) networks” in addition to a range of supervised and unsupervised learning techniques. The advantages and disadvantages of each approach are explored, with a focus on how crucial it is to select the best approach depending on the type of EEG data and the particular emotional awareness task at hand. Our established model effectively categorized emotions into two main groups: arousal (the strength of the emotion) and valence (the pleasantness of the emotion). Surprisingly, our model was able to identify between these high and low valence, as well as high and low arousal states, with an average accuracy of over 92%. In addition, our model was able to classify emotions into four different groups: peak valence—slight arousal (PVSA), peak valence—peak arousal (PVPA), minor valence—slight arousal (SVSA), and slight valence—peak arousal (SVPA). The accuracy in this more thorough categorization was still very good at 84%. This degree of precision demonstrates the model's ability to identify and discriminate between complex emotional states, highlighting its potential in a range of emotion detection applications. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Long short-term memory; Emotional state; Behavioral research; Learning systems; Brain; Support vector machines; Machine-learning; Feature selection; Features selection; Feature extraction; Memory network; EEG signals; Neurophysiology; State Detection; Cognitive function; Cognitive functions; Emotion perception; Forestry; Long term memory; Long-term memory (long short-term memory) network; Long-term memory (LSTM) networks,,,emotion,No,Yes
scopus,Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition,"Nguyen, C.-V.T.; Nguyen, C.-B.; Ha, Q.-T.; Le, D.-T.",2024,,,,,"Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models. We release the code for MultiDAG+CL and experiments: https://github.com/vanntc711/MultiDAG-CL. © 2024 ELRA Language Resource Association: CC BY-NC 4.0.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Directed graphs; Affective Computing; Data handling; Multimodal Emotion Recognition; Language processing; Natural languages; Acyclic graphs; Curricula; Curriculum learning; Curriculum Learning; Data imbalance; Directed acyclic graph; Directed Acyclic Graph; Textual features,,,emotion,No,Yes
scopus,Acoustic and visual geometry descriptor for multi-modal emotion recognition fromvideos,"Ramyasree, K.; Kumar, C.S.",2024,,33,,10.11591/ijeecs.v33.i2.pp960-970,"Recognizing human emotions simultaneously from multiple data modalities (e.g., face, and speech) has drawn significant research interest, and numerous research contributions have been investigated in the affective computing community. However, most methods concentrate less on facial alignment and keyframe selection for audio-visual input. Hence, this paper proposed a new audio-visual descriptor, mainly concentrating on describing the emotion through only a few frames. For this purpose, we proposed a new self-similarity distance matrix (SSDM), which computes the spatial, and temporal distances through landmark points on the facial image. The audio signal is described through an asset of composite features, including statistical features, spectral features, formant frequencies, and energies. A support vector machine (SVM) algorithm is employed to classify both models, and the final results are fused to predict the emotion. Surrey audiovisual expressed emotion (SAVEE) and Ryerson multimedia research lab (RML) datasets are utilized for experimental validation, and the proposed method has shown significant improvement from the state of art methods. © 2024 Institute of Advanced Engineering and Science. All rights reserved.",Acoustic feature Audio and video Decision level fusion Geometric features Key frames Multimodal emotion recognition,,,emotion,No,No
scopus,A Cross-Modal Adaptive Masked Autoencoder for Decoding Emotions With Multimodal Data,"Cheng, C.; Liu, W.; Zhang, Y.; Feng, L.; Jia, Z.",2024,,,,10.1109/TCSS.2024.3415613,"Multimodal emotion recognition (MER) has recently gained much attention since it can leverage information over multiple modalities. However, in real life, we often encounter the problem of missing modalities, as well as modeling the heterogeneity and correlation among multimodal data are challenges. To this end, we propose a unified model called cross-modal adaptive masked autoencoder (CMA-MAE) for incomplete multimodal learning. Our CMA-MAE model comprises a cross-modal adaptive fusion encoder (CMAFE) and a multiview adaptive encoder (MVAE) to capture and fuse the heterogeneity and correlation among multimodal features. Additionally, we design a convolutional decoder that progressive upsampling and fusion with the modality-invariant features to generate robust emotional features from partially observable data. To effectively utilize both data with complete and incomplete modalities for feature learning, we adopt an end-to-end approach that simultaneously optimizes classification and reconstruction tasks. Extensive testing on the DEAP and SEED-IV datasets is conducted to assess our model, with the findings demonstrating that our CMA-MAE model outperforms current leading approaches in both incomplete and complete multimodal learning scenarios. IEEE",Emotion Recognition; Speech recognition; Multi-modal data; Multimodal emotion recognition; Learning systems; Signal encoding; Correlation; Auto encoders; Cross-modal; Multiple modalities; Missing modality; Multi-modal learning; multimodal emotion recognition (MER); Autoencoder; correlation; Decoding; heterogeneity; Heterogeneity; missing modalities,,,emotion,No,No
scopus,Comprehensive Multisource Learning Network for Cross-Subject Multimodal Emotion Recognition,"Chen, C.; Li, Z.; Kou, K.I.; Du, J.; Li, C.; Wang, H.; Vong, C.",2024,,,,10.1109/TETCI.2024.3406422,"Electroencephalography (EEG) signals and eye movement signals, which represent internal physiological responses and external subconscious behaviors, respectively, have been shown to be reliable indicators for recognizing emotions. However, integrating these two modalities across multiple subjects presents several challenges: 1) designing a robust consistency metric that balances the consistency and divergences between heterogeneous modalities across multiple subjects; 2) simultaneously considering intra-modality and inter-modality information across multiple subjects; and 3) overcoming individual differences among multiple subjects and generating subject-invariant representations of the multimodal fused features. To address these challenges associated with multisource data (i.e., multiple modalities and subjects), we propose a novel comprehensive multisource learning network (CMSLNet) for cross-subject multimodal emotion recognition. Specifically, an instance-level adaptive robust consistency metric is first designed to better align the information between EEG signals and eye movement signals, identifying their consistency and divergences across various emotions. Subsequently, an attentive low-rank multimodal fusion (Att-LMF) method is developed to account for individual differences and dynamically learn intra-modality and inter-modality information, resulting in highly discriminative fused features. Finally, domain generalization is utilized to extract subject-invariant representations of the fused features, thus adapting to new subjects and enhancing the model&#x0027;s generalization. Through these elaborate designs, CMSLNet effectively incorporates the information from multisource data, thus significantly improving the accuracy and reliability of emotion recognition. Extensive experiments on two public datasets demonstrate the superior performance of CMSLNet. CMSLNet achieves high accuracies of 83.15&#x0025; on the SEED-IV dataset and 87.32&#x0025; on the SEED-V dataset, surpassing the state-of-the-art methods by 3.62&#x0025; and 4.60&#x0025;, respectively. IEEE",Measurement; Emotion Recognition; Speech recognition; Emotion recognition; Physiology; Multi-modal fusion; Multimodal emotion recognition; Learning systems; Correlation; multimodal emotion recognition; Features extraction; Electroencephalography; Biomedical signal processing; Electrophysiology; Physiological models; Brain modeling; Eye movements; Feature extraction; cross-subject; Cross-subject; domain generalization; Domain generalization; Generalisation; low-rank multimodal fusion; Low-rank multimodal fusion; Multi-Sources; Multisource learning,,,emotion,No,Yes
scopus,Integrating Representation Subspace Mapping with Unimodal Auxiliary Loss for Attention-based Multimodal Emotion Recognition,"Du, X.; Zhang, X.; Wang, D.; Xu, Y.; Wu, Z.; Zhang, S.; Zhao, X.; Yu, J.; Lou, L.",2024,,,,,"Multimodal emotion recognition (MER) aims to identify emotions by utilizing affective information from multiple modalities. Due to the inherent disparities among these heterogeneous modalities, there is a large modality gap in their representations, leading to the challenge of fusing multiple modalities for MER. To address this issue, this work proposes a novel attention-based MER framework associated with audio and text by integrating representation subspace mapping with unimodal auxiliary loss for enhancing multimodal fusion capabilities. Initially, a representation subspace mapping module is proposed to map each modality into two distinct subspaces. One is modality-public, enabling the acquisition of common representations and reducing the discrepancies across modalities. The other is modality-unique, retaining the unique characteristics of each modality while eliminating redundant inter-modal attributes. Then, a cross-modality attention is leveraged to bridge the modality gap in unique representations and facilitate modality adaptation. Additionally, our method designs an unimodal auxiliary loss to remove the redundancy unrelated to emotion classification, resulting in robust and meaningful representations for MER. Comprehensive experiments are conducted on the IEMOCAP and MSP-Improv datasets, and experiment results show that our method achieves superior performance to state-of-the-art MER methods. © 2024 ELRA Language Resource Association: CC BY-NC 4.0.",Emotion Recognition; Speech recognition; Multi-modal fusion; Multimodal emotion recognition; Unimodal; Emotion classification; Multiple modalities; Cross modality; cross-modality attention; Cross-modality attention; fusion; Mapping; Mapping modules; representation subspace mapping; Representation subspace mapping; unimodal auxiliary loss; Unimodal auxiliary loss,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition Using Feature Fusion: An LLM-based Approach,"Om Kumar, C.U.; Gowtham, N.; Zakariah, M.; Almazyad, A.",2024,,,,10.1109/ACCESS.2024.3425953,"Multimodal emotion recognition is a developing field that analyzes emotions through various channels, mainly audio, video, and text. However, existing state-of-the-art systems focus on two to three modalities at the most, utilize traditional techniques, fail to consider emotional interplay, lack the scope to add more modalities, and aren&#x2019;t efficient in predicting emotions accurately. This research proposes a novel approach using rule-based systems to convert non-verbal cues to text, inspired by a limited prior attempt, which lacked proper benchmarking. It achieves efficient multimodal emotion recognition by utilizing distilRoBERTa, a large language model, fine-tuned with a combined textual representation of audio (such as loudness, spectral flux, MFCCs, pitch stability, and emphasis) and visual features (action units) extracted from videos. This approach is evaluated using datasets RAVDESS and BAUM-1. It achieves high accuracy (93.18% in RAVDESS and 93.69% in BAUM-1) on both datasets, performing on par with the SOTA (State-of-the-Art) systems, if not slightly better. Furthermore, the research highlights the potential for incorporating additional modalities by transforming them into text using rule-based systems and utilizing them to refine further pre-trained large language models giving rise to a more comprehensive approach to emotion recognition. Authors",Emotion Recognition; Speech recognition; Emotion recognition; Character recognition; Visual languages; Features extraction; Computational linguistics; Language model; Large language model; Early fusion; Early fusion strategies; Early fusion strategy; Feature extraction with rule-based system; Feature extraction with Rule-based systems; Fusion strategies; Knowledge based systems; Large Language Models; Multimodal models; Rules based systems,,,emotion,No,Yes
scopus,MAGDRA: A Multi-modal Attention Graph Network with Dynamic Routing-By-Agreement for multi-label emotion recognition,"Li, X.; Liu, J.; Xie, Y.; Gong, P.; Zhang, X.; He, H.",2024,,283,,10.1016/j.knosys.2023.111126,"Multimodal multi-label emotion recognition (MMER) is a vital yet challenging task in affective computing. Despite significant progress in previous works, there are three limitations: (i) Limited applicability in real-world scenarios due to the assumption of pre-alignment of multimodal data. (ii) Inadequate utilization of long-term dependencies across modalities. (iii) Insufficient exploitation of correlations among emotion labels. In this paper, to overcome these limitations, a Multi-modal Attention Graph model with Dynamic Routing-by-Agreement (MAGDRA) is proposed. In MAGDRA, the fusion of multi-modal data can be performed without pre-alignment via pseudo-alignment algorithm (PAA). Furthermore, an Expectation-maximized Cross-modal Temporal (ECT) fusion approach is presented to effectively learn the cross-modal interactions and long-term dependencies among visual, audio and textual data. Moreover, to conquer the underconsideration of the correlation among multiple labels, a Reinforced Multi-Label Emotion Detection (RMLED) module is carefully designed. Extensive experiments are conducted on three public benchmark datasets, IEMOCAP, CMU-MOSI, and CMU-MOSEI, the results demonstrate that MAGDRA outperforms the existing methods and has the potential to generalize to multi-modal multi-label tasks in other domains. © 2023",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Multi-modal data; Data fusion; Alignment; Multi-modal data fusion; Dynamic routing; Dynamic routing-by-agreement; Graph attention network; Graph attention networks; Graph model; Multi-labels; Network routing; Pre-alignment,,,emotion,Yes,Yes
scopus,RobinNet: A Multimodal Speech Emotion Recognition System With Speaker Recognition for Social Interactions,"Khurana, Y.; Gupta, S.; Sathyaraj, R.; Raja, S.P.",2024,,11,,10.1109/TCSS.2022.3228649,"It is essential to understand the underlying emotions that are imparted through speech in order to study social communications as well as to generate seamless human-computer interactions. Speech emotion recognition (SER) is a considerably challenging task due to the lack of sufficient data and the complex interdependence of phrases with the context and emotion they imply. This article presents RobinNet: a RoBERTa- and Inception-ResNet-V2-based novel multimodal network for SER. The model employs transfer learning to build two unimodal systems for text and audio features and then incorporates them into a single classifier through Intermediate Fusion. This work has been created after carefully analyzing the performance of various top-performing unimodal systems and then utilizing a fine-tuned RoBERTa-based model to represent the textual features. Furthermore, we utilize an Inception-ResNetV2 pretrained network for Speaker Identification and employ transfer learning to train it for the task of emotion recognition through speech using spectrogram augmentation. The proposed multimodal system combines the two modalities through intermediate fusion and gives out a weighted accuracy (WA) of 72.8% when evaluated against the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results reveal that the proposed multimodal system outperforms state-of-the-art (SOTA) solutions on the benchmark datasets IEMOCAP, multimodal emotion lines dataset (MELD), and CMU-MOSEI. The proposed model utilizes intermediate fusion unlike any of its predecessors that perform late fusion after significant independent processing, thereby improving the overall artificial multimodal representations. © 2014 IEEE.",Emotion Recognition; Speaker recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Human computer interaction; Job analysis; Task analysis; Features extraction; Computational modelling; Audio systems; Transfer learning; Feature extraction; Bit-error rate; multimodal emotion recognition (MER); RoBERTa; Inception-resnetv2; Inception-ResNetV2; intermediate fusion; Intermediate fusion; speaker recognition,,,emotion,No,Yes
scopus,FUNCTIONAL EMOTION TRANSFORMER FOR EEG-ASSISTED CROSS-MODAL EMOTION RECOGNITION,"Jiang, W.-B.; Li, Z.; Zheng, W.-L.; Lu, B.-L.",2024,,,,10.1109/ICASSP48485.2024.10446937,"Multimodal emotion recognition based on electroencephalography (EEG) and eye movements has attracted increasing attention due to their high performance and complementary properties. However, there are two challenges that hinder its practical applications: the inconvenient EEG data collection and high-cost data annotation. In contrast, eye movements are convenient to obtain and process in real scenarios. To combine high performance of EEG and easy setups of eye tracking, we propose a novel EEG-assisted Contrastive Learning Framework with a Functional Emotion Transformer (ECO-FET) for cross-modal emotion recognition. ECO-FET leverages both the functional brain connectivity and the spectral-spatial-temporal domain of EEG signals simultaneously, which dramatically benefit the learning of eye movements. The whole process consists of three phases: pre-training, test, and fine-tuning. ECO-FET exploits the complementary information provided by multiple modalities during pre-training in order to improve the performance of unimodal models. In the pre-training phase, unlabeled EEG and eye movement data are fed into the model to contrastively learn the emotional latent representations between the two modalities, while in the test phase, eye movements and few labeled EEG samples are used to predict different emotions. Experimental results on three public datasets demonstrate that ECO-FET surpasses the state-of-the-art dramatically. © 2024 IEEE.",cross-modal emotion recognition; EEG; eye movement,,,emotion,Yes,Yes
scopus,Emotional Intelligence-Based Music Recommendation System Using Hybrid Deep Neural Network,"Panigrahi, B.S.; Hannah Joyce, K.J.; Punna, T.; Varala, B.K.; Madhavi, B.K.",2024,,,,10.1109/IC-CGU58078.2024.10530669,"This research focuses on advancing emotion recognition techniques, particularly in predicting arousal and valence from multi-channel signals. This study aims to advance techniques in emotion recognition, with a specific focus on the prediction of arousal and valence from multi-channel signals. The resulting emotional states serve as valuable input, seamlessly integrated into collaborative or content-based recommendation engines to enrich their capabilities with nuanced emotional information. The research contributes significantly to the broader field of affective computing, striving to create more sophisticated and emotionally intelligent systems that effectively cater to user preferences and needs. To achieve this, we propose a hybrid deep neural network architecture that combines Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Feedforward Neural Networks to learn intricate patterns of emotion. Emotions, being affective states reflecting an individual's response to mental stimuli, play a central role in this investigation. The primary objective is facilitating the transition towards neutral or positive emotional states, such as Joy and Trust, particularly in patients. For the identification of targeted emotions, we employ a Convoluted Neural Network (CNN) model, trained on meticulously curated datasets containing annotated emotion-labeled audio clips. These clips transform Mel Frequency Spectrograms (MFS) for effective classification based on the specific emotions they represent.  © 2024 IEEE.",Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Learning systems; Affective Computing; Music; Electrophysiology; Physiological models; Convolution; Machine-learning; Recurrent neural networks; Network architecture; Forecasting; Recommender systems; Feedforward neural networks; Galvanic skin response; Intelligent systems; Arousal and valence prediction; Arousal and Valence Prediction; Emotion-aware systems; Emotion-aware technology; Emotion-Aware Technology; Galvanic skin response analyse; Galvanic Skin Response Analysis; Machine learning in emotion-aware system; Machine Learning in Emotion-aware Systems; Multimodal emotion sensing; Multimodal Emotion Sensing; Photo plethysmography; Photo plethysmography for emotion recognition; Photo Plethysmography for Emotion Recognition; Physiological signal processing; Physiological Signal Processing; Recommender Systems; Response analysis; User Modeling; User Modelling,,,emotion,No,No
scopus,Emotional Resonance in Brainwaves: EEG based Classification of Emotional Dynamics,"Manishaa, K.; Sridevi, C.; Kiran, B.; Ganesh Roy, M.T.",2024,,,,10.1109/ICBSII61384.2024.10564040,"This project focuses on combining various machine learning algorithms to classify emotions based on electroencephalogram (EEG) data. In the fields of affective computing, human-computer interface, and healthcare, emotion recognition is significant. The DREAMER (Database for Emotional Analysis in Music Videos) and GAMEEMO datasets, both of which include EEG signals captured during particular stimuli are used in the study. The two datasets are compared at the initial phase of the project in order to figure out which is the most appropriate for additional investigation. The study involves feature extraction, preprocessing, artifact identification, and dataset comparison analysis after dataset selection. Using the selected dataset, several machine learning techniques are used for emotion classification, which include Decision Tree, Random Forest, AdaBoost, Naïve Bayes, and Linear SVM. The results indicate AdaBoost is effective in classifying emotions with the maximum accuracy of 91.7%. Additionally, Adaboost has a F1 score of 94.1% and precision of 88% which tends to be the highest among other algorithms used. Various performance metrics such as F1 Score, Sensitivity, Specificity, Recall and ROC curve are determined for these algorithms, which classify emotions into stress and non-stress classes. Further studies include exploring multimodal approaches and transfer learning to enhance model performance and accurately predict emotions. © 2024 IEEE.",Emotion Recognition; Emotion; Learning systems; Affective Computing; Classification (of information); machine learning; Electroencephalography; F1 scores; Support vector machines; Machine-learning; Feature extraction; stress; Decision trees; Adaptive boosting; Brain wave; DREAMER; GAMEEMO; Human computer interfaces; Machine learning algorithms; non-stress; Non-stress,,,emotion,No,Yes
scopus,Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation,"Meng, T.; Zhang, F.; Shou, Y.; Shao, H.; Ai, W.; Li, K.",2024,,,,10.1109/TASLP.2024.3434495,"Since Multimodal Emotion Recognition in Conversation (MERC) can be applied to public opinion monitoring, intelligent dialogue robots, and other fields, it has received extensive research attention in recent years. Unlike traditional unimodal emotion recognition, MERC can fuse complementary semantic information between multiple modalities (e.g., text, audio, and vision) to improve emotion recognition. However, previous work ignored the inter-modal alignment process and the intra-modal noise information before multimodal fusion but directly fuses multimodal features, which will hinder the model for representation learning. In this study, we have developed a novel approach called Masked Graph Learning with Recursive Alignment (MGLRA) to tackle this problem, which uses a recurrent iterative module with memory to align multimodal features, and then uses the masked GCN for multimodal feature fusion. First, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Second, we build a recurrent iteration module with a memory function, which can use communication between different modalities to eliminate the gap between modalities and achieve the preliminary alignment of features between modalities. Then, a cross-modal multi-head attention mechanism is introduced to achieve feature alignment between modalities and construct a masked GCN for multimodal feature fusion, which can perform random mask reconstruction on the nodes in the graph to obtain better node feature representation. Finally, we utilize a multilayer perceptron (MLP) for emotion recognition. Extensive experiments on two benchmark datasets (i.e., IEMOCAP and MELD) demonstrate that MGLRA outperforms state-of-the-art methods. Our code is publicly available at https:&#x002F;&#x002F;github.com&#x002F;FuchenZhang&#x002F;MGLRA. IEEE",Long short-term memory; Semantics; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Task analysis; multimodal emotion recognition; Features extraction; multimodal fusion; Social aspects; Feature extraction; Intelligent robots; Fuses; Graph representation; Graph representation learning; Information filtering; Iterative methods; Noise; recurrent alignment; Recurrent alignment,,,emotion,No,No
scopus,End-to-end multimodal emotion recognition based on facial expressions and remote photoplethysmography signals,"Li, J.; Peng, J.",2024,,,,10.1109/JBHI.2024.3430310,"Emotion is a complex physiological phenomenon, and a single modality may be insufficient for accurately determining human emotional states. This paper proposes an end-to-end multimodal emotion recognition method based on facial expressions and non-contact physiological signals. Facial expression features and remote photoplethysmography (rPPG) signals are extracted from facial video data, and a transformer-based cross-modal attention mechanism (TCMA) is used to learn the correlation between the two modalities. The results show that the accuracy of emotion recognition can be slightly improved by combining facial expressions with accurate rPPG signals. The performance is further improved with the use of TCMA, for which the binary classification accuracy of valence and arousal is 91.11&#x0025; and 90.00&#x0025;, respectively. Additionally, when experiments are conducted using the whole dataset, an increased accuracy of 7.31&#x0025; and 4.23&#x0025; for the binary classification of valence and arousal, and an improved accuracy of 5.36&#x0025; for the four classifications of valence-arousal are achieved when TCMA is used in modal fusion, compared to using only facial expression modality, which fully demonstrates the effectiveness and robustness of TCMA. This method makes it possible to realize multimodal emotion recognition of facial expressions and contactless physiological signals in reality. IEEE",Remote photoplethysmography; convolutional neural network; emotion; facial expression; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Face recognition; Physiology; adult; diagnosis; female; human; male; videorecording; Classification (of information); multimodal; Features extraction; controlled study; Convolutional neural networks; Biomedical signal processing; Cross-modal; Convolutional neural network; article; residual neural network; binary classification; photoelectric plethysmography; Photoplethysmography; Accuracy; arousal; facial recognition; feature extraction; Feature extraction; Cross-modal attention; Neural networks; Neural-networks; cross-modal attention; crossmodal attention; remote photoplethysmography; Residual neural network; Residual neural networks,,,emotion,No,Yes
scopus,IndEmoVis: A Multimodal Repository for In-Depth Emotion Analysis in Conversational Contexts,"Karani, R.; Desai, S.",2024,,233,,10.1016/j.procs.2024.03.200,"Emotion recognition holds significant importance in human communication, benefiting various domains like human-computer interaction, affective computing, and social robotics. Recent interest lies in exploiting multimodal data, encompassing audio, visual, and other cues, to enhance emotion recognition systems. However, most available datasets predominantly focus on Western cultures, overlooking the diverse emotional expressions in regions like India. Moreover, existing datasets often neglect complex emotions like sympathy and awe. To address these limitations, we introduce ""IndEMoVis,"" a novel multimodal dataset of Indian emotions. It comprises 122 recorded audio visual responses during conversations between pairs of individuals. The dataset includes 61 participants, consisting of 25 females and 36 males aged 18 to 21, primarily from Maharashtra and Gujarat states in India. It encompasses nine emotions: Neutral, Happiness, Sadness, Surprise, Disgust, Anger, Fear, Awe, and Sympathy. The annotation process involves a three-step procedure, ensuring accurate emotion labeling. Additionally, annotations are provided for intensity and confidence levels. IndEMoVis dataset aims to support the research community in affective computing by improving conversation abilities, analyzing emotional intelligence, and evaluating responses in debates. Its cultural relevance and inclusion of complex emotions offer valuable insights into emotion recognition for diverse contexts. © 2024 The Authors. Published by Elsevier B.V.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Affective Computing; Human computer interaction; Human robot interaction; Emotion analysis; Audio systems; Dataset; Multi-modal dataset; Audio-visual; Multimodal Dataset; Audio visual input; Audio Visual Input; Complex emotions; Indian emotion; Indian Emotions,,,emotion,Yes,No
scopus,Enhanced speech emotion recognition using averaged valence arousal dominance mapping and deep neural networks,"Rizhinashvili, D.; Sham, A.H.; Anbarjafari, G.",2024,,,,10.1007/s11760-024-03406-8,"This study delves into advancements in speech emotion recognition (SER) by establishing a novel approach for emotion mapping and prediction using the Valence-Arousal-Dominance (VAD) model. Central to this research is the creation of reliable emotion-to-VAD mappings, achieved by averaging outcomes from multiple pre-trained networks applied to the RAVDESS dataset. This approach adeptly resolves prior inconsistencies in emotion-to-VAD mappings and establishes a dependable framework for SER. The study also introduces a refined SER model, integrating the pre-trained Wave2Vec 2.0 with Long Short-Term Memory (LSTM) networks and linear layers, culminating in an output layer representing valence, arousal, and dominance. Notably, this model exhibits commendable accuracy across various datasets, such as RAVDESS, EMO-DB, CREMA-D, and TESS, thereby showcasing its robustness and adaptability, an improvement over earlier models susceptible to dataset-specific overfitting. The research further unveils a comprehensive speech analysis application, adept at denoising, segmenting, and profiling emotions in speech segments. This application features interactive emotion tracking and sentiment reports, illustrating its practicality in diverse applications. The study recognizes ongoing challenges in SER, especially in managing the subjective nature of emotion perception and integrating multimodal data. Although the research marks a progression in SER technology, it underscores the need for continuous research and careful consideration of ethical aspects in deploying such technologies. This work contributes to the SER domain by introducing a dependable method for emotion mapping, a robust model for emotion recognition, and a user-friendly application for practical implementations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",Long short-term memory; Speech emotion recognition; Emotion Recognition; Deep neural networks; Behavioral research; Multi-modal data; Brain; Recognition models; LSTM; Memory network; Speech analysis; Mapping; De-noising; Diverse applications; Network layers; Output layer; Overfitting; Speech segments; Valence arousal dominance,,,emotion,No,Yes
scopus,FUSING MODALITY-SPECIFIC REPRESENTATIONS AND DECISIONS FOR MULTIMODAL EMOTION RECOGNITION,"Ruan, Y.-P.; Han, S.; Li, T.; Wu, Y.",2024,,,,10.1109/ICASSP48485.2024.10447035,"Multimodal emotion recognition (MER) is important for building humanoid chatbots and has gained increasing attention in recent years. Existing studies have proven that extracting better modality-specific representations, which keep both commonality and individuality information of different modalities, is important for the MER task. However, all these works are restricted in making final predictions based on fusing modality-specific representations, and the effectiveness of the modality-specific decisions has not been studied. In this paper, we propose for the first time to fuse both the modality-specific representations and decisions for the MER task and design a bi-channel fusing network (BCFN). Specifically, a BCFN model first extracts and mixes the modality-specific representations and decisions in two convolutional blocks respectively, and then fuses the two joint multimodal features for the final decision. Extensive experiments are conducted on two MER benchmark datasets with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of our proposed BCFN model and confirm the effectiveness of incorporating modality-specific decisions for the MER task. © 2024 IEEE.",emotion recognition; multimodal; decision; representation,,,emotion,No,No
scopus,Vision-Enabled Large Language and Deep Learning Models for Image-Based Emotion Recognition,"Nadeem, M.; Sohail, S.S.; Javed, L.; Anwer, F.; Saudagar, A.K.J.; Muhammad, K.",2024,,,,10.1007/s12559-024-10281-5,"The significant advancements in the capabilities, reasoning, and efficiency of artificial intelligence (AI)-based tools and systems are evident. Some noteworthy examples of such tools include generative AI-based large language models (LLMs) such as generative pretrained transformer 3.5 (GPT 3.5), generative pretrained transformer 4 (GPT-4), and Bard. LLMs are versatile and effective for various tasks such as composing poetry, writing codes, generating essays, and solving puzzles. Thus far, LLMs can only effectively process text-based input. However, recent advancements have enabled them to handle multimodal inputs, such as text, images, and audio, making them highly general-purpose tools. LLMs have achieved decent performance in pattern recognition tasks (such as classification), therefore, there is a curiosity about whether general-purpose LLMs can perform comparable or even superior to specialized deep learning models (DLMs) trained specifically for a given task. In this study, we compared the performances of fine-tuned DLMs with those of general-purpose LLMs for image-based emotion recognition. We trained DLMs, namely, a convolutional neural network (CNN) (two CNN models were used: CNN1 and CNN2), ResNet50, and VGG-16 models, using an image dataset for emotion recognition, and then tested their performance on another dataset. Subsequently, we subjected the same testing dataset to two vision-enabled LLMs (LLaVa and GPT-4). The CNN2 was found to be the superior model with an accuracy of 62% while VGG16 produced the lowest accuracy with 31%. In the category of LLMs, GPT-4 performed the best, with an accuracy of 55.81%. LLava LLM had a higher accuracy than CNN1 and VGG16 models. The other performance metrics such as precision, recall, and F1-score followed similar trends. However, GPT-4 performed the best with small datasets. The poor results observed in LLMs can be attributed to their general-purpose nature, which, despite extensive pretraining, may not fully capture the features required for specific tasks like emotion recognition in images as effectively as models fine-tuned for those tasks. The LLMs did not surpass specialized models but achieved comparable performance, making them a viable option for specific tasks without additional training. In addition, LLMs can be considered a good alternative when the available dataset is small. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Emotion Recognition; Performance; Speech recognition; Emotion recognition; Deep learning; Learning systems; Convolutional neural networks; Convolutional neural network; Learning models; Statistical tests; Computational linguistics; Language model; Large language model; ChatGPT; Generative artificial intelligence; Image-based; Large language models,,,emotion,No,Yes
scopus,MMRBN: RULE-BASED NETWORK FOR MULTIMODAL EMOTION RECOGNITION,"Chen, X.",2024,,,,10.1109/ICASSP48485.2024.10447930,"Human emotion is usually expressed in multiple modalities, like audio and text. Multimodal methods can boost Emotion Recognition. However, the relationship between audio and text, and their roles in emotion expression have not been fully studied, and hence hinder Multimodal Emotion Recognition (MER). In this work, taking into consideration of the above two things, we propose two rules for MER, which are Rule 1: The audio module should be more expressive than the text module, and Rule 2: The single-modality emotional representation should be dynamically fused into the multimodal emotion representation. Following these two rules, we design the corresponding rule-based multimodal network (MMRBN). Experiment result on the public dataset demonstrates the effectiveness of our proposed rules and MMRBN. © 2024 IEEE.",Emotion Recognition; Multimodal Emotion Recognition; Attention Neural Network,,,emotion,No,Yes
scopus,Incongruity-Aware Cross-Modal Attention for Audio-Visual Fusion in Dimensional Emotion Recognition,"Praveen, R.G.; Alam, J.",2024,,,,10.1109/JSTSP.2024.3422823,"Multimodal emotion recognition has immense potential for the comprehensive assessment of human emotions, utilizing multiple modalities that often exhibit complementary relationships. In video-based emotion recognition, audio and visual modalities have emerged as prominent contact-free channels, widely explored in existing literature. Current approaches typically employ cross-modal attention mechanisms between audio and visual modalities, assuming a constant state of complementarity. However, this assumption may not always hold true, as non-complementary relationships can also manifest, undermining the efficacy of cross-modal feature integration and thereby diminishing the quality of audio-visual feature representations. To tackle this problem, we introduce a novel Incongruity-Aware Cross-Attention (IACA) model, capable of harnessing the benefits of robust complementary relationships while efficiently managing non-complementary scenarios. Specifically, our approach incorporates a two-stage gating mechanism designed to adaptively select semantic features, thereby effectively capturing the inter-modal associations. Additionally, the proposed model demonstrates an ability to mitigate the adverse effects of severely corrupted or missing modalities. We rigorously evaluate the performance of the proposed model through extensive experiments conducted on the challenging RECOLA and Aff-Wild2 datasets. The results underscore the efficacy of our approach, as it outperforms state-of-the-art methods by adeptly capturing inter-modal relationships and minimizing the influence of missing or heavily corrupted modalities. Furthermore, we show that the proposed model is compatible with various cross-modal attention variants, consistently improving performance on both datasets. IEEE",Semantics; Computational modeling; Emotion Recognition; Speech recognition; Emotion recognition; Cross-attention; Features extraction; Computational modelling; Audio acoustics; Audio-visual fusion; Feature extraction; Noise measurement; Adaptation models; Predictive models; Audio-Visual Fusion; Complementary relationship; Cross-Attention; Noise measurements; Non-complementary relationship; Non-complementary relationships; Visualization,,,emotion,No,Yes
scopus,IMPROVING MULTI-MODAL EMOTION RECOGNITION USING ENTROPY-BASED FUSION AND PRUNING-BASED NETWORK ARCHITECTURE OPTIMIZATION,"Wang, H.; Du, J.; Dai, Y.; Lee, C.-H.; Ren, Y.; Liu, Y.",2024,,,,10.1109/ICASSP48485.2024.10447231,"In this study, we aim to improve our recent hierarchical information fusion system for multi-modal emotion recognition challenge (MER 2023) in both efficiency and performance. Specifically, we extract robust acoustic and visual representations from pre-trained models and fuse them together in different structures. Then, an entropy-based fusion approach is proposed to obtain the final prediction of emotion and valence based on multi-label predictions of all different feature fusion structures. Furthermore, to reduce the network redundancy and improve the model generalization in low-resource multimodal data conditions, we propose a novel approach for optimizing the network structure progressively based on structured pruning and learning-rate rewinding. When tested on the dataset of MER 2023, the optimized network structure with entropy-based fusion yields consistent and significant improvements, outperforming the champion system of the MER-MULTI sub-challenge. © 2024 IEEE.",feature fusion; entropy-based fusion; Multi-modal emotion recognition; network architecture optimization; structured pruning,,,emotion,Yes,Yes
scopus,MDEmoNet: A Multimodal Driver Emotion Recognition Network for Smart Cockpit,"Hu, C.; Gu, S.; Yang, M.; Han, G.; Lai, C.S.; Gao, M.; Yang, Z.; Ma, G.",2024,,,,10.1109/ICCE59016.2024.10444365,"The automotive smart cockpit is an intelligent and connected in-vehicle consumer electronics product. It can provide a safe, efficient, comfortable, and enjoyable human-machine interaction experience. Emotion recognition technology can help the smart cockpit better understand the driver's needs and state, improve the driving experience, and enhance safety. Currently, driver emotion recognition faces some challenges, such as low accuracy and high latency. In this paper, we propose a multimodal driver emotion recognition model. To our best knowledge, it is the first time to improve the accuracy of driver emotion recognition by using facial video and driving behavior (including brake pedal force, vehicle Y-Axis position and Z-Axis position) as inputs and employing a multi-Task training approach. For verification, the proposed scheme is compared with some mainstream state-of-The-Art methods on the publicly available multimodal driver emotion dataset PPB-Emo.  © 2024 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; deep learning; Driver emotion recognition; Emotion recognition; Deep learning; Multi-modal fusion; multimodal fusion; Human machine interaction; Automotives; Consumer electronics products; driver emotion recognition; Interaction experiences; smart cockpit; Smart cockpit,,,emotion,No,Yes
scopus,Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities,"Liu, R.; Zuo, H.; Lian, Z.; Schuller, B.W.; Li, H.",2024,,,,10.1109/TAFFC.2024.3378570,"Multimodal emotion recognition (MER) aims to understand the way that humans express their emotions by exploring complementary information across modalities. However, it is hard to guarantee that full-modality data is always available in real-world scenarios. To deal with missing modalities, researchers focused on meaningful joint multimodal representation learning during cross-modal missing modality imagination. However, the cross-modal imagination mechanism is highly susceptible to errors due to the &#x201C;modality gap&#x201D; issue, which affects the imagination accuracy, thus, the final recognition performance. To this end, we introduce the concept of a modality-invariant feature into the missing modality imagination network, which contains two key modules: 1) a novel contrastive learning-based module to extract modality-invariant features under full modalities; 2) a robust imagination module based on imagined invariant features to reconstruct missing information under missing conditions. Finally, we incorporate imagined and available modalities for emotion recognition. Experimental results on benchmark datasets demonstrate that our proposed method outperforms existing state-of-the-art strategies. Compared with our previous work, our extended version is more effective on multimodal emotion recognition with missing modalities. The code is released at <uri>https://github.com/ZhuoYulang/CIF-MMIN</uri>. IEEE",Training; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Features extraction; Benchmarking; Self-supervised learning; Contrastive learning; Performances evaluation; Missing modality imagination; Feature extraction; Benchmark testing; Image reconstruction; Images reconstruction; Invariant feature; Invariant features; Modality gap; Performance evaluation,,,emotion,No,Yes
scopus,GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition,"Li, J.; Wang, X.; Lv, G.; Zeng, Z.",2024,,26,,10.1109/TMM.2023.3260635,"Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. We extract various types of edges from the constructed graph for encoding, thus enabling GNNs to extract crucial contextual and interactive information more accurately when performing message passing. Furthermore, we design a GNN structure called GAT-MLP, which can provide a new unified network framework for multimodal learning. The experimental results on two benchmark datasets show that our GraphCFC outperforms the state-of-the-art (SOTA) approaches.  © 1999-2012 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Directed graphs; Human computer interaction; Oral communication; Task analysis; Cross-modal; Graphic methods; multimodal fusion; Emotion recognition in conversation; Graph neural networks; Message passing; Context models; Complementation; cross-modal feature complementation; Cross-modal feature complementation; graph neural networks,,,emotion,No,Yes
scopus,Mining High-quality Samples from Raw Data and Majority Voting Method for Multimodal Emotion Recognition,"Li, Q.; Gao, Y.; Li, Y.",2023,,,,10.1145/3581783.3612862,"Automatic emotion recognition has a wide range of applications in human-computer interaction. In this paper, we present our work in the Multimodel Emotion Recognition (MER) 2023, which contains three sub-challenges: MER-MULTI, MER-NOISE, and MER-SEMI. We first use a vanilla semi-supervised method to mine high quality samples from the MER-SEMI unlabeled dataset to expand the training set. Specifically, we ensemble three models trained with the official training set by a majority voting method, which is used to select samples with high prediction consistency. The selected samples together with the original training set are further augmented by adding noise. Then, the features of different modalities of expanded dataset are extracted from several pre-trained or fine-tuned models, and they are subsequently used to create different feature combinations to capture more effective emotion representations. Besides, we employ early fusion of different modal features and late fusion of different recognition models to obtain the final prediction. Experimental results show that our proposed method improves the performance over the official baselines by 30.4%, 55.3% and 1.57% for the three sub-challenges and ranks 4, 3, and 5, respectively. The present work sheds light on high-quality data mining and model ensemble by majority voting for multimodal emotion recognition. © 2023 ACM.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Data mining; Human computer interaction; multimodal emotion recognition; Supervised learning; Pre-trained model; Automatic emotion recognition; fine-tune; Fine-tune; High quality; Multi-modelling; pre-trained models; Semi-supervised method; Training sets; Voting method,,,emotion,No,Yes
scopus,Exploring Cross-Modal Inconsistency in Entities and Emotions for Multimodal Fake News Detection,"Wang, L.; Zhang, C.; Xu, H.; Xu, Y.; Wang, S.",2024,,14425 LNCS,,10.1007/978-981-99-8429-9_18,"The automatic detection of multimodal fake news has attracted significant attention recently. Numerous existing methods focus on the fusion of unimodal features to generate multimodal news representations. However, it is possible that these methods have not successfully acquired aligned modal information with sufficient accuracy and failed to effectively leverage the entity inconsistency present across modalities. Besides, there has been a lack of exploration regarding the emotional inconsistency across modalities. To address that, we propose CINEMA, a novel framework to explore cross-modal inconsistency in entities and emotions for multimodal fake news detection. We leverage the cross-modal contrastive learning objective to establish the alignment between the image and text modalities. An entity consistency learning module is developed to learn the cross-modality entity correlations. An emotional consistency learning module is implemented to effectively capture the emotional information within each modality. Finally, we evaluate the performance of CINEMA and conduct a comparative study using two extensively used datasets, Twitter and Weibo. The experimental results unequivocally demonstrate that our proposed CINEMA framework surpasses previous approaches by a substantial margin, establishing new state-of-the-art results on both datasets. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Multi-modal; Multi-modal fusion; Unimodal; Social networking (online); Learn+; Cross-modal; Fake detection; Multi-modal learning; Automatic Detection; Fake news detection; Learning modules; Learning objectives,,,emotion,No,Yes
scopus,A Mapping on Current Classifying Categories of Emotions Used in Multimodal Models for Emotion Recognition,"Gong, Z.; Hu, X.; Yao, M.; Zhu, X.; Hirschberg, J.",2024,,,,,"In Emotion Detection within Natural Language Processing and related multimodal research, the growth of datasets and models has led to a challenge: disparities in emotion classification methods. The lack of commonly agreed upon conventions on the classification of emotions creates boundaries for model comparisons and dataset adaptation. In this paper, we compare the current classification methods in recent models and datasets and propose a valid method to combine different emotion categories. Our proposal arises from experiments across models, psychological theories, and human evaluations, and we examined the effect of proposed mapping on models. © 2024 Association for Computational Linguistics.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Emotion detection; Classification (of information); Emotion classification; Language processing; Natural languages; Computational linguistics; Mapping; Multimodal models; Classification methods; On currents; On-currents,,,emotion,No,No
scopus,MM-EMOR: Multi-Modal Emotion Recognition of Social Media Using Concatenated Deep Learning Networks,"Adel, O.; Fathalla, K.M.; Abo ElFarag, A.",2023,,7,,10.3390/bdcc7040164,"Emotion recognition is crucial in artificial intelligence, particularly in the domain of human–computer interaction. The ability to accurately discern and interpret emotions plays a critical role in helping machines to effectively decipher users’ underlying intentions, allowing for a more streamlined interaction process that invariably translates into an elevated user experience. The recent increase in social media usage, as well as the availability of an immense amount of unstructured data, has resulted in a significant demand for the deployment of automated emotion recognition systems. Artificial intelligence (AI) techniques have emerged as a powerful solution to this pressing concern in this context. In particular, the incorporation of multimodal AI-driven approaches for emotion recognition has proven beneficial in capturing the intricate interplay of diverse human expression cues that manifest across multiple modalities. The current study aims to develop an effective multimodal emotion recognition system known as MM-EMOR in order to improve the efficacy of emotion recognition efforts focused on audio and text modalities. The use of Mel spectrogram features, Chromagram features, and the Mobilenet Convolutional Neural Network (CNN) for processing audio data are central to the operation of this system, while an attention-based Roberta model caters to the text modality. The methodology of this study is based on an exhaustive evaluation of this approach across three different datasets. Notably, the empirical findings show that MM-EMOR outperforms competing models across the same datasets. This performance boost is noticeable, with accuracy gains of an impressive 7% on one dataset and a substantial 8% on another. Most significantly, the observed increase in accuracy for the final dataset was an astounding 18%. © 2023 by the authors.",emotion; social media; Emotion Recognition; Multi-modal; Speech recognition; Emotion; Emotion recognition; Deep learning; Character recognition; Social media; Social networking (online); Human computer interaction; multimodal; recognition; Convolutional neural networks; Data handling; Recognition systems; classification; IEMOCAP; MELD; Mobilenet; MobileNet; Recognition; Roberta; Robertum,,,emotion,No,Yes
scopus,Research on Multimodal Emotion Recognition Based on Fusion of Electroencephalogram and Electrooculography,"Yin, J.; Wu, M.; Yang, Y.; Li, P.; Li, F.; Liang, W.; Lv, Z.",2024,,73,,10.1109/TIM.2024.3370813,"Emotion recognition plays a vital role in building a harmonious society and emotional interaction. Recent research has demonstrated that multimodal interchannel correlations and insufficient emotion elicitation plague deep learning-based emotion identification techniques. To cope with these problems, we propose a multimodal and channel attention fusion transformer (MCAF-Transformer). First, we employ an olfactory video approach to evoke emotional expression more fully and acquire electroencephalogram (EEG) and electrooculography (EOG) signal data. Second, the model makes full use of multimodal channel information, time-domain and spatial-domain information of EEG and EOG signals, captures the correlation of different channels using channel attention, and improves the accuracy of emotion recognition by focusing on the global dependence on the temporal order using the transformer. We conducted extensive experiments on the olfactory video sentiment dataset, and the experimental results were correct at 94.63%. The results show that olfactory videos evoke emotion more adequately than pure videos and that the MCAF-Transformer model significantly outperforms other emotion recognition methods.  © 1963-2012 IEEE.",Transformer; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; multimodal; Features extraction; Electroencephalography; Biomedical signal processing; Electrophysiology; Brain modeling; transformer; Time domain analysis; Feature extraction; EEG and EOG; electroencephalogram (EEG) and electrooculography (EOG); olfactory video emotion evocation; Olfactory video emotion evocation; Time-domain analysis,,,emotion,No,Yes
scopus,Symbolic Graphic Emotion Design in Gift Packaging Based on Multimodal Emotion Fusion,"Yang, C.",2024,,9,,10.2478/amns.2023.2.01042,"This paper firstly discusses the value significance of emotional packaging design, outlines its design principles and methods, and explains the thinking structure of emotional design. Secondly, the feature detection of symbolic graphics is realized using a mixture of regression and segmentation, and a multilevel cross-modal perceptual emotion recognition model is constructed based on the collaborative attention mechanism of the cross-modal Transformer and recurrent neural network. Finally, a dataset test and empirical analysis were conducted to verify the model’s effectiveness in this paper. The results show that the multimodal fusion model has better recognition performance than the unimodal and bimodal models, with F1 values of 0.914, 0.923, and 0.902 for neutral, positive, and negative emotions, respectively. Symbolic graphics in the emotional design of gift packages are mainly expressed in the form of intuition, vividness, and validity to emphasize the diversity of symbolic graphics in the emotional design of gift packages. This shows that multimodal emotional fusion can be realized to analyze the emotional design of symbolic graphics of gift packaging and promote the emotional innovation design of symbolic graphics of gift packaging. © 2023 Chun Yang, published by Sciendo.",Attention mechanisms; Emotion Recognition; Multi-modal; Cross-modal; Multi-modality; Multimodality; Recurrent neural networks; Statistical tests; Affective design; Collaborative attention; Collaborative attention mechanism; Collaborative attention mechanisms; Design; Emotional design; Gift packaging; Packaging; Symbolic graphic; Symbolic graphics,,,emotion,No,Yes
scopus,A Hybrid Deep Learning Emotion Classification System Using Multimodal Data,"Kim, D.-H.; Son, W.-H.; Kwak, S.-S.; Yun, T.-H.; Park, J.-H.; Lee, J.-D.",2023,,23,,10.3390/s23239333,"This paper proposes a hybrid deep learning emotion classification system (HDECS), a hybrid multimodal deep learning system designed for emotion classification in a specific national language. Emotion classification is important in diverse fields, including tailored corporate services, AI advancement, and more. Additionally, most sentiment classification techniques in speaking situations are based on a single modality: voice, conversational text, vital signs, etc. However, analyzing these data presents challenges because of the variations in vocal intonation, text structures, and the impact of external stimuli on physiological signals. Korean poses challenges in natural language processing, including subject omission and spacing issues. To overcome these challenges and enhance emotion classification performance, this paper presents a case study using Korean multimodal data. The case study model involves retraining two pretrained models, LSTM and CNN, until their predictions on the entire dataset reach an agreement rate exceeding 0.75. Predictions are used to generate emotional sentences appended to script data, which are further processed using BERT for final emotion prediction. The research result is evaluated by using categorical cross-entropy (CCE) to measure the difference between the model’s predictions and actual labels, F1 score, and accuracy. According to the evaluation, the case model outperforms the existing KLUE/roBERTa model with improvements of 0.5 in CCE, 0.09 in accuracy, and 0.11 in F1 score. As a result, the HDECS is expected to perform well not only on Korean multimodal datasets but also on sentiment classification considering the speech characteristics of various languages and regions. © 2023 by the authors.",Long short-term memory; Humans; Emotions; emotion; Multi-modal; BERT; deep learning; Deep learning; Deep Learning; human; Multi-modal data; Learning systems; Classification (of information); multimodal; F1 scores; Sentiment classification; Emotion classification; Forecasting; Case-studies; Natural language processing systems; entropy; Entropy; Text processing; Asian; Asian People; Communication; Cross entropy; emotion classification; Emotion classification systems; interpersonal communication,,,emotion,Yes,Yes
scopus,Modeling Hierarchical Uncertainty for Multimodal Emotion Recognition in Conversation,"Chen, F.; Shao, J.; Zhu, A.; Ouyang, D.; Liu, X.; Shen, H.T.",2024,,54,,10.1109/TCYB.2022.3185119,"Approximating the uncertainty of an emotional AI agent is crucial for improving the reliability of such agents and facilitating human-in-the-loop solutions, especially in critical scenarios. However, none of the existing systems for emotion recognition in conversation (ERC) has attempted to estimate the uncertainty of their predictions. In this article, we present HU-Dialogue, which models hierarchical uncertainty for the ERC task. We perturb contextual attention weight values with source-adaptive noises within each modality, as a regularization scheme to model context-level uncertainty and adapt the Bayesian deep learning method to the capsule-based prediction layer to model modality-level uncertainty. Furthermore, a weight-sharing triplet structure with conditional layer normalization is introduced to detect both invariance and equivariance among modalities for ERC. We provide a detailed empirical analysis for extensive experiments, which shows that our model outperforms previous state-of-the-art methods on three popular multimodal ERC datasets. © 2013 IEEE.",Humans; Emotions; emotion; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; human; Emotion recognition in conversation; Uncertainty; Adaptation models; Predictive models; Context models; uncertainty; Uncertainty analysis; Bayes method; Bayes theorem; Bayes Theorem; Bayesian; Bayesian deep learning; Capsule network; capsule network (CapsNet); Conditional layer normalization; conditional layer normalization (CLN); emotion recognition in conversation (ERC); Normalisation; reproducibility; Reproducibility of Results,,,emotion,No,No
scopus,Cross-Modal Dynamic Transfer Learning for Multimodal Emotion Recognition,"Hong, S.; Kang, H.; Cho, H.",2024,,12,,10.1109/ACCESS.2024.3356185,"Multimodal Emotion Recognition is an important research area for developing human-centric applications, especially in the context of video platforms. Most existing models have attempted to develop sophisticated fusion techniques to integrate heterogeneous features from different modalities. However, these fusion methods can affect performance since not all modalities help figure out the semantic alignment for emotion prediction. We observed that the 8.0% of misclassified instances' performance is improved for the existing fusion model when one of the input modalities is masked. Based on this observation, we propose a representation learning method called Cross-modal DynAmic Transfer learning (CDaT), which dynamically filters the low-confident modality and complements it with the high-confident modality using uni-modal masking and cross-modal representation transfer learning. We train an auxiliary network that learns model confidence scores to determine which modality is low-confident and how much the transfer should occur from other modalities. Furthermore, it can be used with any fusion model in a model-agnostic way because it leverages transfer between low-level uni-modal information via probabilistic knowledge transfer loss. Experiments have demonstrated the effect of CDaT with four different state-of-the-art fusion models on the CMU-MOSEI and IEMOCAP datasets for emotion recognition.  © 2013 IEEE.",Semantics; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Affective Computing; multimodal emotion recognition; Features extraction; Cross-modal; Affective computing; Computational modelling; Knowledge management; Transfer learning; Feature extraction; cross-modal knowledge transfer; Cross-modal knowledge transfer; Knowledge transfer; model confidence; Model confidence; Probabilistic logics,,,emotion,No,Yes
scopus,HEADSET: Human Emotion Awareness under Partial Occlusions Multimodal DataSET,"Lohesara, F.G.; Freitas, D.R.; Guillemot, C.; Eguiazarian, K.; Knorr, S.",2023,,29,,10.1109/TVCG.2023.3320236,"The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new multimodal database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.  © 2023 IEEE.",Humans; Emotions; emotion; Face recognition; human; Classification (of information); face; Face; Textures; algorithm; Algorithms; Multi-modal dataset; multimodal dataset; Statistical tests; Virtual reality; Three dimensional displays; Three-dimensional display; Cameras; computer graphics; Computer Graphics; diagnostic imaging; Extended reality; Helmet mounted displays; light field; Light fields; Movement; movement (physiology); Point cloud compression; Point-clouds; Resist; virtual reality; volumetric video; Volumetric video; Volumetrics; X reality,,,emotion,No,Yes
scopus,Music Emotion Classification Algorithm Based on Multimodal Deep Learning,"Huang, C.",2024,,950 LNNS,,10.1007/978-3-031-55848-1_1,"In response to the problem of existing algorithms that only consider single-modal features of music or videos, leading to low classification efficiency, this paper proposes a music emotion classification algorithm based on multimodal deep learning. Firstly, a two-dimensional audio convolutional neural network is used, and Mel spectrograms are utilized to learn these audio features. Then, we employ multimodal fusion techniques to integrate audio and video features and design a multimodal deep learning classification algorithm for music emotion classification. This paper constructs a diverse music video dataset to address the lack of labelled music video datasets. Experimental validation is conducted on these datasets to verify the effectiveness of the proposed algorithm, and the impact of different optimizers on the performance of single-modal classification models is compared and analyzed. The experimental results demonstrate that the multimodal classifier achieves the best classification performance compared to the single-modal emotion classifier. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Multi-modal; Deep learning; Multi-modal fusion; Classification (of information); Music; Music emotion classifications; Convolutional neural networks; Learning algorithms; Audio acoustics; Neural-networks; neural network; Classification algorithm; Fusion techniques; multimodal deep learning; Multimodal deep learning; Multimodal fusion technique; multimodal fusion techniques; music emotion classification algorithm; Music emotion classification algorithm; Single-modal,,,emotion,Yes,Yes
scopus,Incorporating Spiking Neural Network for Dynamic Vision Emotion Analysis,"Wang, B.; Liang, X.",2024,,14437 LNCS,,10.1007/978-981-99-8558-6_29,"In the domain of affective computing, researchers have sought to enhance the performance of models and algorithms by leveraging the complementarity of multimodal information. However, the rapid emergence of new modalities has outpaced the development of suitable datasets, posing a challenge in keeping up with the advancements in modal sensing technology. The collection and analysis of multimodal data present intricate and substantial tasks. To address the partial missing data challenge within the research community, we have curated a novel homogeneous multimodal gesture emotion recognition dataset, augmenting existing datasets through meticulous analysis. This dataset not only fills the gaps in homogeneous multimodal data but also opens up new avenues for emotion recognition research. Additionally, we propose a pseudo dual-flow network based on this dataset, establishing its potential application in the affective computing community. Experimental findings indicate the feasibility of utilizing traditional visual information and spiking visual information derived from homogeneous multimodal data for visual emotion recognition. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2024.",Emotion Recognition; Performance; Speech recognition; Emotion recognition; Modal analysis; Multi-modal data; Affective Computing; Emotion analysis; Neural networks; Neural-networks; Dynamic vision; Dynamic vision sensor; Dynamic vision sensors; Spiking neural network; Visual information,,,emotion,No,Yes
scopus,A customizable framework for multimodal emotion recognition using ensemble of deep neural network models,"Dixit, C.; Satapathy, S.M.",2023,,29,,10.1007/s00530-023-01188-6,"Multimodal emotion recognition of videos of human oration, commonly called opinion videos, has a wide scope of applications across all domains. Here, the speakers express their views or opinions about various topics. This field is being researched by many with the aim of introducing accurate and efficient architectures for the same. This study also carries the same objective while exploring novel concepts in the field of emotion recognition. The proposed framework uses cross-dataset training and testing, so that the resultant architecture and models are unrestricted by the domain of input. It uses benchmark datasets and ensemble learning to make sure that even if the individual models are slightly biased, they can be countered by the learnings of the other models. Therefore, to achieve this objective with the mentioned novelties, three benchmark datasets, ISEAR, RAVDESS, and FER-2013, are used to train independent models for each of the three modalities of text, audio, and images. Another dataset is used in addition to the ISEAR dataset to train the text model. They are then combined and tested on the benchmark multimodal dataset of CMU-MOSEI. For the text analysis model, ELMo embedding and RNN are used, for audio, a simple DNN is used and for image emotion recognition, a 2D CNN is used after pre-processing. They are aggregated using the stacking technique for the final result. The complete architecture can be used as a partially pre-trained algorithm for the prediction of individual modalities, and partially trainable for stacking the results to get efficient emotion prediction based on input quality. The accuracy obtained on the CMU-MOSEI data set is 86.60% and the F1-score for the same is 0.84. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Emotion Recognition; Deep neural networks; Speech recognition; CNN; Emotion recognition; Character recognition; Multimodal emotion recognition; Network architecture; Stackings; Statistical tests; Stacking; RNN; Benchmark datasets; Deep neural network; Cross-dataset; Customizable; ELMo; Neural network model; Scope of application,,,emotion,No,Yes
scopus,"Emotion Recognition in Conversations: A Survey Focusing on Context, Speaker Dependencies, and Fusion Methods","Fu, Y.; Yuan, S.; Zhang, C.; Cao, J.",2023,,12,,10.3390/electronics12224714,"As a branch of sentiment analysis tasks, emotion recognition in conversation (ERC) aims to explore the hidden emotions of a speaker by analyzing the sentiments in utterance. In addition, emotion recognition in multimodal data from conversation includes the text of the utterance and its corresponding acoustic and visual data. By integrating features from various modalities, the emotion of utterance can be more accurately predicted. ERC research faces challenges in context construction, speaker dependency design, and multimodal heterogeneous feature fusion. Therefore, this review starts by defining the ERC task, developing the research work, and introducing the utilized datasets in detail. Simultaneously, we analyzed context modeling in conversations, speaker dependency, and methods for fusing multimodal information concerning existing research work for evaluation purposes. Finally, this review also explores the research, application challenges, and opportunities of ERC. © 2023 by the authors.",multimodal data; feature extraction; context construct; emotion recognition in conversation; fusion method; speaker dependency,,,emotion,No,No
scopus,Emotion-Aware Multimodal Fusion for Meme Emotion Detection,"Sharma, S.; S, R.; Akhtar, M.S.; Chakraborty, T.",2024,,,,10.1109/TAFFC.2024.3378698,"The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public&#x0027;s opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding. In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We then present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism. Our investigation establishes ALFRED&#x0027;s superiority over existing baselines by 4.94&#x0025; F1. Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task. We then discuss ALFRED&#x0027;s domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets - HarMeme and Dank Memes, over other baselines. Further, we analyze ALFRED&#x0027;s interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis. IEEE",social media; Emotion Recognition; Modal analysis; Multi-modal fusion; Social media; Social networking (online); 'current; Emotion detection; Emotion analysis; Multi-modality; Large datasets; Multi-modal dataset; information fusion; Pre-training; Meme; multimodality; memes; Political parties,,,emotion,No,No
scopus,Multimodal Depression Detection Network Based on Emotional and Behavioral Features in Conversations,"Wang, P.; Yang, B.; Wang, S.; Zhu, X.; Ni, R.; Yang, C.",2024,,1998,,10.1007/978-981-99-9109-9_44,"Early detection of depression has always been a challenge. Currently, research on automatic depression detection mainly focuses on using low-level features such as audio, text, or video from interview dialogue as input data, ignoring some high-level features contained in the dialogue. We proposes a multimodal depression detection method for extracting emotional and behavioral features from dialogue and detecting early depression. Specifically, we design an emotional feature extraction module and a behavioral feature extraction module, which input the extracted emotional and behavioral features as high-level features into the depression detection network. In this process, a weighted attention fusion module is used to guide the learning of text and audio modalities and predict the final result. Experimental results on the public dataset DAIC-WOZ show that the extracted emotional and behavioral features effectively complement the high-level semantics missing in the network. Our proposed method improves the F1-score by 6% compared to traditional approaches. The experimental data also indicate the importance of the model’s detection results in early depression detection. This technology has certain application value in professional fields such as caregiving, emotional interaction, psychological diagnosis, and treatment. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2024.",Semantics; Multi-modal; Deep learning; Features extraction; Feature fusion; Features fusions; Depression detection; Extraction; Feature extraction; Multi-modal learning; Multimodal learning; Behavioral features; Detection networks; High-level features; Interview dialog; Interview dialogue,,,emotion,No,Yes
scopus,A multimodal fusion emotion recognition method based on multitask learning and attention mechanism,"Xie, J.; Wang, J.; Wang, Q.; Yang, D.; Gu, J.; Tang, Y.; Varatnitski, Y.I.",2023,,556,,10.1016/j.neucom.2023.126649,"With new developments in the field of human–computer interaction, researchers are now paying attention to emotion recognition, especially multimodal emotion recognition, as emotion is a multidimensional expression. In this study, we propose a multimodal fusion emotion recognition method (MTL-BAM) based on multitask learning and the attention mechanism to tackle the major problems encountered in multimodal emotion recognition tasks regarding the lack of consideration of emotion interactions among modalities and the focus on emotion similarity among modalities while ignoring the differences. By improving the attention mechanism, the emotional contribution of each modality is further analyzed so that the emotional representations of each modality can learn from and complement each other to achieve better interactive fusion effect, thereby building a multitask learning framework. By introducing three types of monomodal emotion recognition tasks as auxiliary tasks, the model can detect emotion differences. Simultaneously, the label generation unit is introduced into the auxiliary tasks, and the monomodal emotion label value can be obtained more accurately through two proportional formulas while preventing the zero value problem. Our results show that the proposed method outperforms selected state-of-the-art methods on four evaluation indexes of emotion classification (i.e., accuracy, F1 score, MAE, and Pearson correlation coefficient). The proposed method achieved accuracy rates of 85.36% and 84.61% on the published multimodal datasets of CMU-MOSI and CMU-MOSEI, respectively, which are 2–6% higher than those of existing state-of-the-art models, demonstrating good multimodal emotion recognition performance and strong generalizability. © 2023 Elsevier B.V.",emotion; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; human; Multi-modal fusion; Multimodal emotion recognition; Learning systems; Human computer interaction; Multimodal; learning; attention; human experiment; Attention mechanism; Multitask learning; article; Recognition methods; Correlation methods; correlation coefficient; Learning mechanism; Multi-dimensional expressions; Multitasking learning,,,emotion,No,Yes
scopus,A Multi-Level Alignment and Cross-Modal Unified Semantic Graph Refinement Network for Conversational Emotion Recognition,"Zhang, X.; Cui, W.; Hu, B.; Li, Y.",2024,,,,10.1109/TAFFC.2024.3354382,"Emotion recognition in conversation (ERC) based on multiple modalities has attracted enormous attention. However, most research simply concatenated multimodal representations, generally neglecting the impact of cross-modal correspondences and uncertain factors, and leading to the cross-modal misalignment problems. Furthermore, recent methods only considered simple contextual features, commonly ignoring semantic clues and resulting in an insufficient capture of the semantic consistency. To address these limitations, we propose a novel multi-level alignment and cross-modal unified semantic graph refinement network (MA-CMU-SGRNet) for ERC task. Specifically, a multi-level alignment (MA) is first designed to bridge the gap between acoustic and lexical modalities, which can effectively contrast both the instance-level and prototype-level relationships, separating the multimodal features in the latent space. Second, a cross-modal uncertainty-aware unification (CMU) is adopted to generate a unified representation in joint space considering the ambiguity of emotion. Finally, a dual-encoding semantic graph refinement network (SGRNet) is investigated, which includes a syntactic encoder to aggregate information from near neighbors and a semantic encoder to focus on useful semantically close neighbors. Extensive experiments on three multimodal public datasets show the effectiveness of our proposed method compared with the state-of-the-art methods, indicating its potential application in conversational emotion recognition. Implementation codes can be available at <uri>https://github.com/zxiaohen/MA-CMU-SGRNet</uri>. IEEE",Semantics; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Signal encoding; Task analysis; Cross-modal; Graphic methods; multimodal fusion; Self-supervised learning; Uncertainty; Alignment; Context models; Uncertainty analysis; Context modeling; cross-modal alignment; Cross-modal alignment; semantic refinement; Semantic refinement; Syntactics,,,emotion,No,No
scopus,Real-time emotional health detection using fine-tuned transfer networks with multimodal fusion,"Sharma, A.; Sharma, K.; Kumar, A.",2023,,35,,10.1007/s00521-022-06913-2,"Recognizing and regulating human emotion or a wave of riding emotions are a vital life skill as it can play an important role in how a person thinks, behaves and acts. Accurate real-time emotion detection can revolutionize the human–computer interaction industry and has the potential to provide a proactive approach to mental health care. Several untapped sources of data, including social media data (psycholinguistic markers), multimodal data (audio and video signals) combined with the sensor-based psychophysiological and brain signals, help to comprehend the affective states and emotional experiences. In this work, we propose a model that utilizes three modalities, i.e., visual (facial expression and body gestures), audio (speech) and text (spoken content), to classify emotion into discrete categories based on Ekman’s model with an additional category for ‘neutral’ state. Transfer learning has been used with multistage fine-tuning for each modality instead of training on a single dataset to make the model generalizable. The use of multiple modalities allows integration of heterogeneous data from different sources effectively. The results of the three modalities are combined at the decision-level using weighted fusion technique. The proposed EmoHD model compares favorably to the state-of-the-art technique on two benchmark datasets MELD and IEMOCAP. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Health care; Emotion Recognition; Multi-modal; Emotion recognition; Multi-modal fusion; Human computer interaction; Multimodal; Transfer learning; Real- time; Human emotion; Health care detection; Health care Detection; Life skills; Real-time emotion detection; Transfer network,,,emotion,No,No
scopus,Automatic detection of artifacts and improved classification models for emotional activity detection from multimodal physiological data,"Prabhakaran, S.; Ayyamperumal, N.K.",2023,,45,,10.3233/JIFS-232662,"This manuscript proposes an automated artifacts detection and multimodal classification system for human emotion analysis from human physiological signals. First, multimodal physiological data, including the Electrodermal Activity (EDA), electrocardiogram (ECG), Blood Volume Pulse (BVP) and respiration rate signals are collected. Second, a Modified Compressed Sensing-based Decomposition (MCSD) is used to extract the informative Skin Conductance Response (SCR) events of the EDA signal. Third, raw features (edge and sharp variations), statistical and wavelet coefficient features of EDA, ECG, BVP, respiration and SCR signals are obtained. Fourth, the extracted raw features, statistical and wavelet coefficient features from all physiological signals are fed into the parallel Deep Convolutional Neural Network (DCNN) to reduce the dimensionality of feature space by removing artifacts. Fifth, the fused artifact-free feature vector is obtained for neutral, stress and pleasure emotion classes. Sixth, an artifact-free feature vector is used to train the Random Forest Deep Neural Network (RFDNN) classifier. Then, a trained RFDNN classifier is applied to classify the test signals into different emotion classes. Thus, leveraging the strengths of both RF and DNN algorithms, more comprehensive feature learning using multimodal psychological data is achieved, resulting in robust and accurate classification of human emotional activities. Finally, an extensive experiment using the Wearable Stress and Affect Detection (WESAD) dataset shows that the proposed system outperforms other existing human emotion classification systems using physiological data. © 2023-IOS Press. All rights reserved.",Deep neural networks; Multi-modal; Physiological signals; Physiology; Classification (of information); Convolutional neural networks; Biomedical signal processing; Physiological models; Convolution; Convolutional neural network; Electrocardiograms; physiological signals; Random forests; Compressed sensing; Compressed-Sensing; deep convolutional neural network; Deep convolutional neural network; Emotional reactivity; modified compressed sensing; Modified compressed sensing; Motion artifact; motion artifacts; random forest deep neural network; Random forest deep neural network,,,emotion,No,No
scopus,FMFN: A Fuzzy Multimodal Fusion Network for Emotion Recognition in Ensemble Conducting,"Han, X.; Chen, F.; Ban, J.",2024,,,,10.1109/TFUZZ.2024.3373125,"Conducting and interacting with an orchestra is a multimodal process that integrates channels such as music, visual cues, posture, and gestures to convey artistic intent accurately. For robots, discerning human emotions from these channels can enhance human-machine interactions. Currently, gesture recognition systems in orchestras focus more on rhythm, speed, and dynamics, while studying emotional factors in orchestra conducting music requires more profound research. We introduced the Facial Expression and Orchestra Gesture Emotion (FEGE) dataset, consisting of eight different emotions for recognition. This paper introduces a Fuzzy Multimodal Fusion Network (FMFN) based on fuzzy logic, which operates in multi-feature spaces and is designed for emotion recognition in bimodal tasks involving facial expressions and orchestra-conducting gestures. The network maps facial expressions and gestures into a multi-feature space through bimodal processing, learns unique and shared representations, and decodes them using classifiers optimized by FMFN parameters. Finally, it processes data uncertainty and fuzziness using a fuzzy logic system, improving the classification decision process to enhance the robustness and adaptability of emotion recognition tasks in bimodal visual modalities. Experimental results on the FEGE dataset confirmed the effectiveness of our network. The proposed bimodal fusion network achieved an accuracy of 89.16&#x0025; in bimodal emotion recognition, which is approximately a 21&#x0025; improvement over single-modal recognition results. This approach can also be better applied to human-machine interaction systems, particularly in orchestra conducting training, aiming to enhance the most critical emotional factors conveyed during the conducting process, thus elevating the depth of artistic intent. IEEE",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Multi-modal fusion; Human computer interaction; Human robot interaction; Job analysis; Task analysis; Features extraction; Feature extraction; Man machine systems; Computer circuits; Fuzzy logic; Fuzzy systems; Fuzzy-Logic; Emotional recognition; Emotional Recognition; Conductor; Conductors; Ensemble conducting; Ensemble Conducting; Fuzzy Logic; Mathematical models; Multimodal fusion network; Multimodal Fusion Network,,,emotion,No,Yes
scopus,Animated Pedagogical Agents Performing Affective Gestures Extracted from the GEMEP Dataset: Can People Recognize Their Emotions?,"Mukanova, M.; Adamo, N.; Mousas, C.; Choi, M.; Hauser, K.; Mayer, R.; Zhao, F.",2024,,565 LNICST,,10.1007/978-3-031-55312-7_20,"The study reported in the paper focused on applying a set of affective body gestures extracted from the Geneva Multimodal Emotion Portrayals (GEMEP) dataset to two pedagogical animated agents in an online lecture context and studying the effects of those gestures on subjects’ perception of the agents’ emotions. 131 participants completed an online survey where they watched different animations featuring a female and a male animated agent expressing six emotions (anger, joy, sadness, disgust, fear, and surprise) while delivering a lecture segment. After watching the animations, subjects were asked to identify the agents’ emotions. Findings showed that only one expression of the angry emotion by the female agent was recognized with an acceptable level of accuracy (recognition rate >75%), while the remaining emotions showed low recognition rates ranging from 1.5% to 64%. A mapping of the results on Russel’s Circumplex model of emotion showed that participants’ identification of levels of emotion arousal and valence was slightly more accurate than recognition of emotion quality but still low (recognition rates <75% for 5 out of 6 emotions). Results suggest that hand and arm gestures alone are not sufficient for conveying the agent’s emotion type and the levels of emotion valence and arousal. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2024.",Emotion Recognition; Multi-modal; Behavioral research; Emotion recognition; Affective body gesture; Affective Body Gestures; Affective Gesture; Animated agents; Animated pedagogical agent; Animated Pedagogical Agents; Body gesture; GEMEP database; Geneva multimodal emotion portrayal database; Online surveys; Pedagogical agents,,,emotion,No,Yes
scopus,A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations,"Ma, H.; Wang, J.; Lin, H.; Zhang, B.; Zhang, Y.; Xu, B.",2024,,26,,10.1109/TMM.2023.3271019,"Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT) for the task. The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers, and learns weights between modalities dynamically by designing a hierarchical gated fusion strategy. Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision. Specifically, we introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality. Experiments on IEMOCAP and MELD datasets demonstrate that SDT outperforms previous state-of-the-art baselines.  © 1999-2012 IEEE.",Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Modal interactions; Job analysis; Modal representation; Oral communication; Task analysis; Multimodal emotion recognition in conversation; Multimodal emotion recognition in conversations; multimodal fusion; Distillation; Context models; intra- and inter-modal interactions; Intra-modal and inter-modal interaction; modal representation,,,emotion,No,No
scopus,Wearable-based human flow experience recognition enhanced by transfer learning methods using emotion data,"Irshad, M.T.; Li, F.; Nisar, M.A.; Huang, X.; Buss, M.; Kloep, L.; Peifer, C.; Kozusznik, B.; Pollak, A.; Pyszka, A.; Flak, O.; Grzegorzek, M.",2023,,166,,10.1016/j.compbiomed.2023.107489,"Background: Flow experience is a specific positive and affective state that occurs when humans are completely absorbed in an activity and forget everything else. This state can lead to high performance, well-being, and productivity at work. Few studies have been conducted to determine the human flow experience using physiological wearable sensor devices. Other studies rely on self-reported data. Methods: In this article, we use physiological data collected from 25 subjects with multimodal sensing devices, in particular the Empatica E4 wristband, the Emotiv Epoc X electroencephalography (EEG) headset, and the Biosignalplux RespiBAN – in arithmetic and reading tasks to automatically discriminate between flow and non-flow states using feature engineering and deep feature learning approaches. The most meaningful wearable device for flow detection is determined by comparing the performances of each device. We also investigate the connection between emotions and flow by testing transfer learning techniques involving an emotion recognition-related task on the source domain. Results: The EEG sensor modalities yielded the best performances with an accuracy of 64.97%, and a macro Averaged F1 (AF1) score of 64.95%. An accuracy of 73.63% and an AF1 score of 72.70% were obtained after fusing all sensor modalities from all devices. Additionally, our proposed transfer learning approach using emotional arousal classification on the DEAP dataset led to an increase in performances with an accuracy of 75.10% and an AF1 score of 74.92%. Conclusion: The results of this study suggest that effective discrimination between flow and non-flow states is possible with multimodal sensor data. The success of transfer learning using the DEAP emotion dataset as a source domain indicates that emotions and flow are connected, and emotion recognition can be used as a latent task to enhance the performance of flow recognition. © 2023 The Author(s)","affect; convolutional neural network; Machine Learning; Adult; Female; Humans; Young Adult; Male; Emotions; Machine learning; emotion; Emotion Recognition; Performance; Speech recognition; adult; Deep learning; female; human; male; Multimodal sensing; physiology; Learning systems; Classification (of information); Article; machine learning; electroencephalography; Electroencephalography; procedures; Electrophysiology; F1 scores; human experiment; Physiological models; Machine-learning; signal processing; Signal Processing, Computer-Assisted; Transfer learning; arousal; feature extraction; young adult; Neural networks; Wearable sensors; arithmetic; Artificial neural network; Flow; flow experience; Flow experience; Human flow experience; image segmentation; Physiological response; Physiological responses; reading; transfer of learning; wearable computer; Wearable Electronic Devices",,,emotion,No,Yes
scopus,A Versatile Multimodal Learning Framework for Zero-Shot Emotion Recognition,"Qi, F.; Zhang, H.; Yang, X.; Xu, C.",2024,,34,,10.1109/TCSVT.2024.3362270,"Multi-modal Emotion Recognition (MER) aims to identify various human emotions from heterogeneous modalities. With the development of emotional theories, there are more and more novel and fine-grained concepts to describe human emotional feelings. Real-world recognition systems often encounter unseen emotion labels. To address this challenge, we propose a versatile zero-shot MER framework to refine emotion label embeddings for capturing inter-label relationships and improving discrimination between labels. We integrate prior knowledge into a novel affective graph space that generates tailored label embeddings capturing inter-label relationships. To obtain multimodal representations, we disentangle the features of each modality into egocentric and altruistic components using adversarial learning. These components are then hierarchically fused using a hybrid co-attention mechanism. Furthermore, an emotion-guided decoder exploits label-modal dependencies to generate adaptive multimodal representations guided by emotion embeddings. We conduct extensive experiments with different multimodal combinations, including visual-acoustic and visual-textual inputs, on four datasets in both single-label and multi-label zero-shot settings. Results demonstrate the superiority of our proposed framework over state-of-the-art methods.  © 1991-2012 IEEE.",Semantics; Transformer; Emotion Recognition; Embeddings; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Job analysis; Task analysis; Features extraction; transformer; Feature extraction; Multi-modal learning; Zero-shot learning; Circuits and systems; Timing circuits; zero-shot learning,,,emotion,No,No
scopus,MALN: Multimodal Adversarial Learning Network for Conversational Emotion Recognition,"Ren, M.; Huang, X.; Liu, J.; Liu, M.; Li, X.; Liu, A.-A.",2023,,33,,10.1109/TCSVT.2023.3273577,"Multimodal emotion recognition in conversations (ERC) aims to identify the emotional state of constituent utterances expressed by multiple speakers in dialogue from multimodal data. Existing multimodal ERC approaches focus on modeling the global context of the dialogue and neglect to mine the characteristic information from the corresponding utterances expressed by the same speaker. Additionally, information from different modalities exhibits commonality and diversity for emotional expression. The commonality and diversity of multimodal information are compensated for each other but not effectively exploited in previous multimodal ERC works. To tackle these issues, we propose a novel Multimodal Adversarial Learning Network (MALN). MALN first mines the speaker's characteristics from context sequences and then incorporate them with the unimodal features. Afterward, we design a novel adversarial module AMDM to exploit both commonality and diversity from the unimodal features. Finally, AMDM fuses different modalities to generate refined utterance representations for emotion classification. Extensive experiments are conducted on two public multimodal ERC datasets, IEMOCAP and MELD. Through the experiments, MALN shows its superiority over the state-of-the-art methods.  © 2023 IEEE.",Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Learning systems; Social networking (online); Job analysis; Oral communication; Task analysis; multimodal fusion; Machine-learning; Emotion recognition in conversation; Emotion recognition in conversations; E-learning; adversarial learning; Adversarial learning; Adversarial machine learning,,,emotion,No,No
scopus,Thesis Proposal: Detecting Empathy Using Multimodal Language Model,"Hasan, M.R.; Hossain, M.Z.; Krishna, A.; Rahman, S.; Gedeon, T.",2024,,,,,"Empathy is crucial in numerous social interactions, including human-robot, patient-doctor, teacher-student, and customer-call centre conversations. Despite its importance, empathy detection in videos continues to be a challenging task because of the subjective nature of empathy and often remains under-explored. Existing studies have relied on scripted or semi-scripted interactions in text-, audio-, or video-only settings that fail to capture the complexities and nuances of real-life interactions. This PhD research aims to fill these gaps by developing a multimodal language model (MMLM) that detects empathy in audiovisual data. In addition to leveraging existing datasets, the proposed study involves collecting real-life interaction video and audio. This study will leverage optimisation techniques like neural architecture search to deliver an optimised small-scale MMLM. Successful implementation of this project has significant implications in enhancing the quality of social interactions as it enables real-time measurement of empathy and thus provides potential avenues for training for better empathy in interactions. © 2024 Association for Computational Linguistics.",Multi-modal; Human robot interaction; Computational linguistics; Language model; Teachers'; Audio-visual data; Call centers; Human robots; Neural architectures; Optimization techniques; Small scale; Social interactions,,,empathy,No,No
scopus,Research on Emotion Classification Based on Multi-modal Fusion,"Zhihua, X.; Radzi, N.H.M.; Hashim, H.",2024,,21,,10.21123/bsj.2024.9454,"Nowadays, people’s expression on the Internet is no longer limited to text, especially with the rise of the short video boom, leading to the emergence of a large number of modal data such as text, pictures, audio, and video. Compared to single mode data, the multi-modal data always contains massive information. The mining process of multi-modal information can help computers to better understand human emotional characteristics. However, because the multi-modal data show obvious dynamic time series features, it is necessary to solve the dynamic correlation problem within a single mode and between different modes in the same application scene during the fusion process. To solve this problem, in this paper, a feature extraction framework of the three-dimensional dynamic expansion is established based on the common multi-modal data, for example video, sound, text. Based on the framework, a multi-modal fusion-matched framework based on spatial and temporal feature enhancement, respectively to solve the dynamic correlation within and between modes, and then model the short and long term dynamic correlation information between different modes based on the proposed framework. Multiple group experiments performed on MOSI datasets show that the emotion recognition model constructed based on the framework proposed here in this paper can better utilize the more complex complementary information between different modal data. Compared with other multi-modal data fusion models, the spatial-temporal attention-based multimodal data fusion framework proposed in this paper significantly improves the emotion recognition rate and accuracy when applied to multi-modal emotion analysis, so it is more feasible and effective. © 2022 The Author(s). Published by College of Science for Women, University of Baghdad.",Dynamic correlation; Feature matching; Match fusion; Multi-modal emotion classification; Temporal attention,,,emotion,No,Yes
scopus,Affect-GCN: a multimodal graph convolutional network for multi-emotion with intensity recognition and sentiment analysis in dialogues,"Firdaus, M.; Singh, G.V.; Ekbal, A.; Bhattacharyya, P.",2023,,82,,10.1007/s11042-023-14885-1,"Emotion classification along with sentimental analysis in dialogues is a complex task that has currently attained immense popularity. When communicating their thoughts and feelings, humans are prone to having many emotions of varying intensities. The task is complicated and fascinating since emotions in a dialogue utterance can be independent or based on the preceding utterances. Additional details such as audio and video, along with text facilitates in the identification of the right emotions with the corresponding intensity and appropriate sentiments in a dialogue. In this work, we focus on the task of predicting multiple emotions and their corresponding intensity along with sentiments in a given utterance of a dialogue. With the release of MEISD dataset, the task of simultaneously predicting the sentiments along with multiple emotions with intensity from a given utterance of a conversation utilizing the knowledge from textual, audio and visual cues has gained significance in conversational systems. We design an Affect-GCN framework that utilizes an RNN-GCN network as an utterance encoder followed by Multimodal Factorized Bilinear (MFB) pooling for enhance representation of different modalities. The proposed Affect-GCN framework shows an improvement of 0.7 in terms of Jaccard index for multi-label emotion classification while an increase of 0.3 for intensity prediction. Experimental analysis shows that our proposed Affect-GCN framework outperforms the existing approaches and several baselines for the task of multi-label emotion classification, intensity prediction and sentiment analysis in dialogues. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Emotion Recognition; Multi-modal; Emotion; Modal analysis; Sentiment analysis; Classification (of information); Convolution; Audio systems; Emotion classification; Forecasting; Convolutional networks; Graph convolutional network; Classification; Conversational AI; Intensity; Multi-modal factorized bilinear  pooling; Multi-modal Factorized Bilinear (MFB) pooling; Sentiment,,,emotion,Yes,No
scopus,Speech emotion recognition using multimodal feature fusion with machine learning approach,"Panda, S.K.; Jena, A.K.; Panda, M.R.; Panda, S.",2023,,82,,10.1007/s11042-023-15275-3,"Speech-based emotional state recognition must have a significant impact on artificial intelligence as machine learning advances. When it comes to emotion recognition, proper feature selection is critical. As a result, feature fusion technology is offered in this work as a means of achieving high prediction accuracy by emphasizing the extraction of sole features. Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Mel Spectrogram, Short-time Fourier transform (STFT) and Root Mean Square (RMS) are extracted, and four different feature fusion techniques are used on five standard machine learning classifiers: XGBoost, Support Vector Machine (SVM), Random Forest, Decision-Tree (D-Tree), and K Nearest Neighbor (KNN). The successful use of feature fusion techniques on our suggested classifier yields a satisfactory recognition rate of 99.64% on the female only dataset (TESS), 91% on SAVEE (male only dataset) and 86% on CREMA-D (both male and female) dataset. The proposed model shows that effective feature fusion improves the accuracy and applicability of emotion detection systems. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Speech emotion recognition; Emotion Recognition; Speech recognition; Learning systems; Classification (of information); Feature fusion; Features fusions; Support vector machines; Support vectors machine; Machine-learning; Feature Selection; Decision trees; Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficients; Feature fusion (FF); Mel frequency cepstral coefficients; Nearest neighbor search; Speech emotion recognition (SER); Support vector machine; Xgboost; XGBoost; Zero crossing rate; Zero Crossing Rate,,,emotion,No,Yes
scopus,Utilizing Quantum Particle Swarm Optimization for Multimodal Fusion of Gestures and Facial Expressions in Ensemble Conducting for Emotional Recognition,"Han, X.; Chen, F.; Ban, J.",2024,,2007 CCIS,,10.1007/978-981-97-0576-4_4,"The conductor-orchestra interaction is a multimodal process, integrating channels like music, visual cues, postures, and gestures to convey artistic intent accurately. For robots, discerning human emotions from these channels enhances human-machine interaction. Current gesture recognition systems in ensembles prioritize rhythm, tempo, and dynamics; research on the emotional factors of ensembles conducting in music needs to be more extensive. We introduce the Facial Expression and Ensemble Conducting Gesture (FEGE) dataset, comprising eight distinct emotions for recognition. This article presents a Quantum Particle Swarm Optimization Algorithm (QPSO)-based parameter optimization for a Multimodal Fusion Network (QMFN) operating in a multi-feature space, aiming for emotional recognition in dual visual tasks. The network maps conduct facial expressions and gestures into a multi-feature space via dual-modality processing. It learns distinct and shared representations and decodes them using classifiers optimized through QPSO parameters. Experiments on the FEGE dataset validate our network’s efficacy. The proposed bimodal fusion network achieves an 83.17% accuracy in dual visual emotion recognition, marking about a 15% enhancement over single-modal recognition results. The proposed method can also be better applied to human-computer interaction systems for ensemble conducting training, aiming to enhance the deeper artistic intent conveyed by the most crucial emotional factors during the conducting process. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Emotion Recognition; Face recognition; Facial Expressions; Multi-modal fusion; Human computer interaction; Human robot interaction; Feature space; Multifeatures; Emotional recognition; Emotional Recognition; Ensemble conducting; Ensemble Conducting; Multimodal fusion network; Multimodal Fusion Network; Gesture recognition; Emotional factors; Particle swarm optimization (PSO); Quanta particle swarm optimizations; Quantum Particle Swarm Optimization; Quantum particle swarm optimization algorithm; Vision,,,emotion,No,Yes
scopus,InMDb: Indian Movie Database for Emotion Analysis,"Garg, R.; Thareja, R.; Bisht, M.; Singh, M.; Arora, S.; Shukla, J.",2023,,,,10.1145/3627631.3627665,"Cinematic experiences, characterized by intricate audio-visual stimuli, foster profound emotional engagement. However, the correlation between audience emotions, physiological responses, film genres, and ratings, particularly in the underexplored Bollywood context, remains largely uncharted. Understanding this intricate interplay can provide filmmakers valuable insights for content adaptation. Addressing this research gap, we introduce ""InMDB: Indian Movie DataBase,"" a comprehensive multimodal dataset that examines emotional responses elicited by Bollywood trailers, using both self-reported measures and physiological data. Our meticulous statistical analysis of the dataset deepens the understanding of how emotions and their subsequent physiological responses correlate with, and potentially influence, film ratings and categories, offering novel insights into emotional engagement in the cinematic context. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Emotion Recognition; Physiology; Emotion analysis; Physiological models; Multi-modal dataset; Emotional response; Research gaps; Physiological response; Audio-visual stimulus; Cinematics; Content adaptation; Emotional engagements; Measure data; Motion pictures,,,emotion,No,No
scopus,Speech Emotion Recognition in Multimodal Environments with Transformer: Arabic and English Audio Datasets,"Mohamed, E.A.; Koura, A.; Kayed, M.",2024,,15,,10.14569/IJACSA.2024.0150359,"Speech Emotion Recognition (SER) is a fast-developing area of study with a primary goal of automatically identifying and analyzing the emotional states expressed in speech. Emotions are crucial in human communication as they impact the effectiveness and meaning of linguistic expressions. SER aims to create computational approaches and models to detect and interpret emotions from speech signals. One of the primary applications of SER is evident in the field of Human-Computer Interaction (HCI), where it can be used to develop interactive systems that adapt to the user's emotional state based on their voice. This paper investigates the use of speech data for speech emotion recognition. Additionally, we applied a transformation process to convert the speech data into 2D images. Subsequently, we compared the outcomes of this transformation with the original speech data, aligning the comparison with a dataset containing labeled speech samples in both Arabic and English. Our experiments compare three methods: a transformer-based model, a Vision Transformer (ViT) based model, and a wave2vec-based model. The transformer model is trained from scratch on two significant audio datasets: the Arabic Natural Audio Dataset (ANAD) and the Toronto Emotional Speech Set (TESS), while the vision transformer is evaluated alongside wave2vec as part of transfer learning. The results are impressive. The transformer model achieved remarkable accuracies of 94% and 99% on ANAD and TESS datasets, respectively. Additionally, ViT demonstrates strong capabilities, achieving accuracies of 88% and 98% on the ANAD and TESS datasets, respectively. To assess the transfer learning potential, we also explore the Wave2Vector model with fine-tuning. However, the findings suggest limited success, achieving only a 56% accuracy rate on the ANAD dataset. © (2024), (Science and Information Organization). All Rights Reserved.",Speech emotion recognition; Emotion Recognition; Emotional state; Speech recognition; Multimodal emotion recognition; Learning systems; Signal encoding; Human computer interaction; multimodal emotion recognition; Metadata; Data handling; Emotional speech; Fine tuning; Speech data; fine-tuning; Toronto; transformer encoder; Transformer encoder; Transformer modeling; Tuning; wav2vec; Wav2vec,,,emotion,Yes,Yes
scopus,EMERSK -Explainable Multimodal Emotion Recognition With Situational Knowledge,"Palash, M.; Bhargava, B.",2024,,26,,10.1109/TMM.2023.3304015,"Automatic emotion recognition has recently gained significant attention due to the growing popularity of deep learning algorithms. One of the primary challenges in emotion recognition is effectively utilizing the various cues (modalities) available in the data. Another challenge is providing a proper explanation of the outcome of the learning. To address these challenges, we present Explainable Multimodal Emotion Recognition with Situational Knowledge (EMERSK), a generalized and modular system for human emotion recognition and explanation using visual information. Our system can handle multiple modalities, including facial expressions, posture, and gait, in a flexible and modular manner. The network consists of different modules that can be added or removed depending on the available data. We utilize a two-stream network architecture with convolutional neural networks (CNNs) and encoder-decoder style attention mechanisms to extract deep features from face images. Similarly, CNNs and recurrent neural networks (RNNs) with Long Short-term Memory (LSTM) are employed to extract features from posture and gait data. We also incorporate deep features from the background as contextual information for the learning process. The deep features from each module are fused using an early fusion network. Furthermore, we leverage situational knowledge derived from the location type and adjective-noun pair (ANP) extracted from the scene, as well as the spatio-temporal average distribution of emotions, to generate explanations. Ablation studies demonstrate that each sub-network can independently perform emotion recognition, and combining them in a multimodal approach significantly improves overall recognition performance. Extensive experiments conducted on various benchmark datasets, including GroupWalk, validate the superior performance of our approach compared to other state-of-the-art methods.  © 1999-2012 IEEE.",Long short-term memory; Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Deep learning; Deep Learning; Multimodal emotion recognition; Features extraction; Multimodal; Benchmarking; Convolution; Convolutional neural network; LSTM; Learning algorithms; Network architecture; Knowledge management; Feature extraction; Convolutional neural network (CNN),,,emotion,No,Yes
scopus,Multi-modal Emotion Recognition Based on Hypergraph,"Zong, L.-L.; Zhou, J.-H.; Xie, Q.-J.; Zhang, X.-C.; Xu, B.",2023,,46,,10.11897/SP.J.1016.2023.02520,"With the rapid progress of artificial intelligence technology, machines need to recognize users’ emotions to provide users with a better human-computer interaction experience. Therefore, emotion recognition has become one of the active fields of artificial intelligence. Traditional emotion recognition is mostly based on text modality. Compared with single modality, multi-modal emotion recognition has the advantages of data complementarity and model robustness. In multi-modal emotion recognition, feature fusion between modalities determines the effect of emotion recognition. Recently, graph-based intra-modality fusion has attracted much attention of related research, which uses graphs of binary relationships between two modalities. When processing data of three or more modalities, the graph can hardly effectively establish the feature fusion between all modalities without introducing redundant information, limiting the performance of multi-modal emotion recognition. Therefore, it is necessary to design more effective method to model and fuse multi-modal emotion features. To solve this problem, this paper proposes an emotion recognition model Multi-modal Emotion Recognition Based on Hypergraph （MORAH） which introduces hypergraph to establish multivariate relations among multi-modal data instead of binary relations and achieves efficient multi-modal feature fusion. Specifically, the model divides multi-modal feature fusion into two stages： the hyperedge construction stage and the hypergraph learning stage. In the hyperedge construction stage, we aggregate the information of each time step in the sequence through the capsule network and establish the graph of a single modality. Then, we use graph convolution for the second aggregation, which is used as the basis for establishing hypergraph in the next stage. Benefiting from the graph capsule aggregation method, the model can work with aligned data and unaligned data at the same time, without manual alignment of unaligned data. In the hypergraph learning stage, we not only establish the association between the nodes of different modalities of the same sample but also establish the association between all modalities of the same sample. At the same time, we use hierarchical multi-level hyperedges to avoid too smooth node embedding and the simple hypergraph convolution method to fuse the high-level features between modalities, ensuring that all node features are only updated when necessary in the hypergraph convolution process. Simplified graph convolution can guarantee the effect of emotion recognition and improve the training speed without nonlinear activation and convolution filter matrix. Comprehensive experiments on two benchmark datasets show that the proposed model makes full use of the multiple relations between multi-modal data by using hypergraph. Compared with the existing advanced methods, MORAH improves the binary accuracy by 1.3% and F1-score by 1.1% on the unaligned data of the CMU-MOSI dataset. On the unaligned data of the CMU-MOSEI dataset, MORAH improves the binary accuracy and the F1-score by 0. 2%, respectively. To demonstrate the generality of the hypergraph learning stage in various multimodal tasks, we apply the hierarchical multi-level hyperedges to the emotion recognition in conversation （ERC）. The experimental results indicate that MORAH can improve the performance of ERC to a certain extent. This suggests that the MORAH model can function as a universal tool to assist downstream natural language processing tasks. © 2023 Science Press. All rights reserved.",Emotion Recognition; Performance; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Modal analysis; Character recognition; Learning systems; Artificial intelligence; Human computer interaction; Features fusions; Graphic methods; Convolution; Data handling; Multi-modal learning; Graph theory; Capsule network; capsule network; Hyper graph; hyperedge extension; Hyperedge extension; Hyperedges; hypergraph learning; Hypergraph learning; multi-modal learning,,,emotion,No,Yes
scopus,Transformer-Based Self-Supervised Multimodal Representation Learning for Wearable Emotion Recognition,"Wu, Y.; Daoudi, M.; Amad, A.",2024,,15,,10.1109/TAFFC.2023.3263907,"Recently, wearable emotion recognition based on peripheral physiological signals has drawn massive attention due to its less invasive nature and its applicability in real-life scenarios. However, how to effectively fuse multimodal data remains a challenging problem. Moreover, traditional fully-supervised based approaches suffer from overfitting given limited labeled data. To address the above issues, we propose a novel self-supervised learning (SSL) framework for wearable emotion recognition, where efficient multimodal fusion is realized with temporal convolution-based modality-specific encoders and a transformer-based shared encoder, capturing both intra-modal and inter-modal correlations. Extensive unlabeled data is automatically assigned labels by five signal transforms, and the proposed SSL model is pre-trained with signal transformation recognition as a pretext task, allowing the extraction of generalized multimodal representations for emotion-related downstream tasks. For evaluation, the proposed SSL model was first pre-trained on a large-scale self-collected physiological dataset and the resulting encoder was subsequently frozen or fine-tuned on three public supervised emotion recognition datasets. Ultimately, our SSL-based method achieved state-of-the-art results in various emotion classification tasks. Meanwhile, the proposed model was proved to be more accurate and robust compared to fully-supervised methods on low data regimes.  © 2010-2012 IEEE.",Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Physiological signals; Physiology; Multi-modal fusion; Signal encoding; Task analysis; Features extraction; Metadata; Physiological models; multimodal fusion; Self-supervised learning; Supervised learning; physiological signals; Extraction; Wearable technology; Feature extraction; transformers; self-supervised learning,,,emotion,Yes,No
scopus,A multimodal emotion recognition system using deep convolution neural networks,"Almulla, M.A.",2024,,,,10.1016/j.jer.2024.03.021,"Despite the progress in computer-technology, with regard to Human-Computer Interaction (HCI), emotion recognition is still a challenging problem. In this paper, we present a novel multimodal emotion recognition system capable of recognizing emotions from audio, video, and text data using deep convolution neural networks. The system is able to recognize happy, angry, sad, afraid, disgust, surprise and neutral emotions. We used three datasets to train and test the system, one set for each of the three input formats. The results show a recognition accuracy rate of 100% for audio, 69% for video, and 64% for text. When applying the decision-level fusion, the recorded accuracy rate is 80%. These results confirm that the system is effective in recognizing human emotions. © 2024 The Authors",Multimodal emotion recognition; Deep convolution neural networks,,,emotion,No,Yes
scopus,A Multimodal Driver Emotion Recognition Algorithm Based on the Audio and Video Signals in Internet of Vehicles Platform,"Ying, N.; Jiang, Y.; Guo, C.; Zhou, D.; Zhao, J.",2024,,,,10.1109/JIOT.2024.3363176,"Driving can take up a substantial part of daily life and frequently trigger negative emotions like anger or anxiety, which have a significant adverse impact on driving safety as well as long-term human health. To identify driver emotions, thereby improving the safety and humanization of intelligent driving, we explore how to model the discriminative emotion features from both speech and facial expressions in this work. More specifically, an effective attention-based network for facial expression and a lightweight speech emotion network are proposed, separately. Then, audio and video features are combined at the feature level to construct our multimodal driver emotion recognition model. This paper proposes a new audio feature extractor that uses a multi-scale residual structure to extract spectrogram features. In terms of video, a set of frame sequences using Local Binary Pattern Histograms (LBPH) is obtained through preprocessing, which generates a fixed-dimensional feature representation. These features are then input into a fine-tuned ResNet18 model to analyze spatial information. This model is further augmented by integrating both a temporal attention module and a Gated Recurrent Unit (GRU), enhancing its capability to create a highly discriminative video representation. Additionally, we propose an Internet of Vehicles (IoV) platform, specifically designed for driver emotion recognition. The IoV platform consists of sensor layer, data acquisition and transport layer, server layer and data application layer. The IoV platform uses sensors to collect multimodal data from drivers, which can provide data support for the proposed multimodal driver emotion recognition algorithm. The performance of this proposed algorithm is evaluated on two multimodal emotional datasets, Ryerson Audio-Visual Dataset of Emotional Speech and Song (RAVDESS) and Surrey Audio-Visual Expressed Emotion (SAVEE), using a variety of performance indicators. Compared to other baseline methods, this proposed multimodal model achieves state-of-the-art results on the RAVDESS and SAVEE datasets, demonstrating superior recognition accuracy with rates of 0.93 and 0.99, respectively. Additionally, it exhibits precision scores of 0.93 on RAVDESS and 0.99 on SAVEE, along with exceptional specificity scores of 0.99 and 1.00, respectively. IEEE",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Multimodal emotion recognition; Affective Computing; Features extraction; Multimodal Emotion Recognition; Data acquisition; Feature extraction; Audio-visual; Random forests; Driver emotion; Driver Emotion; Driving safety; Driving Safety; Intelligent driving; Intelligent Driving; Internet of vehicle; Internet of Vehicles; Local binary pattern; Servers; Vehicles,,,emotion,No,Yes
scopus,Speech Emotion Recognition using Threshold Fusion for Enhancing Audio Sensitivity,"Luo, Z.; Christiansson, S.; Ladóczki, B.; Komatani, K.",2023,,,,10.1145/3611380.3628557,"Speech Emotion Recognition (SER) has found applications in various fields. However, most SER studies exhibit a bias towards the text modality, which can lead to incorrect recognition when nonverbal audio features convey the primary emotional information. To address this issue, we propose a two-step solution to enhance the audio emotion sensitivity of SER models. First, we use a parallel emotional speech dataset (ESD), which contains identical speech content pronounced with different emotions, to pretrain a speech content-independent emotion recognition model, named the Audio Sensitive Network (ASN). Second, we propose a novel threshold fusion technique utilizing the Tree-structured Parzen Estimator (TPE) to optimize different thresholds for each predictive label, integrating the ASN with baseline SER classifiers. To demonstrate the efficacy of our approach, we conduct experiments on the IEMOCAP and ESD datasets. The results reveal that our novel method enhances audio sensitivity by enhancing the performance of existing SER classifiers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Character recognition; multimodal; Recognition models; Neural-networks; Nonverbals; Emotional speech; Electrostatic devices; Incorrect recognition; neural networks; Speech content; threshold fusion; Threshold fusion,,,emotion,Yes,Yes
scopus,Emotion generation method in online physical education teaching based on data mining of teacher-student interactions,"Zhao, Y.; Kong, X.; Zheng, W.; Ahmad, S.",2024,,10,,10.7717/peerj-cs.1814,"Different from conventional educational paradigms, online education lacks the direct interplay between instructors and learners, particularly in the sphere of virtual physical education. Regrettably, extant research seldom directs its focus toward the intricacies of emotional arousal within the teacher-student course dynamic. The formulation of an emotion generation model exhibits constraints necessitating refinement tailored to distinct educational cohorts, disciplines, and instructional contexts. This study proffers an emotion generation model rooted in data mining of teacher-student course interactions to refine emotional discourse and enhance learning outcomes in the realm of online physical education. This model includes techniques for data preprocessing and augmentation, a multimodal dialogue text emotion recognition model, and a topic-expanding emotional dialogue generation model based on joint decoding. The encoder assimilates the input sentence into a fixed-length vector, culminating in the final state, wherein the vector produced by the context recurrent neural network is conjoined with the preceding word’s vector and employed as the decoder’s input. Leveraging the long-short-term memory neural network facilitates the modeling of emotional fluctuations across multiple rounds of dialogue, thus fulfilling the mandate of emotion prediction. The evaluation of the model against the DailyDialog dataset demonstrates its superiority over the conventional end-to-end model in terms of loss and confusion values. Achieving an accuracy rate of 84.4%, the model substantiates that embedding emotional cues within dialogues augments response generation. The proposed emotion generation model augments emotional discourse and learning efficacy within online physical education, offering fresh avenues for refining and advancing emotion generation models. © 2024 Zhao et al. All Rights Reserved.",Artificial Intelligence; Emotion Recognition; Character recognition; Learning systems; Data mining; Sentiment analysis; Machine-learning; Recurrent neural networks; Natural languages; Sentiment Analysis; E-learning; Teachers'; Decoding; Students; Algorithm and analyse of algorithm; Algorithms and Analysis of Algorithms; Analysis of algorithms; Data mining and machine learning; Data Mining and Machine Learning; Emotion generation; Emotion generation model; Natural Language and Speech; Natural speech; Online physical education teaching; Physical education teachings; Student interactions; Teacher-student interaction,,,emotion,No,Yes
scopus,GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information Fusion for Conversational Emotion Detection,"Li, J.; Wang, X.; Lv, G.; Zeng, Z.",2024,,15,,10.1109/TAFFC.2023.3261279,"Multimodal Emotion Recognition in Conversation (ERC) plays an influential role in the field of human-computer interaction and conversational robotics since it can motivate machines to provide empathetic services. Multimodal data modeling is an up-and-coming research area in recent years, which is inspired by human capability to integrate multiple senses. Several graph-based approaches claim to capture interactive information between modalities, but the heterogeneity of multimodal data makes these methods prohibit optimal solutions. In this article, we introduce a multimodal fusion approach named Graph and Attention based Two-stage Multi-source Information Fusion (GA2MIF) for emotion detection in conversation. Our proposed method circumvents the problem of taking heterogeneous graph as input to the model while eliminating complex redundant connections in the construction of graph. GA2MIF focuses on contextual modeling and cross-modal modeling through leveraging Multi-head Directed Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal ATtention networks (MPCATs), respectively. Extensive experiments on two public datasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the capacity to validly capture intra-modal long-range contextual information and inter-modal complementary information, as well as outperforms the prevalent State-Of-The-Art (SOTA) models by a remarkable margin.  © 2010-2012 IEEE.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Data mining; Cross-modal interaction; Directed graphs; Human computer interaction; Human robot interaction; Oral communication; Graphic methods; Computational modelling; multimodal fusion; Information fusion; Emotion recognition in conversation; Graph neural networks; Context models; graph neural networks; emotion recognition in conversation; Cross-modal interactions; multi-head attention mechanism; Multi-head attention mechanism,,,emotion,No,No
scopus,"Multimodal Emotion Recognition Based on Facial Expressions, Speech, and EEG","Pan, J.; Fang, W.; Zhang, Z.; Chen, B.; Zhang, Z.; Wang, S.",2024,,5,,10.1109/OJEMB.2023.3240280,"Goal: As an essential human-machine interactive task, emotion recognition has become an emerging area over the decades. Although previous attempts to classify emotions have achieved high performance, several challenges remain open: 1) How to effectively recognize emotions using different modalities remains challenging. 2) Due to the increasing amount of computing power required for deep learning, how to provide real-time detection and improve the robustness of deep neural networks is important. Method: In this paper, we propose a deep learning-based multimodal emotion recognition (MER) called Deep-Emotion, which can adaptively integrate the most discriminating features from facial expressions, speech, and electroencephalogram (EEG) to improve the performance of the MER. Specifically, the proposed Deep-Emotion framework consists of three branches, i.e., the facial branch, speech branch, and EEG branch. Correspondingly, the facial branch uses the improved GhostNet neural network proposed in this paper for feature extraction, which effectively alleviates the overfitting phenomenon in the training process and improves the classification accuracy compared with the original GhostNet network. For work on the speech branch, this paper proposes a lightweight fully convolutional neural network (LFCNN) for the efficient extraction of speech emotion features. Regarding the study of EEG branches, we proposed a tree-like LSTM (tLSTM) model capable of fusing multi-stage features for EEG emotion feature extraction. Finally, we adopted the strategy of decision-level fusion to integrate the recognition results of the above three modes, resulting in more comprehensive and accurate performance. Result and Conclusions: Extensive experiments on the CK+, EMO-DB, and MAHNOB-HCI datasets have demonstrated the advanced nature of the Deep-Emotion method proposed in this paper, as well as the feasibility and superiority of the MER approach.  © 2020 IEEE.",facial expressions; Long short-term memory; Emotion Recognition; Performance; Deep neural networks; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Multimodal emotion recognition; Features extraction; speech; Electroencephalography; electroencephalogram; Electrophysiology; Brain; Brain modeling; Convolution; Extraction; Feature extraction; Emotion feature; Ghostnet; Human-Machine Interactive,,,emotion,No,Yes
scopus,"A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods","Pan, B.; Hirota, K.; Jia, Z.; Dai, Y.",2023,,561,,10.1016/j.neucom.2023.126866,"Affective computing is one of the most important research fields in modern human–computer interaction (HCI). The goal of affective computing is to study and develop the theories, methods, and systems that can recognize, explain, process, and simulate human emotions. As a branch of affective computing, emotion recognition aims to enlighten the machine/computer automatically analyzing human emotions, which has received increasing attention from researchers in various fields. Human beings generally observe and understand the emotional states of one person by integrating the perceived information from his/her facial expressions, voice tone, speech content, behavior, or physiological features. To imitate the emotion observation manner of humans, researchers have been devoted to constructing multimodal emotion recognition models by fusing information from two or more modalities. In this paper, we provide a comprehensive review of multimodal emotion recognition from the perspectives of multimodal datasets, data preprocessing, unimodal feature extraction, and multimodal information fusion methods in recent decades. Furthermore, challenges and future research directions of the topic are specified and discussed. The main motivations of this review are to conclude the recent emergence of abundant works on multimodal emotion recognition and to provide potential guidance to researchers in the related field for understanding the pipeline and mainstream approaches to multimodal emotion recognition. © 2023 Elsevier B.V.",Machine learning; emotion; facial expression; Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; adult; female; human; male; Multimodal emotion recognition; Affective Computing; Human computer interaction; Classification (of information); learning; speech; human experiment; Physiological models; Computation theory; classifier; Information fusion; feature extraction; voice; Multimodal information fusion; Human emotion; Fusion methods; Classifier; Feature learning; Human being; motivation; pipeline; Pre-processing method; Research fields; short survey,,,emotion,No,No
scopus,Emotion recognition from unimodal to multimodal analysis: A review,"Ezzameli, K.; Mahersia, H.",2023,,99,,10.1016/j.inffus.2023.101847,"The omnipresence of numerous information sources in our daily life brings up new alternatives for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data, the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep learning play in this? What is multimodality? How did it progress? What are the methods of information fusion? What are the most used datasets in each modality and in multimodal recognition? We can understand and compare the various methods by answering these questions. © 2023 Elsevier B.V.",Emotion Recognition; Speech recognition; Emotion recognition; Modal analysis; Deep learning; Unimodal; Affective Computing; Affective computing; Multi-modality; Multimodality; Multimodal analysis; Fusion; Daily lives; Ehealth; Information sources; Modality,,,emotion,No,No
scopus,Identifying multimodal misinformation leveraging novelty detection and emotion recognition,"Kumari, R.; Ashok, N.; Agrawal, P.K.; Ghosal, T.; Ekbal, A.",2023,,61,,10.1007/s10844-023-00789-x,"With the growing presence of multimodal content on the web, a specific category of fake news is rampant on popular social media outlets. In this category of fake online information, real multimedia contents (images, videos) are used in different but related contexts with manipulated texts to mislead the readers. The presence of seemingly non-manipulated multimedia content reinforces the belief in the associated fabricated textual content. Detecting this category of misleading multimedia fake news is almost impossible without relevance to any prior knowledge. In addition to this, the presence of highly novel and emotion-invoking contents can fuel the rapid dissemination of such fake news. To counter this problem, in this paper, we first introduce a novel multimodal fake news dataset that includes background knowledge (from authenticate sources) of the misleading articles. Second, we design a multimodal framework using Supervised Contrastive Learning (SCL) based novelty detection and Emotion Prediction tasks for fake news detection. We perform extensive experiments to reveal that our proposed model outperforms the state-of-the-art (SOTA) models. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Emotion Recognition; Multi-modal; Emotion; Emotion recognition; Deep learning; Contrastive learning; Fake detection; Background information; Misinformation detection; Multimedia contents; Novelty; Novelty detection,,,emotion,No,No
scopus,"The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation","Christ, L.; Amiriparian, S.; Baird, A.; Kathan, A.; Müller, N.; Klug, S.; Gagne, C.; Tzirakis, P.; Stappen, L.; Meßner, E.-M.; König, A.; Cowen, A.; Cambria, E.; Schuller, B.W.",2023,,,,10.1145/3606039.3613114,"The Multimodal Sentiment Analysis Challenge (MuSe) 2023 is a set of shared tasks addressing three different contemporary multimodal affect and sentiment analysis problems: In the Mimicked Emotions Sub-Challenge (MuSe-Mimic), participants predict three continuous emotion targets. This sub-challenge utilises the Hume-Vidmimic dataset comprising of user-generated videos. For the Cross-Cultural Humour Detection Sub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football Coach Humour (Passau-SFCH) dataset is provided. Participants predict the presence of spontaneous humour in a cross-cultural setting. The Personalisation Sub-Challenge (MuSe-Personalisation) challenge is based on the Ulm-Trier Social Stress Test (Ulm-TSST) dataset, featuring recordings of subjects in a stressed situation. Here, arousal and valence signals are to be predicted, whereas parts of the test labels are made available in order to facilitate personalisation. MuSe 2023 seeks to bring together a broad audience from different research communities such as audio-visual emotion recognition, natural language processing, signal processing, and health informatics. In this baseline paper, we introduce the datasets, sub-challenges, and provided feature sets. As a competitive baseline system, a Gated Re-current Unit (GRU)-Recurrent Neural Network (RNN) is employed. On the respective sub-challenges' test datasets, it achieves a mean (across three continuous intensity targets) Pearson's Correlation Coefficient of .4727 for MuSe-Mimic, an Area Under the Curve (AUC) value of .8310 for MuSe-Humour and Concordance Correlation Coefficient (CCC) values of .7482 for arousal and .7827 for valence in the MuSe-Personalisation sub-challenge.  © 2023 ACM.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Modal analysis; Multi-modal fusion; multimodal sentiment analysis; Affective Computing; Sentiment analysis; multimodal fusion; Multimodal sentiment analyse; Recurrent neural networks; Statistical tests; Challenge; Signal processing; Correlation methods; Humor detection; affective computing; Personalizations; benchmark; Benchmark; challenge; humour detection; Sports; Time series analysis,,,emotion,Yes,Yes
scopus,DGNN: Dependency Graph Neural Network for Multimodal Emotion Recognition in Conversation,"Zhang, Z.; Wang, X.; Yuan, L.; Miao, G.; Liu, M.; Yun, W.; Wu, G.",2024,,1963 CCIS,,10.1007/978-981-99-8138-0_8,"For emotion recognition in conversation (ERC), the modeling of conversational dependency plays a crucial role. Existing methods often directly connect multimodal information and then build a graph neural network based on a fixed number of past and future utterances. The former leads to the lack of interaction between modalities, and the latter is less consistent with the logic of the conversation. Therefore, in order to better build conversational dependency, we propose a Dependency Graph Neural Network (DGNN) for ERC. First, we present a cross-modal fusion transformer for modeling dependency between different modalities of the same utterance. Then, we design a directed graph neural network model based on the adaptive window for modeling dependency between different utterances. The results of the extensive experiments on two benchmark datasets demonstrate the superiority of the proposed model. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal information; Multimodal emotion recognition; Directed graphs; Cross-modal; Emotion recognition in conversation; Graph neural networks; Adaptive window; Adaptive windows; Cross-modal fusion transformer; Dependency graphs; Model dependencies,,,emotion,No,No
scopus,Going Beyond Closed Sets: A Multimodal Perspective for Video Emotion Analysis,"Pu, H.; Sun, Y.; Song, R.; Chen, X.; Jiang, H.; Liu, Y.; Cao, Z.",2024,,14430 LNCS,,10.1007/978-981-99-8537-1_19,"Emotion analysis plays a crucial role in understanding video content. Existing studies often approach it as a closed set classification task, which overlooks the important fact that the emotional experiences of humans are so complex and difficult to be adequately expressed in a limited number of categories. In this paper, we propose MM-VEMA, a novel MultiModal perspective for Video EMotion Analysis. We formulate the task as a crossmodal matching problem within a joint multimodal space of videos and emotional experiences (e.g. emotional words, phrases, sentences). By finding experiences that closely match each video in this space, we can derive the emotions evoked by the video in a more comprehensive manner. To construct this joint multimodal space, we introduce an efficient yet effective method that manipulates the multimodal space of a pre-trained vision-language model using a small set of emotional prompts. We conduct experiments and analyses to demonstrate the effectiveness of our methods. The results show that videos and emotional experiences are well aligned in the joint multimodal space. Our model also achieves state-of-the-art performance on three public datasets. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Emotion Recognition; Multi-modal; Emotion; Modal analysis; Emotion analysis; Multimodal; Cross-modal; Video; Computer vision; Classification tasks; Video contents; Closed set; Closed-set classifications; Emotional experiences,,,emotion,No,Yes
scopus,AIA-Net: Adaptive Interactive Attention Network for Text-Audio Emotion Recognition,"Zhang, T.; Li, S.; Chen, B.; Yuan, H.; Philip Chen, C.L.",2023,,53,,10.1109/TCYB.2022.3195739,"Emotion recognition based on text-audio modalities is the core technology for transforming a graphical user interface into a voice user interface, and it plays a vital role in natural human-computer interaction systems. Currently, mainstream multimodal learning research has designed various fusion strategies to learn intermodality interactions but hardly considers that not all modalities play equal roles in emotion recognition. Therefore, the main challenge in multimodal emotion recognition is how to implement effective fusion algorithms based on the auxiliary structure. To address this problem, this article proposes an adaptive interactive attention network (AIA-Net). In AIA-Net, text is treated as a primary modality, and audio is an auxiliary modality. AIA-Net adapts to textual and acoustic features with different dimensions and learns their dynamic interactive relations in a more flexible way. The interactive relations are encoded as interactive attention weights to focus on the acoustic features that are effective for textual emotional representations. AIA-Net performs well in adaptively assisting the textual emotional representation with the acoustic emotional information. Moreover, multiple collaborative learning (co-learning) layers of AIA-Net achieve multiple multimodal interactions and the deep bottom-up evolution of emotional representations. Experimental results on three benchmark datasets demonstrate the great effectiveness of the proposed method over the state-of-the-art methods. © 2013 IEEE.",Humans; Emotions; emotion; Transformer; Emotion Recognition; Speech recognition; Emotion recognition; human; Character recognition; Multimodal emotion recognition; Human computer interaction; Correlation; multimodal emotion recognition; Features extraction; learning; Learning; algorithm; Algorithms; Audio systems; acoustics; Audio acoustics; Acoustics; Adaptive interactive attention; Adaptive interactive attention (AIA); auxiliary structure; Auxiliary structure; Collaborative learning; Computer Systems; Emotional representations; Graphical user interfaces; Multiple collaborative learning  layer; multiple collaborative learning (co-learning) layers,,,emotion,No,No
scopus,Multimodal Emotion Recognition in Conversation Based on Hypergraphs,"Li, J.; Mei, H.; Jia, L.; Zhang, X.",2023,,12,,10.3390/electronics12224703,"In recent years, sentiment analysis in conversation has garnered increasing attention due to its widespread applications in areas such as social media analytics, sentiment mining, and electronic healthcare. Existing research primarily focuses on sequence learning and graph-based approaches, yet they overlook the high-order interactions between different modalities and the long-term dependencies within each modality. To address these problems, this paper proposes a novel hypergraph-based method for multimodal emotion recognition in conversation (MER-HGraph). MER-HGraph extracts features from three modalities: acoustic, text, and visual. It treats each modality utterance in a conversation as a node and constructs intra-modal hypergraphs (Intra-HGraph) and inter-modal hypergraphs (Inter-HGraph) using hyperedges. The hypergraphs are then updated using hypergraph convolutional networks. Additionally, to mitigate noise in acoustic data and mitigate the impact of fixed time scales, we introduce a dynamic time window module to capture local-global information from acoustic signals. Extensive experiments on the IEMOCAP and MELD datasets demonstrate that MER-HGraph outperforms existing models in multimodal emotion recognition tasks, leveraging high-order information from multimodal data to enhance recognition capabilities. © 2023 by the authors.",emotion recognition; hypergraph; hypergraph attention mechanism; multi-modal,,,emotion,No,No
scopus,EmoffMeme: identifying offensive memes by leveraging underlying emotions,"Kumari, G.; Bandyopadhyay, D.; Ekbal, A.",2023,,82,,10.1007/s11042-023-14807-1,"Facebook, Twitter, Instagram, and other social media sites allow anonymity and independence. People exert their right to free expression without fear of repercussions. However, in the absence of thorough surveillance, people have fallen prey to offensiveness, trolls, and social media predators. Memes, a type of multimodal media, are becoming increasingly popular online. While most memes are meant to be humorous, some use dark humor to disseminate offensive content. Our present research focuses on learning the dependency and correlation between the three tasks, viz., detecting offensive memes, classifying offensive memes into fine-grained categories, and detecting emotions in a meme. For this, we created EmoffMeme, a large-scale multimodal dataset for Hindi. We aim at gaining insight into hidden social media users’ emotions by studying the meme’s text and image. We present an end-to-end multitasking deep neural network-based CLIP (Contrastive Language-Image Pre-training) model to solve the above correlated tasks simultaneously. We also employ Multimodal Factorized Bilinear (MFB) pooling to incorporate one common portrayal of a meme’s textual and visual part. We demonstrated the effectiveness of our work through extensive experiments. The evaluation shows that the proposed multitask framework yields better performance for the primary task, i.e., offensiveness identification, with the help of secondary task, i.e., emotion analysis. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Facebook; Deep neural networks; Multi-modal; Social media; Social networking (online); Multi tasks; Multimodal; Large dataset; Fine grained; Large-scales; Multi-task; Multimodal factorized bilinear; Offensive meme detection; Research focus,,,emotion,No,No
scopus,Multimodal Arabic emotion recognition using deep learning,"Al Roken, N.; Barlas, G.",2023,,155,,10.1016/j.specom.2023.103005,"Emotion Recognition has been an active area for decades due to the complexity of the problem and its significance in human–computer interaction. Various methods have been employed to tackle this problem, leveraging different inputs such as speech, 2D and 3D images, audio signals, and text, all of which can convey emotional information. Recently, researchers have started combining multiple modalities to enhance the accuracy of emotion classification, recognizing that different emotions may be better expressed through different input types. This paper introduces a novel Arabic audio-visual natural-emotion dataset, investigates two existing multimodal classifiers, and proposes a new classifier trained on our Arabic dataset. Our evaluation encompasses different aspects, including variations in visual dataset sizes, joint and disjoint training, single and multimodal networks, as well as consecutive and overlapping segmentation. Through 5-fold cross-validation, our proposed classifier achieved exceptional results with an average F1-score of 0.912 and an accuracy of 0.913 for natural emotion recognition. © 2023 Elsevier B.V.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Human computer interaction; Classification (of information); Human–computer interaction; Speech communication; Computer vision; Audio signal; 2D images; 3-D image; 3D-images; Active area; Audiovisual; Natural emotions; Petroleum reservoir evaluation; Speech signal processing,,,emotion,No,Yes
scopus,DER-GCN: Dialog and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialog Emotion Recognition,"Ai, W.; Shou, Y.; Meng, T.; Li, K.",2024,,,,10.1109/TNNLS.2024.3367940,"With the continuous development of deep learning (DL), the task of multimodal dialog emotion recognition (MDER) has recently received extensive research attention, which is also an essential branch of DL. The MDER aims to identify the emotional information contained in different modalities, e.g., text, video, and audio, and in different dialog scenes. However, the existing research has focused on modeling contextual semantic information and dialog relations between speakers while ignoring the impact of event relations on emotion. To tackle the above issues, we propose a novel dialog and event relation-aware graph convolutional neural network (DER-GCN) for multimodal emotion recognition method. It models dialog relations between speakers and captures latent event relations information. Specifically, we construct a weighted multirelationship graph to simultaneously capture the dependencies between speakers and event relations in a dialog. Moreover, we also introduce a self-supervised masked graph autoencoder (SMGAE) to improve the fusion representation ability of features and structures. Next, we design a new multiple information Transformer (MIT) to capture the correlation between different relations, which can provide a better fuse of the multivariate information between relations. Finally, we propose a loss optimization strategy based on contrastive learning to enhance the representation learning ability of minority class features. We conduct extensive experiments on the benchmark datasets, Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Multimodal EmotionLines Dataset (MELD), which verify the effectiveness of the DER-GCN model. The results demonstrate that our model significantly improves both the average accuracy and the <inline-formula> <tex-math notation=""LaTeX"">$F1$</tex-math> </inline-formula> value of emotion recognition. Our code is publicly available at https://github.com/yuntaoshou/DER-GCN. IEEE",Semantics; Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Transformers; Job analysis; Task analysis; Features extraction; Auto encoders; Convolution; Self-supervised learning; Contrastive learning; Extraction; Feature extraction; Neural networks; Context models; Multimodal dialogue; Context modeling; event extraction; Events extractions; Masked graph autoencoder; masked graph autoencoders (MGAEs); Multimodal dialog emotion recognition; multimodal dialog emotion recognition (MDER); Multiple information transformer; multiple information Transformer (MIT),,,emotion,No,Yes
scopus,Muti-modal Emotion Recognition via Hierarchical Knowledge Distillation,"Sun, T.; Wei, Y.; Ni, J.; Liu, Z.; Song, X.; Wang, Y.; Nie, L.",2024,,,,10.1109/TMM.2024.3385180,"Due to its wide applications, multimodal emotion recognition has gained increasing research attention. Although existing methods have achieved compelling success with various multimodal fusion methods, they overlook that the dominated modality (e.g., text) may cause a shortcut and hence negatively affect the representation learning of other modalities (e.g., image and audio). To alleviate such a problem, we resort to the knowledge distillation to narrow the gap between different modalities. In particular, we develop a new hierarchical knowledge distillation model for multi-modal emotion recognition (HKD-MER), consisting of three components, feature extraction, hierarchical knowledge distillation, and attentive multi-modal fusion. As the major contribution in our proposed model, the hierarchical knowledge distillation is designed to transfer the knowledge from the dominant modality to the others at both the feature and label levels. It boosts the performance of non-dominated modalities by modeling the inter-modal relation between different modalities. We have justified the effectiveness of our proposed model over two benchmark datasets. IEEE",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Electronic mail; Modal representation; Features extraction; Optimisations; Contrastive learning; Extraction; Distillation; Feature extraction; E-learning; Predictive models; Optimization; Visualization; Knowledge distillation; Knowledge Distillation; Acoustics; Contrastive Learning; Multi-modal representation learning; Multi-modal Representation Learning,,,emotion,No,Yes
scopus,Hybrid multi-modal emotion recognition framework based on InceptionV3DenseNet,"Alamgir, F.M.; Alam, M.S.",2023,,82,,10.1007/s11042-023-15066-w,"Emotion recognition is one of the most complex research areas as individuals express emotional cues based on several modalities such as audio, facial expressions, and language. The recognition of emotion from one of the modalities is not always feasible as the single modalities are disturbed by several factors. The existing models cannot attain the maximum accuracy in exactly identifying the expressions of individuals. In this paper, a novel hybrid multi-modal emotion recognition framework InceptionV3DenseNet is proposed for improving the recognition accuracy. Initially contextual features are extracted from different modalities such as video, audio and text. From the video modality, the features such as shot length, lighting key, motion and color are extracted. Zero-crossing rate, Mel frequency cepstral coefficient (MFCC), energy and pitch are extracted from the audio modality and the unigram, bigram and TF-IDF are extracted from the textual modality. In feature extraction, high level features are extracted with better generalization capability. The extracted features are fused using the multi-set integrated canonical correlation analysis (MICCA) and are provided as the input to the proposed hybrid network model. It detects the correlation between multimodal features to provide better performance with single learning phase. Then the proposed hybrid deep learning model is utilized to classify emotional states by considering the accuracy and reliability. The work simulations are conducted in the MATLAB platform and evaluated using the MELD and RAVDESS datasets. The outcomes proved that the proposed model is more efficient and accurate than the compared models and attained an overall accuracy rate of 74.87% in MELD and 95.25% in RAVDESS. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Classification (of information); Features extraction; Feature fusion; Features fusions; Extraction; Feature extraction; Classification; Textual features; Multi-modal emotion recognition; Audio features; MATLAB; Research areas; Simulation platform; Video features,,,emotion,No,Yes
scopus,Emotional Video Captioning with Vision-Based Emotion Interpretation Network,"Song, P.; Guo, D.; Yang, X.; Tang, S.; Wang, M.",2024,,33,,10.1109/TIP.2024.3359045,"Effectively summarizing and re-expressing video content by natural languages in a more human-like fashion is one of the key topics in the field of multimedia content understanding. Despite good progress made in recent years, existing efforts usually overlooked the emotions in user-generated videos, thus making the generated sentence a bit boring and soulless. To fill the research gap, this paper presents a novel emotional video captioning framework in which we design a Vision-based Emotion Interpretation Network to effectively capture the emotions conveyed in videos and describe the visual content in both factual and emotional languages. Specifically, we first model the emotion distribution over an open psychological vocabulary to predict the emotional state of videos. Then, guided by the discovered emotional state, we incorporate visual context, textual context, and visual-Textual relevance into an aggregated multimodal contextual vector to enhance video captioning. Furthermore, we optimize the network in a new emotion-fact coordinated way that involves two losses-Emotional Indication Loss and Factual Contrastive Loss, which penalize the error of emotion prediction and visual-Textual factual relevance, respectively. In other words, we innovatively introduce emotional representation learning into an end-To-end video captioning network. Extensive experiments on public benchmark datasets, EmVidCap and EmVidCap-S, demonstrate that our method can significantly outperform the state-of-The-Art methods by a large margin. Quantitative ablation studies and qualitative analyses clearly show that our method is able to effectively capture the emotions in videos and thus generate emotional language sentences to interpret the video content.  © 1992-2012 IEEE.",Humans; Emotions; Psychology; Cognition; emotion; Emotion Recognition; Emotion recognition; human; Job analysis; Task analysis; Visual languages; Emotion analysis; benchmarking; Benchmarking; Video recording; cognition; emotion analysis; Coordinated optimization; emotion-fact coordinated optimization; Emotion-fact coordinated optimization; Emotional video captioning; Generator; Vision based; Vocabulary,,,emotion,No,No
scopus,Auditive Emotion Recognition for Emphatic AI-Assistants,"Duwenbeck, R.; Kirchner, E.A.",2024,,,,10.1007/s13218-023-00828-3,"This paper briefly introduces the Project “AudEeKA”, whose aim is to use speech and other bio signals for emotion recognition to improve remote, but also direct, healthcare. This article takes a look at use cases, goals and challenges, of researching and implementing a possible solution. To gain additional insights, the main-goal of the project is divided into multiple sub-goals, namely speech emotion recognition, stress detection and classification and emotion detection from physiological signals. Also, similar projects are considered and project-specific requirements stemming from use-cases introduced. Possible pitfalls and difficulties are outlined, which are mostly associated with datasets. They also emerge out of the requirements, their accompanying restrictions and first analyses in the area of speech emotion recognition, which are shortly presented and discussed. At the same time, first approaches to solutions for every sub-goal, which include the use of continual learning, and finally a draft of the planned architecture for the envisioned system, is presented. This draft presents a possible solution for combining all sub-goals, while reaching the main goal of a multimodal emotion recognition system. © 2024, The Author(s).",Speech emotion recognition; Emotion Recognition; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Emotion detection; Stress detection; Biosignals; Continual learning; Stress classifications; Subgoals,,,emotion,No,No
scopus,Emotion recognition from multiple physiological signals using intra- and inter-modality attention fusion network,"Gong, L.; Chen, W.; Li, M.; Zhang, T.",2024,,144,,10.1016/j.dsp.2023.104278,"Recently, many studies have shown that integrating multiple modalities can more accurately and robustly identify human emotions compared with a single modality. However, how to fully utilize the heterogeneity and correlation of multiple modalities to improve emotion recognition performance remains a challenge. Given this, we propose a novel multimodal fusion method that considers both heterogeneity and correlation simultaneously, and realizes an end-to-end multimodal emotion recognition model using intra- and inter-modality attention fusion network. Firstly, the dual-stream feature extractor is designed to extract emotional features from raw EEG signals and peripheral physiological signals (PPS) separately. Then, the inter-modality fusion module is released to capture the correlation and complementarity of the two features. Meanwhile, the intra-modality encoding module is added to preserve the heterogeneity information of each feature. Finally, their joint loss function is applied to train the model. The proposed model has been extensively validated on DEAP and DREAMER multimodal datasets, with an average accuracy of 97.97%/98.02% on valence/arousal dimension for DEAP dataset and 99.47%/99.47% on valence/arousal dimension for DREAMER dataset, which outperforms the state-of-the-art multimodal methods. Additionally, we also explore the best combination of EEG and each peripheral physiological signal (e.g., EOG, EMG signals), which can assist in the development of a low-cost and more effective multimodal emotion analysis system. The proposed method can provide new insights into multimodal fusion research for emotion recognition. © 2023 Elsevier Inc.",Transformer; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Multi-modal fusion; Intermodality; Electroencephalography; Biomedical signal processing; Electroencephalogram; Electroencephalogram (EEG); Multiple modalities; Peripheral physiological signal; Data fusion; Multimodal data fusion; Peripheral physiological signal (PPS),,,emotion,No,Yes
scopus,Semantic fusion of facial expressions and textual opinions from different datasets for learning-centered emotion recognition,"Cárdenas-López, H.M.; Zatarain-Cabada, R.; Barrón-Estrada, M.L.; Mitre-Hernández, H.",2023,,27,,10.1007/s00500-023-08076-1,"Learning-centered emotions have a significant role in the cognitive process in learning. For this reason, it is relevant that virtual learning environments consider the cognitive and affective aspects of the student. Methods of artificial intelligence such as the recognition of facial expressions, and sentimental analysis have proven to be an excellent alternative in the automatic recognition of emotions. However, learning-centered emotions and opinion-based sentiment dataset commonly contain single modalities. At the same time, single modalities cannot effectively represent complex emotions in real life. This work presents three different fusion methods applied to three image-based and text-based dataset for learning-centered emotion recognition. Using some conventional deep learning architectures, the three new multimodal datasets showed promising results when compared with similar architectures trained in unimodal information. The improvement of one of the methods (embedding-based representation) was 4% compared to single-modality hyperparameter optimization. The main objective of this study is to benchmark the viability of semantic fusion of multimodal learning-centered emotional data from the different datasets for intelligent tutoring system applications. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Semantics; Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Character recognition; Learning systems; Convolutional neural networks; Benchmarking; Convolutional neural network; Computer aided instruction; Network architecture; Fuzzy logic; Fuzzy neural networks; Fuzzy-Logic; Semantic Web; Cognitive process; Dataset semantic fusion; Intelligent tutoring; Intelligent tutoring system; Intelligent tutoring systems; Long short-term memories; Semantic fusion; Tutoring system,,,emotion,No,No
scopus,Classification of Patient Emotions in Nonverbal Communication Based on Machine Learning,"Kosachev, I.S.; Smetanina, O.N.",2023,,33,,10.1134/S1054661823030215,"Abstract: This article is devoted to solving the problem of classifying a patient’s emotions in nonverbal communication based on machine learning. The article reflects the current state of the problem in the field of classification of human emotions in nonverbal communication and gives the formulation of the problem and the results of experimental studies for finding the most high-quality model that allows for the classification of emotions. The resulting model has an architecture that includes two recurrent subnets with an attention mechanism, the outputs of which are combined and fed to a fully connected classification layer. As a result, the obtained model on the validation dataset has accuracy of 0.77. The model was developed in Python using the TensorFlow and Keras frameworks. To extract the image of the face, the BlazeFace model from the MediaPipe framework was used. © 2023, Pleiades Publishing, Ltd.",Machine learning; Multi-modal data; multimodal data; 'current; Classification (of information); machine learning; Machine-learning; Human emotion; Neural-networks; Abstracting; High quality; neural networks; Classification of human emotion; classification of human emotions; Non-verbal communications; nonverbal communication; On-machines; Quality modeling,,,emotion,No,Yes
scopus,Investigations in Emotion Aware Multimodal Gender Prediction Systems from Social Media Data,"Suman, C.; Chaudhari, R.; Saha, S.; Kumar, S.; Bhattacharyya, P.",2023,,10,,10.1109/TCSS.2022.3158605,"Gender plays a crucial role in improving the performance quality of personalized systems. Privacy and anonymity allow users to hide their details. Based on the intuition that post contents of male and female users differ, we can predict the gender of the social media account holder via their corresponding posts. These posts can be multimodal (text + image) in nature. We investigate various emotion-assisted multimodal gender prediction models in this article. The developed models use gated recurrent units (GRUs) and ResNets for extracting features from the tweets and the images, respectively. The distribution of emotion categories is related to the gender of the target person. In response to these findings, this article describes the first attempt to use multimodal (image and text posted) information for gender prediction in a multitask setting with emotion recognition as an auxiliary task. The enriched PAN-2018 dataset with gender and emotion labels is used to train gender and emotion networks. Several models were developed to improve the gender prediction task by generating better emotion-aware features. The results show that the proposed multimodal emotion detection model outperforms single-modal (text and image)-based models and state-of-the-art systems on the benchmark PAN-2018 dataset.  © 2014 IEEE.",Natural language processing; Emotion Recognition; Speech recognition; Emotion recognition; Character recognition; Social networking (online); Emotion detection; Job analysis; Task analysis; Features extraction; natural language processing; Forecasting; Language processing; Natural languages; Feature extraction; Neural networks; Predictive models; Neural-networks; Gender predictions; Online systems; neural networks; gender prediction; Neural network.; Probability distributions; Probability: distributions,,,emotion,Yes,Yes
scopus,ECMER: Edge-Cloud Collaborative Personalized Multimodal Emotion Recognition Framework in the Internet of Vehicles,"Zhang, P.; Fu, M.; Zhao, R.; Wu, D.; Zhang, H.; Yang, Z.; Wang, R.",2023,,37,,10.1109/MNET.003.2300012,"Real-time driver emotion recognition and timely risk warning can effectively reduce the incidence of traffic accidents. However, existing emotion recognition methods obtain emotion features from human physiological signals and are unsuitable for complex scenarios in the Internet of Vehicles (loV). Moreover, the existing methods in the IoV cannot fully use the resources of edge devices for mining the driver's personalities, resulting in limited accuracy. To address the problem, we propose a novel Edge-Cloud Collaborative Multimodal Emotion Recognition Framework (ECMER). The driver's facial expression and audio data are loaded to the edge for preliminary calculation, including coarse-grained facial expression recognition and driver's personality features extraction, which is uploaded to the cloud for cross-fusion. Specifically, a personality-coupled driver's emotion recognition method is proposed, and the Big Five Model is introduced from the psychological perspective. The facial expression features contained in images and audio features in videos are employed to calculate the driver's personality features, which are further fused with multimodal features. Subsequently, a hierarchical multi-granularity driver emotion recognition method is designed where the real-time coarsegran-ularity driver emotion recognition is conducted by edge devices to reduce the data transmission pressure and cloud computing load. The empirical results on real-world datasets demonstrate that the performance of driver emotion recognition under this architecture is improved. © 1986-2012 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Physiological signals; Audio data; Multimodal emotion recognition; Real- time; Recognition methods; Emotion feature; Edge clouds; Facial expression data; Risk warnings,,,emotion,No,Yes
scopus,Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant Features,"Augusma, A.; Vaufreydaz, D.; Letué, F.",2023,,,,10.1145/3577190.3616546,"This paper explores privacy-compliant group-level emotion recognition ""in-the-wild""within the EmotiW Challenge 2023. Group-level emotion recognition can be useful in many fields including social robotics, conversational agents, e-coaching and learning analytics. This research imposes itself using only global features avoiding individual ones, i.e. all features that can be used to identify or track people in videos (facial landmarks, body poses, audio diarization, etc.). The proposed multimodal model is composed of a video and an audio branches with a cross-attention between modalities. The video branch is based on a fine-tuned ViT architecture. The audio branch extracts Mel-spectrograms and feed them through CNN blocks into a transformer encoder. Our training paradigm includes a generated synthetic dataset to increase the sensitivity of our model on facial expression within the image in a data-driven way. The extensive experiments show the significance of our methodology. Our privacy-compliant proposal performs fairly on the EmotiW challenge, with 79.24% and 75.13% of accuracy respectively on validation and test set for the best models. Noticeably, our findings highlight that it is possible to reach this accuracy level with privacy-compliant features using only 5 frames uniformly distributed on the video.  © 2023 ACM.",Robotics; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Social robotics; Multimodal; Conversational agents; Global feature; Group emotion recognition in-the-wild.; Group emotions; Group level; Privacy safe; Transformer network; Transformer networks,,,emotion,No,Yes
scopus,Data augmentation and enhancement for multimodal speech emotion recognition,"Setyono, J.C.; Zahra, A.",2023,,12,,10.11591/eei.v12i5.5031,"Humans’ fundamental need is interaction with each other such as using conversation or speech. Therefore, it is crucial to analyze speech using computer technology to determine emotions. The speech emotion recognition (SER) method detects emotions in speech by examining various aspects. SER is a supervised method to decide the emotion class in speech. This research proposed a multimodal SER model using one of the deep learning based enhancement techniques, which is the attention mechanism. Additionally, this research addresses the imbalanced dataset problem in the SER field using generative adversarial networks (GAN) as a data augmentation technique. The proposed model achieved an excellent evaluation performance of 0.96 or 96% for the proposed GAN configuration. This work showed that the GAN method in the multimodal SER model could enhance performance and create a balanced dataset. © 2023, Institute of Advanced Engineering and Science. All rights reserved.",Speech emotion recognition; Deep learning; Attention mechanism; Transfer learning; Data augmentation; Imbalanced dataset; Pre-trained transformer model,,,emotion,No,Yes
scopus,Multimodal Prompt Learning in Emotion Recognition Using Context and Audio Information,"Jeong, E.; Kim, G.; Kang, S.",2023,,11,,10.3390/math11132908,"Prompt learning has improved the performance of language models by reducing the gap in language model training methods of pre-training and downstream tasks. However, extending prompt learning in language models pre-trained with unimodal data to multimodal sources is difficult as it requires additional deep-learning layers that cannot be attached. In the natural-language emotion-recognition task, improved emotional classification can be expected when using audio and text to train a model rather than only natural-language text. Audio information, such as voice pitch, tone, and intonation, can give more information that is unavailable in text to predict emotions more effectively. Thus, using both audio and text can enable better emotion prediction in speech emotion-recognition models compared to semantic information alone. In this paper, in contrast to existing studies that use multimodal data with an additional layer, we propose a method for improving the performance of speech emotion recognition using multimodal prompt learning with text-based pre-trained models. The proposed method is using text and audio information in prompt learning by employing a language model pre-trained on natural-language text. In addition, we propose a method to improve the emotion-recognition performance of the current utterance using the emotion and contextual information of the previous utterances for prompt learning in speech emotion-recognition tasks. The performance of the proposed method was evaluated using the English multimodal dataset MELD and the Korean multimodal dataset KEMDy20. Experiments using both the proposed methods obtained an accuracy of 87.49%, (Formula presented.) score of 44.16, and weighted (Formula presented.) score of 86.28. © 2023 by the authors.",multimodal; natural language processing; audio processing; prompt learning; speech emotion recognition,,,emotion,No,Yes
scopus,Annotations from speech and heart rate: impact on multimodal emotion recognition,"Sharma, K.; Chanel, G.",2023,,,,10.1145/3577190.3614165,"The focus of multimodal emotion recognition has often been on the analysis of several fusion strategies. However, little attention has been paid to the effect of emotional cues, such as physiological and audio cues, on external annotations used to generate the Ground Truths (GTs). In our study, we analyze this effect by collecting six continuous arousal annotations for three groups of emotional cues: speech only, heartbeat sound only and their combination. Our results indicate significant differences between the three groups of annotations, thus giving three distinct cue-specific GTs. The relevance of these GTs is estimated by training multimodal machine learning models to regress speech, heart rate and their multimodal fusion on arousal. Our analysis shows that a cue(s)-specific GT is better predicted by the corresponding modality(s). In addition, the fusion of several emotional cues for the definition of GTs allows to reach a similar performance for both unimodal models and multimodal fusion. In conclusion, our results indicates that heart rate is an efficient cue for the generation of a physiological GT; and that combining several emotional cues for GTs generation is as important as performing input multimodal fusion for emotion prediction.  © 2023 Owner/Author.",Machine learning; Emotion Recognition; Speech recognition; Modal analysis; Multi-modal fusion; Multimodal emotion recognition; Affective Computing; machine learning; multimodal fusion; Machine-learning; Dataset; dataset; affective computing; Annotation; annotations; Ground truth; Heart; Heart-rate; social cues; Social cues; social signals; Social signals,,,emotion,No,Yes
scopus,Graph to Grid: Learning Deep Representations for Multimodal Emotion Recognition,"Jin, M.; Li, J.",2023,,,,10.1145/3581783.3612074,"Multimodal emotion recognition based on electroencephalogram (EEG) and compensating physiological signals (e.g., eye tracking) has shown potential in the diagnosis and rehabilitation tracking of depression. Since the multi-channel EEG signals are generally processed as one-dimensional (1-D) graph-like features, existing approaches can only adopt underdeveloped shallow models to recognize emotions. However, these simple models have difficulty decoupling complex emotion patterns due to their limited representation capacity. To address this problem, we propose the graph-to-grid (G2G), a concise and plug-and-play module that transforms the 1-D graph-like data into the two-dimensional (2-D) grid-like data via the numerical relation coding. After that, the well developed deep models, e.g., ResNet can be used to downstream tasks. In addition, G2G simplifies the previous complex multimodal fusion into an input matrix augmentation operation, which greatly reduces the difficulty of model design and parameter tuning. Extensive results on three public datasets (SEED, SEED5 and MPED) indicate that the proposed approach achieves state-of-the-art emotion recognition accuracy in both unimodal and multimodal settings, with good cross-session generalization ability. G2G enables the development of more appropriate multimodal emotion recognition algorithms for follow-up studies. Our code is publicly available at https://github.com/Jinminbox/G2G. © 2023 Owner/Author.",Emotion Recognition; Speech recognition; Eye tracking; Physiological signals; Deep learning; Multimodal emotion recognition; multimodal emotion recognition; Electroencephalography; Biomedical signal processing; electroencephalogram; Electrophysiology; Complex emotions; Brain computer interface; brain-computer interface; Decouplings; Electroencephalogram signals; Eye-tracking; Multi channel; One-dimensional; Plug-and-play; Simple modeling,,,emotion,No,Yes
scopus,Audio-Visual Fusion for Emotion Recognition in the Valence-Arousal Space Using Joint Cross-Attention,"Gnana Praveen, R.; Cardinal, P.; Granger, E.",2023,,5,,10.1109/TBIOM.2022.3233083,"Automatic emotion recognition (ER) has recently gained much interest due to its potential in many real-world applications. In this context, multimodal approaches have been shown to improve performance (over unimodal approaches) by combining diverse and complementary sources of information, providing some robustness to noisy and missing modalities. In this paper, we focus on dimensional ER based on the fusion of facial and vocal modalities extracted from videos, where complementary audio-visual (A-V) relationships are explored to predict an individual's emotional states in valence-arousal space. Most state-of-the-art fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. To address this problem, we introduce a joint cross-attentional model for A-V fusion that extracts the salient features across A-V modalities, and allows to effectively leverage the inter-modal relationships, while retaining the intra-modal relationships. In particular, it computes the cross-attention weights based on correlation between the joint feature representation and that of individual modalities. Deploying the joint A-V feature representation into the cross-attention module helps to simultaneously leverage both the intra and inter modal relationships, thereby significantly improving the performance of the system over the vanilla cross-attention module. The effectiveness of our proposed approach is validated experimentally on challenging videos from the RECOLA and AffWild2 datasets. Results indicate that our joint cross-attentional A-V fusion model provides a cost-effective solution that can outperform state-of-the-art approaches, even when the modalities are noisy or absent. Code is available at https://github.com/praveena2j/Joint-Cross-Attention-for-Audio-Visual-Fusion. © 2019 IEEE.",Transformer; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Deep learning; Multi-modal fusion; Cross-attention; Features extraction; Computational modelling; multimodal fusion; Audio acoustics; Video; Predictive models; Cost effectiveness; Noise measurements; cross-attention; Dimensional emotion recognition; joint representation; Joint representation,,,emotion,No,Yes
scopus,THFN: Emotional health recognition of elderly people using a Two-Step Hybrid feature fusion network along with Monte-Carlo dropout,"Jothimani, S.; Premalatha, K.",2023,,86,,10.1016/j.bspc.2023.105116,"The influence of technology's progress on the ability to read people's emotions has received increased attention in recent years. It is essential to identify the elderly's feelings because it indicates mental wellness. This study proposes a unique approach to micro-information extraction using a cross-model attention mechanism and a two-step hybrid feature fusion Network (THFN). To predict feelings across many channels, the study suggests using a cross-model attention network. The model improves the accuracy of emotion prediction in the elderly by employing a fusion mechanism at both the feature level and the decision level. In addition, Monte-Carlo dropout (MCD) in the deep neural model is employed for improved classification and uncertainty prediction. As an activation function, the first-order derivative of Mish (Fmish) helps to boost classification accuracy. ElderReact is a brand-new multimodal dataset which is employed for senior citizens' responses to stimuli designed to evoke emotions. This research contributes to the emotional lives of the elderly, which helps to design better involvements to improve their mental health. The experimental results show that the proposed approach achieves a precision of 0.99, Recall of 0.99, F1-score of 1.00 and 99.8% accuracy, the sensitivity of 99.7% and specificity of 98.9% in emotion classification, and 100% accuracy in emotional health prediction. From the experimental results, it is proved that the proposed model performs better than the previously suggested models. © 2023 Elsevier Ltd",convolutional neural network; Elderly people; Attention; emotion; Attention mechanisms; Emotion Recognition; Multi-modal; Emotion recognition; human; Cross model; Multimodal; Article; controlled study; Features fusions; Hybrid features; Forecasting; mental health; Feature Fusion; aged; attention network; diagnostic test accuracy study; Elderly health; Elderly Health; emotional stability; measurement precision; Monte Carlo method; Monte Carlo methods; sensitivity and specificity; Technology progress,,,emotion,No,Yes
scopus,Multimodal emotion recognition using cross modal audio-video fusion with attention and deep metric learning,"Mocanu, B.; Tapu, R.; Zaharia, T.",2023,,133,,10.1016/j.imavis.2023.104676,"In the last few years, the multi-modal emotion recognition has become an important research issue in the affective computing community due to its wide range of applications that include mental disease diagnosis, human behavior understanding, human machine/robot interaction or autonomous driving systems. In this paper, we introduce a novel end-to-end multimodal emotion recognition methodology, based on audio and visual fusion designed to leverage the mutually complementary nature of features while maintaining the modality-specific information. The proposed method integrates spatial, channel and temporal attention mechanisms into a visual 3D convolutional neural network (3D-CNN) and temporal attention into an audio 2D convolutional neural network (2D-CNN) to capture the intra-modal features characteristics. Further, the inter-modal information is captured with the help of an audio-video (A-V) cross-attention fusion technique that effectively identifies salient relationships across the two modalities. Finally, by considering the semantic relations between the emotion categories, we design a novel classification loss based on an emotional metric constraint that guides the attention generation mechanisms. We demonstrate that by exploiting the relations between the emotion categories our method yields more discriminative embeddings, with more compact intra-class representations and increased inter-class separability. The experimental evaluation carried out on the RAVDESS (The Ryerson Audio-Visual Database of Emotional Speech and Song), and CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset) datasets validates the proposed methodology, which leads to average accuracy scores of 89.25% and 84.57%, respectively. In addition, when compared to state-of-the-art techniques, the proposed solution shows superior performances, with gains in accuracy ranging in the [1.72%, 11.25%] interval. © 2023",Semantics; Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Audio videos; Deep learning; Multimodal emotion recognition; Human robot interaction; Convolutional neural networks; Cross-modal; Convolution; Convolutional neural network; Diagnosis; Cross-modal fusion; Temporal attention; Channel attention; Emotional metric constraint; Spatial attention,,,emotion,No,Yes
scopus,Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition,"Li, B.; Fei, H.; Liao, L.; Zhao, Y.; Teng, C.; Chua, T.-S.; Ji, D.; Li, F.",2023,,,,10.1145/3581783.3612053,"It has been a hot research topic to enable machines to understand human emotions in multimodal contexts under dialogue scenarios, which is tasked with multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received consistent attention in recent years, where a diverse range of methods has been proposed for securing better task performance. Most existing works treat MM-ERC as a standard multimodal classification problem and perform multimodal feature disentanglement and fusion for maximizing feature utility. Yet after revisiting the characteristic of MM-ERC, we argue that both the feature multimodality and conversational contextualization should be properly modeled simultaneously during the feature disentanglement and fusion steps. In this work, we target further pushing the task performance by taking full consideration of the above insights. On the one hand, during feature disentanglement, based on the contrastive learning technique, we devise a Dual-level Disentanglement Mechanism (DDM) to decouple the features into both the modality space and utterance space. On the other hand, during the feature fusion stage, we propose a Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism (CRM) for multimodal and context integration, respectively. They together schedule the proper integrations of multimodal and context features. Specifically, CFM explicitly manages the multimodal feature contributions dynamically, while CRM flexibly coordinates the introduction of dialogue contexts. On two public MM-ERC datasets, our system achieves new state-of-the-art performance consistently. Further analyses demonstrate that all our proposed mechanisms greatly facilitate the MM-ERC task by making full use of the multimodal and context features adaptively. Note that our proposed methods have the great potential to facilitate a broader range of other conversational multimodal tasks. © 2023 ACM.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Multimodal features; Multi-modal learning; Human emotion; multimodal learning; Fusion mechanism; Context features; Hot research topics; Task performance,,,emotion,No,Yes
scopus,SMIN: Semi-Supervised Multi-Modal Interaction Network for Conversational Emotion Recognition,"Lian, Z.; Liu, B.; Tao, J.",2023,,14,,10.1109/TAFFC.2022.3141237,"Conversational emotion recognition is a crucial research topic in human-computer interactions. Due to the heavy annotation cost and inevitable label ambiguity, collecting large amounts of labeled data is challenging and expensive, which restricts the performance of current fully-supervised methods in this domain. To address this problem, researchers attempt to distill knowledge from unlabeled data via semi-supervised learning. However, most of these semi-supervised methods ignore multimodal interactive information, although recent works have proven that such interactive information is essential for emotion recognition. To this end, we propose a novel framework to seamlessly integrate semi-supervised learning with multimodal interactions, called 'Semi-supervised Multi-modal Interaction Network (SMIN)'. SMIN contains two well-designed semi-supervised modules, 'Intra-modal Interactive Module (IIM)' and 'Cross-modal Interactive Module (CIM)' to learn intra- and cross-modal interactions. These two modules leverage additional unlabeled data to extract emotion-salient representations. To capture additional contextual information, we utilize the hierarchical recurrent networks followed with the hybrid fusion strategy to integrate multimodal features. These multimodal features are further utilized for conversational emotion recognition. Experimental results on four benchmark datasets (i.e., IEMOCAP, MELD, CMU-MOSI and CMU-MOSEI) demonstrate that SMIN succeeds over existing state-of-the-art strategies on emotion recognition.  © 2010-2012 IEEE.",Semi-supervised learning; Emotion Recognition; Speech recognition; Emotion recognition; Cross-modal interaction; Modal interactions; Human computer interaction; Supervised learning; Multimodal Interaction; Interaction networks; conversational emotion recognition; Conversational emotion recognition; cross-modal interaction; Interactive computer systems; intra-modal interaction; Intra-modal interaction; Semi-supervised; semi-supervised learning; Semi-supervised multi-modal interaction network; Semi-supervised multi-modal interaction network (SMIN),,,emotion,Yes,Yes
scopus,Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation,"Anand, S.; Devulapally, N.K.; Bhattacharjee, S.D.; Yuan, J.",2023,,,,10.1145/3581783.3612517,"Evaluating speaker emotion in conversations is crucial for various applications requiring human-computer interaction. However, co-occurrences of multiple emotional states (e.g. 'anger' and 'frustration' may occur together or one may influence the occurrence of the other) and their dynamic evolution may vary dramatically due to the speaker's internal (e.g., influence of their personalized socio-cultural-educational and demographic backgrounds) and external contexts. Thus far, the previous focus has been on evaluating only the dominant emotion observed in a speaker at a given time, which is susceptible to producing misleading classification decisions for difficult multi-labels during testing. In this work, we present Self-supervised Multi- Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously. The proposed Multimodal Distillation Loss calibrates the fusion network by minimizing the Kullback-Leibler divergence with the peer networks. Additionally, each peer network is conditioned using a self-supervised contrastive objective to improve the generalization across diverse socio-demographic speaker backgrounds. By enabling peer collaborative learning that allows each network to independently learn their mode-specific discriminative patterns,SeMUL-PCD is effective across different conversation environments. In particular, the model not only outperforms the current state-of-the-art models on several large-scale public datasets (e.g., MOSEI, EmoReact and ElderReact), but with around 17% improved weighted F1-score in the cross-dataset experimental settings. The model also demonstrates an impressive generalization ability across age and demography-diverse populations. © 2023 ACM.",Transformer; Emotion Recognition; Emotional state; Multi-modal; Learning systems; Human computer interaction; Classification (of information); Emotion analysis; transformer; Large dataset; Distillation; emotion analysis; Population statistics; Multi-labels; Knowledge distillation; Collaborative learning; Co-occurrence; collaborative learning; knowledge distillation; multi-label classification; Multi-label classifications; Peer network; Population dynamics,,,emotion,Yes,Yes
scopus,"A Survey of Deep Learning-Based Multimodal Emotion Recognition: Speech, Text, and Face","Lian, H.; Lu, C.; Li, S.; Zhao, Y.; Tang, C.; Zong, Y.",2023,,25,,10.3390/e25101440,"Multimodal emotion recognition (MER) refers to the identification and understanding of human emotional states by combining different signals, including—but not limited to—text, speech, and face cues. MER plays a crucial role in the human–computer interaction (HCI) domain. With the recent progression of deep learning technologies and the increasing availability of multimodal datasets, the MER domain has witnessed considerable development, resulting in numerous significant research breakthroughs. However, a conspicuous absence of thorough and focused reviews on these deep learning-based MER achievements is observed. This survey aims to bridge this gap by providing a comprehensive overview of the recent advancements in MER based on deep learning. For an orderly exposition, this paper first outlines a meticulous analysis of the current multimodal datasets, emphasizing their advantages and constraints. Subsequently, we thoroughly scrutinize diverse methods for multimodal emotional feature extraction, highlighting the merits and demerits of each method. Moreover, we perform an exhaustive analysis of various MER algorithms, with particular focus on the model-agnostic fusion methods (including early fusion, late fusion, and hybrid fusion) and fusion based on intermediate layers of deep models (encompassing simple concatenation fusion, utterance-level interaction fusion, and fine-grained interaction fusion). We assess the strengths and weaknesses of these fusion strategies, providing guidance to researchers to help them select the most suitable techniques for their studies. In summary, this survey aims to provide a thorough and insightful review of the field of deep learning-based MER. It is intended as a valuable guide to aid researchers in furthering the evolution of this dynamic and impactful field. © 2023 by the authors.",deep learning; multimodal emotion recognition; survey; fusion method,,,emotion,No,No
scopus,DBT: multimodal emotion recognition based on dual-branch transformer,"Yi, Y.; Tian, Y.; He, C.; Fan, Y.; Hu, X.; Xu, Y.",2023,,79,,10.1007/s11227-022-05001-5,"There are very few labeled datasets in speech emotion recognition. The reason is that emotion is subjective and requires much time for labeling experts to identify emotion categories, while the wav2vec2.0 model is a general model for obtaining speech representations through self-supervised training. Therefore, we try to apply it to speech-emotion recognition tasks. We propose a multimodal dual-branch transformer network. For the speech processing branch, first, we use wav2vec2.0 to extract speech features. Then, a fine-tuning strategy and a self-attention-based interlayer feature fusion strategy are used. Second, a fully convolutional classification network is used for emotion classification. Then, we use RoBERTa for text emotion recognition and bimodal fusion by an improved weighted Dempster–Shafer (DS) strategy. In addition, we propose an accuracy-weighted label smoothing method, which can improve recognition accuracy. We perform comprehensive experiments on two benchmarks: IEMOCAP and CASIA, covering both Chinese and English datasets. The experimental results show that the proposed method has higher accuracy than state-of-the-art methods. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Speech emotion recognition; Emotion Recognition; Speech recognition; Character recognition; Multimodal emotion recognition; Speech processing; Fine tuning; Adaptive interlayer fusion; Dempster-shafer; Labeled dataset; Model fine-tuning; wav2vec2.0; Wav2vec2.0; Weighted dempste–shaf strategy; Weighted DS strategy; Weighted label smoothing,,,emotion,No,Yes
scopus,A Multi-task Model for Emotion and Offensive Aided Stance Detection of Climate Change Tweets,"Upadhyaya, A.; Fisichella, M.; Nejdl, W.",2023,,,,10.1145/3543507.3583860,"In this work, we address the United Nations Sustainable Development Goal 13: Climate Action by focusing on identifying public attitudes toward climate change on social media platforms such as Twitter. Climate change is threatening the health of the planet and humanity. Public engagement is critical to address climate change. However, climate change conversations on Twitter tend to polarize beliefs, leading to misinformation and fake news that influence public attitudes, often dividing them into climate change believers and deniers. Our paper proposes an approach to classify the attitude of climate change tweets (believe/deny/ambiguous) to identify denier statements on Twitter. Most existing approaches for detecting stances and classifying climate change tweets either overlook deniers' tweets or do not have a suitable architecture. The relevant literature suggests that emotions and higher levels of toxicity are prevalent in climate change Twitter conversations, leading to a delay in appropriate climate action. Therefore, our work focuses on learning stance detection (main task) while exploiting the auxiliary tasks of recognizing emotions and offensive utterances. We propose a multimodal multitasking framework MEMOCLiC that captures the input data using different embedding techniques and attention frameworks, and then incorporates the learned emotional and offensive expressions to obtain an overall representation of the features relevant to the stance of the input tweet. Extensive experiments conducted on a novel curated climate change dataset and two benchmark stance detection datasets (SemEval-2016 and ClimateStance-2022) demonstrate the effectiveness of our approach. © 2023 ACM.",Emotion Recognition; emotion recognition; Emotion recognition; Social networking (online); Social media platforms; climate change; Climate change; Climate models; Multi-task model; offensive language; Offensive languages; Polaris; Public attitudes; Public engagement; stance detection; Stance detection; Twitter; United Nations,,,emotion,No,No
scopus,MahaEmoSen: Towards Emotion-aware Multimodal Marathi Sentiment Analysis,"Chaudhari, P.; Nandeshwar, P.; Bansal, S.; Kumar, N.",2023,,22,,10.1145/3618057,"With the advent of the Internet, social media platforms have witnessed an enormous increase in user-generated textual and visual content. Microblogs on platforms such as Twitter are extremely useful for comprehending how individuals feel about a specific issue through their posted texts, images, and videos. Owing to the plethora of content generated, it is necessary to derive an insight of its emotional and sentimental inclination. Individuals express themselves in a variety of languages and, lately, the number of people preferring native languages has been consistently increasing. Marathi language is predominantly spoken in the Indian state of Maharashtra. However, sentiment analysis in Marathi has rarely been addressed. In light of the above, we propose an emotion-aware multimodal Marathi sentiment analysis method (MahaEmoSen). Unlike the existing studies, we leverage emotions embedded in tweets besides assimilating the content-based information from the textual and visual modalities of social media posts to perform a sentiment classification. We mitigate the problem of small training sets by implementing data augmentation techniques. A word-level attention mechanism is applied on the textual modality for contextual inference and filtering out noisy words from tweets. Experimental outcomes on real-world social media datasets demonstrate that our proposed method outperforms the existing methods for Marathi sentiment analysis in resource-constrained circumstances.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Multi-modal; Emotion; Modal analysis; Multi-modal data; Social media; Social networking (online); Sentiment analysis; Classification (of information); Social media platforms; User-generated; Data classification; emotions; Marathi; multimodal data classification; Multimodal data classification,,,emotion,No,No
scopus,Detection of Conversational Health in a Multimodal Conversation Graph by Measuring Emotional Concordance,"Suresh, K.; Patil, M.D.; Madhu, S.; Mahamuni, Y.; Das, B.",2023,,,,10.1145/3589572.3589588,"With the advent of social media and technology, the increased connections between individuals and organizations have led to a similar increase in the number of conversations. These conversations, in most cases are bimodal in nature, consisting of both images and text. Existing work in multimodal conversation typically focuses on individual utterances rather than the overall dialogue. The aspect of conversational health is important in many real world conversational uses cases including the emerging world of Metaverse. The work described in this paper investigates conversational health from the viewpoint of emotional concordance in bimodal conversations modelled as graphs. Using this framework, an existing multimodal dialogue dataset has been reformatted as a graph dataset that is labelled with the emotional concordance score. In this work, determination of conversational health has been framed as a graph classification problem. A graph neural network based model using algorithms such as Graph Convolution Network and Graph Attention Network is then used to detect the emotional concordance or discordance based upon the multimodal conversation that is provided. The model proposed in this paper achieves an overall F1 Score of 0.71 for equally sized class training and testing size, which offers improved results compared to previous models using the same benchmark dataset.  © 2023 ACM.",Emotion Recognition; Multi-modal; Emotion recognition; Social media; Multimodal; Statistical tests; Graph neural networks; AS graph; Graph; Graph classification; Graphs; Metaverses; Multimodal conversation; Real-world; Social technologies,,,emotion,Yes,Yes
scopus,Subject wise data augmentation based on balancing factor for quaternary emotion recognition through hybrid deep learning model,"Singh, K.; Ahirwal, M.K.; Pandey, M.",2023,,86,,10.1016/j.bspc.2023.105075,"An electroencephalogram (EEG) identifies neuronal activity as electrical currents produced by a group of specialized pyramidal cells within the brain due to synchronized activity. EEG signal contains essential information about brain activity and is often used to measure the enthusiasm of individual behavioural feelings and polarity. Recently, the researcher focused on classifying EEG emotions from multimodal DEAP data sets into binary (high and low) and ternary (low, medium and high) classes based on valence and arousal scales. However, for deep and intrinsic emotion recognition, multiclass classification is preferred. But due to the limited data sample and improper class distribution, multiclass classification is more challenging. This paper proposes a data augmentation-based hybrid deep learning model to classify EEG signals over four different emotional stages, i.e. happy (H), relaxed (R), anger (A) and sad (S). The hybrid model encapsulates a convolutional neural network (CNN) and bi-directional long short-term memory (Bi-LSTM) to recognize EEG samples’ emotions. Furthermore, the proposed model performs segmentation to enhance EEG data samples, and SMOTE (Synthetic minority oversampling technique) is committed to achieving alleviated skewness in EEG samples among four emotion classes. The experiments are conducted on all the data of 32 subjects before and after processing the data augmentation algorithm on the DEAP data. The average improvement in precision, recall, F1-score, accuracy, and specificity is 6.02%, 9.44%, 7.98%, 6.01%, and 2.66% respectively after data augmentation, and statistically varies with the balancing factor. © 2023 Elsevier Ltd",Long short-term memory; convolutional neural network; emotion; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; adult; Deep learning; female; human; male; Learning systems; Classification (of information); controlled study; Electroencephalography; Convolutional neural networks; Biomedical signal processing; electroencephalogram; Brain; Convolutional neural network; Data handling; article; short term memory; anger; multiclass classification; Classification; Data augmentation; Electroencephalogram signals; diagnostic test accuracy study; 1D-CNN; 1d-convolutional neural network; Arousal; Balancing factor; BI-LSTM; clinical article; EEG signal; recall; Valence,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition in Noisy Environment Based on Progressive Label Revision,"Li, S.; Lian, H.; Lu, C.; Zhao, Y.; Tang, C.; Zong, Y.; Zheng, W.",2023,,,,10.1145/3581783.3612867,"The multimodal emotion recognition has attracted more attention in recent decades. Though remarkable progress has been achieved with the rapid development of deep learning, existing methods are still hard to tackle noise problems that occurred commonly in emotion recognition's practical application. To improve the robustness of the multimodal emotion recognition algorithm, we propose an MLP-based label revision algorithm. The framework consists of three complementary feature extraction networks that were verified in MER2023. After that, an MLP-based attention network with specially designed loss functions was used to fuse features from different modalities. Finally, the scheme that used the output probability of each emotion to revise the sample's output category was employed to revise the test set's label obtained by classifier. The samples that are most likely to be affected by noise and misclassified have a chance to get correct classification. The best experimental result shows that the F1-score of our algorithm on the test dataset of the MER 2023 Noise subchallenge is 86.35 and combined metric is 0.6694, which ranks 2nd at the MER 2023 NOISE subchallenge. © 2023 ACM.",Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Classification (of information); multimodal emotion recognition; Features fusions; feature fusion; Statistical tests; Neural-networks; neural networks; label revision; Label revision; modality robustness; Modality robustness; Noise problems; Noisy environment; Recognition algorithm; Revision algorithms,,,emotion,Yes,Yes
scopus,A multimodal fusion-based deep learning framework combined with keyframe extraction and spatial and channel attention for group emotion recognition from videos,"Qi, S.; Liu, B.",2023,,26,,10.1007/s10044-023-01178-4,"Video-based group emotion recognition is an important research area in computer vision and is of great significance for the intelligent understanding of videos and human–computer interactions. Previous studies have adopted the traditional two-stage shallow pipeline to extract visual or audio features and train classifiers. A single feature or two are insufficient to comprehensively represent video information. In addition, sparse expression of emotions has not been addressed effectively. Therefore, in this study, we propose a novel deep convolutional neural networks (CNNs) architecture for video-based group emotion recognition that fuses multimodal feature information such as vision, audio, optical flow, and face. To address the problem of sparse emotional expressions in videos, we constructed an improved keyframe extraction algorithm for a visual stream to extract keyframes with more emotional features. A subnetwork incorporating spatial and channel attention was designed to automatically concentrate on the regions and channels carrying distinctive information in each keyframe to more accurately represent the emotional features of the visual stream. The proposed model was used to conduct extensive experiments on a video group affect dataset. It outperformed other video-based group emotion recognition methods. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Channel attention; Spatial attention; Keyframe extraction algorithm; Multimodal feature fusion; Video-based group emotion recognition,,,emotion,No,No
scopus,Multi-loop graph convolutional network for multimodal conversational emotion recognition,"Ren, M.; Huang, X.; Li, W.; Liu, J.",2023,,94,,10.1016/j.jvcir.2023.103846,"Emotion recognition in conversations (ERC) has gained increasing research attention in recent years due to its wide applications in a surge of emerging tasks, such as social media analysis, dialog generation, and recommender systems. Since constituent utterances in a conversation are closely semantic-related, the constituent utterances’ emotional states are also closely related. In our consideration, this correlation could serve as a guide for the emotion recognition of constituent utterances. Accordingly, we propose a novel approach named Semantic-correlation Graph Convolutional Network (SC-GCN) to take advantage of this correlation for the ERC task in multimodal scenario. Specifically, we first introduce a hierarchical fusion module to model the dynamics among the textual, acoustic and visual features and fuse the multimodal information. Afterward, we construct a graph structure based on the speaker and temporal dependency of the dialog. We put forward a novel multi-loop architecture to explore the semantic correlations by the self-attention mechanism and enhance the correlation information via multiple loops. Through the graph convolution process, the proposed SC-GCN finally obtains a refined representation of each utterance, which is used for the final prediction. Extensive experiments are conducted on two benchmark datasets and the experimental results demonstrate the superiority of our SC-GCN. © 2023 Elsevier Inc.",Semantics; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Multi-modal sentiment analyse; Sentiment analysis; Graphic methods; Convolution; Convolutional networks; Graph convolutional network; Conversational emotion recognition; Correlation graphs; Multi-modal sentiment analysis; Multiloop; Social media analysis,,,emotion,No,No
scopus,HAAN-ERC: hierarchical adaptive attention network for multimodal emotion recognition in conversation,"Zhang, T.; Tan, Z.; Wu, X.",2023,,35,,10.1007/s00521-023-08638-2,"Multimodal emotional expressions affect the progress of conversation in complex ways in our lives. For multimodal emotion recognition in conversation (ERC), previous studies focus on modeling partial influences of speaker and modality to infer emotion states in historical context based on traditional modeling units. However, with the tremendous success of Transformer in broad fields, how to effectively model intra- and inter-speaker, intra- and intermodal influences in historical dialog context based on Transformer is still not been tackled. In this paper, we propose a novel methodology HAAN-ERC, which hierarchically uses dialogue context information to model intra-speaker, inter-speaker, intra-modal, and intermodal influences to infer the emotional state of speakers. Meanwhile, we propose an adaptive attention mechanism, which can be trained in an end-to-end manner and automatically makes the unique decision for each speaker to omit redundant or valueless utterances from historical contexts in multiple hierarchies for adaptive fusion. The performance of HAAN-ERC is comprehensively evaluated on two popular multimodal ERC datasets of IEMOCAP and MELD, and achieves new state-of-the-art results. The encouraging results prove the validity of our HAAN-ERC. Our original codes will be publicly available at https://github.com/TAN-OpenLab/HAAN-ERC . © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Information fusion; Multi-modality; Multimodality; Context information; Context-based; Conversation; Emotional expressions; Novel methodology; Traditional models,,,emotion,No,Yes
scopus,EmotiW 2023: Emotion Recognition in the Wild Challenge,"Dhall, A.; Singh, M.; Goecke, R.; Gedeon, T.; Zeng, D.; Wang, Y.; Ikeda, K.",2023,,,,10.1145/3577190.3616545,"This paper describes the 9th Emotion Recognition in the Wild (EmotiW) challenge, which is being run as a grand challenge at the 25th ACM International Conference on Multimodal Interaction 2023. EmotiW challenge focuses on affect related benchmarking tasks and comprises of two sub-challenges: a) User Engagement Prediction in the Wild, and b) Audio-Visual Group-based Emotion Recognition. The purpose of this challenge is to provide a common platform for researchers from diverse domains. The objective is to promote the development and assessment of methods, which can predict engagement levels and/or identify perceived emotional well-being of a group of individuals in real-world circumstances. We describe the datasets, the challenge protocols and the accompanying sub-challenge.  © 2023 ACM.",Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Affective Computing; Multimodal Interaction; Audio-visual; Group emotions; Engagement; Interactive computer systems; Common platform; Grand Challenge; Group Emotions; Group-based; User engagement,,,emotion,No,No
scopus,Emotion recognition based on multiple physiological signals,"Li, Q.; Liu, Y.; Yan, F.; Zhang, Q.; Liu, C.",2023,,85,,10.1016/j.bspc.2023.104989,"Physiological signals can more realistically reflect human emotional states. To overcome the limitations imposed in single-modal emotion recognition, emotion recognition of multimodal physiological signals has received increasingly widespread attention. However, the original fusion models usually ignore the different distributions of multiple signals and how to capture complementary features from multimodal information effectively. This paper proposes an effective classification model for multimodal physiological signals to address the above issues based on modeling the heterogeneity and correlation between multimodal signals. First, differential entropy features are extracted from Electroencephalography (EEG) signals and peripheral physiological signals (PPS) such as Electrocardiographic (ECG) signals, Electromyographic (EMG) signals, and other physiological signals. Then, according to the different distributions and frequency characteristics of the acquired signals, the EEG signal features are made into a three-dimensional feature map and input to the neural network to extract the frequency spatial dimension features. Further temporal features are extracted from the peripheral physiological signals using a long and short-term memory network. Finally, the EEG and peripheral physiological signal features were fused and input to a multimodal long and short-term memory network to extract the association between different modalities and perform classification. The experiments were conducted on the benchmark DEAP dataset, and the results showed that the classification accuracy of the proposed model in this paper was 95.89% and 94.99% in the arousal dimension and the valence dimension, respectively, which were 2.77% and 3.11% higher compared to the unimodal EEG model, respectively. This paper also analyzed the effects of different peripheral physiological signals on emotion recognition. © 2023 Elsevier Ltd",emotion; Emotion Recognition; Emotional state; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; human; Classification (of information); Features extraction; Multimodal; controlled study; electroencephalography; Electroencephalography; Biomedical signal processing; EEG; Electrophysiology; human experiment; Brain; article; short term memory; electrocardiography; Peripheral physiological signal; Peripheral physiological signals; arousal; feature extraction; Feature extraction; Memory network; entropy; Signal features; Different distributions; Long and short term memory; signal transduction,,,emotion,No,Yes
scopus,Driver Emotion Recognition With a Hybrid Attentional Multimodal Fusion Framework,"Mou, L.; Zhao, Y.; Zhou, C.; Nakisa, B.; Rastgoo, M.N.; Ma, L.; Huang, T.; Yin, B.; Jain, R.; Gao, W.",2023,,14,,10.1109/TAFFC.2023.3250460,"Negative emotions may induce dangerous driving behaviors leading to extremely serious traffic accidents. Therefore, it is necessary to establish a system that can automatically recognize driver emotions so that some actions can be taken to avoid traffic accidents. Existing studies on driver emotion recognition have mainly used facial data and physiological data. However, there are fewer studies on multimodal data with contextual characteristics of driving. In addition, fully fusing multimodal data in the feature fusion layer to improve the performance of emotion recognition is still a challenge. To this end, we propose to recognize driver emotion using a novel multimodal fusion framework based on convolutional long-short term memory network (ConvLSTM), and hybrid attention mechanism to fuse non-invasive multimodal data of eye, vehicle, and environment. In order to verify the effectiveness of the proposed method, extensive experiments have been carried out on a dataset collected using an advanced driving simulator. The experimental results demonstrate the effectiveness of the proposed method. Finally, a preliminary exploration on the correlation between driver emotion and stress is performed. © 2010-2012 IEEE.",Long short-term memory; Attention mechanisms; Emotion Recognition; Speech recognition; Driver emotion recognition; Emotion recognition; Multi-modal data; Multi-modal fusion; Data mining; Features extraction; Attention mechanism; Brain; multimodal fusion; Convolution; Feature extraction; Anxiety disorder; driver emotion recognition; Accidents; convolutional long short term memory; Convolutional long short term memory; driver stress; Driver stress,,,emotion,No,Yes
scopus,Enhanced Emotion Recognition for Women and Children Safety Prediction using Deep Network,"Wagh, N.R.; Sutar, S.R.",2023,,11,,,"The most difficult research problem is ensuring the safety of women and children. Multimodal emotion recognition is a difficult task. One of the most important and widely used research domains in HCI is multimodal data, which includes audio, video, text, facial expression, body motions, bio-signals, and physiological data. This data is used to forecast the safety of women and children. Rigid research has been proposed in this context. To create the best multimodal model for emotion recognition combining picture, text, audio, and video modalities, a novel deep learning model is developed, and a thorough analysis of data, feature, and model-level fusion is undertaken. Separate innovative feature extractor networks are suggested specifically for picture, text, audio, and video data. Then, at the model level, an ideal multimodal emotion identification model is developed by combining information from images, text, voice, and video. Three benchmark multimodal datasets, IEMOCAP, Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion (SAVEE), are used to evaluate the performances of the suggested models. On the IEMOCAP, SAVEE, and RAVDESS datasets, the suggested models obtain high predicted accuracies of 96%, 97%, and 97%, respectively. By contrasting the outcomes with those of the current emotion recognition models, the models' efficacy and optimality are also confirmed. Women and Children Safety Prediction employs multimodal Enhanced emotion recognition. © 2023, Ismail Saritas. All rights reserved.",deep learning; multimodal; fusion; audio-visual media; Facial Expression Recognition; women safety,,,emotion,No,Yes
scopus,Negative emotion recognition using multimodal physiological signals for advanced driver assistance systems,"Hieida, C.; Yamamoto, T.; Kubo, T.; Yoshimoto, J.; Ikeda, K.",2023,,28,,10.1007/s10015-023-00858-y,"Recent advanced driver assistance systems’ (ADASs) control cars to avoid accidents, but few of them consider driver’s comfort. To realize comfortable driving, an ADAS must sense the driver’s emotions, especially when they are negative. Since emotions are reflected in a person’s physiological signals, they are informative for sensing emotions. However, it is unclear which signals are most useful for detecting a driver’s negative emotions. To examine the usefulness of each physiological signal, we implemented an emotion classifier (negative or non-negative) using sparse logistic regression for multimodal signals. This classifier was trained using a multimodal physiological signal dataset with negative emotion labels collected, while subjects were driving a vehicle. The resulting classifier successfully classifies emotions with an area under the curve of 0.74 and identifies the physiological signals that are useful for detecting negative emotions. © 2023, International Society of Artificial Life and Robotics (ISAROB).",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Classification (of information); Regression analysis; Accidents; Advanced driver assistance systems; Areas under the curves; Automobile drivers; Logistic regression; Logistics regressions; Negative emotion; Negative emotions; Non negatives; System control,,,emotion,Yes,No
scopus,CREATION AND ANALYSIS OF MULTIMODAL EMOTION RECOGNITION CORPUS WITH INDIAN ACTORS,"Anadkat, K.; Diwanji, H.; Modasiya, S.; Mehta, M.",2023,,18,,10.24412/1932-2321-2023-374-269-283,"Emotion recognition plays an important role in many real-life application areas of artificial intelligence like human-computer interactions, autism detection, stress and depression detection, measuring mental health, and suicide prevention. Emotion state of a person can be decided by the facial expression, tone of voice, words of speech, and body gestures when they are having a face-to-face conversation. People widely use social media platforms to post their feelings and mood through status. So, the status text can be used to identify the emotional state of a person. Physiological signals (EEG, ECG, and EDA) can identify the emotional state more accurately as people cannot be faked during the data collection but it is difficult to collect data. Many unimodal and multimodal datasets are publicly available but still, there is a strong need to create a multimodal dataset that consists of all the important modalities for the identification of emotional state. In this paper, first, we have reviewed all the available unimodal and multimodal datasets, then in the next section, we discuss the method to prepare the multimodal dataset. The data of four different modalities like facial expressions, audio, social media text, and EEG have been collected from seven different actors of different age groups and of different demographic regions. The dataset is non-spontaneous and contains discrete emotion labels like happy, sad, and angry. The procedure to create a dataset of different modalities include steps like capturing data, pre-processing, feature extraction, and storing to the relevant format. In last, to observe effect of different emotions, analysis of proposed multimodal database is carried out using efficient image, speech and text parameters. © 2023 The Author(s).",Emotion recognition; Unimodal; Multimodal; EEG; Feature extraction; Modality,,,emotion,Yes,No
scopus,Two Birds With One Stone: Knowledge-Embedded Temporal Convolutional Transformer for Depression Detection and Emotion Recognition,"Zheng, W.; Yan, L.; Wang, F.-Y.",2023,,14,,10.1109/TAFFC.2023.3282704,"Depression is a critical problem in modern society that affects an estimated 350 million people worldwide, causing feelings of sadness and a lack of interest and pleasure. Emotional disorders are gaining interest and are closely entwined with depression, because one contributes to an understanding of the other. Despite the achievements in the two separate tasks of emotion recognition and depression detection, there has not been much prior effort to build a unified model that can connect these two tasks with different modalities, including multimedia (text, audio, and video) and unobtrusive physiological signals (e.g., electroencephalography). We propose a novel temporal convolutional transformer with knowledge embedding to address the joint task of depression detection and emotion recognition. This approach not only learns multimodal embeddings across domains via the temporal convolutional transformer but also exploits special-domain knowledge from medical knowledge graphs to improve the performance of detection and recognition. It is essential that the features learned by our method can be perceived as a priori and are suitable for increasing the performance of other related tasks. Our method illustrates the case of 'two birds with one stone' in the sense that two or more tasks can be efficiently handled with our unique model, which captures effective features. Experimental results on ten real-world datasets show that the proposed approach significantly outperforms other state-of-the-art approaches. On the other hand, experiments in which our methodology is applied to other reasoning tasks show that our approach effectively supports model reasoning related to emotion and improves its performance.  © 2010-2012 IEEE.",Depression; Transformer; Emotion Recognition; Embeddings; Multi-modal; Speech recognition; Emotion recognition; Character recognition; Multimodal emotion recognition; Task analysis; multimodal emotion recognition; Electroencephalography; Electrophysiology; Physiological models; Brain modeling; Convolution; transformer; Birds; Domain Knowledge; joint learning; Joint learning; knowledge embedding; Knowledge embedding; Multimodal depression detection,,,emotion,No,Yes
scopus,Uni2Mul: A Conformer-Based Multimodal Emotion Classification Model by Considering Unimodal Expression Differences with Multi-Task Learning,"Zhang, L.; Liu, C.; Jia, N.",2023,,13,,10.3390/app13179910,"Multimodal emotion classification (MEC) has been extensively studied in human–computer interaction, healthcare, and other domains. Previous MEC research has utilized identical multimodal annotations (IMAs) to train unimodal models, hindering the learning of effective unimodal representations due to differences between unimodal expressions and multimodal perceptions. Additionally, most MEC fusion techniques fail to consider the unimodal–multimodal inconsistencies. This study addresses two important issues in MEC: learning satisfactory unimodal representations of emotion and accounting for unimodal–multimodal inconsistencies during the fusion process. To tackle these challenges, the authors propose the Two-Stage Conformer-based MEC model (Uni2Mul) with two key innovations: (1) in stage one, unimodal models are trained using independent unimodal annotations (IUAs) to optimize unimodal emotion representations; (2) in stage two, a Conformer-based architecture is employed to fuse the unimodal representations learned in stage one and predict IMAs, accounting for unimodal–multimodal differences. The proposed model is evaluated on the CH-SIMS dataset. The experimental results demonstrate that Uni2Mul outperforms baseline models. This study makes two key contributions: (1) the use of IUAs improves unimodal learning; (2) the two-stage approach addresses unimodal–multimodal inconsistencies during Conformer-based fusion. Uni2Mul advances MEC by enhancing unimodal representation learning and Conformer-based fusion. © 2023 by the authors.",conformer; multi-task; multimodal emotion classification; pre-trained model,,,emotion,No,No
scopus,The Emotions of the Crowd: Learning Image Sentiment from Tweets via Cross-Modal Distillation,"Serra, A.; Carrara, F.; Tesconi, M.; Falchi, F.",2023,,372,,10.3233/FAIA230503,"Trends and opinion mining in social media increasingly focus on novel interactions involving visual media, like images and short videos, in addition to text. In this work, we tackle the problem of visual sentiment analysis of social media images - specifically, the prediction of image sentiment polarity. While previous work relied on manually labeled training sets, we propose an automated approach for building sentiment polarity classifiers based on a cross-modal distillation paradigm; starting from scraped multimodal (text + images) data, we train a student model on the visual modality based on the outputs of a textual teacher model that analyses the sentiment of the corresponding textual modality. We applied our method to randomly collected images crawled from Twitter over three months and produced, after automatic cleaning, a weakly-labeled dataset of ~1.5 million images. Despite exploiting noisy labeled samples, our training pipeline produces classifiers showing strong generalization capabilities and outperforming the current state of the art on five manually labeled benchmarks for image sentiment polarity prediction. © 2023 The Authors.",Multi-modal; Social media; Social networking (online); Sentiment analysis; Classification (of information); Cross-modal; Distillation; Image analysis; Training sets; Media images; Automated approach; Opinion mining; Trend minings; Visual media,,,emotion,Yes,No
scopus,Emotion Expression Estimates to Measure and Improve Multimodal Social-Affective Interactions,"Brooks, J.A.; Tiruvadi, V.; Baird, A.; Tzirakis, P.; Li, H.; Gagne, C.; Oh, M.; Cowen, A.",2023,,,,10.1145/3610661.3616129,"Large language models (LLMs) are being adopted in a wide range of applications, but an understanding of other social-affective signals is needed to support effective human-computer-interaction (HCI) in multimodal interfaces. In particular, robust, accurate measurements of human emotional expression can be used to tailor responses to human values and preferences. In this paper, we present two models available from an API-based suite of emotional expression models that measure nuanced facial and vocal signals, providing rich, high-dimensional emotional expression estimates (EEEs). We demonstrate the ability of EEEs to provide insight into two established datasets and present methods for integrating EEEs into large language model (LLM) applications. We discuss how this approach is a step towards more reliable tools for clinical screening and scientific study, as well as empathic digital assistants that can be used in therapeutic settings. © 2023 ACM.",Emotion Recognition; Multi-modal; Emotion recognition; Affective Computing; Human computer interaction; Sentiment analysis; Multimodal sentiment analyse; Large dataset; Mental health; Mental Health; Emotion expression; Computational linguistics; Language model; Multimodal Sentiment Analysis; Economic and social effects; Emotional expressions; Emotion science; Emotion Science,,,emotion,No,No
scopus,Impact of lockdown on Generation-Z: a fuzzy based multimodal emotion recognition approach using CNN,"Hore, S.; Bhattacharya, T.",2023,,82,,10.1007/s11042-023-14543-6,"The primary direction of most of the research done so far on the effects of Lockdown due to pandemic have been limited to areas such as clinical studies, possible impact on the global economy, or issues related to migrant workers. However, during this period, little attempt has been made to understand the emotions of Generation Z, one of the prime victims of this pandemic. Members of this generation were born after 1996. So, most of them are studying in various schools, colleges, or universities. In the proposed work, the emotions of some students of an engineering college in West Bengal, India, have been analyzed. A multimodal approach has been applied to obtain vivid pictures of 74 students’ minds. The valence-arousal inspired Organize-Split-Fuse (OSF) model has been proposed to achieve this objective. Two conventional Convolutional Neural Network (CNN) models have been employed separately, to classify human emotions using Acoustic Information (AcI) and Facial Expressions (FE) from the generated dataset. The employed models have achieved satisfactory performance (91% and 72.7% accuracy respectively) on the benchmark dataset. Afterward, classified emotions have been organized and split successfully. Finally, a fuzzy rule-based classification system has been used to fuse both emotions at the decision level. The results show that junior students have higher positivity and less Neutral emotions than the senior. In-depth analysis shows that boys are more apprehensive than girls while girls have a more optimistic outlook for the future. The year-wise observations show the chaotic state of students’ minds. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Facial expressions; Emotion Recognition; CNN; Emotion; Facial Expressions; Multimodal emotion recognition; Classification (of information); Benchmarking; Convolutional neural network; Economic and social effects; Students; Acoustic information; Clinical research; Clinical study; Fuzzy; Fuzzy inference; Generation-Z; Global economies; Global issues; Locks (fasteners),,,emotion,No,Yes
scopus,GraphMFT: A graph network based multimodal fusion technique for emotion recognition in conversation,"Li, J.; Wang, X.; Lv, G.; Zeng, Z.",2023,,550,,10.1016/j.neucom.2023.126427,"Multimodal machine learning is an emerging area of research, which has received a great deal of scholarly attention in recent years. Up to now, there are few studies on multimodal Emotion Recognition in Conversation (ERC). Since Graph Neural Networks (GNNs) possess the powerful capacity of relational modeling, they have an inherent advantage in the field of multimodal learning. GNNs leverage the graph constructed from multimodal data to perform intra- and inter-modal information interaction, which effectively facilitates the integration and complementation of multimodal data. In this work, we propose a novel Graph network based Multimodal Fusion Technique (GraphMFT) for emotion recognition in conversation. Multimodal data can be modeled as a graph, where each data object is regarded as a node, and both intra- and inter-modal dependencies existing between data objects can be regarded as edges. GraphMFT utilizes multiple improved graph attention networks to capture intra-modal contextual information and inter-modal complementary information. In addition, the proposed GraphMFT attempts to address the challenges of existing graph-based multimodal conversational emotion recognition models such as MMGCN. Empirical results on two public multimodal datasets reveal that our model outperforms the State-Of-The-Art (SOTA) approaches with the accuracy of 67.90% and 61.30%. © 2023 Elsevier B.V.",Machine learning; emotion; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; artificial neural network; human; Multi-modal fusion; Network-based; Graph networks; machine learning; human experiment; Graphic methods; Multimodal fusion; Machine-learning; article; conversation; Emotion recognition in conversation; Multimodal machine learning; Graph neural networks; Fusion techniques; attention network,,,emotion,No,Yes
scopus,An Assessment of In-the-Wild Datasets for Multimodal Emotion Recognition,"Aguilera, A.; Mellado, D.; Rojas, F.",2023,,23,,10.3390/s23115184,"Multimodal emotion recognition implies the use of different resources and techniques for identifying and recognizing human emotions. A variety of data sources such as faces, speeches, voices, texts and others have to be processed simultaneously for this recognition task. However, most of the techniques, which are based mainly on Deep Learning, are trained using datasets designed and built in controlled conditions, making their applicability in real contexts with real conditions more difficult. For this reason, the aim of this work is to assess a set of in-the-wild datasets to show their strengths and weaknesses for multimodal emotion recognition. Four in-the-wild datasets are evaluated: AFEW, SFEW, MELD and AffWild2. A multimodal architecture previously designed is used to perform the evaluation and classical metrics such as accuracy and F1-Score are used to measure performance in training and to validate quantitative results. However, strengths and weaknesses of these datasets for various uses indicate that by themselves they are not appropriate for multimodal recognition due to their original purpose, e.g., face or speech recognition. Therefore, we recommend a combination of multiple datasets in order to obtain better results when new samples are being processed and a good balance in the number of samples by class. © 2023 by the authors.",Emotion Recognition; Performance; Speech recognition; Deep learning; Deep Learning; Multimodal emotion recognition; multimodal emotion recognition; F1 scores; Data-source; Condition; Controlled conditions; In-the-wild dataset; in-the-wild datasets; Multimodal architectures; Recognizing Human Emotion,,,emotion,No,Yes
scopus,Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation,"Zou, S.; Huang, X.; Shen, X.",2023,,,,10.1145/3581783.3611805,"Emotion Recognition in Conversation (ERC) plays an important role in driving the development of human-machine interaction. Emotions can exist in multiple modalities, and multimodal ERC mainly faces two problems: (1) the noise problem in the cross-modal information fusion process, and (2) the prediction problem of less sample emotion labels that are semantically similar but different categories. To address these issues and fully utilize the features of each modality, we adopted the following strategies: first, deep emotion cues extraction was performed on modalities with strong representation ability, and feature filters were designed as multimodal prompt information for modalities with weak representation ability. Then, we designed a Multimodal Prompt Transformer (MPT) to perform cross-modal information fusion. MPT embeds multimodal fusion information into each attention layer of the Transformer, allowing prompt information to participate in encoding textual features and being fused with multi-level textual information to obtain better multimodal fusion features. Finally, we used the Hybrid Contrastive Learning (HCL) strategy to optimize the model's ability to handle labels with few samples. This strategy uses unsupervised contrastive learning to improve the representation ability of multimodal fusion and supervised contrastive learning to mine the information of labels with few samples. Experimental results show that our proposed model outperforms state-of-the-art models in ERC on two benchmark datasets. © 2023 ACM.",Transformer; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multi-modal fusion; Learning systems; Cross-modal; transformer; Information fusion; Emotion recognition in conversation; Multiple modalities; Human machine interaction; emotion recognition in conversation; hybrid contrastive learning; Hybrid contrastive learning; multimodal prompt information; Multimodal prompt information; Noise pollution,,,emotion,No,Yes
scopus,Multimodal Facial Emotion Recognition Using Improved Convolution Neural Networks Model,"Udeh, C.P.; Chen, L.; Du, S.; Li, M.; Wu, M.",2023,,27,,10.20965/jaciii.2023.p0710,"In the quest for human-robot interaction (HRI), leading to the development of emotion recognition, learning, and analysis capabilities, robotics plays a significant role in human perception, attention, decision-making, and social communication. However, the accurate recognition of emotions in HRI remains a challenge. This is due to the coexistence of multiple sources of information in utilizing multimodal facial expressions and head poses as multiple convolutional neural networks (CNN) and deep learning are combined. This research analyzes and improves the robustness of emotion recognition, and proposes a novel approach that optimizes traditional deep neural networks that fall into poor local optima when optimizing the weightings of the deep neural network using standard methods. The proposed approach adaptively finds the better weightings of the network, resulting in a hybrid genetic algorithm with stochastic gradient descent (HGASGD). This hybrid algorithm combines the inherent, implicit parallelism of the genetic algorithm with the better global optimization of stochastic gradient descent (SGD). An experiment shows the effectiveness of our proposed approach in providing complete emotion recognition through a combination of multimodal data, CNNs, and HGASGD, indicating that it represents a powerful tool in achieving interactions between humans and robotics. To validate and test the effectiveness of our proposed approach through experiments, the performance and reliability of our approach and two variants of HGASGD FER are compared using a large dataset of facial images. Our approach integrates multimodal information from facial expressions and head poses, enabling the system to recognize emotions better. The results show that CNN-HGASGD outperforms CNNs-SGD and other existing state-of-the-art methods in terms of FER. © Fuji Technology Press Ltd.",facial expression; Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Face recognition; Facial Expressions; Human robot interaction; Decision making; Convolution; Convolutional neural network; Statistical tests; Facial emotions; Humans-robot interactions; Head pose; convolution neural network; Convolution neural network; Genetic algorithms; Global optimization; head pose; Hybrid genetic algorithms; stochastic gradient descent; Stochastic gradient descent; Stochastic models; Stochastic systems,,,emotion,No,Yes
scopus,Empirical study of emotional state recognition using multimodal fusion framework,"Singh, M.K.",2023,,,,10.1145/3591156.3591160,"Recent trends towards emotion recognition are not only limited to text, audio and visual modalities; instead, physiological signals are tempting attention of many researchers in challenging field of human-computer interaction. In this paper, an empirical analysis is conducted with static patterns of physiological signals over time for human emotional state using machine learning approach. It is also investigated in this study that which brain lobe is most efficient to emotions. Here, subjective ratings are transformed to 3-dimensional VAD space which are then grouped into five discrete emotion labels using three clustering techniques via k-means, k-medoids and fuzzy c-means. Various features are extracted from EEG and other peripheral signals separately and are validated with the labels obtained from these clustering mechanisms using traditional classification algorithms. The aim of this study is to evaluate the performance of EEG signals and peripheral signals individually and analyze the results when these modalities are fused together. It is observed that the results obtained with multimodal fusion acquires the highest accuracy of 91.90% with 0.98 AUC score for ensemble subspace-KNN classifier when validated using clustered labels of fuzzy c-means via city-block metric. The study is conducted on DEAP dataset using 32 subjects and 15 named emotions only. It is also observed that temporal regions of brain are mostly correlated to emotions. Hence, these temporal EEG channels can be utilized for human emotion recognition towards human-computer interaction.  © 2023 ACM.",Emotion Recognition; Emotional state; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Physiology; Character recognition; Multi-modal fusion; Human computer interaction; Biomedical signal processing; Physiological models; Multimodal fusion; Emotion models; physiological signals; EEG signals; Fuzzy systems; K-means clustering; Cluster analysis; clustering techniques; Clustering techniques; discrete emotion model; Discrete emotion model; distance metric; Distance metrics; Peripheral signal; peripheral signals,,,emotion,Yes,Yes
scopus,Real-time Emotion Pre-Recognition in Conversations with Contrastive Multi-modal Dialogue Pre-training,"Ju, X.; Zhang, D.; Zhu, S.; Li, J.; Li, S.; Zhou, G.",2023,,,,10.1145/3583780.3615024,"This paper presents our pioneering effort in addressing a new and realistic scenario in multi-modal dialogue systems called Multi-modal Real-time Emotion Pre-recognition in Conversations (MREPC). The objective is to predict the emotion of a forthcoming target utterance that is highly likely to occur. We believe that this task can enhance the dialogue system's understanding of the interlocutor's state of mind, enabling it to prepare an appropriate response in advance. However, addressing MREPC poses the following challenges: 1) Previous studies on emotion elicitation typically focus on textual modality and perform sentiment forecasting within a fixed contextual scenario. 2) Previous studies on multi-modal emotion recognition aim to predict the emotion of existing utterances, making it difficult to extend these approaches to MREPC due to the absence of the target utterance. To tackle these challenges, we construct two benchmark multi-modal datasets for MREPC and propose a task-specific multi-modal contrastive pre-training approach. This approach leverages large-scale unlabeled multi-modal dialogues to facilitate emotion pre-recognition for potential utterances of specific target speakers. Through detailed experiments and extensive analysis, we demonstrate that our proposed multimodal contrastive pre-training architecture effectively enhances the performance of multi-modal real-time emotion pre-recognition in conversations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Emotion elicitation; Speech processing; Forecasting; Contrastive learning; Real- time; Dialogue systems; Pre-training; Real time systems; multi-modal; Interactive computer systems; Conversation; contrastive learning; conversations; emotion pre-recognition; Emotion pre-recognition; Realistic scenario,,,emotion,No,Yes
scopus,MMATERIC: Multi-Task Learning and Multi-Fusion for AudioText Emotion Recognition in Conversation,"Liang, X.; Zou, Y.; Zhuang, X.; Yang, J.; Niu, T.; Xu, R.",2023,,12,,10.3390/electronics12071534,"The accurate recognition of emotions in conversations helps understand the speaker’s intentions and facilitates various analyses in artificial intelligence, especially in human–computer interaction systems. However, most previous methods need more ability to track the different emotional states of each speaker in a dialogue. To alleviate this dilemma, we propose a new approach, Multi-Task Learning and Multi-Fusion AudioText Emotion Recognition in Conversation (MMATERIC) for emotion recognition in conversation. MMATERIC can refer to and combine the benefits of two distinct tasks: emotion recognition in text and emotion recognition in speech, and production of fused multimodal features to recognize the emotions of different speakers in dialogue. At the core of MATTERIC are three modules: an encoder with multimodal attention, a speaker emotion detection unit (SED-Unit), and a decoder with speaker emotion detection Bi-LSTM (SED-Bi-LSTM). Together, these three modules model the changing emotions of a speaker at a given moment in a conversation. Meanwhile, we adopt multiple fusion strategies in different stages, mainly using model fusion and decision stage fusion to improve the model’s accuracy. Simultaneously, our multimodal framework allows features to interact across modalities and allows potential adaptation flows from one modality to another. Our experimental results on two benchmark datasets show that our proposed method is effective and outperforms the state-of-the-art baseline methods. The performance improvement of our method is mainly attributed to the combination of three core modules of MATTERIC and the different fusion methods we adopt in each stage. © 2023 by the authors.",multimodal fusion; emotion recognition in conversation; multi-task learning; Multi-Task Learning and Multi-Fusion for AudioText Emotion Recognition in Conversation,,,emotion,No,Yes
scopus,MMFN: Emotion recognition by fusing touch gesture and facial expression information,"Li, Y.-K.; Meng, Q.-H.; Wang, Y.-X.; Hou, H.-R.",2023,,228,,10.1016/j.eswa.2023.120469,"Human social interaction is a multimodal process that integrates several communication channels, such as speech, facial expressions, body gestures, and touch. For intelligent robots, the ability to recognize human emotions based on these channels helps in realizing natural human-robot interaction. The existing works on multimodal emotion recognition (MER) mainly focus on facial expression, speech, text, and electrophysiological signals. However, there are few works on the implementation of MER based on touch. In this work, we established a facial expression and touch gesture emotion (FETE) dataset comprising six basic discrete emotions, which was made publicly available for the first time. In addition, a multi-eigenspace based multimodal fusion network (MMFN) was proposed for tactile-visual bimodal emotion recognition. The proposed MMFN projects the tactile and visual modalities in multi-eigenspace to learn the specific and shared representations and employs multiple classifiers to decode these features. We demonstrated the effectiveness of our network through experiments on the FETE dataset. The tactile-visual dual-modal emotion recognition using MMFN achieved an accuracy of 80.18% and an improvement of approximately 15% over the single-modality results. © 2023 Elsevier Ltd",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Facial expression; Facial Expressions; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Human robot interaction; Electrophysiology; Speech communication; Humans-robot interactions; Intelligent robots; Human social interactions; Gesture recognition; Bimodal emotion recognition; Eigenspaces; Human–robot interaction; Multi-eigenspace; Touch gesture,,,emotion,No,Yes
scopus,Exploring the Contextual Factors Affecting Multimodal Emotion Recognition in Videos,"Bhattacharya, P.; Gupta, R.K.; Yang, Y.",2023,,14,,10.1109/TAFFC.2021.3071503,"Emotional expressions form a key part of user behavior on today's digital platforms. While multimodal emotion recognition techniques are gaining research attention, there is a lack of deeper understanding on how visual and non-visual features can be used to better recognize emotions in certain contexts, but not others. This study analyzes the interplay between the effects of multimodal emotion features derived from facial expressions, tone and text in conjunction with two key contextual factors: i) gender of the speaker, and ii) duration of the emotional episode. Using a large public dataset of 2,176 manually annotated YouTube videos, we found that while multimodal features consistently outperformed bimodal and unimodal features, their performance varied significantly across different emotions, gender and duration contexts. Multimodal features performed particularly better for male speakers in recognizing most emotions. Furthermore, multimodal features performed particularly better for shorter than for longer videos in recognizing neutral and happiness, but not sadness and anger. These findings offer new insights towards the development of more context-aware emotion recognition and empathetic systems.  © 2010-2012 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Affective Computing; Sentiment analysis; Features extraction; Affective computing; sentiment analysis; Large dataset; Video; Feature extraction; Human emotion; Digital devices; Multi-modal recognition; affect sensing and analysis; Affect sensing and analysis; Device for affective computing; High performance computing; Modeling human emotion; modelling human emotions; multi-modal recognition; Performance computing; Technology &; technology & devices for affective computing,,,emotion,Yes,Yes
scopus,Augmenting ECG Data with Multiple Filters for a Better Emotion Recognition System,"Hasnul, M.A.; Ab. Aziz, N.A.; Abd. Aziz, A.",2023,,48,,10.1007/s13369-022-07585-9,"A physiological-based emotion recognition system (ERS) with a unimodal approach such as an electrocardiogram (ECG) is not as popular compared to a multimodal approach. However, a single modality has the advantage of lower development and computational cost. Therefore, this study focuses on a unimodal ECG-based ERS. The ECG-based ERS has the potential to become the next mass-adopted consumer application due to the wide availability of wearable and mobile ECG devices in the market. Currently, ECG-inclusive affective datasets are limited, and many of the existing datasets have small sample sizes. Hence, ECG-based ERS studies are stunted by the lack of quality data. A novel multi-filtering augmentation technique is proposed here to increase the sample size of the ECG data. This technique augments the ECG signals by cleaning the data in different ways. Three small ECG datasets labelled according to emotion state are used in this study. The benefit of the proposed augmentation techniques is measured using the classification accuracy of five machine learning algorithms; k-nearest neighbours (KNN), support vector machine, decision tree, random forest and multilayer perceptron. The results show that with the proposed technique, there is a significant improvement in performance for all the datasets and classifiers. KNN classifier improved the most with the augmented data and the reported classification accuracies of over 90%. © 2023, King Fahd University of Petroleum & Minerals.",Machine learning; Emotion recognition; Affective computing; Electrocardiogram; Augmentation; Filter,,,emotion,Yes,Yes
scopus,"A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations","Zhang, Y.; Wang, J.; Liu, Y.; Rong, L.; Zheng, Q.; Song, D.; Tiwari, P.; Qin, J.",2023,,93,,10.1016/j.inffus.2023.01.005,"Sarcasm, sentiment and emotion are tightly coupled with each other in that one helps the understanding of another, which makes the joint recognition of sarcasm, sentiment and emotion in conversation a focus in the research in artificial intelligence (AI) and affective computing. Three main challenges exist: Context dependency, multimodal fusion and multitask interaction. However, most of the existing works fail to explicitly leverage and model the relationships among related tasks. In this paper, we aim to generically address the three problems with a multimodal joint framework. We thus propose a multimodal multitask learning model based on the encoder–decoder architecture, termed M2Seq2Seq. At the heart of the encoder module are two attention mechanisms, i.e., intramodal (Ia) attention and intermodal (Ie) attention. Ia attention is designed to capture the contextual dependency between adjacent utterances, while Ie attention is designed to model multimodal interactions. In contrast, we design two kinds of multitask learning (MTL) decoders, i.e., single-level and multilevel decoders, to explore their potential. More specifically, the core of a single-level decoder is a masked outer-modal (Or) self-attention mechanism. The main motivation of Or attention is to explicitly model the interdependence among the tasks of sarcasm, sentiment and emotion recognition. The core of the multilevel decoder contains the shared gating and task-specific gating networks. Comprehensive experiments on four bench datasets, MUStARD, Memotion, CMU-MOSEI and MELD, prove the effectiveness of M2Seq2Seq over state-of-the-art baselines (e.g., CM-GCN, A-MTL) with significant improvements of 1.9%, 2.0%, 5.0%, 0.8%, 4.3%, 3.1%, 2.8%, 1.0%, 1.7% and 2.8% in terms of Micro F1. © 2023 Elsevier B.V.",Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Learning systems; Signal encoding; Affective Computing; Sentiment analysis; Affective computing; Learning models; Multitask learning; Multilevels; Decoding; Linearization; Multimodal sarcasm recognition; Single level,,,emotion,No,Yes
scopus,M2ER: Multimodal Emotion Recognition Based on Multi-Party Dialogue Scenarios,"Zhang, B.; Yang, X.; Wang, G.; Wang, Y.; Sun, R.",2023,,13,,10.3390/app132011340,"Researchers have recently focused on multimodal emotion recognition, but issues persist in recognizing emotions in multi-party dialogue scenarios. Most studies have only used text and audio modality, ignoring the video modality. To address this, we propose M2ER, a multimodal emotion recognition scheme based on multi-party dialogue scenarios. Addressing the issue of multiple faces appearing in the same frame of the video modality, M2ER introduces a method using multi-face localization for speaker recognition to eliminate the interference of non-speakers. The attention mechanism is used to fuse and classify different modalities. We conducted extensive experiments in unimodal and multimodal fusion using the multi-party dialogue dataset MELD. The results show that M2ER achieves superior emotion recognition in both text and audio modalities compared to the baseline model. The proposed method using speaker recognition in the video modality improves emotion recognition performance by 6.58% compared to the method without speaker recognition. In addition, the multimodal fusion based on the attention mechanism also outperforms the baseline fusion model. © 2023 by the authors.",attention mechanism; emotion recognition; multimodal; feature extraction; speaker recognition; feature-level fusion,,,emotion,No,Yes
scopus,Emotion recognition framework using multiple modalities for an effective human–computer interaction,"Moin, A.; Aadil, F.; Ali, Z.; Kang, D.",2023,,79,,10.1007/s11227-022-05026-w,"Human emotions are subjective reactions to objects or events that are related to diverse physiological, behavioral and intellectual changes. The research community is gaining more interest in emotion recognition due to its vast applications including human–computer interaction, virtual reality, self-driving, digital content entertainment, human behavior monitoring, and medicine. Electroencephalogram (EEG) signals that are collected from the brain are playing a massive part in the advancement of brain–computer interface systems. The current techniques that are using EEG signals for emotion recognition are lacking in subject-independent or cross-subject emotion analysis. Additionally, there is a lack of multimodal approaches that combine EEG data with other modalities. In view of the stated deficiencies, this study presents an efficient multimodal strategy for cross-subject emotion recognition utilizing EEG and facial gestures. The proposed method fuses the spectral and statistical features extracted from the EEG data with a histogram of oriented gradients and local binary patterns features extracted from the facial images. Following on, support vector machines, k-nearest neighbor, and ensemble are employed for emotion classification. Additionally, the class misbalance problem is solved using the up-sampling approach. The accuracy of the suggested method is assessed on the dataset of emotion analysis using physiological signals with tenfold cross-validation. The findings of the research study are promising, with the highest accuracy of 97.25% for valence and 96.1% for arousal, respectively. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Machine learning; Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Face recognition; Modal analysis; Multimodal emotion recognition; Data mining; Human computer interaction; Emotion analysis; Electroencephalography; Biomedical signal processing; Electroencephalogram; Support vector machines; Machine-learning; Human–computer interaction; Electroencephalogram (EEG); Multiple modalities; Virtual reality; Human emotion; Facial feature; Nearest neighbor search; Local binary pattern; Electroencephalogram signals; Facial features; Research communities,,,emotion,No,Yes
scopus,Audio-Visual Emotion Recognition With Preference Learning Based on Intended and Multi-Modal Perceived Labels,"Lei, Y.; Cao, H.",2023,,14,,10.1109/TAFFC.2023.3234777,"This article introduces a novel preference learning framework that simultaneously considers both the intended and the perceived labels while addressing the mismatches between them. Based on analyzing the discrepancies and agreements between the intended and the perceived labels in different modalities of audio-only, visual-only, and audio-visual, as well as the consistency among the perceptual ratings of all raters, we propose three sets of pair-wise ranking rules to generate multi-scale relevant scores for preference learning, scaling from sketchy manner to detailed manner. Three ranking models with support vector machine (SVM), deep neural networks (DNN), and gradient boosting decision trees (GBDT) are developed. Our results demonstrate that all three preference learning models significantly outperform the conventional classifiers baselines, and the LambdaMART model with gradient boosting decision trees achieves the best performance. The improvement from the preference learning models confirm the benefits of complementary information provided by different types of labels. We also observe additional improvement from the detailed 'complex ranking rules', particular with the best LambdaMART model, which suggests that we should treat intended and perceived labels in single-model & multi-modal differently. We further discuss the complementary of different ranking models, and obtain the best overall accuracy of 85.06% on CREMA-D dataset when combining the two best ranking models-LambdaMART and RankNet-together, which is significantly better than the 76.19% accuracy attained by the baseline models. Finally, we perform the cross-corpus emotion recognition experiments by training emotion rankers on CREMA-D dataset and tested the ranking-based emotion classifier on the SAVEE dataset that do not have perceived labels annotated. © 2010-2012 IEEE.",Emotion Recognition; Deep neural networks; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Job analysis; Task analysis; Classification (of information); multimodal emotion recognition; Support vector machines; Predictive models; Decision trees; Neural-networks; Intended label; Intended labels; Lambdamart; LambdaMART; Perceived label; perceived labels; preference learning; Preference learning; RankNet; RankSVM,,,emotion,Yes,Yes
scopus,Fusing generative adversarial network and temporal convolutional network for Mandarin emotion recognition,"Li, H.-F.; Zhang, X.-Y.; Duan, S.-F.; Jia, H.-R.; Liang, H.-Z.",2023,,57,,10.3785/j.issn.1008-973X.2023.09.018,"An emotion recognition system that integrates acoustic and articulatory feature conversions was proposed in order to investigate the influence of acoustic and articulatory conversions on Mandarin emotion recognition. Firstly, a multimodal emotional Mandarin database was recorded based on the human articulation mechanism. Then, a bi-directional mapping generative adversarial network (Bi-MGAN) was designed to solve the feature conversion problem with bimodality, and the generator loss functions and the mapping loss functions were proposed to optimise the network. Finally, a residual temporal convolutional network based on the feature-dimension attention (ResTCN-FDA) was constructed to use attention mechanisms to adaptively assign different weights to different variety features and different dimension channels. Experimental results show that the conversion accuracy of Bi-MGAN outperforms the current optimal algorithms for conversion network in both the forward and the reverse mapping tasks. The evaluation metrics of ResTCN-FDA on a given emotion dataset is much higher than traditional emotion recognition algorithms. The real features fused with the mapped features resulted in a significant increase in the accuracy of the emotions being recognized correctly, and the positive effect of mapping on Mandarin emotion recognition was demonstrated. © 2023 Zhejiang University. All rights reserved.",attention mechanism; Attention mechanisms; Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Network-based; Bi-directional; Convolution; Convolutional networks; Loss functions; Mapping; Acoustic and articulatory conversion; acoustic and articulatory conversions; cycle generative adversarial network; Cycle generative adversarial network; Feature conversion; Generative adversarial networks; temporal convolutional network; Temporal convolutional network,,,emotion,No,Yes
scopus,Self-supervised representation learning using multimodal Transformer for emotion recognition,"Goetz, T.; Arora, P.; Erick, F.X.; Holzer, N.; Sawant, S.",2023,,,,10.1145/3615834.3615837,"In this paper, we present a Modality-Agnostic Transformer based Self-Supervised Learning (MATS2L) for emotion recognition using physiological signals. The proposed approach consists of two stages: a) Pretext stage, where the transformer model is pre-trained with unlabeled physiological signal data using masked signal prediction as pre-training task and form contextualized signal representations. b) Downstream stage, where self-supervised learning (SSL) representations extracted from a pre-trained model are utilized for emotion recognition tasks. Modality-agnostic approach allows the transformer model to focus on exploring mutual features among different physiological signals and learning more meaningful embeddings to estimate emotions effectively. We conduct several experiments on a public dataset WESAD and perform comparisons with fully supervised and other competitive SSL approaches. Experimental results showed that the proposed approach is capable of learning meaningful features and superior to other competitive SSL approaches. Moreover, a transformer model trained on SSL features outperforms fully supervised transformer model. We also present detailed ablation studies to prove the robustness of our approach.  © 2023 Owner/Author.",Transformer; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Biomedical signal processing; Physiological models; Self-supervised learning; Supervised learning; Transformer modeling; Modality-agnostic; Signal data; Signal prediction; Supervised learning approaches,,,emotion,No,No
scopus,"Emoji, Sentiment and Emotion Aided Cyberbullying Detection in Hinglish","Maity, K.; Saha, S.; Bhattacharyya, P.",2023,,10,,10.1109/TCSS.2022.3183046,"The advent of the Internet is a boon to society. However, many of its banes cannot be undermined, cyberbullying being one of them. The emotional state and sentiment of a person have a significant influence on the intended content. The current work is the first attempt in investigating the role of sentiment and emotion information for identifying cyberbullying in the Indian scenario. From Twitter, a benchmark Hind-English code-mixed corpus called BullySentEmo has been developed as there is no dataset available labeled with bully, sentiment, and emotion. Moreover, emoji information available with tweet texts can provide better understanding of user intention. The developed dataset consists of both modalities, tweet text, and emoji. In India, the majority of communication on different social media platforms is based on Hindi and English and language switching is a common practice in digital communication. An attention-based multimodal, adversarial multitasking framework is proposed for cyberbully detection (CBD) considering two auxiliary tasks: sentiment analysis (SA) and emotion recognition (ER). Experimental results indicate that compared to unimodal and single-task variants, the proposed framework improves the performance of the main task, i.e., CBD, by 3.59% and 2.56% in terms of accuracy and F1-Score, respectively. Furthermore, two different benchmark datasets (Twitter dataset and Aggression dataset) have been considered to show the robustness of our proposed model.  © 2014 IEEE.",emotion; Emotion; Social networking (online); Sentiment analysis; Task analysis; Features extraction; Feature extraction; Bit error rate; Bit-error rate; Blogs; Sentiment; Annotation; Code mixed; Computer crime; Cyber bullying; cyberbullying; Multitasking; multitasking (MT); sentiment,,,emotion,Yes,Yes
scopus,Multimodal emotion recognition based on audio and text by using hybrid attention networks,"Zhang, S.; Yang, Y.; Chen, C.; Liu, R.; Tao, X.; Guo, W.; Xu, Y.; Zhao, X.",2023,,85,,10.1016/j.bspc.2023.105052,"Multimodal Emotion Recognition (MER) has recently become a popular and challenging topic. The most key challenge in MER is how to effectively fuse multimodal information. Most of prior works may not fully consider the inter-modal and intra-modal attention mechanism to jointly learn intra-modal and inter-modal emotional salient information for further improving the performance of MER. To address this problem, this paper proposes a new MER framework based on audio and text by using Hybrid Attention Networks (MER-HAN). The proposed MER-HAN combines three different attention mechanisms such as the local intra-modal attention, the cross-modal attention, and the global inter-modal attention to effectively learn intra-modal and inter-modal emotional salient features for MER. Specifically, an Audio and Text Encoder (ATE) block equipped with deep learning techniques with the local intra-modal attention mechanism is initially designed to learn high-level audio and text feature representations from the corresponding audio and text sequences, respectively. Then, a Cross-Modal Attention (CMA) block is presented to jointly capture high-level shared feature representations across audio and text modalities. Finally, a Multimodal Emotion Classification (MEC) block with the global inter-modal attention mechanism is provided to obtain final MER results. Extensive experiments conducted on two public multimodal emotional datasets, i.e., IEMOCAP and MELD datasets, show the advantage of the proposed MER-HAN on MER tasks. © 2023 Elsevier Ltd",emotion; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; deep learning; Deep learning; human; Character recognition; Multimodal emotion recognition; Learn+; attention; Cross-modal; human experiment; article; Cross-modal attention; crossmodal attention; Feature representation; attention network; Global inter-modal attention; Local intra-modal attention,,,emotion,No,Yes
scopus,Multimodal Emotion Detection via Attention-Based Fusion of Extracted Facial and Speech Features,"Mamieva, D.; Abdusalomov, A.B.; Kutlimuratov, A.; Muminov, B.; Whangbo, T.K.",2023,,23,,10.3390/s23125475,"Methods for detecting emotions that employ many modalities at the same time have been found to be more accurate and resilient than those that rely on a single sense. This is due to the fact that sentiments may be conveyed in a wide range of modalities, each of which offers a different and complementary window into the thoughts and emotions of the speaker. In this way, a more complete picture of a person’s emotional state may emerge through the fusion and analysis of data from several modalities. The research suggests a new attention-based approach to multimodal emotion recognition. This technique integrates facial and speech features that have been extracted by independent encoders in order to pick the aspects that are the most informative. It increases the system’s accuracy by processing speech and facial features of various sizes and focuses on the most useful bits of input. A more comprehensive representation of facial expressions is extracted by the use of both low- and high-level facial features. These modalities are combined using a fusion network to create a multimodal feature vector which is then fed to a classification layer for emotion recognition. The developed system is evaluated on two datasets, IEMOCAP and CMU-MOSEI, and shows superior performance compared to existing models, achieving a weighted accuracy WA of 74.6% and an F1 score of 66.1% on the IEMOCAP dataset and a WA of 80.7% and F1 score of 73.7% on the CMU-MOSEI dataset. © 2023 by the authors.","attention mechanism; Humans; Emotions; Facial Expression; emotion; facial expression; Attention mechanisms; Emotion Recognition; Emotional state; Multi-modal; Speech recognition; CNN; Face recognition; Facial Expressions; human; Multimodal emotion recognition; Emotion detection; multimodal emotion recognition; recognition; Recognition, Psychology; speech; Speech; F1 scores; Speech features; Feature extraction; Facial feature; Analysis of data; facial feature; speech feature",,,emotion,No,Yes
scopus,Emotion-Recognition Algorithm Based on Weight-Adaptive Thought of Audio and Video,"Cheng, Y.; Zhou, D.; Wang, S.; Wen, L.",2023,,12,,10.3390/electronics12112548,"Emotion recognition commonly relies on single-modal recognition methods, such as voice and video signals, which demonstrate a good practicability and universality in some scenarios. Nevertheless, as emotion-recognition application scenarios continue to expand and the data volume surges, single-modal emotion recognition proves insufficient to meet people’s needs for accuracy and comprehensiveness when the amount of data reaches a certain scale. Thus, this paper proposes the application of multimodal thought to enhance emotion-recognition accuracy and conducts corresponding data preprocessing on the selected dataset. Appropriate models are constructed for both audio and video modalities: for the audio-modality emotion-recognition task, this paper adopts the “time-distributed CNNs + LSTMs” model construction scheme; for the video-modality emotion-recognition task, the “DeepID V3 + Xception architecture” model construction scheme is selected. Furthermore, each model construction scheme undergoes experimental verification and comparison with existing emotion-recognition algorithms. Finally, this paper attempts late fusion and proposes and implements a late-fusion method based on the idea of weight adaptation. The experimental results demonstrate the superiority of the multimodal fusion algorithm proposed in this paper. When compared to the single-modal emotion-recognition algorithm, the accuracy of recognition is increased by almost 4%, reaching 84.33%. © 2023 by the authors.",multimodal; LSTM; DeepID V3; time-distributed CNNs; Xception,,,emotion,No,Yes
scopus,CSAT-FTCN: A Fuzzy-Oriented Model with Contextual Self-attention Network for Multimodal Emotion Recognition,"Jiang, D.; Liu, H.; Wei, R.; Tu, G.",2023,,15,,10.1007/s12559-023-10119-6,"Multimodal emotion analysis has become a hot trend because of its wide applications, such as the question-answering system. However, in a real-world scenario, people usually have mixed or partial emotions about evaluating objects. In this paper, we introduce a fuzzy temporal convolutional network based on contextual self-attention (CSAT-FTCN) to address these challenges, which has a membership function modeling various fuzzy emotions for understanding emotions in a more profound sense. Moreover, the CSAT-FTCN can obtain the dependency relationships of target utterances on internal own key information and external contextual information to understand emotions in a more profound sense. Additionally, as for multi-modality data, we introduce an attention fusion (ATF) mechanism to capture the dependency relationship between different modality information. The experimental results show that our CSAT-FTCN outperforms state-of-the-art models on tested datasets. The CSAT-FTCN network provides a novel method for multimodal emotion analysis. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Emotion Recognition; Multi-modal; Modal analysis; Multimodal emotion recognition; Emotion analysis; Convolution; Multimodal emotion analyse; Multimodal emotion analysis; Convolutional networks; Fuzzy neural networks; Fusion mechanism; Fuzzy inference; Temporal convolutional network; Attention fusion mechanism; Dependency relationship; Fuzzy network; Fuzzy networks; Membership functions,,,emotion,No,No
scopus,Multimodal Emotion-Cause Pair Extraction in Conversations,"Wang, F.; Ding, Z.; Xia, R.; Li, Z.; Yu, J.",2023,,14,,10.1109/TAFFC.2022.3226559,"Conversation is an important form of human communication and contains a large number of emotions. It is interesting to discover emotions and their causes in conversations. Conversation in its natural form is multimodal. Many studies have been carried out on multimodal emotion recognition in conversations, yet there is still a lack of work on multimodal emotion cause analysis. In this article, we introduce a new task named Multimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly extract emotions and the corresponding causes from conversations reflected in multiple modalities (i.e., text, audio and video). We accordingly construct a multimodal conversational emotion cause dataset, Emotion-Cause-in-Friends, which contains 9,794 multimodal emotion-cause pairs among 13,619 utterances in the Friends sitcom. We benchmark the task by establishing two baseline systems including a heuristic approach considering inherent patterns in the location of causes and emotions and a deep learning approach that incorporates multimodal features for emotion-cause pair extraction, and conduct the human performance test for comparison. Furthermore, we investigate the effect of multimodal information, explore the potential of incorporating commonsense knowledge, and perform the task under both Static and Real-time settings.  © 2010-2012 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Modal analysis; Deep learning; Data mining; Affective Computing; Job analysis; Oral communication; Task analysis; Emotion analysis; Features extraction; Benchmarking; Affective computing; Extraction; Feature extraction; Multi-modal learning; multimodal learning; emotion analysis; Annotation; emotion cause extraction; Emotion cause extraction; emotion-cause pair extraction; Emotion-cause pair extraction; Heuristic methods,,,emotion,No,Yes
scopus,Multimodal speech emotion recognition based on multi-scale MFCCs and multi-view attention mechanism,"Feng, L.; Liu, L.-Y.; Liu, S.-L.; Zhou, J.; Yang, H.-Q.; Yang, J.",2023,,82,,10.1007/s11042-023-14600-0,"In recent years, speech emotion recognition (SER) increasingly attracts attention since it is a key component of intelligent human-computer interaction and sophisticated dialog systems. To obtain more abundant emotional information, a great number of studies in SER pay attention to the multimodal systems which utilize other modalities such as text and facial expression to assist the speech emotion recognition. However, it is difficult to structure a fusion mechanism which can selectively extract abundant emotion-related features from different modalities. To tackle this issue, we develop a multimodal speech emotion recognition model based on multi-scale MFCCs and multi-view attention mechanism, which is able to extract abundant audio emotional features and comprehensively fuse emotion-related features from four aspects (i.e., audio self-attention, textual self-attention, audio attention based on textual content, and textual attention based on audio content). Under different audio input conditions and attention configurations, it can be observed that the best emotion recognition accuracy can be achieved by jointly utilizing four attention modules and three different scales of MFCCs. In addition, based on multi-task learning, we regard the gender recognition as an auxiliary task to learn gender information. To further improve the accuracy of emotion recognition, a joint loss function based on softmax cross-entropy loss and center loss is used. The experiments are conducted on two different datasets (IEMOCAP and MSP-IMPROV). The experimental results demonstrate that the proposed model outperforms the previous models on IEMOCAP dataset, while it obtains the competitive performance on MSP-IMPROV dataset. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Speech emotion recognition; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Character recognition; Human computer interaction; Emotional information; Multi-views; Dialogue systems; Multi-scales; Mulit-scale MFCC; Mulit-scale MFCCs; Multi-view attention,,,emotion,No,Yes
scopus,Agreement and disagreement between major emotion recognition systems,"Vogel, C.; Ahmad, K.",2023,,276,,10.1016/j.knosys.2023.110759,"The evaluation of systems that claim to recognize emotions expressed by human beings is a contested and complex task: The early pioneers in this field gave the impression that these systems will eventually recognize a flash of anger, suppressed glee/happiness, momentary disgust or contempt, lurking fear, or sadness in someone's face or voice (Picard and Klein, 2002; Schuller et al., 2011). Emotion recognition systems are trained on ‘labelled’ databases — collection of video/audio recording comprising images and voices of humans enacting one emotional state. Machine learning programmes then regress the pixel distributions or wave forms against the labels. The system is then said to have learnt how to recognize and interpret human emotions and rated using information science metrics. These systems are adopted by the world at large for applications ranging from autistic spectrum communications to teaching and learning, and onwards to covert surveillance. The training databases depend upon human emotions recorded in ideal conditions — faces looking at the camera and centrally located, voices articulated through noise-cancelling microphones. Yet there are reports that the posed training data set, that is racially-skewed and gender unbalanced, does not prepare these systems to cope with data-in-the-wild and that expression-unrelated variations (like illumination, head pose, and identity bias (Li and Deng, 2020)) can impact their performance as well. Deployments of these systems tend to adopt one or other and apply it to data collected outside laboratory conditions and use the resulting classifications in subsequent processing. We have devised a testing method that helps to quantify the similarities and differences of facial emotion recognition systems (FER) and speech emotion recognition systems (SER). We report on the development of a data base comprising videos and sound track of 64 politicians and 7 government spokespersons (25 F, 46 M; 34 White Europeans, 19 East Asians, and 18 South Asians), ranging in age from 32–85 years, and each of the 71 has on average three 180 s videos; a total of 16.66 h of data. We have compared the performance of two FERs (Emotient and Affectiva) and two SERs (OpenSmile and Vokaturi) on our data by analysing emotions reported by these systems on a frame-by-frame basis. We have analysed the directly observable head movements, and the indirectly observable muscle movement parts of the face and for the muscle movements in the vocal tract. There was marked disagreement in emotions recognized, and the differences were exacerbated more women than for men, and more for South and East Asians than for White Europeans. Levels of agreement and disagreement on both high-level (i.e. emotion labels) and lower-level features (e.g. Euler angles of head movement) are shown. We show that inter-system disagreement may also be used as an effective response variable in reasoning about data features that influence disagreement. We argue that reliability of subsequent processing in approaches that adopt these systems may be enhanced by restricting action to cases where systems agree within a given tolerance level. This paper may be considered as a foray into the greater debate about the so-called algorithmic (un)fairness and data bias in the development and deployment of machine learning systems of which FERs and SERs are a good exemplar. © 2023 The Author(s)",Emotion Recognition; Performance; Speech recognition; Emotion recognition; Face recognition; Learning systems; Security systems; Recognition systems; Emotion recognition system; Human emotion; Communications data; Emotion processing; Emotion recognition systems; Head movements; Multi-modal communication data; Multimodal communications,,,emotion,Yes,Yes
scopus,SMFNM: Semi-supervised multimodal fusion network with main-modal for real-time emotion recognition in conversations,"Yang, J.; Dong, X.; Du, X.",2023,,35,,10.1016/j.jksuci.2023.101791,"Real-time emotion recognition in conversations (ERC), which relies on only the historical utterances to achieve ERC, has recently gained increasing attention due to its significance in providing real-time empathetic services. Although utilizing multimodal information can mitigate the issues of unimodal approaches, few real-time ERC studies consider the differences in representation ability of different modalities and explore comprehensive conversational context from different perspectives based on different structures. Furthermore, the heavy annotation cost makes it difficult to collect sufficient labeled data, which also limits the performance of current supervised ERC approaches. To address these issues, we propose a novel framework SMFNM for real-time ERC, which integrates semi-supervised learning with multimodal fusion under the guidance of main-modal. Specifically, SMFNM utilizes additional unlabeled data to extract high-quality intra-modal representations, and implements cross-modal interaction to capture complementary information to enhance the audio representations. Then SMFNM employs the directed acyclic graph and the Gated Recurrent Units for exploring more accurate conversational context from both the multimodal and main-modal perspectives, respectively. Finally, these two types of contextual features are fused for emotion identification. Extensive experiments on benchmark datasets (i.e., IEMOCAP (4-way), IEMOCAP (6-way) and MELD) demonstrate the effectiveness, superiority and rationality of our SMFNM. © 2023 The Author(s)",Semi-supervised learning; Multimodal interaction; Multimodal fusion network; Main modal; Real-time Emotion recognition in conversations,,,emotion,Yes,Yes
scopus,"MuSe 2023 Challenge: Multimodal Prediction of Mimicked Emotions, Cross-Cultural Humour, and Personalised Recognition of Affects","Amiriparian, S.; Christ, L.; König, A.; Cowen, A.; Meßner, E.-M.; Cambria, E.; Schuller, B.W.",2023,,,,10.1145/3581783.3610943,"The 4th Multimodal Sentiment Analysis Challenge (MuSe) focuses on Multimodal Prediction of Mimicked Emotions, Cross-Cultural Humour, and Personalised Recognition of Affects. The workshop takes place in conjunction with ACM Multimedia'23. We provide three datasets as part of the challenge: (i) The Hume-Vidmimic dataset which offers 30+ hours of expressive behaviour data from 557 participants. It involves mimicking and rating emotions: Approval, Disappointment, and Uncertainty. This multimodal resource is valuable for studying human emotional expressions. (ii) The 2023 edition of the Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset comprises German football press conference recordings within the training set, while videos of English football press conferences are included in the unseen test set. This unique configuration offers a cross-cultural evaluation environment for humour recognition. (iii) The Ulm-Trier Social Stress Test (Ulm-TSST) dataset contains recordings of subjects under stress. It involves arousal and valence signals, with some test labels provided to aid personalisation. Based on these datasets, we formulate three multimodal affective computing challenges: (1) Mimicked Emotions Sub-Challenge (MuSe-Mimic) for categorical emotion prediction, (2) Cross-Cultural Humour Detection Sub-Challenge (MuSe-Humour) for cross-cultural humour detection, and (3) Personalisation Sub-Challenge (MuSe-Personalisation) for personalised dimensional emotion recognition. In this summary, we outline the challenge's motivation, participation guidelines, conditions, and results. © 2023 Owner/Author.","Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Multi-modal fusion; multimodal sentiment analysis; Affective Computing; Sentiment analysis; multimodal fusion; Multimodal sentiment analyse; Forecasting; Statistical tests; Challenge; affective computing; Personalizations; challenge; Sports; Emotion mimic, cross-cultural humor detection; emotion mimics, cross-cultural humour detection; Presses (machine tools); summary paper; Summary paper",,,emotion,Yes,No
scopus,Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks,"Aslam, A.; Sargano, A.B.; Habib, Z.",2023,,144,,10.1016/j.asoc.2023.110494,"There has been a growing interest in multimodal sentiment analysis and emotion recognition in recent years due to its wide range of practical applications. Multiple modalities allow for the integration of complementary information, improving the accuracy and precision of sentiment and emotion recognition tasks. However, working with multiple modalities presents several challenges, including handling data source heterogeneity, fusing information, aligning and synchronizing modalities, and designing effective feature extraction techniques that capture discriminative information from each modality. This paper introduces a novel framework called “Attention-based Multimodal Sentiment Analysis and Emotion Recognition (AMSAER)” to address these challenges. This framework leverages intra-modality discriminative features and inter-modality correlations in visual, audio, and textual modalities. It incorporates an attention mechanism to facilitate sentiment and emotion classification based on visual, textual, and acoustic inputs by emphasizing relevant aspects of the task. The proposed approach employs separate models for each modality to automatically extract discriminative semantic words, image regions, and audio features. A deep hierarchical model is then developed, incorporating intermediate fusion to learn hierarchical correlations between the modalities at bimodal and trimodal levels. Finally, the framework combines four distinct models through decision-level fusion to enable multimodal sentiment analysis and emotion recognition. The effectiveness of the proposed framework is demonstrated through extensive experiments conducted on the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. The results confirm a notable performance improvement compared to state-of-the-art methods, attaining 85% and 93% accuracy for sentiment analysis and emotion classification, respectively. Additionally, when considering class-wise accuracy, the results indicate that the “angry” emotion and “positive” sentiment are classified more effectively than the other emotions and sentiments, achieving 96.80% and 93.14% accuracy, respectively. © 2023 Elsevier B.V.",Semantics; Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Sentiment analysis; Data handling; Emotion classification; Multiple modalities; Data-source; Accuracy and precision; Discriminative features; Feature extraction techniques; Hierarchical systems; Multimodal attention,,,emotion,No,Yes
scopus,WiFE: WiFi and Vision Based Unobtrusive Emotion Recognition via Gesture and Facial Expression,"Gu, Y.; Zhang, X.; Yan, H.; Huang, J.; Liu, Z.; Dong, M.; Ren, F.",2023,,14,,10.1109/TAFFC.2023.3285777,"Emotion plays a critical role in making the computer more human-like. As the first and most essential step, emotion recognition emerges recently as a hot but relatively nascent topic, i.e., current research mainly focuses on single modality (e.g., facial expression) while human emotion expressions are multi-modal in nature. To this end, we propose an unobtrusive emotion recognition system leveraging two emotion-rich and tightly-coupled modalities, i.e., gesture and facial expression. The system design faces two major challenges, namely, how to capture the emotional expression in both modalities without disturbing the subject and how to leverage the relationship between modalities for recognizing the emotion. For the former, we explore WiFi and vision for unobtrusive and contactless gesture and facial expression sensing, respectively. For the latter, we propose a novel deep learning framework named Multi-Source Learning (MSL) to efficiently exploit both self-correlation in the modality and cross-correlation between modalities for fine-grained emotion recognition. To evaluate the proposed method, we prototype the system on low-cost commodity WiFi and vision devices, build a first-of-its-kind WiFi-Vision emotion dataset, and conduct extensive experiments. Empirical results not only verify the effectiveness of WiFE in emotion recognition, but also confirm the superiority of multi-modality over single-modality.  © 2010-2012 IEEE.",facial expression; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Face recognition; Facial Expressions; Deep learning; multimodal; Features extraction; Feature extraction; Decoding; Gesture recognition; channel state information; Channel state information; Channel-state information; Discrete wavelet transforms; Discrete-wavelet-transform; Gestures recognition; Wireless fidelities; Wireless local area networks (WLAN),,,emotion,No,No
scopus,Hierarchical extreme puzzle learning machine-based emotion recognition using multimodal physiological signals,"Pradhan, A.; Srivastava, S.",2023,,83,,10.1016/j.bspc.2023.104624,"Detection of exact emotions through multi-modal physiological signals provides relevant information for different processes. Numerous computational approaches have been presented for the precise analysis of emotion types. But due to some problems like ruined signal quality, increased time consumption, and the necessity of high storage space, classification accuracy's efficiency worsens. Hence, this research classified multi-modal physiological signals based on machine and deep learning (DL) models. The proposed work implements the Hierarchical Extreme Puzzle Learning Machine (HEPLM) approach to classify the actual output of embedded emotions. The proposed work comprises four steps: pre-processing, signal-to-image conversion, feature extraction, and classification. Pre-processing is carried out using Savitzky-Golay smoothing filtering (SGF) for the removal of noise and to increase signal quality. Hybrid wavelet scattering with Synchro squeezing Wavelet Transform approach converts the signal into an image. In feature extraction process, the valuable features are extracted using ResNet-152 and the Inception v3 model, whereas the features are combined through an ensemble approach. HEPLM is used in the final classification process, combining Puzzle Optimization Algorithm (POA) and Hierarchical Extreme Learning Machine (HELM) to reduce feature dimensionality and improve classification accuracy. The dataset adopted in the proposed work is Wearable Stress and Affect Detection (WESAD) to collect multi-modal physiological signals. The presentation of the projected work is assessed with metrics like accuracy, recall, precision, F1 score, kappa, and so on. The proposed effort demonstrates better results of emotion classification when compared to the existing methods by holding 96.29% of accuracy. © 2023 Elsevier Ltd",Image processing; emotion; Emotion Recognition; Multi-modal; Speech recognition; deep learning; Emotion recognition; Physiological signals; Physiology; Deep learning; human; Learning systems; machine learning; Biomedical signal processing; human experiment; Physiological models; algorithm; article; residual neural network; Extraction; feature extraction; Feature extraction; Classification accuracy; Wavelet transforms; noise; physiological stress; Knowledge acquisition; Learning machines; recall; Ensemble; Filtering; filtration; Optimal feature; Optimal features; Pre-processing; Signal conversion; Signal quality; wavelet transform,,,emotion,No,Yes
scopus,Investigating Multisensory Integration in Emotion Recognition Through Bio-Inspired Computational Models,"Benssassi, E.M.; Ye, J.",2023,,14,,10.1109/TAFFC.2021.3106254,"Emotion understanding represents a core aspect of human communication. Our social behaviours are closely linked to expressing our emotions and understanding others' emotional and mental states through social signals. The majority of the existing work proceeds by extracting meaningful features from each modality and applying fusion techniques either at a feature level or decision level. However, these techniques are incapable of translating the constant talk and feedback between different modalities. Such constant talk is particularly important in continuous emotion recognition, where one modality can predict, enhance and complement the other. This article proposes three multisensory integration models, based on different pathways of multisensory integration in the brain; that is, integration by convergence, early cross-modal enhancement, and integration through neural synchrony. The proposed models are designed and implemented using third-generation neural networks, Spiking Neural Networks (SNN). The models are evaluated using widely adopted, third-party datasets and compared to state-of-the-art multimodal fusion techniques, such as early, late and deep learning fusion. Evaluation results show that the three proposed models have achieved comparable results to the state-of-the-art supervised learning techniques. More importantly, this article demonstrates plausible ways to translate constant talk between modalities during the training phase, which also brings advantages in generalisation and robustness to noise.  © 2010-2012 IEEE.",Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Deep learning; Data mining; Features extraction; Integration; Brain; Brain modeling; Support vector machines; Support vectors machine; Feature extraction; Graph neural networks; Neural-networks; Spiking neural network; graph neural network; Multisensory integration; mutilsensory integration; Mutilsensory integration; neural synchrony; Neural synchrony,,,emotion,No,No
scopus,Applying Segment-Level Attention on Bi-Modal Transformer Encoder for Audio-Visual Emotion Recognition,"Hsu, J.-H.; Wu, C.-H.",2023,,14,,10.1109/TAFFC.2023.3258900,"Emotions can be expressed through multiple complementary modalities. This study selected speech and facial expressions as modalities by which to recognize emotions. Current audiovisual emotion recognition models perform supervised learning using signal-level inputs. Such models are presumed to characterize the temporal relationships in signals. In this study, supervised learning was performed on segment-level signals, which are more granular than signal-level signals, to precisely train an emotion recognition model. Effectively fusing multimodal signals is challenging. In this study, sequential segments of audiovisual signals were obtained, and features were extracted and applied to estimate segment-level attention weights according to the emotional consistency of the two modalities using a neural tensor network. A proposed bimodal Transformer Encoder was trained using signal-level and segment-level emotion labels in which temporal context was incorporated into the signals to improve upon existing emotion recognition models. In bimodal emotion recognition, the experimental results demonstrated that the proposed method achieved 74.31% accuracy (3.05% higher than the method of fusing correlation features) on the audio-visual emotion dataset BAUM-1, which is based on fivefold cross-validation, and 76.81% accuracy (2.57% higher than the Multimodal Transformer Encoder) on the multimodal emotion data set CMU-MOSEI, which is composed of training, validation, and testing sets.  © 2010-2012 IEEE.",Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Signal encoding; Features extraction; Recognition models; Supervised learning; Statistical tests; Feature extraction; Audio-visual; Audio-visual emotion recognition; bi-modal transformer; Bi-modal transformer; Image segmentation; Images segmentations; segment-level attention; Segment-level attention,,,emotion,Yes,Yes
scopus,"A multi-task, multi-modal approach for predicting categorical and dimensional emotions","Ispas, A.-R.; Deschamps-Berger, T.; Devillers, L.",2023,,,,10.1145/3610661.3616190,"Speech emotion recognition (SER) has received a great deal of attention in recent years in the context of spontaneous conversations. While there have been notable results on datasets like the well-known corpus of naturalistic dyadic conversations, IEMOCAP, for both the case of categorical and dimensional emotions, there are few papers which try to predict both paradigms at the same time. Therefore, in this work, we aim to highlight the performance contribution of multi-task learning by proposing a multi-task, multi-modal system that predicts categorical and dimensional emotions. The results emphasise the importance of cross-regularisation between the two types of emotions. Our approach consists of a multi-task, multi-modal architecture that uses parallel feature refinement through self-attention for the feature of each modality. In order to fuse the features, our model introduces a set of learnable bridge tokens that merge the acoustic and linguistic features with the help of cross-attention. Our experiments for categorical emotions on 10-fold validation yield results comparable to the current state-of-the-art. In our configuration, our multi-task approach provides better results compared to learning each paradigm separately. On top of that, our best performing model achieves a high result for valence compared to the previous multi-task experiments. © 2023 ACM.",Speech emotion recognition; Attention mechanisms; Emotion Recognition; Performance; Speech recognition; Multi-modal fusion; Learning systems; Multi tasks; Multi-modal approach; Linguistics; Multitask learning; Multimodal system; Attention Mechanisms; Multi-modal Fusion; Multi-Task Learning; Regularisation; Speech Emotion Recognition; Spontaneous conversations,,,emotion,No,Yes
scopus,Whose emotion matters? Speaking activity localisation without prior knowledge,"Carneiro, H.; Weber, C.; Wermter, S.",2023,,545,,10.1016/j.neucom.2023.126271,"The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more closely match the transcribed utterances given in the MELD dataset. Finally, we devise a model for emotion recognition in conversations trained on the realigned MELD-FAIR videos, which outperforms state-of-the-art models for ERC based on vision alone. This indicates that localising the source of speaking activities is indeed effective for extracting facial expressions from the uttering speakers and that faces provide more informative visual cues than the visual features state-of-the-art models have been using so far. The MELD-FAIR realignment data, and the code of the realignment procedure and of the emotional recognition, are available at https://github.com/knowledgetechnologyuhh/MELD-FAIR. © 2023 The Author(s)",emotion; facial expression; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Facial Expressions; human; videorecording; State of the art; speech; human experiment; Recognition models; Multi-modality; Multimodality; article; conversation; Active speaker detection; automatic speech recognition; Forced alignment; genetic transcription; Localisation; Speaker detection; vision,,,emotion,Yes,No
scopus,Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels,"Jiang, W.-B.; Liu, X.-H.; Zheng, W.-L.; Lu, B.-L.",2023,,,,10.1145/3581783.3613797,"Emotion recognition from physiological signals is a topic of widespread interest, and researchers continue to develop novel techniques for perceiving emotions. However, the emergence of deep learning has highlighted the need for high-quality emotional datasets to accurately decode human emotions. In this study, we present a novel multimodal emotion dataset that incorporates electroencephalography (EEG) and eye movement signals to systematically explore human emotions. Seven basic emotions (happy, sad, fear, disgust, surprise, anger, and neutral) are elicited by a large number of 80 videos and fully investigated with continuous labels that indicate the intensity of the corresponding emotions. Additionally, we propose a novel Multimodal Adaptive Emotion Transformer (MAET), that can flexibly process both unimodal and multimodal inputs. Adversarial training is utilized in MAET to mitigate subject discrepancy, which enhances domain generalization. Our extensive experiments, encompassing both subject-dependent and cross-subject conditions, demonstrate MAET's superior performance in handling various inputs. The filtering of data for high emotional evocation using continuous labels proved to be effective in the experiments. Furthermore, the complementary properties between EEG and eye movements are observed. Our code is available at https://github.com/935963004/MAET. © 2023 ACM.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Deep learning; Electroencephalography; Electrophysiology; Eye movements; Dataset; Human emotion; dataset; High quality; Basic emotions; continuous label; Continuous label; eeg; Eeg; eye movements; Novel techniques,,,emotion,Yes,Yes
scopus,Brain-Machine Coupled Learning Method for Facial Emotion Recognition,"Liu, D.; Dai, W.; Zhang, H.; Jin, X.; Cao, J.; Kong, W.",2023,,45,,10.1109/TPAMI.2023.3257846,"Neural network models of machine learning have shown promising prospects for visual tasks, such as facial emotion recognition (FER). However, the generalization of the model trained from a dataset with a few samples is limited. Unlike the machine, the human brain can effectively realize the required information from a few samples to complete the visual tasks. To learn the generalization ability of the brain, in this article, we propose a novel brain-machine coupled learning method for facial emotion recognition to let the neural network learn the visual knowledge of the machine and cognitive knowledge of the brain simultaneously. The proposed method utilizes visual images and electroencephalogram (EEG) signals to couple training the models in the visual and cognitive domains. Each domain model consists of two types of interactive channels, common and private. Since the EEG signals can reflect brain activity, the cognitive process of the brain is decoded by a model following reverse engineering. Decoding the EEG signals induced by the facial emotion images, the common channel in the visual domain can approach the cognitive process in the cognitive domain. Moreover, the knowledge specific to each domain is found in each private channel using an adversarial strategy. After learning, without the participation of the EEG signals, only the concatenation of both channels in the visual domain is used to classify facial emotion images based on the visual knowledge of the machine and the cognitive knowledge learned from the brain. Experiments demonstrate that the proposed method can produce excellent performance on several public datasets. Further experiments show that the proposed method trained from the EEG signals has good generalization ability on new datasets and can be applied to other network models, illustrating the potential for practical applications.  © 1979-2012 IEEE.","Humans; Emotions; emotion; Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; artificial neural network; human; Neural Networks, Computer; Learning systems; Job analysis; Task analysis; electroencephalography; Electroencephalography; procedures; Electrophysiology; Brain; Brain modeling; algorithm; Algorithms; transfer learning; Transfer learning; facial recognition; Facial emotion recognition; Multi-modal learning; Brain-machine intelligence; EEG signals; Facial emotions; Cognitive systems; multimodal learning; Decoding; brain; diagnostic imaging; Vision; Cognitive process; Electroencephalogram signals; Facial Recognition; facial emotion recognition; Machine intelligence; privileged learning; Privileged learning; Reverse engineering",,,emotion,No,Yes
scopus,Exploring the Potential of Multimodal Emotion Recognition for Hearing-Impaired Children Using Physiological Signals and Facial Expressions,"Takir, S.; Toprak, E.; Uluer, P.; Erol Barkana, D.; Kose, H.",2023,,,,10.1145/3610661.3616240,"This study proposes an approach for emotion recognition in children with hearing impairments by utilizing physiological and facial cues and fusing them using machine learning techniques. The study is a part of a child-robot interaction project to support children with hearing impairments with affective applications in clinical setups and hospital environments and improve their social well-being. Physiological signals and facial expressions of children were collected and annotated by the collaborating psychologists as pleasant, unpleasant, and neutral, using the video recordings of the sessions. Both single and multimodal approaches are used to classify emotions using this data. The model trained using only facial expression features yielded a result of 43.67%. When only physiological data was used, the result increased to 58.68%. Finally, when the features of these two different modalities were fused in the feature layer, the accuracy further increased to 74.96% demonstrating that the multimodal approach for this data set has significantly improved the recognition of pleasant, unpleasant, and neutral emotions in children with hearing impairments. © 2023 Owner/Author.",facial expression; Emotion Recognition; Speech recognition; Face recognition; Facial Expressions; Physiological signals; Multimodal emotion recognition; Learning systems; Emotion detection; Multi-modal approach; Video recording; Multi-modality; physiological signals; sensor fusion; multimodality; Audition; Sensor fusion; child-machine interaction; Child-machine interaction; emotion detection; Hearing impaired; Hearing impairments,,,emotion,Yes,Yes
scopus,Emotion and Stress Recognition Utilizing Galvanic Skin Response and Wearable Technology: A Real-time Approach for Mental Health Care,"Hosseini, E.; Fang, R.; Zhang, R.; Rafatirad, S.; Homayoun, H.",2023,,,,10.1109/BIBM58861.2023.10386049,"In modern society, people are exposed to various stressors and negative emotions daily and they may cause mental and physical diseases such as depression, anxiety, high blood pressure, heart attacks, and stroke. Therefore, this paper delves into the potential of modern wearable technologies as a tool for real-time health monitoring. The advent of ubiquitous sensing has ushered in an era where physiological and behavioral measurements can be continuously recorded in daily life. One significant physiological marker is the Galvanic Skin Response (GSR), which exhibits noteworthy changes under different emotional states. We propose a machine learning-based emotion recognition framework. It includes a preprocessing stage that eliminates noise and extracts 87 features from the GSR data. To account for individual differences in physiological responses, we also introduce a novel normalization procedure per subject. Finally, a subset of dominant and discriminative features enhances the proposed framework's performance. We conducted experiments on two datasets, the wearable stress and affect detection dataset (WESAD) for stress detection, and the multimodal MAHNOB-HCI dataset for emotion recognition. The results show that the Leave-One-Out method is capable of detecting stress with 97.03% accuracy. Moreover, the proposed method classifies arousal and valence with an accuracy of 82.20% and 82.57%, respectively.  © 2023 IEEE.",Machine learning; Health care; Emotion Recognition; Speech recognition; Emotion recognition; Physiological signals; Electrophysiology; Physiological models; Wearable technology; Real- time; Mental health; Galvanic skin response; Physiological Signals; Stress detection; Real time systems; Applied machine learning; Applied Machine Learning; Blood pressure; Exposed to; Health care information system; Health care information systems; Stress Detection; Stress recognition,,,emotion,No,Yes
scopus,Mutual Cross-Attention in Dyadic Fusion Networks for Audio-Video Emotion Recognition,"Luo, J.; Phan, H.; Wang, L.; Reiss, J.",2023,,,,10.1109/ACIIW59127.2023.10388147,"Multimodal emotion recognition is a challenging problem in the research fields of human-computer interaction and pattern recognition. How to efficiently find a common subspace among the heterogeneous multimodal data is still an open problem for audio-video emotion recognition. In this work, we propose an attentive audio-video fusion network in an emotional dialogue system to learn attentive contextual dependency, speaker information, and the interaction of audio-video modalities. We employ pre-trained models, wav2vec, and Distract your Attention Network, to extract high-level audio and video representations, respectively. By using weighted fusion based on a cross-attention module, the cross-modality encoder focuses on the inter-modality relations and selectively captures effective information among the audio-video modality. Specifically, bidirectional gated recurrent unit models capture long-term contextual information, explore speaker influence, and learn intra-and inter-modal interactions of the audio and video modalities in a dynamic manner. We evaluate the approach on the MELD dataset, and the experimental results show that the proposed approach achieves state-of-the-art performance on the dataset.  © 2023 IEEE.",attention mechanism; Attention mechanisms; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Audio videos; Deep learning; Multimodal emotion recognition; Affective Computing; Human computer interaction; Modality Fusion; Learn+; Speech processing; Audio and video; affective computing; Research fields; modality fusion,,,emotion,No,Yes
scopus,Emotion Classification Through Speech Data Analysis,"Marcos, L.; Mai, K.V.; Abhari, A.",2023,,,,10.1109/WSC60868.2023.10408424,"Good quality healthcare services require effective communication between the patient and the healthcare provider. This work will help improve the areas of healthcare systems automation and optimization by applying Speech Emotion Recognition (SER) in health consultations to prevent miscommunication between patients and healthcare providers. Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D) was used to compare the performances of different machine learning models in classifying emotions. Before feeding the raw dataset to the models, exploratory data analysis was done to determine features that should be considered for future analysis. Our results showed that depending on the emotion, there are some syllables in the text that were emphasized or took time to be pronounced by the speaker. After data analysis, the dataset was fed into different models and determined that the Support Vector Machine (SVM) is a machine-learning model for SER.  © 2023 IEEE.",Speech emotion recognition; Health care; Emotion Recognition; Speech recognition; Learning systems; Classification (of information); Support vector machines; Data handling; Emotion classification; Machine learning models; Speech data; Effective communication; Health care providers; Healthcare services; Healthcare systems; Quality healthcare; System automa-tion,,,emotion,No,No
scopus,Bio-Signal Based Multimodal Fusion with Bilinear Model for Emotion Recognition,"Singh, A.; Holzer, N.; Gotz, T.; Wittenberg, T.; Gob, S.; Sawant, S.; Salman, M.-M.; Pahl, J.",2023,,,,10.1109/BIBM58861.2023.10385273,"In this paper, we propose a feature-based multimodal fusion deep-learning approach to combine biosignals, such as electrocardiogram (ECG) and galvanic skin response (GSR) for effective emotion recognition. We make use of a bilinear fusion method to fuse ECG and GSR based features and to jointly capture and complement the complex information from both modalities to enhance the performance of an emotion recognition system. The performance of the proposed fusion approach is evaluated on the MAHNOB-HCI dataset and compared with existing multimodal fusion approaches. Experimental results show that the combination of ECG and GSR features have a high discriminating ability in classifying (or predicting) the emotional states 'Arousal' and 'Valence' better than that of a single modality based emotion recognition. We also provide the model's uncertainty analysis to understand how confident the obtained predictions are.  © 2023 IEEE.",Emotion Recognition; Performance; Speech recognition; Emotion recognition; Deep learning; Multi-modal fusion; Electrophysiology; Multimodal fusion; Electrocardiograms; ECG; Galvanic skin response; GSR; Uncertainty analysis; Fusion methods; Biosignals; Learning approach; Bilinear models; Bio-signals; Deep feature; Deep features; Feature-based,,,emotion,No,Yes
scopus,Real-Time Facial Emotion Detection System Using Multimodal Fusion Deep Learning Architecture,"Sridhar, K.V.; Thripurala, S.",2023,,,,10.1109/ELEXCOM58812.2023.10370457,"One of the crucial applications in the area of computer vision is face emotion detection. Systems for detecting facial emotions are frequently utilized in fields of study such as the detection of human social and physiological interactions and the diagnosis of mental diseases. Identifying facial emotions with high accuracy is a challenging task. The capacity of the deep learning model to extract facial features from the input images determines how accurately emotions are detected. The deep learning model used in this work for emotion detection is the convolution and attention network (CoAtNet), which combines the convolutional neural networks (CNNs) with the vision transformers (ViTs). The FER2013 dataset is the one used for model training and validation. This work also includes training on various CNN models such as VGG16, ResNet50, DenseNet121, and Xception. Among all the models, the CoAtNet model provided the highest validation accuracy of 80.314%. To improve accuracy, the facial landmarks of the images are extracted and incorporated into the deep learning network through the Multimodal Fusion approach. By incorporating the facial landmarks through multimodal fusion, the validation accuracies of the deep learning networks are improved, and the highest accuracy of 83.458% is obtained with the CoAtN et model. Using the CoAtNet model, a face emotion detection system is implemented on the Raspberry Pi hardware, which detects the emotion of a person by capturing real-time images.  © 2023 IEEE.",Face recognition; Deep learning; Multi-modal fusion; Learning systems; Emotion detection; Convolutional neural networks; Convolution; Convolutional neural network; Image enhancement; Diagnosis; Facial emotions; Multimodal Fusion; Computer hardware; Convolution and attention network; Convolution and Attention Network (CoAtNet); Convolutional Neural Networks (CNNs); Face emotion detection; Face Emotion Detection; Facial landmark; Facial Landmarks; Raspberry pi hardware; Raspberry Pi Hardware; Vision transformer; Vision Transformers (ViTs),,,emotion,No,No
scopus,Multimodal Emotion Recognition Based on Deep Temporal Features Using Cross-Modal Transformer and Self-Attention,"Maji, B.; Swain, M.; Guha, R.; Routray, A.",2023,,,,10.1109/ICASSP49357.2023.10096937,"Multimodal speech emotion recognition (MSER) is an emerging and challenging field of research due to its more robust characteristics than unimodal. However, in multimodal approaches, the interactive relations for model building using different modalities of speech representations for emotion recognition have not been well investigated yet. To address this issue, we introduce a new approach to capturing the deep temporal features of audio and text. The audio features are learned with a convolution neural network (CNN) and a Bi-directional Gated Recurrent Unit (Bi-GRU) network. The textual features are represented by GloVe word embedding along with Bi-GRU. A cross-modal transformers block is designed for multimodal learning to capture better inter-and intra-interactions and temporal information between the audio and textual features. Further, a self-Attention (SA) network is employed to select more important emotional information from the fused multimodal features. We evaluate the proposed method on the IEMOCAP dataset on four emotion classes (i.e., angry, neutral, sad, and happy). The proposed method performs significantly better than the most recent state-of-The-Art MSER methods. © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Multimodal emotion recognition; Bi-directional; Cross-modal; Recurrent neural networks; Self-attention; Cross-modal transformer; Computer vision; Textual features; Audio features; cross-modal transformer; self-Attention; Temporal features,,,emotion,No,No
scopus,Multimodal Speech Emotion Recognition Using Modality-Specific Self-Supervised Frameworks,"Patamia, R.A.; Santos, P.E.; Acheampong, K.N.; Ekong, F.; Sarpong, K.; Kun, S.",2023,,,,10.1109/SMC53992.2023.10394418,"Emotion recognition is a topic of significant interest in assistive robotics due to the need to equip robots with the ability to comprehend human behavior, facilitating their effective interaction in our society. Consequently, efficient and dependable emotion recognition systems supporting optimal human-machine communication are required. Multi-modality (including speech, audio, text, images, and videos) is typically exploited in emotion recognition tasks. Much relevant research is based on merging multiple data modalities and training deep learning models utilizing low-level data representations. However, most existing emotion databases are not large (or complex) enough to allow machine learning approaches to learn detailed representations. This paper explores modality-specific pre-trained transformer frameworks for self-supervised learning of speech and text representations for data-efficient emotion recognition while achieving state-of-the-art performance in recognizing emotions. This model applies feature-level fusion using nonverbal cue data points from motion capture to provide multimodal speech emotion recognition. The model was trained using the publicly available IEMOCAP dataset, achieving an overall accuracy of 77.58% for four emotions, outperforming state-of-the-art approaches  © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Deep learning; Character recognition; Human robot interaction; Multi-modality; Recognition systems; Assistive robotics; Effective interactions; Human behaviors; Human-machine communication; Speech audio,,,emotion,No,Yes
scopus,AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations,"Anand, S.; Devulapally, N.K.; Bhattacharjee, S.D.; Yuan, J.; Chang, Y.-P.",2023,,,,10.1109/BigMM59094.2023.00013,"Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network (MAN) that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN ""injects""cross-modal attention via its Peripheral keyvalue pairs within each layer of a mode-specific Central query network. The resulting cross-attended mode-specific descriptors are then combined using an Adaptive Fusion (AF) technique that enables the model to integrate the discriminative and complementary mode-specific data patterns within an instance-specific multimodal descriptor. Given a dialogue represented by a sequence of utterances, the proposed AMuSE (Adaptive Multimodal Analysis for Speaker Emotion) model condenses both spatial (within-mode and within-utterance) and temporal (across-mode and across-utterances in the sequence) features into two dense descriptors: speaker-level and utterance-level. This helps not only in delivering better classification performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy) in large-scale public datasets (MELD and IEMOCAP) but also helps the users in understanding the reasoning behind each emotion prediction made by the model via its Multimodal Explainability Visualization module. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Audio videos; Multimodal sensing; Character recognition; Learning systems; Cross-modal interaction; Classification (of information); Large datasets; Multimodal analysis; Human machine interaction; Descriptors; Behavioral patterns; Multimodal Sensing; Supervised Learning; Temporal evolution,,,emotion,No,Yes
scopus,Quaternary classification of emotions based on electroencephalogram signals using hybrid deep learning model,"Singh, K.; Ahirwal, M.K.; Pandey, M.",2023,,14,,10.1007/s12652-022-04495-4,"Recognizing emotions from electroencephalography (EEG) signals is a trustworthy and reliable method to monitor the mental health of patients and the enthusiasm of individual behavioral feelings and polarity. Recently, the researcher focused on classifying EEG emotions from multimodal DEAP data set into binary (high and low) and ternary (low, medium and high) classes based on valence and arousal scale. However, for deep and intrinsic emotion recognition, multiclass classification is preferred. But non-linearity and non-stationary nature of EEG make it more challenging. Therefore, this paper proposes a hybrid deep learning model of one-dimensional Convolutional Neural Network (1DCNN) and Bidirectional Long Short-Term Memory (BI-LSTM) for multiclass emotion classification. The hybrid model linearly distributed desired class labels over two-dimensional emotion space (valence and arousal) as happy, relaxed, anger, and sad. The simulation results of the proposed model for binary, ternary and quaternary classification of emotion acquire 91.31%, 89.32% and 88.19% accuracy, respectively. Subject wise analysis of results has also been performed. The novelty of this work is the quaternary classification of emotions. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Long short-term memory; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Learning systems; Classification (of information); Electroencephalography; Convolutional neural networks; Biomedical signal processing; Electrophysiology; Learning models; Classification; Electroencephalogram signals; 1D-CNN; Arousal; BI-LSTM; EEG signal; Valence; 1d-CNN; Bidirectional long short-term memory; Classification of emotions; Electroencephalography signal,,,emotion,Yes,Yes
scopus,Multimodal Approach to Emotion Recognition using Deep Learning,"Ajay, K.; Naseeba, B.; Challa, N.P.; Arun Karthick, A.",2023,,,,10.1109/ICIMIA60377.2023.10426592,"The ability to recognize emotions is a complex and challenging task. Traditional approaches rely on handcrafted features, which are often not robust to variations in facial expressions and text. Deep learning methods have shown promise, but they have typically focused on a single modality. In this research, a deep learning-based multimodal approach to emotion identification is proposed. The proposed approach first uses two separate deep learning models to extract features from facial expressions and text. The features from the two models are then fused to make a final emotion prediction. The proposed method was tested on two datasets that were made available to the public, and it achieved competitive results on both datasets. The proposed approach is more robust to variations in facial expressions and text than traditional and single-modality deep learning approaches. It can also better capture the complex interplay between facial expressions and text. The proposed approach is a promising new approach to emotion recognition with the potential to be used in a variety of applications.  © 2023 IEEE.",Facial expressions; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Character recognition; Learning systems; Sentiment analysis; Multi-modal approach; Fusion model; Multimodal deep learning; Learning methods; Text sentiment analyse; Text sentiment analysis; Traditional approaches,,,emotion,No,No
scopus,One for All: A Unified Generative Framework for Image Emotion Classification,"Shi, G.; Deng, S.; Wang, B.; Feng, C.; Zhuang, Y.; Wang, X.",2023,,,,10.1109/TCSVT.2023.3341840,"Image Emotion Classification (IEC) is an essential research area, offering valuable insights into user emotional states for a wide range of applications, including opinion mining, recommendation systems, and mental health treatment. The challenges associated with IEC are mainly attributed to the complexity and ambiguity of human emotions, the lack of a universally accepted emotion model, and excessive dependence on prior knowledge. To address these challenges, we propose a novel Unified Generative framework for Image Emotion Classification (UGRIE), which is capable of simultaneously modeling various emotion models and capturing intricate semantic relationships between emotion labels. Our approach employs a flexible natural language template, converting the IEC task into a template-filling process that can be easily adapted to accommodate a diverse range of IEC tasks. To further enhance the performance, we devise a mapping mechanism to seamlessly integrate the multimodal pre-training model CLIP with the text generation pre-training model BART, thus leveraging the strengths of both models. A comprehensive set of experiments conducted on multiple public datasets demonstrates that our proposed method consistently outperforms existing approaches to a large margin in supervised settings, exhibits remarkable performance in low-resource scenarios, and unifies distinct emotion models within a single, versatile framework. IEEE",Semantics; Psychology; Data models; Emotion Recognition; Emotion recognition; Job analysis; Task analysis; Classification (of information); Large dataset; Emotion classification; Multi-modal learning; Adaptation models; Image classification; Pre-training; multi-modal learning; IEC; Image emotion classification; images emotion classification; Pre-training model; Training model,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition Dataset in the Wild (MERDWild),"Martinez, F.; Aguilera, A.; Mellado, D.",2023,,,,10.1109/CHILECON60335.2023.10418672,"Multimodal emotion recognition involves identifying human emotions in specific situations using artificial intelligence across multiple modalities. MERDWild, a multimodal emotion recognition dataset, addresses the challenge of unifying, cleaning, and transforming three datasets collected in uncontrolled environments with the aim of integrating and standardizing a database that encompasses three modalities: facial images, audio, and text. A methodology is presented that combines information from these modalities, utilizing ï¿½in-the-wildï¿½ datasets including AFEW, AffWild2, and MELD. MERDWild consists of 15 873 audio samples, 905 281 facial images, and 15 321 sentences, all of them considered usable quality data. The project outlines the entire process of data cleaning, transformation, normalization, and quality control, resulting in a unified structure for recognizing seven emotions.  © 2023 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Character recognition; Multimodal emotion recognition; Metadata; Multiple modalities; Human emotion; In-the-wild dataset; Audio samples; Datum transformation; Facial images; in-the-wild dataset; multimodal sources; Multimodal sources; Quality data,,,emotion,No,No
scopus,MULTIMODAL APPROACH FOR EMOTION RECOGNITION USING FEATURE FUSION,"Chowanda, A.",2023,,17,,10.24507/icicel.17.02.181,"Emotion recognition has been a challenge. Multimodality approach in emotion classification has been used in many research to improve the recognition performance. Nevertheless, there is a lack of understanding between how the multimodality affects the performance of the model. This paper uses IEMOCAP as dataset and creates several unimodal model and multimodal model resulting in combination of the top unimodal model for emotion recognition with feature fusion method which merges features from different models. After evaluating the models, this paper analyzes the connection of every unimodality involved and its implication to multimodality built. This paper also applies audio augmentation to reducing overfitting in model’s prediction. The top result of multimodal model consisting of 3 modalities achieves F1 score of 71.25% and the model consisting of 2 modalities achieves F1 score of 76.5%. ©2023 ICIC International.",Emotion recognition; Deep learning; Text classification; Image classification; Multimodal classification; Audio augmentations; Audio classification,,,emotion,No,Yes
scopus,Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition,"Vazquez-Rodriguez, J.; Lefebvre, G.; Cumin, J.; Crowley, J.L.",2023,,,,10.1109/ACII59096.2023.10388079,"Decades of research indicate that emotion recognition is more effective when drawing information from multiple modalities. But what if some modalities are sometimes missing? To address this problem, we propose a novel Transformer-based architecture for recognizing valence and arousal in a time-continuous manner even with missing input modalities. We use a coupling of cross-attention and self-attention mechanisms to emphasize relationships between modalities during time and enhance the learning process on weak salient inputs. Experimental results on the Ulm-TSST dataset show that our model exhibits an improvement of the concordance correlation coefficient evaluation of 37% when predicting arousal values and 30% when predicting valence values, compared to a late-fusion baseline approach.  © 2023 IEEE.",Machine Learning; Transformer; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Affective Computing; Transformers; Machine-learning; Multimodal Emotion Recognition; Multiple modalities; Input modalities; Learning process; Missing inputs,,,emotion,No,No
scopus,Multi-Still: A lightweight Multi-modal Cross Attention Knowledge Distillation method for the Real-Time Emotion Recognition Service in Edge-to-Cloud Continuum,"Jo, H.-K.; Seo, Y.; Hong, C.S.; Huh, E.-N.",2023,,,,10.1109/ATC58710.2023.10318524,"Recent advances in big data and artificial intelligence have led to active research in emotion recognition based on multimodal transformer models. Although these multimodal transformer models demonstrate high performance, their applications into real-time services are challenging due to their heavy computational requirements. Therefore, this study proposes a Multi-Still method, which transfers the multimodal knowledge of a teacher model to a student model using the knowledge distillation method supporting edge to cloud continuum environment. Multi-Still trained by text and voice data from Korean multimodal emotional datasets (KEMDy19, KEMDy20) both teacher and student. As a result, the student model transferred knowledge from the teacher model showed a 21% increase in number of inferences per second compared to the teacher model, 70.31% reduction in network size, and 65% reduction in the number of parameters. Nevertheless, it shows similar accuracy to the teacher model. We provide real-time emotion recognition services for the lightweight resources in edge continuum by efficiently learning multimodal data through knowledge distillation.  © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; multimodal; Distillation; Real-time emotion recognition; Knowledge distillation; Student Modeling; Students; Transformer modeling; knowledge distillation; % reductions; Distillation method; Information services; lightweight model; Lightweight model; Teacher models,,,emotion,No,Yes
scopus,Multimodal Approach: Emotion Recognition from Audio and Video Modality,"Kumar, H.; Martin, A.",2023,,,,10.1109/ICECA58529.2023.10395255,"Emotion recognition is a domain of artificial intelligence that recognizes human emotions similar to the cognitive capabilities of Affective computing. It is an interdisciplinary field of computer science that is emerging as a new area for research. Recognizing emotions from different input sources using computational intelligence is an advanced application of artificial intelligence and has benefitted numerous applications in human-computer interaction, healthcare, and psychology. The proposed work extensively studies a multimodal approach to emotion recognition that takes input from audio and video modalities. The study critically analyses the performance and improvement of emotion recognition deep learning models and fusion techniques of audio-video input modalities. The proposed approach demonstrates the effectiveness of the multimodal approach and the challenges of multimodal fusion while building a model on RAVDESS, and IEMOCAP dataset. In essence, the research strives to contribute to the growing body of knowledge in the field of audio-based emotion recognition by providing a comprehensive analysis of feature extraction techniques and deep learning models. By understanding the strengths and limitations of each approach, researchers and practitioners can make informed decisions when developing systems that accurately capture and interpret emotions from audio and video modalities. Conclusively, the advancement has the potential to enhance human-computer interaction, improve mental health assessment, and enrich various other domains where emotion plays a pivotal role.  © 2023 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Deep Learning; Multi-modal fusion; Learning systems; Affective Computing; Human computer interaction; Features extraction; Multi-modal approach; Learning models; Audio systems; Extraction; Feature extraction; Human emotion; Audio and video; Feature Extraction; Multimodal fusion formatting; Multimodal Fusion formatting,,,emotion,No,Yes
scopus,Multimodal Multifaceted Music Emotion Recognition Based on Self-Attentive Fusion of Psychology-Inspired Symbolic and Acoustic Features,"Zhao, J.; Yoshii, K.",2023,,,,10.1109/APSIPAASC58517.2023.10317539,"This paper describes automatic music emotion recognition (MER) that aims to estimate the valence and arousal (V/A) scores of a piece of piano music. The emotion is multifaceted in nature; it is rendered by various features that are often mutually dependent and inherent in music composition and performance. A basic approach to MER is to train a deep neural network (DNN) that extracts latent features representing the emotion as a whole and estimates the V/A scores, using only a limited amount of audio data with imbalanced V/A annotations. Such a black-box approach, however, suffers from limited performance and interpretability. To overcome these limitations, in this paper we propose a multimodal multifaceted MER method that fuses various kinds of musically-meaningful symbolic and acoustic features extracted from both MIDI and audio data, respectively, based on the expert knowledge of musical psychology. More specifically, our method separately extracts the affective features representing the rhythm, dynamics, melody, harmony, and tone color of a piano piece as the main factors affecting the emotion and integrates them with a self-attention mechanism that can learn the complicated cross-modal relationships. The experiments using the common EMOPIA dataset showed that the proposed model achieved the state-of-the-art V/A classification accuracy of 69.2% and that the multimodal and multifaceted feature fusions contributed to the performance improvement. © 2023 IEEE.",Emotion Recognition; Performance; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Audio data; Classification (of information); Music; Audio acoustics; Music emotions; Acoustic features; Music composition; Music performance; Piano music; Symbolic features,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition based on 2D Kernel Density Estimation for Multiple Labels Fusion,"Luo, Z.; Komatani, K.",2023,,,,10.1109/APSIPAASC58517.2023.10317262,"Psychological emotion can be typically categorized into discrete emotion states, such as anger, happiness, and neutrality labels, as well as estimated as degrees of a 2D continuous valence-arousal (VA) space. Previous studies on multimodal emotion recognition have employed fusion mechanisms for multiple modalities but treated emotion labels and VA degrees as separate recognition tasks. By modeling the relationship between these different labels, it becomes possible to leverage training datasets with different types of labels for improving multimodal emotion recognition. In this study, we explore the utilization of multiple labels by employing a 2D Kernel Density Estimation (2D-KDE) method to mathematically model their relations. Subsequently, we propose a label fusion layer (LFL) based on these relations to adjust the predicted probabilities of emotion states obtained from existing baselines of multimodal emotion recognition networks. Through extensive experiments, we demonstrate the effectiveness of our proposed model in improving emotion recognition performance and achieving superior results on the IEMOCAP and OMG-Emotion datasets. © 2023 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multiple modalities; Fusion mechanism; Fusion layers; Kernel Density Estimation; Kernel Density Estimation methods; Label fusions; Multiple labels; Statistics; Training dataset,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition using Acoustic and Visual Features,"Radha, N.; Swathika, R.; Shreya, P.S.",2023,,,,10.1109/ICDSAAI59313.2023.10452635,"Emotions are strong messengers that convey our inner experiences, wants, and aspirations. Furthermore, correctly understanding the emotions allows us to negotiate life's problems, make educated decisions, build meaningful connections with others, and develop emotional intelligence. This research aims at automatically determining the emotion of a person accurately using multimodal emotion recognition strategy which is a fusion of acoustic and visual modalities. The RAVDESS dataset has been used for the purpose of emotion detection. Machine learning algorithms such SVM, Random Forest, KNN, Gradient Boosting, MLP, Decision Tree, Nave Bayes, and Ensemble Learning techniques were used for testing and training to identify emotion from the auditory components. The Le-Net 5 model was used to identify emotion from visual imagery. Metrics like accuracy, confusion matrix and training testing validation loss were used to evaluate the performance of these models. The proposed technique uses high-quality audio and video data, with the acoustic ensemble method attaining 65% accuracy and the video CNN model obtaining an accuracy of 86%. The recognition accuracy increases to 94.5% when the acoustic and visual components are combined at model level. © 2023 IEEE.",Emotion Recognition; Speech recognition; Multimodal emotion recognition; Learning systems; Support vector machines; Support vectors machine; Audio acoustics; Decision trees; Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficients; Adaptive boosting; Support Vector Machine; Nearest neighbor search; Acoustic features; K-near neighbor; K-Nearest Neighbors; Lenet-5; LeNet-5; Machine components; Mel Frequency Cepstral Coefficients; Multi-Layer Perceptron; Multilayers perceptrons; Nearest-neighbour; Visual feature,,,emotion,No,Yes
scopus,"Multimodal music emotion recognition in Indonesian songs based on CNN-LSTM, XLNet transformers","Sams, A.S.; Zahra, A.",2023,,12,,10.11591/eei.v12i1.4231,"Music carries emotional information and allows the listener to feel the emotions contained in the music. This study proposes a multimodal music emotion recognition (MER) system using Indonesian song and lyrics data. In the proposed multimodal system, the audio data will use the mel spectrogram feature, and the lyrics feature will be extracted by going through the tokenizing process from XLNet. Convolutional long short term memory network (CNN-LSTM) performs the audio classification task, while XLNet transformers performs the lyrics classification task. The outputs of the two classification tasks are probability weight and actual prediction with the value of positive, neutral, and negative emotions, which are then combined using the stacking ensemble method. The combined output will be trained into an artificial neural network (ANN) model to get the best probability weight output. The multimodal system achieves the best performance with an accuracy of 80.56%. The results showed that the multimodal method of recognizing musical emotions gave better performance than the single modal method. In addition, hyperparameter tuning can affect the performance of multimodal systems. © 2023, Institute of Advanced Engineering and Science. All rights reserved.",Multimodal; CNN-LSTM; Indonesian song dataset; Mel spectrogram; Music emotion recognition; Stacking ensemble method; XLNet transformers,,,emotion,No,Yes
scopus,A multi-modal deep learning system for Arabic emotion recognition,"Abu Shaqra, F.; Duwairi, R.; Al-Ayyoub, M.",2023,,26,,10.1007/s10772-022-09981-w,"Emotion analysis is divided into emotion detection, where the system detects if there is an emotional state, and emotion recognition where the system identifies the label of the emotion. In this paper, we provide a multimodal system for emotion detection and recognition using Arabic dataset. We evaluated the performance of both audio and visual data as a unimodal system, then, we exposed the impact of integrating the information sources into one model. We examined the effect of gender identification on the performance. Our results show that identifying speaker’s gender beforehand increases the performance of emotion recognition especially for the models that rely on audio data. Comparing the audio-based system with the visual-based system demonstrates that each model performs better for a specific emotional label. 70% of the angry labels were predicted correctly using the audio model while this percentage was less using the visual model (63%). The accuracy obtained for the surprise class was (40.6%) using the audio model while it was (56.2%) using the visual model. The combination of both modalities improves accuracy. Our final result for the multimodal system was (75%) for the emotion detection task and (60.11%) for emotion recognition task and these results are among the top results achieved in this field and the first which focus on Arabic content. Specifically, the novelty of this work is expressed by exploiting deep learning and multimodal models in emotion analysis and applying it on a natural audio and video dataset for Arabic speaking persons. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Facial expressions; Speech emotion recognition; Emotion Recognition; Performance; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Modal analysis; Audio data; Deep learning; Learning systems; Emotion detection; Emotion analysis; Multi-modality; Multimodal system; Emotional arabic dataset; Emotional Arabic datasets; Multimodalities,,,emotion,Yes,Yes
scopus,Multimodal emotion recognition based on feature selection and extreme learning machine in video clips,"Pan, B.; Hirota, K.; Jia, Z.; Zhao, L.; Jin, X.; Dai, Y.",2023,,14,,10.1007/s12652-021-03407-2,"Multimodal fusion-based emotion recognition has attracted increasing attention in affective computing because different modalities can achieve information complementation. One of the main challenges for reliable and effective model design is to define and extract appropriate emotional features from different modalities. In this paper, we present a novel multimodal emotion recognition framework to estimate categorical emotions, where visual and audio signals are utilized as multimodal input. The model learns neural appearance and key emotion frame using a statistical geometric method, which acts as a pre-processer for saving computation power. Discriminative emotion features expressed from visual and audio modalities are extracted through evolutionary optimization, and then fed to the optimized extreme learning machine (ELM) classifiers for unimodal emotion recognition. Finally, a decision-level fusion strategy is applied to integrate the results of predicted emotions by the different classifiers to enhance the overall performance. The effectiveness of the proposed method is demonstrated through three public datasets, i.e., the acted CK+ dataset, the acted Enterface05 dataset, and the spontaneous BAUM-1s dataset. An average recognition rate of 93.53% on CK+, 91.62% on Enterface05, and 60.77% on BAUM-1s are obtained. The emotion recognition results acquired by fusing visual and audio predicted emotions are superior to both recognition of unimodality and concatenation of individual features. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Affective Computing; Multimodal fusion; Feature selection; Feature Selection; Features selection; Optimization; Complementation; Extreme learning machine; Knowledge acquisition; Learning machines; Video-clips; Evolutionary optimization; Evolutionary optimizations,,,emotion,No,Yes
scopus,Multi-Modal Based Fusion Emotion Identification Using Inception V3 Algorithm,"Subhasri, V.P.; Srivarshini, R.; Grace, R.K.",2023,,,,10.1109/ICSSAS57918.2023.10331693,"Multimodal emotion recognition aims to identify human emotions from various modalities, such as facial expressions, voice and text. Combining multiple modalities yields more accurate and reliable recognition results. Data collection, feature extraction, and classification are essential processes in emotion recognition. Datasets are used for data collection, while feature extraction extracts relevant features for emotional states. Deep learning techniques, such as Inception and deep learning models, are employed for classification. Multimodal emotion recognition can benefit industries like healthcare, education, and entertainment, but still faces challenges such as dataset size, feature extraction methods, and contextual data. A multimodal emotion recognition system combines multiple models to accurately identify emotions and improve performance. © 2023 IEEE.",Facial expressions; Emotion Recognition; Multi-modal; Speech recognition; Facial Expressions; Deep learning; Character recognition; Multimodal emotion recognition; Learning systems; Classification (of information); Features extraction; Learning models; Data acquisition; Data collection; Extraction; Feature extraction; Classification; Emotion identifications; Modality; Deep learning model; Deep learning models; Inception; Modalities,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition Based on Hybrid Fusion,"Yang, G.; Yang, D.; Li, J.; Wang, G.",2023,,,,10.1109/MLBDBI60823.2023.10481889,"Accurate emotion recognition can greatly improve the reliability of interpersonal communication and mental illness diagnosis. Aiming at the problem that the accuracy of single-modal emotion recognition is difficult to improve, a multimodal emotion recognition method is proposed by fusing speech, facial expression and EEG. The attention mechanism is introduced in the feature layer fusion, and the recognition accuracy is improved by designing the optimal weight allocation algorithm of the decision layer fusion. The results show that the recognition accuracy is 94.53% on the training set MAHNOB-HCI, 92.89% on the SEED dataset, and 91.54% on the self-built dataset, which indicates that the recognition accuracy is improved compared with the single-modal emotion recognition, and has good generalization ability. © 2023 IEEE.",Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Attention mechanism; Hybrid fusions; Diseases; Single-modal; Mental illness; Decision layers; Decision-layer; Inter-personal communications; Recognition accuracy,,,emotion,No,Yes
scopus,Emo-BERT: A Multi-Modal Teacher Speech Emotion Recognition Method,"Zhao, G.; Zhang, Y.; Chu, J.",2023,,,,10.1109/WAIE60568.2023.00022,"In the field of education, the existing automatic teacher emotion recognition is mainly realized by unimodal information such as audio, text, and facial expression. The effectiveness of multimodal speech emotion recognition methods has been demonstrated in the industrial field. To construct an accurate and feasible multimodal speech emotion recognition method, a neural network named Emo-BERT, which combines audio features with text content for teacher speech emotion recognition is proposed. The proposed Emo-BERT can deeply integrate the audio and text features of teacher utterance, and realize more efficient teacher speech emotion recognition. The audio encoder in Emo-BERT can effectively integrate the frame-level features and utterance-level features of teacher utterance through the attention pooling block, so that Emo-BERT can select valuable features more effectively. The proposed Emo-Bert achieves unweighted accuracy of 77.6% and 64.0% on IEMOCAP and MELD. A teacher emotion dataset (TED) was established from real teaching scenes to evaluate proposed method. The proposed method fills the gap of multimodal teacher speech emotion recognition, our proposed method has higher accuracy. © 2023 IEEE.",attention mechanism; Speech emotion recognition; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; deep learning; Deep learning; Character recognition; Multimodal emotion recognition; multimodal emotion recognition; Speech emotions; Teachers'; Recognition methods; Audio features; teacher speech emotion; Teacher speech emotion,,,emotion,No,Yes
scopus,Multimodal Speech Emotion Recognition Using Deep Learning and the Impact of Data Balancing,"Ahmed, T.; Begum, I.; Mia, M.S.; Tasnim, W.",2023,,,,10.1109/STI59863.2023.10464522,"In recent years, many studies have investigated the potential uses of recognizing emotions from speech in a variety of sectors, attracting a lot of attention to this topic. The performance of Speech Emotion Recognition (SER) is significantly impacted by effective emotional feature extraction from speech. In this paper, we have presented a novel approach of multimodal SER using the interactive emotional dyadic motion capture database. Our primary focus lies in overcoming the common challenges of imbalanced dataset in emotion recognition research, which often leads to reduce the accuracy. To address this issue, we have employed a data balancing technique. Our multimodal model initially has shown promising results even before the implementation of DB techniques, but the incorporation of DB techniques marked the turning point in our study and has been able to significantly improve the model's performances such as, precision to 77.64 %, recall to 77.21 %, F1-score to 77.42 % and accuracy to 79.32%. These outcomes unequivocally show the value of DB in overcoming the constraints of unbalanced dataset and support the efficacy of our proposed multimodal strategy for SER, along with providing insightful information for prospective uses in real-world situations as well as future developments in this field.  © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Deep Learning; Features extraction; Imbalanced dataset; Speech Emotion Recognition; Data balancing; Data Balancing; Motion capture; Recognizing emotions,,,emotion,No,Yes
scopus,Emotion Recognition Based on Wasserstein Distance Fusion of Audiovisual Features,"Ai, N.; Zhang, S.; Yi, N.; Ma, Z.",2023,,,,10.1109/RCAE59706.2023.10398807,"The task of multimodal emotion recognition aims to discern human emotions by utilizing both speech and facial expressions. Given the significance of complementarity between different modal sources, fusion has emerged as a prominent area of research in multimodal emotion recognition. However, when merging multiple modalities, two distinct challenges arise: the high dimensionality of feature vectors and redundancy of information. Furthermore, this complexity introduces intricacies into the data. To address these challenges, we introduce a model based on the Wasserstein distance. This model extracts meaningful features from each modality by minimizing the Wasserstein distance, reducing information redundancy through the selection of these meaningful features. Subsequently, the effective features from both modalities are integrated, partially mitigating the high dimensionality of the feature data and simplifying it. Finally, a classification task is applied to the integrated effective features to determine the ultimate emotional category. Experimental results conducted on the RA VDESS dataset demonstrate that our model outperforms post-fusion models by achieving an accuracy 3.7 % higher, reaching an impressive accuracy rate of 85.2%. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Facial Expressions; Multimodal emotion recognition; multimodal; Audio-visual features; Multiple modalities; Redundancy; Human emotion; fusion; Features vector; High dimensionality; Wasserstein distance,,,emotion,No,Yes
scopus,Multimodal emotion recognition based on long-distance modeling and multi-source data fusion,"Cai, J.; Lin, Y.; Ma, R.",2023,,,,10.1109/CIPAE60493.2023.00100,"With the prevalence of social media, people are increasingly inclined to express emotions in various forms such as text, images, audio, and video, which has made emotion recognition tasks more complex and challenging. Although traditional deep learning algorithms such as CNN and RNN have been widely used in multimodal emotion recognition, there are still challenges in fusing multimodal features and handling long-distance dependencies which affect the accuracy of emotion classification. To address these issues, this paper proposes a multimodal emotion recognition model based on long-distance modeling and multi-source data fusion. The model uses Transformer to encode concatenated multimodal feature vectors, and uses multi-head attention mechanism for feature fusion. It also introduces cross-layer residual connections to enhance the model's generalization ability. Finally, a multilayer perceptron is used to learn the complex relationship between feature vectors and achieve more accurate emotion classification results. The results on the IEMOCAP dataset show that the model's average accuracy reaches 59.3% and F1 value reaches 56.8%. Compared with the baseline methods, the proposed model improves the accuracy, demonstrating its effectiveness in emotion recognition tasks. © 2023 IEEE.",Transformer; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Character recognition; Multimodal emotion recognition; Emotion classification; Data fusion; Multimodal features; Features vector; Multi-head attention mechanism; Multi-head Attention Mechanism; Multisource data; Residual connection; Residual Connection,,,emotion,No,Yes
scopus,Integration of Facial and Speech Expressions for Multimodal Emotional Recognition,"Ruangdit, T.; Sungkhin, T.; Phenglong, W.; Phaisangittisagul, E.",2023,,,,10.1109/TENCON58879.2023.10322416,"The detection and interpretation of human emotions using a variety of sensory cues, including visual, auditory, body language, and physiological indicators, is commonly referred to as emotional recognition. This technology has a broad range of applications, including medical diagnosis, customer service satisfaction, and intelligent call center routing. While facial expressions represent one of the most conspicuous emotional cues utilized in emotional recognition, the integration of additional expressions may enhance its efficacy. To this end, this study proposes an integration of facial and speech expressions as emotional features for the prediction of emotions in machine learning models. The emotional predictions from facial and speech recognition models are subsequently combined to generate the final emotional prediction. Several facial and speech benchmark datasets are employed to evaluate the proposed methodology. The experimental results demonstrate significant promise and the continued development of emotional recognition as a supplementary system for assisting doctors in clinical and research studies appears feasible.  © 2023 IEEE.",Customer satisfaction; facial expression; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Integration; Speech features; Forecasting; Diagnosis; Human emotion; Emotional recognition; Body language; Clinical research; face mesh; Face mesh; Sensory cues; speech expression; Speech expression; speech features,,,emotion,No,No
scopus,Multimodal Learning with Incompleteness towards Multimodal Sentiment Analysis and Emotion Recognition Task,"Nguyen, C.-V.T.; Nguyen, H.-S.; Le, D.-T.; Ha, Q.-T.",2023,,,,10.1109/KSE59128.2023.10299456,"Multimodal machine learning tasks have gained significant popularity and demonstrated promising results in mul-timodal data analysis. However, existing multimodal approaches primarily focus on scenarios where all modalities have complete data, neglecting the challenges posed by missing data. In real-world settings, certain modalities may lack parallel sequences due to noise during data collection and preprocessing, leading to random missingness in each modality. In this paper, we propose an integrated model MM-Align+ that leverages information from three input modalities: audio, visual, and text, to address two important multimodal tasks: sentiment analysis and emotion recognition under missing data with varying rates. We conduct extensive experiments on three public datasets, namely MOSEI, MOSI, and MELD. The preliminary results demonstrate the promising performance of the MM-Align+ model and suggest potential for further improvements and novel insights.  © 2023 IEEE.",Machine learning; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Character recognition; multimodal sentiment analysis; Sentiment analysis; Multimodal sentiment analyse; Machine-learning; Missing modality; Multimodal machine learning; missing modality; alignment dynamics; Alignment dynamics; Missing data; multimodal machine learning,,,emotion,No,Yes
scopus,Joint modelling of audio-visual cues using attention mechanisms for emotion recognition,"Ghaleb, E.; Niehues, J.; Asteriadis, S.",2023,,82,,10.1007/s11042-022-13557-w,"Emotions play a crucial role in human-human communications with complex socio-psychological nature. In order to enhance emotion communication in human-computer interaction, this paper studies emotion recognition from audio and visual signals in video clips, utilizing facial expressions and vocal utterances. Thereby, the study aims to exploit temporal information of audio-visual cues and detect their informative time segments. Attention mechanisms are used to exploit the importance of each modality over time. We propose a novel framework that consists of bi-modal time windows spanning short video clips labeled with discrete emotions. The framework employs two networks, with each one being dedicated to one modality. As input to a modality-specific network, we consider a time-dependent signal deriving from the embeddings of the video and audio modalities. We employ the encoder part of the Transformer on the visual embeddings and another one on the audio embeddings. The research in this paper introduces detailed studies and meta-analysis findings, linking the outputs of our proposition to research from psychology. Specifically, it presents a framework to understand underlying principles of emotion recognition as functions of three separate setups in terms of modalities: audio only, video only, and the fusion of audio and video. Experimental results on two datasets show that the proposed framework achieves improved accuracy in emotion recognition, compared to state-of-the-art techniques and baseline methods not using attention mechanisms. The proposed method improves the results over baseline methods by at least 5.4%. Our experiments show that attention mechanisms reduce the gap between the entropies of unimodal predictions, which increases the bimodal predictions’ certainty and, therefore, improves the bimodal recognition rates. Furthermore, evaluations with noisy data in different scenarios are presented during the training and testing processes to check the framework’s consistency and the attention mechanism’s behavior. The results demonstrate that attention mechanisms increase the framework’s robustness when exposed to similar conditions during the training and the testing phases. Finally, we present comprehensive evaluations of emotion recognition as a function of time. The study shows that the middle time segments of a video clip are essential in the case of using audio modality. However, in the case of video modality, the importance of time windows is distributed equally. © 2022, The Author(s).",Attention mechanisms; Emotion Recognition; Embeddings; Speech recognition; Emotion recognition; Affective Computing; Human computer interaction; Affective computing; Multi-modal learning; Multimodal learning; Audio-visual; Video cameras; Video-clips; Function evaluation; Time segments; Time windows; Visual cues,,,emotion,Yes,Yes
scopus,A Privacy-Preserving Multi-Task Learning Framework for Emotion and Identity Recognition from Multimodal Physiological Signals,"Benouis, M.; Can, Y.S.; Andre, E.",2023,,,,10.1109/ACIIW59127.2023.10388160,"The increasing popularity of empathetic sensors can play a significant role in the affective computing era. Recognizing human emotion from these unobtrusive devices is an important building block in this context. Multi-task learning has been studied extensively for various machine learning tasks, including affective computing, which uses the shared information between related tasks to improve performance. Since the physiological data from the mentioned sensors contain private data, they can also lead to privacy threats by exposing highly sensitive information. To address this issue, we combine differential privacy and federated learning approaches with multi-task learning to efficiently recognize the user's mental stress while perturbing private user identity information. More concretely, the proposed framework improves the performance of emotion recognition tasks by taking advantage of multi-task learning and preserving privacy. We extensively evaluate our framework with the employed dataset: results show an accurate emotion recognition of 90% while limiting the re-identification accuracies to 47%.  © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Learning systems; Affective Computing; Multitask learning; Learning frameworks; Recognizing Human Emotion; Building blockes; Identity recognition; Privacy preserving; Privacy-preserving techniques,,,emotion,No,Yes
scopus,A Multitask Framework for Emotion Recognition Using EEG and Eye Movement Signals with Adversarial Training and Attention Mechanism,"Liu, W.; Luo, Y.; Lu, Y.; Lu, Y.",2023,,,,10.1109/BIBM58861.2023.10385505,"Affective brain-computer interface is one of the research frontiers which could promote the development of artificial intelligence and which might help the diagnosis and treatment of mental health diseases. In the field of emotion recognition with EEG and eye movement signals, however, it is challenging to build a model that can extract emotion-related information, fuse multiple modalities, and preserve the complementary characteristics at the same time. In this paper, we proposed a multi-task framework with adversarial training and attention mechanism (ATAM) for emotion recognition using EEG and eye movement signals. An adversarial training scheme is designed by maximizing mutual information loss within the same modalities and minimizing the cosine similarity loss between different modalities. With this design, the proposed ATAM not only preserved emotional information in EEG and eye movement signals, but also kept the complementary property between these two modalities. Attention mechanism was used to fuse multimodal features adaptively. Our evaluation of the model using the SEED and SEED-IV datasets revealed that the proposed method surpassed existing methodologies in recognition accuracy. Further analysis of the loss curves and attentional weight distributions indicated the effectiveness of ATAM in transforming and incorporating multi-modal properties. The adversarial training set-up, epitomized in our ablation study, was necessary for good performance.  © 2023 IEEE.",attention mechanism; emotion; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion; Emotion recognition; Multi tasks; Biomedical signal processing; EEG; eye movement; Eye movements; adversarial training; Multiple modalities; Mental health; Diagnosis; Training schemes; Brain computer interface; Adversarial training; Complementary characteristics; Research frontiers,,,emotion,No,Yes
scopus,Music Emotion Analysis Based on Multimodal Intelligence,"Bao, W.",2023,,228,,10.1016/j.procs.2023.11.064,"In response to the problem of low classification efficiency in existing algorithms that only consider single modal features of music or video, the author proposes a music emotion classification algorithm based on multimodal deep learning. Firstly, a two-dimensional audio Convolutional neural network is designed, which takes the audio Mel spectrum as the input to learn the audio characteristics of music; Secondly, a video neural network was designed to learn the spatiotemporal characteristics of music videos; Adopting multimodal fusion technology, we fused audio and video features and designed a multimodal deep learning classification algorithm to classify music emotions. In response to the lack of labeled music video datasets, the author constructed a diverse music video dataset. Based on this dataset, experiments were conducted to verify the effectiveness of the proposed algorithm, and the impact of different optimizers on the performance of single mode classification models was compared and analyzed. The experimental results show that compared with a single modal sentiment classifier, a multimodal classifier can achieve the best classification performance.  © 2023 The Authors. Published by Elsevier B.V.",Emotion Recognition; Multi-modal; Modal analysis; Deep learning; Classification (of information); Learn+; Multimodal; Music; Convolutional neural networks; Learning algorithms; Audio acoustics; Video dataset; Music emotions; Classification algorithm; Single-modal; Analyse; Analysis; Intelligence; Music video,,,emotion,Yes,Yes
scopus,Multistage linguistic conditioning of convolutional layers for speech emotion recognition,"Triantafyllopoulos, A.; Reichel, U.; Liu, S.; Huber, S.; Eyben, F.; Schuller, B.W.",2023,,5,,10.3389/fcomp.2023.1072479,"Introduction: The effective fusion of text and audio information for categorical and dimensional speech emotion recognition (SER) remains an open issue, especially given the vast potential of deep neural networks (DNNs) to provide a tighter integration of the two. Methods: In this contribution, we investigate the effectiveness of deep fusion of text and audio features for categorical and dimensional SER. We propose a novel, multistage fusion method where the two information streams are integrated in several layers of a DNN, and contrast it with a single-stage one where the streams are merged in a single point. Both methods depend on extracting summary linguistic embeddings from a pre-trained BERT model, and conditioning one or more intermediate representations of a convolutional model operating on log-Mel spectrograms. Results: Experiments on the MSP-Podcast and IEMOCAP datasets demonstrate that the two fusion methods clearly outperform a shallow (late) fusion baseline and their unimodal constituents, both in terms of quantitative performance and qualitative behavior. Discussion: Overall, our multistage fusion shows better quantitative performance, surpassing alternatives on most of our evaluations. This illustrates the potential of multistage fusion in better assimilating text and audio information. Copyright © 2023 Triantafyllopoulos, Reichel, Liu, Huber, Eyben and Schuller.",machine learning; natural language processing; multimodal fusion; speech emotion recognition; speech processing,,,emotion,No,Yes
scopus,CH-MEAD: A Chinese Multimodal Conversational Emotion Analysis Dataset with Fine-Grained Emotion Taxonomy,"Ruan, Y.-P.; Zheng, S.-K.; Huang, J.; Zhang, X.; Liu, Y.; Li, T.",2023,,,,10.1109/APSIPAASC58517.2023.10317512,"Emotion recognition in conversations (ERC) is important for building human-like chatbots. Much research work has been devoted to the English community, however, there is still missing large-scale multimodal ERC dataset for the Chinese community. On the other hand, emotion states in the dynamic flow of conversations are typically subtle and complex. But currently most adopted emotion taxonomy for emotion recognition in conversations is based on Ekman's six basic emotion categories [1], which cannot cover the diverse set of emotions occurring in multi-turn dialogues, such as the emerging states of anxiety, puzzled and so on. In this paper, we present a Chinese multimodal conversational emotion analysis dataset (CH-MEAD), which contains 25, 292 video segments (utterances) labeled for 26 emotion categories (including neutral) and are collected from more than 400 Chinese TV series, movies and show. To our best knowledge, the CH-MEAD is the first large-scale Chinese multimodal conversational dataset with fine-grained emotion taxonomy, which is suitable for studying the speaker's subtle and complex emotion states in dialogues and will also promote the development of multimodal ERC in Chinese research community. Based on the CH-MEAD dataset, we propose a speaker-aware contextual multimodal fusion (SCMF) network and demonstrate its efficiency over the current SOTA models in our experiments. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; Taxonomies; Emotion analysis; Large dataset; Complex networks; Chatbots; Fine grained; Large-scales; Dynamic flows; Human like; Still missing,,,emotion,Yes,No
scopus,BEmoFusionNet: A Deep Learning Approach for Multimodal Emotion Classification in Bangla Social Media Posts,"Taheri, Z.S.; Roy, A.C.; Kabir, A.",2023,,,,10.1109/ICCIT60459.2023.10441295,"Multimodal emotion classification, incorporating both image and text modalities, has gained significant attention due to the exponential growth of multimedia data. This research aims to develop a robust system for multimodal emotion classification in Bangla social media content, utilizing both image and text data. Despite limited resources, understanding emotions in Bangla is crucial for mental health interventions. Unlike previous studies focused on well-resourced languages, this research addresses the gap by specifically targeting regional languages like Bangla, going beyond the traditional positive, negative, and neutral classes. This work presents a multimodal Bangla social post dataset containing 4660 samples. Transfer learning techniques, utilizing pre-trained models like ResNet50, VGG16, and InceptionV3, extract visual features, while deep learning architectures such as BiLSTM and CNN are employed for textual content analysis. Multimodal learning techniques, including feature fusion and decision fusion, are explored to combine visual and textual representations. We evaluated the feature fusion of InceptionV3 and BiLSTM features on our Bangla social media post dataset. Our approach achieved a weighted f1-score of 77.50%.  © 2023 IEEE.",Natural language processing; Multi-modal; Deep learning; Learning systems; Social media; Social networking (online); Multimodal; Features fusions; Learning algorithms; Emotion classification; Language processing; Natural languages; Transfer learning; Natural language processing systems; Feature Fusion; Text processing; Learning techniques; Learning approach; Decisions fusion; Decision Fusion,,,emotion,No,Yes
scopus,A systematic survey on multimodal emotion recognition using learning algorithms,"Ahmed, N.; Aghbari, Z.A.; Girija, S.",2023,,17,,10.1016/j.iswa.2022.200171,"Emotion recognition is the process to detect, evaluate, interpret, and respond to people's emotional states and emotions, ranging from happiness to fear to humiliation. The COVID- 19 epidemic has provided new and essential impetus for emotion recognition research. The numerous feelings and thoughts shared and posted on social networking sites throughout the COVID-19 outbreak mirrored the general public's mental health. To better comprehend the existing ecology of applied emotion recognition, this work presents an overview of different emotion acquisition tools that are readily available and provide high recognition accuracy. It also compares the most widely used emotion recognition datasets. Finally, it discusses various machine and deep learning classifiers that can be employed to acquire high level features for classification. Different data fusion methods are also explained in detail highlighting their benefits and limitations. © 2022 The Author(s)",Machine learning; Emotion Recognition; Emotional state; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Learning systems; Machine-learning; Learning algorithms; Data fusion; Virtual reality; E-learning; Fine grained; Data fusion methods; Fine grained emotion; Fine grained emotions; General publics; Social-networking,,,emotion,No,Yes
scopus,A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning,"Ai, W.; Zhang, F.; Meng, T.; Shou, Y.; Shao, H.; Li, K.",2023,,,,10.1109/ICPADS60453.2023.00067,"In terms of human-computer interaction, it is becoming more and more important to correctly understand the user's emotional state in a conversation, so the task of multimodal emotion recognition (MER) started to receive more attention. However, existing emotion classification methods usually perform classification only once. Sentences are likely to be misclassified in a single round of classification. Previous work usually ignores the similarities and differences between different morphological features in the fusion process. To address the above issues, we propose a two-stage emotion recognition model based on graph contrastive learning (TS-GCL). First, we encode the original dataset with different preprocessing modalities. Second, a graph contrastive learning (GCL) strategy is introduced for these three modal data with other structures to learn similarities and differences within and between modalities. Finally, we use MLP twice to achieve the final emotion classification. This staged classification method can help the model to better focus on different levels of emotional information, thereby improving the performance of the model. Extensive experiments show that TS-GCL has superior performance on IEMOCAP and MELD datasets compared with previous methods. © 2023 IEEE.",Emotion Recognition; Performance; Speech recognition; Modal analysis; Multimodal emotion recognition; Learning systems; Human computer interaction; Classification (of information); multimodal emotion recognition; Graphic methods; Recognition models; Emotion classification; Convolutional networks; Graph convolutional network; Classification methods; Model-based OPC; graph contrastive learning; Graph contrastive learning; graph convolutional network; two-stage classification; Two-stage classification,,,emotion,No,Yes
scopus,Emotional speech-based personality prediction using NPSO architecture in deep learning,"Rangra, K.; Kadyan, V.; Kapoor, M.",2023,,25,,10.1016/j.measen.2022.100655,"Speech is an effective way for analyzing mental and psychological health of a speaker's. Automatic speech recognition has been efficiently investigated for human-computer interaction and understanding the emotional & psychological anatomy of human behavior. Emotions and personality are studied to have a strong link while analyzing the prosodic speech parameters. The work proposes a novel personality and emotion classification model using PSO (particle swarm optimization) based CNN (convolution neural network): (NPSO) that predicts both (emotion and personality) The model is computationally efficient and outperforms language models. Cepstral speech features MFCC (mel frequency cepstral constants) is used to predict emotions with 90% testing accuracy and personality with 91% accuracy on SAVEE(Surrey Audio-Visual Expressed Emotion) individually. The correlation between emotion and personality is identified in the work. The experiment uses the four corpora SAVEE, RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song), CREMAD (Crowd-sourced Emotional Multimodal Actors Dataset, TESS (Toronto emotional speech set) corpus, and the big five personality model for finding associations among emotions and personality traits. Experimental results show that the classification accuracy scores for combined datasets are 74% for emotions and 89% for Personality classifications. The proposed model works on seven emotions and five classes of personality. Results prove that MFCC is enough effective in characterizing and recognizing emotions and personality simultaneously. © 2022",Emotion Recognition; Speech recognition; CNN; Behavioral research; Deep learning; Human computer interaction; Classification (of information); Speech features; Emotion classification; Big five; Particle swarm optimization (PSO); Convolution neural network; Cepstral; Emotion-classification; Mel frequencies; Mel frequency cepstral constant; MFCC; OCEAN (big five); OCEAN (Big five); Particle swarm; Particle swarm optimization; Personality-classification; PSO; Swarm optimization,,,emotion,No,Yes
scopus,FV2ES: A Fully End2End Multimodal System for Fast Yet Effective Video Emotion Recognition Inference,"Wei, Q.; Huang, X.; Zhang, Y.",2023,,69,,10.1109/TBC.2022.3215245,"In the latest social networks, more and more people prefer to express their emotions in videos through text, speech, and rich facial expressions. Multimodal video emotion analysis techniques can help understand users' inner world automatically based on human expressions and gestures in images, tones in voices, and recognized natural language. However, in the existing research, the acoustic modality has long been in a marginal position as compared to visual and textual modalities. That is, it tends to be more difficult to improve the contribution of the acoustic modality for the whole multimodal emotion recognition task. Besides, although better performance can be obtained by introducing common deep learning methods, the complex structures of these training models always result in low inference efficiency, especially when exposed to high-resolution and long-length videos. Moreover, the lack of a fully end-to-end multimodal video emotion recognition system hinders its application. In this paper, we designed a fully multimodal video-to-emotion system (named FV2ES) for fast yet effective recognition inference, whose benefits are threefold: (1) The adoption of the hierarchical attention method upon the sound spectra breaks through the limited contribution of the acoustic modality, and outperforms the existing models' performance on both IEMOCAP and CMU-MOSEI datasets; (2) the introduction of the idea of multi-scale for visual extraction while single-branch for inference brings higher efficiency and maintains the prediction accuracy at the same time; (3) the further integration of data pre-processing into the aligned multimodal learning model allows the significant reduction of computational costs and storage space.  © 1963-12012 IEEE.",emotion; Transformer; Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion; Emotion recognition; Deep learning; Character recognition; Data mining; Features extraction; Multimodal; Data handling; Extraction; Feature extraction; Digital storage; Data visualization; End to end; Efficiency; fully end-to-end; Fully end-to-end; inference; Inference,,,emotion,No,Yes
scopus,Hybrid Multi-Task Learning for End-To-End Multimodal Emotion Recognition,"Chen, J.; Li, Y.; Zhao, Z.; Liu, X.; Wen, Z.; Tao, J.",2023,,,,10.1109/APSIPAASC58517.2023.10317160,"Multimodal emotion recognition plays a pivotal role in the advancement of natural human-computer interaction systems. Recent studies have attempted to apply multi-task learning to emotion recognition. However, the multi-task shared feature extractor of traditional methods needs to integrate the feature representations of different tasks, which may lead to the feature extractor failing to focus on the learning of emotion representations. To address this problem, we propose a hybrid multi-task learning framework for end-to-end multimodal emotion recognition, in which the primary task is emotion classification, and the auxiliary tasks are emotion regression and gender classification. This framework consists of two networks specialized in gender and emotion recognition, where the latter transfers knowledge from the former through our proposed Deep Aggregation LSTM (DA-LSTM). The DA-LSTM could more precisely capture emotional information in discourse by aggregating emotion and gender feature extractors. Experimental results on a commonly used dataset IEMOCAP demonstrate the effectiveness of our proposed method. © 2023 IEEE.",Long short-term memory; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Human computer interaction; Multi tasks; Multitask learning; End to end; Feature representation; Emotion representation; Feature extractor; Human-computer interaction system; Natural human computer interactions,,,emotion,No,No
scopus,Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition,"Li, D.; Wang, Y.; Funakoshi, K.; Okumura, M.",2023,,,,,"Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion recognition (JOYFUL), where multimodality fusion, contrastive learning, and emotion recognition are jointly optimized. Specifically, we first design a new multimodal fusion mechanism that can provide deep interaction and fusion between the global contextual and uni-modal specific features. Then, we introduce a graph contrastive learning framework with inter-view and intra-view contrastive losses to learn more distinguishable representations for samples with different sentiments. Extensive experiments on three benchmark datasets indicate that JOYFUL achieved state-of-the-art (SOTA) performance compared to all baselines. ©2023 Association for Computational Linguistics.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; 'current; Modality Fusion; Benchmarking; Graphic methods; Multiple modalities; Computational linguistics; Human machine interaction; Contextual feature; Graph-based methods; ITS applications; Multi-modality fusion,,,emotion,No,Yes
scopus,A multimodal hyperlapse method based on video and songs’ emotion alignment,"de Matos, D.; Ramos, W.; Silva, M.; Romanhol, L.; Nascimento, E.R.",2023,,166,,10.1016/j.patrec.2022.08.014,"With the recent growth in the use of social media and new digital devices like smartphones and wearable cameras, people are often recording long first-person videos of their daily activities. These videos are usually very long and tiring to watch, bringing the need to speed them up. Recent fast-forward methods do not consider the background music to be inserted into the video, which could make them even more enjoyable. In this paper, we present a new fast-forward method that considers the information present in the video and the background music. We use neural networks to automatically recognize the emotions induced in the video and song and combine the contents in the accelerated video through a new method of frame selection that has as main objective to maximize the similarity of the induced emotions. We present quantitative and qualitative experiments on a large dataset with different videos and songs, showing that our method achieves the best performance in matching emotion similarity, also keeping the video's visual quality. © 2022 Elsevier B.V.",Multi-modal; Social media; Large dataset; First person; Digital devices; Acoustic features; Background musics; Dynamic time warping; Fast forward; First-person video; First-Person video; Hyperlapse; Multimodal frame sampling,,,emotion,No,Yes
scopus,Research on Emotion Recognition Based on Parameter Transfer and Res2Net,"Wu, Y.; Liu, W.; Li, Q.",2023,,,,10.1109/ICFTIC59930.2023.10455873,"In the field of emotion recognition, physiological signals have gradually become a hot object of research because they can objectively reflect real emotions. However, it is difficult to describe emotions completely and accurately with a single signal. Multiple physiological signal fusion models establish a unified classification model through the consistency and complementarity of different physiological signals to improve recognition performance. However, the current multi-modal physiological signal emotion recognition has insufficient information exchange. Aiming at the above problems, this paper proposes a multi-modal physiological signal emotion classification model. Feature extraction of four modes of physiological signals of EEG, EOG, EMG and Skin Electricity is carried out by the method of parameter migration, which saves network training time and improves learning performance. Emotion recognition of multi-modal physiological signals is realized by using Res2Net to more fully mine emotional feature information and fuse it effectively. The experimental results show that the proposed model achieves 95.27% and 94.71% accuracy in the binary classification task of arousal and potency of the DEAP dataset, respectively.  © 2023 IEEE.",Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Deep learning; Multi-modal fusion; Classification (of information); Biomedical signal processing; Physiological models; Multimodal fusion; Fusion model; Signal fusions; Classification models; Parameter transfers,,,emotion,No,Yes
scopus,Enhancing Emotion Recognition using Contrastive Learning Representation of Paired Peripheral Physiological and EEG Signal,"Laksana, I.K.P.B.; Yonatan, M.A.; Parimartha, P.A.; Hongastu, V.A.; Parmonangan, I.H.",2023,,,,10.1109/ICCED60214.2023.10425227,"Human emotions play a fundamental role in human-computer interaction, and accurately recognizing emotions is essential for enhancing user-friendly experiences in HCI applications. Emotions are complex and cannot be simply identified by a single modality, making multimodal approaches crucial. In this study, we propose a cross-modal representation learning approach for emotion recognition using peripheral physiological signals (PPS) and electroencephalogram (EEG). We focus on leveraging PPS, which is easier to obtain in real-life scenarios due to wearable technologies' availability. Through contrastive learning with paired EEG signals, we aim to enhance the capabilities of PPS representation and achieve comparable performance to EEG-based models in downstream unsegmented emotion recognition tasks. We utilized the DEAP dataset and low or high arousal category for the downstream task. The results of the study demonstrate the successful learning of PPS representations that closely resemble EEG signals representations, leading to comparable performance on downstream tasks. © 2023 IEEE.",Emotion Recognition; Performance; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Physiology; Human computer interaction; Electroencephalography; Biomedical signal processing; EEG; Contrastive learning; Wearable technology; Human emotion; Down-stream; Representation learning; Electroencephalogram signals; Peripheral signal; contrastive learning; representation learning; peripheral signal; Signal representations,,,emotion,No,Yes
scopus,Audio-Visual Emotion Recognition Based on Multi-Scale Channel Attention and Global Interactive Fusion,"Zhang, P.; Zhao, H.; Li, M.; Chen, Y.; Zhang, J.; Wang, F.; Wu, X.",2023,,,,10.1109/SMC53992.2023.10394129,"Facial expressions and speech are the most natural and common ways for humans to express their emotions. Automatic audio-visual emotion recognition has attracted a lot of attention in recent years. However, previous methods cannot effectively exploit the complementarity between modalities when performing feature fusion, and the resulting fused feature representations may contain redundant information from different modalities. This paper proposes an audio-visual emotion recognition model based on multi-scale channel attention (MCA) and global interactive fusion (GIF). The MCA module is designed to extract modal key emotional features at multiple contextual scales to express human emotions. Then, feature fusion is performed by using the GIF module, which is implemented in two steps. In the first step, features are fused through a global interactive attention layer, which considers the global interactive information of both intra and inter modalities, and reduces the redundancy of fused feature representations by fusing only the attention scores. In the second step, multiple convolutional neural networks with different kernel sizes are used to further learn the multi-scale emotional information in the fused features that is meaningful for both modalities. The proposed model is verified on two multimodal emotion datasets, the RAVDESS and the SAVEE, and achieves accuracies of 84.12% and 98.71%, respectively, with only 2M parameters.  © 2023 IEEE.",attention mechanism; convolutional neural network; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Facial Expressions; Multimodal emotion recognition; multimodal emotion recognition; Convolutional neural networks; Features fusions; Convolution; Convolutional neural network; Recognition models; feature fusion; Audio-visual; Multi-scales; Feature representation,,,emotion,No,Yes
scopus,Exploring Emotion and Sentiment Landscape of Depression: A Multimodal Analysis Approach,"Anekar, D.; Deshpande, Y.; Suryawanshi, R.; Waman, R.; Divekar, V.; Salunke, R.",2023,,,,10.1109/GCAT59970.2023.10353344,"Conventionally depression detection was done through extensive clinical interviews, wherein the subject's responses are studied by the psychologist to determine his/her mental state. In our model, we try to imbibe the sentiment analysis and emotion detection approach by capturing video and analyzing facial and audio input to identify the symptoms of depression. The system is built with a video capturing containing an emotion recognition, utilizing Haar Cascade classifier, CNN models, and an audio processing model using Google API, NLP forwarded with Naïve Bayes Classifier. We have used FER 2013 dataset. The multimodal approach gives almost 77% accuracy, which can be integrated with the self-assessment questionnaire to improve the accuracy of depression detection. Many depressed individuals cannot receive the right care since there are few objective ways to assess depression. The results of this model with minor modifications will be incorporated into the system for early detection along with intervention for depression and anxiety amongst youth well-being in our society. © 2023 IEEE.",depression; Depression; emotion; Emotion Recognition; Multi-modal; CNN; Emotion; Modal analysis; Sentiment analysis; Classification (of information); multimodal; Multimodal analysis; NLP; Sentiment; sentiment; Analysis approach; Clinical interview; Haar; Mental state,,,emotion,No,Yes
scopus,Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts,"Singh, G.V.; Ghosh, S.; Verma, A.; Painkra, C.; Ekbal, A.",2023,,,,,"Due to its growing impact on public opinion, hate speech on social media has garnered increased attention. While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content. The interpretability of such models has received very little attention, despite the social and legal consequences of erroneous predictions. In this work, we present a novel problem of Distress Identification and Cause Extraction (DICE) from multimodal online posts. We develop a multi-task deep framework for the simultaneous detection of distress content and identify connected causal phrases from the text using emotional information. The emotional information is incorporated into the training process using a zero-shot strategy, and a novel mechanism is devised to fuse the features from the multimodal inputs. Furthermore, we introduce the first-of-its-kind Distress and Cause annotated Multimodal (DCaM) dataset of 20,764 social media posts. We thoroughly evaluate our proposed method by comparing it to several existing benchmarks. Empirical assessment and comprehensive qualitative analysis demonstrate that our proposed method works well on distress detection and cause extraction tasks, improving F1 and ROS scores by 1.95% and 3%, respectively, relative to the best-performing baseline. The code and the dataset can be accessed from the following link: https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DICE. ©2023 Association for Computational Linguistics.",Multi-modal; Social media; Social networking (online); Multi tasks; Automated methods; Textual content; Interpretability; Emotional information; Social aspects; Extraction; Public opinions; Zero-shot learning; Simultaneous detection; Training process,,,emotion,Yes,No
scopus,Audio-Video Based Fusion Mechanism Using Deep Learning for Categorical Emotion Recognition,"Kumar, H.; Martin, A.",2023,,,,10.1109/ICSSAS57918.2023.10331779,"Multimodal emotion recognition is an advanced version of an artificial emotional intelligence system. Various solutions have been proposed to enhance the performance and emotion recognition rate. Multimodal is also one of the recent advanced research areas to classify and recognize emotions in realtime, consisting of physiological signals, facial expressions, audio-visual, audio-text, and video-text fusion. The proposed work uses advanced deep learning models to recognize the seven categorical emotions from the audio and video modality. Different modalities' feature extraction techniques and landmarks, such as Audio and Video, play a vital role in recognizing emotions. It also illustrates the Audio-video-based feature fusion mechanism. It computes the complementary relationship between the different modalities and shows the improved performance using the RESNET20 model for audio modality and the 3DCNN model for video modality on the RAVDESS dataset. Opensmile toolkit is used to extract the compelling features from the audio model and fuse them with extracted features of video modality. Feature fusion mechanism fuses the extracted features of audio and video data to classify and enhance the accuracy of recognized categorical emotions. It also briefly discusses the various audio-video features fusion approaches, computational accuracy and robustness of the proposed model. It shows a result of 71.3% and 69.7%. Accuracy on validation and test datasets outperforms the state-of-the-art models in the multimodal emotion recognition domain with a fusion mechanism. © 2023 IEEE.",Facial expressions; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Physiological signals; Physiology; Audio videos; Deep learning; Character recognition; Multi-modal fusion; Classification (of information); Multimodal; Features fusions; Multimodal fusion; Audio and video; Neural-networks; Fusion mechanism; Neural network,,,emotion,No,Yes
scopus,Transformer-based Self-supervised Representation Learning for Emotion Recognition Using Bio-signal Feature Fusion,"Sawant, S.S.; Erick, F.X.; Arora, P.; Pahl, J.; Foltyn, A.; Holzer, N.; Götz, T.",2023,,,,10.1109/ACIIW59127.2023.10388149,"In this paper, we present a new emotion recognition framework that utilizes transformer based self-supervised representations from different bio-signals and combines them into a fused representation for the task of emotion recognition. Specifically, we explore a cross-attention based fusion mechanism that can explore mutual features among different bio-signals and learn more meaningful embeddings to estimate emotions effectively. Extensive experiments on a public dataset WESAD outperform the performance of fully supervised method for emotion recognition tasks and demonstrate the benefits of self-supervised features in recognizing different emotions. We also present a series of ablation studies to validate the proposed approach.  © 2023 IEEE.",Transformer; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Affective Computing; Learn+; multimodal; Features fusions; transformer; Self-supervised learning; Supervised learning; Signal features; Fusion mechanism; fusion; affective computing; self-supervised learning; Biosignals,,,emotion,No,Yes
scopus,FERLP: Facial Emotion Recognition Based on Landmark Points using Artificial Intelligence and Machine Learning,"Hossain, M.A.; Osman, M.H.; Hamdan, A.A.; Abdelhag, M.E.; Kechadi, M.T.",2023,,,,10.1109/ICCCNT56998.2023.10308392,"In the field of emotion processing, it can be challenging to determine how to identify a person's state of mind from their realistic expressions of emotion when working in an environment with several dimensions. There have been various investigations and projects done to develop strategies for generating accurate predictions; nevertheless, very little has been done to put the distinctive characteristics of the face of a person to use when interpreting and reacting to the emotional expressions of other people. An emotional picture retrieval approach is proposed for use in multimodal emotion detection in this body of work. This tactic is predicated on the observations that facial characteristics are present at all times and that the human face possesses regional redundancy that is independent of facial characteristics. Our approach makes use of landmark points to strip away any superfluous information on facial expressions, allowing us to zero in on the most appealing examples from each individual frame. If we make use of the landmarks, it's possible that we'll be able to forecast facial expressions with a greater degree of accuracy. This is especially important for long-term emotion prediction because it helps us restore time and space representations when an emotional face is introduced. The preliminary findings derived from a standard dataset indicate that the suggested strategy outperforms its rivals with regard to the accuracy of the predictions it generates. © 2023 IEEE.",Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Multimodal emotion recognition; multimodal emotion recognition; Forecasting; Facial emotion recognition; Facial emotions; Emotion predictions; facial emotion recognition; Artificial intelligence learning; emotion prediction; landmark point detection; Landmark point detection; Point detection,,,emotion,No,Yes
scopus,Attention-based 3D convolutional recurrent neural network model for multimodal emotion recognition,"Du, Y.; Li, P.; Cheng, L.; Zhang, X.; Li, M.; Li, F.",2023,,17,,10.3389/fnins.2023.1330077,"Introduction: Multimodal emotion recognition has become a hot topic in human-computer interaction and intelligent healthcare fields. However, combining information from different human different modalities for emotion computation is still challenging. Methods: In this paper, we propose a three-dimensional convolutional recurrent neural network model (referred to as 3FACRNN network) based on multimodal fusion and attention mechanism. The 3FACRNN network model consists of a visual network and an EEG network. The visual network is composed of a cascaded convolutional neural network–time convolutional network (CNN-TCN). In the EEG network, the 3D feature building module was added to integrate band information, spatial information and temporal information of the EEG signal, and the band attention and self-attention modules were added to the convolutional recurrent neural network (CRNN). The former explores the effect of different frequency bands on network recognition performance, while the latter is to obtain the intrinsic similarity of different EEG samples. Results: To investigate the effect of different frequency bands on the experiment, we obtained the average attention mask for all subjects in different frequency bands. The distribution of the attention masks across the different frequency bands suggests that signals more relevant to human emotions may be active in the high frequency bands γ (31–50 Hz). Finally, we try to use the multi-task loss function Lc to force the approximation of the intermediate feature vectors of the visual and EEG modalities, with the aim of using the knowledge of the visual modalities to improve the performance of the EEG network model. The mean recognition accuracy and standard deviation of the proposed method on the two multimodal sentiment datasets DEAP and MAHNOB-HCI (arousal, valence) were 96.75 ± 1.75, 96.86 ± 1.33; 97.55 ± 1.51, 98.37 ± 1.07, better than those of the state-of-the-art multimodal recognition approaches. Discussion: The experimental results show that starting from the multimodal information, the facial video frames and electroencephalogram (EEG) signals of the subjects are used as inputs to the emotion recognition network, which can enhance the stability of the emotion network and improve the recognition accuracy of the emotion network. In addition, in future work, we will try to utilize sparse matrix methods and deep convolutional networks to improve the performance of multimodal emotion networks. Copyright © 2024 Du, Li, Cheng, Zhang, Li and Li.",attention mechanism; convolutional neural network; emotion; facial expression; deep learning; emotion recognition; human; multimodal emotion recognition; Article; information processing; attention; electroencephalogram; electroencephalogram (EEG); accuracy; arousal; valence (emotion); entropy; recurrent neural network; convolutional neural network (CNN); emotion regulation; 3D convolutional recurrent neural network model; 3D feature construction module; differential entropy; frequency analysis; frequency band attention; implementation science; multimodal recognition; object-based attention; self attention; signal detection; visual network; visual network model,,,emotion,No,Yes
scopus,Bimodal Emotion Recognition Based on Vocal and Facial Features,"Wozniak, M.; Sakowicz, M.; Ledwosinski, K.; Rzepkowski, J.; Czapla, P.; Zaporowski, S.",2023,,225,,10.1016/j.procs.2023.10.247,"Emotion recognition is a crucial aspect of human communication, with applications in fields such as psychology, education, and healthcare. Identifying emotions accurately is challenging, as people use a variety of signals to express and perceive emotions. In this study, we address the problem of multimodal emotion recognition using both audio and video signals, to develop a robust and reliable system that can recognize emotions even when one modality is absent. To achieve this goal, we propose a novel architecture based on well-designed feature extractors for each modality and use model-level fusion based on a TFusion block to combine the information from both sources. To be more efficient in real-world scenarios, we trained our model on a compound dataset consisting of RAVDESS, RML, and eNTERFACE'05. It is then evaluated and compared to the state-of-the-art models. We find that our approach performs close to the modern solutions and can recognize emotions accurately when one of the modalities is missing. Additionally, we have developed a real-time emotion recognition application as a part of this work. © 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Deep learning; Deep Learning; Multimodal emotion recognition; Affective Computing; Human communications; Facial feature; Bimodal emotion recognition; In-field; Psychology educations; Tfusion; TFusion,,,emotion,No,No
scopus,"Multimodal Emotion Recognition in Video, Audio, and Text using Deep and Transfer Learning","Panchal, V.; Deshpande, T.; Kandoi, T.; Deulkar, K.; Narvekar, M.",2023,,,,10.1109/ICACTA58201.2023.10392410,"Emotions play an important role in an efficient communication process. Many researchers have studied emotion recognition, however, the majority of their work has concentrated on one modality like text, or numerous modalities like text and speech, but just for one language. This imposes a restriction on the ability to recognize speech, text, or video. Considering multiple modalities at once is the answer to this. There is little study that takes into account all three modalities simultaneously. Therefore, in this paper, we propose a multimodal approach that detects the emotion expressed by a user in a video input where we will not only analyze the facial expression but also take the speech and the text into account while predicting the emotion and expand it to cross-language scenarios. The video input taken detects facial structure and the respective emotion by using some well-known deep learning models. For speech emotion prediction we use three datasets out of which one of them is a different language to implement the cross-language and cross-corpus scenario. The text extracted from audio is given as input to emotion recognition models within the Machine Learning and Deep/Transfer Learning domains. Our results suggest that the multimodal system has a lot of credibility due to the consideration of all 3 modalities and give us a much more accurate context or breakdown of the emotions interpreted from the user input. © 2023 IEEE.",Long short-term memory; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Character recognition; Learning systems; multimodal; Audio; Computational linguistics; FER; RoBERTa; CNN-LSTM; audio; Cross languages; cross-corpus; Cross-corpus; cross-language; Resnet-50; ResNet-50; text emotion recognition; Text emotion recognition,,,emotion,No,No
scopus,Fine-tuning the Wav2Vec2 Model for Automatic Speech Emotion Recognition System,"Kayande, D.; Sonowal, I.B.; Bhukya, R.K.",2023,,,,10.1109/O-COCOSDA60357.2023.10482992,"The speech emotion recognition (SER) has gained pivotal attention on various applications in human-computer interaction and affective computing. In these days, there has been a growing interest in developing robust and accurate systems for identifying emotions from speech utterances. In this work, a novel approach to Wav2Vec2 architecture is used to demonstrate the SER system performance. The Wav2Vec2 model is used to extract the speech features from utterances and feed to feed forward network to identify the emotions accurately on the two datasets, namely, Toronto emotional speech set (TESS) and Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D). Wav2Vec2 implements a contrastive learning target during the pre-training stage. The CREMA-D achieved an accuracy of 76%. Additionally, the weighted F1 score, precision, and recall were, respectively, 0.76, 0.77, and 0.77. On the other hand, on the TESS dataset achieved an accuracy of 99%, the F1 score was 0.99. Furthermore, the weighted recall and precision were both 0.99 and 0.99. © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Affective Computing; Human computer interaction; F1 scores; Human-Computer Interaction; Emotional speech; Fine tuning; Toronto; Speech Emotion Recognition; Automatic speech; Speech emotion recognition systems; Wav2vec2; Wav2Vec2,,,emotion,No,Yes
scopus,"Multimodal, multiview and multitasking depression detection framework endorsed with auxiliary sentiment polarity and emotion detection","Gupta, S.; Singh, A.; Ranjan, J.",2023,,14,,10.1007/s13198-023-01861-z,"The impact of online social media has aided the users in sharing of knowledge, mood, feelings, and interests to the large volume of audience. The mental health of a person can be easily identified by analysing these expressions consisting of different modalities (text and emojis/emoticons). This research work aims to investigate the mood disorder like depression, low mood and other symptoms using tweets and emoticons. The present work curated the twitter based SentiEmoDD dataset as a benchmark for depression detection, labelled with sentiments analysis, emotions detection and other symptoms important for depression detection. The evolved dataset is equipped with both modalities (text and emojis) of tweets. A novel approach has been proposed based on the multi-view ensemble learning model contemplated to attain the information available in different modalities of a sentence for better depression detection. The proposed approach extracts the results from inter ensemble learning model and intra ensemble learning model. The experimental results clearly indicates that multimodal, multi-view and multitasking proposed framework provides an accuracy of 88.29% for the primary task of depression detection SVM linear kernel function. The stacking technique used here, provides the accuracy of 87.69% to detect depression using the proposed algorithm considering all the expressions of emoji and text combinations. © 2023, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.",Emotion Recognition; Multi-modal; Learning systems; Social networking (online); Emotion detection; Sentiment analysis; Multimodal; Support vector machines; Learning models; Depression detection; Online social medias; Ensemble learning; Multi-views; Multitasking; Detection framework; Large volumes; Multiview,,,emotion,Yes,No
scopus,Using Auxiliary Tasks In Multimodal Fusion of Wav2vec 2.0 And Bert for Multimodal Emotion Recognition,"Sun, D.; He, Y.; Han, J.",2023,,,,10.1109/ICASSP49357.2023.10096586,"The lack of data and the difficulty of multimodal fusion have always been challenges for multimodal emotion recognition (MER). In this paper, we propose to use pre-trained models as upstream network, wav2vec 2.0 for audio modality and BERT for text modality, and finetune them in downstream task of MER to cope with the lack of data. For the difficulty of multimodal fusion, we use a K-layer multi-head attention mechanism as a downstream fusion module. Starting from the MER task itself, we design two auxiliary tasks to alleviate the insufficient fusion between modalities and guide the network to capture and align emotion-related features. Compared to the previous state-of-the-art models, we achieve a better performance by 78.42% Weighted Accuracy (WA) and 79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset. © 2023 IEEE.",Attention mechanisms; Emotion Recognition; Speech recognition; BERT; Fusion modules; Multi-modal fusion; Multimodal emotion recognition; Cross-attention; State of the art; Computer vision; Down-stream; Wav2vec2.0; Auxiliary task,,,emotion,No,Yes
scopus,Multilevel Transformer for Multimodal Emotion Recognition,"He, J.; Wu, M.; Li, M.; Zhu, X.; Ye, F.",2023,,,,10.1109/ICASSP49357.2023.10097110,"Multimodal emotion recognition has attracted much attention recently. Fusing multiple modalities effectively with limited labeled data is a challenging task. Considering the success of pre-trained model and fine-grained nature of emotion expression, we think it is reasonable to take these two aspects into consideration. Unlike previous methods that mainly focus on one aspect, we introduce a novel multi-granularity framework, which combines fine-grained representation with pre-trained utterance-level representation. Inspired by Transformer TTS, we propose a multilevel transformer model to perform fine-grained multimodal emotion recognition. Specifically, we explore different methods to incorporate phoneme-level embedding with word-level embedding. To perform multi-granularity learning, we simply combine multilevel transformer model with Bert. Extensive experimental results show that multilevel transformer model outperforms previous state-of-the-art approaches on IEMOCAP dataset. Multi-granularity model achieves additional performance improvement. © 2023 IEEE.",Emotion Recognition; Embeddings; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multilevels; Fine grained; Bert; fine-grained interaction; Fine-grained interaction; highway network; Highway networks; Multi-granularity; multi-granularity emotion recognition; Multi-granularity emotion recognition; multilevel transformer; Multilevel transformer,,,emotion,Yes,Yes
scopus,A Comparison of Time-based Models for Multimodal Emotion Recognition,"Kesim, E.; Helli, S.S.; Cavsak, S.N.; Tanberk, S.",2023,,,,10.1109/ASYU58738.2023.10296562,"Emotion recognition has become an important research topic in the field of human-computer interaction. Studies on audio and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions. In this study, the performance of different sequence models which are frequently used in literature is compared for multi-modal emotion recognition problems. The audio and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models. The sequence models are GRU, Transformer, LSTM, and Max Pooling. Accuracy, precision, harmonic, and macro F1 Score values of all models were calculated. The multi-modal CREMA-D dataset was used in the experiments. As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in harmonic F1 score, LSTM-based architecture with 0.699 in precision and 0.678 in macro F1 score, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620. As a result, it has been observed that the sequence models performances are close to each other. Also, it should be noted that maxpooling has similar performance to the other models although it is a predefined layer. © 2023 IEEE.",Long short-term memory; Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Audio videos; Deep learning; Multi-modal fusion; Human computer interaction; Transformers; Multimodal fusion; Sequence models; Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficients; Automatic audio-video emotion recognition; Automatic audio-video emotion recognition (AVER),,,emotion,No,Yes
scopus,Multi-Modal Network based on Spatio-temporal and Attention for Emotion Recognition,"Zhang, Y.; Chen, W.; Cheng, C.",2023,,,,10.1109/BIBM58861.2023.10385831,"Multi-modal signals are more powerful for emotion recognition since they can provide richer emotion-related information. However, the heterogeneity and correlation of multimodal inputs in emotion recognition are difficult to explain. Additionally, how to model the temporal-spectral-spatial characteristics of electroencephalogram (EEG) signals with brain electrode dynamics and asymmetry is a challenge. To this end, we propose a multi-modal network that makes use of spatio-temporal features and attention mechanisms. The multi-scale temporal and asymmetric spatial learning module extracts EEG temporalspectral and spatial asymmetry features, while the multi-view attention-enhanced convolutional learning module captures key image features from multiple perspectives and frameworks. Moreover, a cross-modal attention fusion module is designed to integrate the extracted EEG and image features, enabling collaborative emotion recognition. Extensive ablation and comparative experiments on the DEAP and SEED-IV datasets demonstrate the superiority of the proposed model in feature learning and multimodal fusion compared to existing state-of-the-art methods.  © 2023 IEEE.",attention mechanism; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Multi-modal fusion; Network-based; Electroencephalography; Biomedical signal processing; Electroencephalogram; electroencephalogram (EEG); Image enhancement; Image features; multi-modal fusion; Learning modules; Multi-modal signal; Multi-modal signals; Multimodal network,,,emotion,No,No
scopus,Feature Refinement via Canonical Correlation Analysis for Multimodal Emotion Recognition,"Cho, S.; Wee, K.; Chun, T.Y.",2023,,,,10.23919/ICCAS59377.2023.10316831,"Multimodal Emotion Recognition (MER) aims to identify human emotions by combining information from a variety types of modalities. Generally, fusing multiple modalities improves the performance by providing more information and complementing the deficiencies among modalities. However, there are inconsistency and sensor noise problems among different modalities in the real-world. In this paper, we present a method to identify inconsistent and noisy modality elements via Canonical Correlation Analysis for multimodal emotion recognition. Our method first computes correlation scores between different modalities at the level of elements of each modality features. We then refine the features by excluding elements with low correlation scores by considering them as ineffectual elements. We demonstrate the efficiency of our method on the benchmarks CMU-MOSEI and IEMOCAP datasets in multi-modal sequences for various fusion methods.  © 2023 ICROS.",Emotion Recognition; Performance; Speech recognition; Modal analysis; Multimodal emotion recognition; Multiple modalities; Human emotion; Correlation methods; Real-world; Noise problems; canonical correlation analysis; Canonical correlations analysis; feature refinement; Feature refinement; Sensors noise; Variety types,,,emotion,No,Yes
scopus,Estimation of Facial Emotion Based on Landmark Points by Applying Artificial Intelligence and Machine Learning,"Osman Abdalraheem, M.H.; Hossain, M.A.; Ahmed Hamdan, A.; Kechadi, M.T.; Limker, S.",2023,,,,10.1109/ICCUBEA58933.2023.10392279,"In the science of emotion processing, it is hard to figure out how to predict an emotion from naturalistic facial expressions in a multidimensional space. While many studies and works have been done to develop methods for making accurate predictions, relatively little has been done to put the uniqueness of the human face to use in reading and responding to people's expressions of emotion. This work proposes an emotional image retrieval strategy for multimodal emotion detection. This strategy is based on the fact that face features are always there and that the human face has regional redundancy that is independent of face features. Our method uses landmark points to get rid of unnecessary information about facial expressions and focus on the most attractive ones in each frame. It's possible that by making use of the landmarks, we'll be able to predict facial emotions more precisely. This is especially helpful for long-term emotion prediction since it allows us to recover time and space representations by introducing an emotional face. The preliminary results based on a benchmark dataset show that the proposed method exceeds its competitors in terms of prediction accuracy.  © 2023 IEEE.",Machine learning; facial expression; Emotion Recognition; Face recognition; Facial Expressions; Machine-learning; Forecasting; Facial emotions; Emotion predictions; Computer vision; Emotion processing; Artificial intelligence learning; emotion prediction; Face features; Human faces; Image retrieval; landmark point; Landmark point; LBP method,,,emotion,No,Yes
scopus,Semisupervised Deep Features of Time-Frequency Maps for Multimodal Emotion Recognition,"Zali-Vargahan, B.; Charmin, A.; Kalbkhani, H.; Barghandan, S.",2023,,2023,,10.1155/2023/3608115,"Traditional approaches for emotion recognition utilize unimodal physiological signals. The effectiveness of such systems is affected by some limitations. To overcome them, this paper proposes a new method based on time-frequency maps that extract the features from multimodal biological signals. At first, the fusion of electroencephalogram (EEG) and peripheral physiological signal (PPS) is performed, and then, the two-dimensional discrete orthonormal Stockwell transform (2D-DOST) of the multimodal signal matrix is calculated to obtain time-frequency maps. A convolutional neural network (CNN) is then utilized to extract the local deep features from the absolute output of the 2D-DOST. Since there are uninformative deep features, the semisupervised dimension reduction scheme reduces them by balancing the generalization and discrimination. Finally, the classifier recognizes the emotion. The Bayesian optimizer finds the proper SSDR and classifier parameter values to maximize the recognition accuracy. The performance of the proposed method is evaluated on the DEAP dataset considering the two- and four-class scenarios through extensive simulations. This dataset consists of electroencephalograph (EEG) signals in 32 channels and peripheral physiological signals (PPSs) in eight channels from 32 subjects. The proposed method reaches the accuracy of 0.953 and 0.928 for two- and four-class scenarios, respectively. The results indicate the efficiency of the multimodal signals for detecting emotions compared to that of unimodal signals. Also, the results indicate that the proposed method outperforms the recently introduced ones.  © 2023 Behrooz Zali-Vargahan et al.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Unimodal; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Semi-supervised; Traditional approaches; Biological signals; Time-frequency map; Two-dimensional,,,emotion,No,Yes
scopus,How You Feelin'? Learning Emotions and Mental States in Movie Scenes,"Srivastava, D.; Singh, A.K.; Tapaswi, M.",2023,,2023-June,,10.1109/CVPR52729.2023.00248,"Movie story analysis requires understanding characters' emotions and mental states. Towards this goal, we formulate emotion understanding as predicting a diverse and multi-label set of emotions at the level of a movie scene and for each character. We propose EmoTx, a multimodal Transformer-based architecture that ingests videos, multiple characters, and dialog utterances to make joint predictions. By leveraging annotations from the MovieGraphs dataset [72], we aim to predict classic emotions (e.g. happy, angry) and other mental states (e.g. honest, helpful). We conduct experiments on the most frequently occurring 10 and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies and comparison against adapted state-of-the-art emotion recognition approaches shows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scores reveals that expressive emotions often look at character tokens while other mental states rely on video and dialog cues. © 2023 IEEE.",language; Vision; reasoning,,,emotion,Yes,No
scopus,Multimodal rough set transformer for sentiment analysis and emotion recognition,"Sun, X.; He, H.; Tang, H.; Zeng, K.; Shen, T.",2023,,,,10.1109/CCIS59572.2023.10263177,"Sentiment analysis and emotion recognition are crucial tasks that utilize multimodal information. Transformer models have shown exceptional performance in multimodal fusion. However, traditional dot product transformers do not tolerate uncertainty inside sentiment analysis and emotion recognition data. In this study, we introduce rough set self-Attention and rough set cross-Attention mechanisms for multimodal sentiment analysis and emotion recognition. A common concept is established based on granulation relations to extract important features through approximation. We then investigate a multimodal fusion transformer network based on rough set theory, which facilitates the interaction of multimodal information and feature guidance through rough set cross-Attention. Our empirical findings demonstrate that this is the first integration of rough set theory and transformer mechanisms for multimodal sentiment analysis and emotion recognition. Compared to traditional transformer fusion methods, our model can handle uncertain information and provides stronger global relationship guidance, allowing for better extraction of semantic information from multimodal data. We evaluate our model on sentiment analysis and emotion recognition experiments using the CMU-MOSEI and MELD datasets. The results show that our method outperforms state-of-The-Art networks. © 2023 IEEE.",Semantics; Transformer; Emotion Recognition; Rough set; Rough set theory; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Multi-modal information; Multi-modal fusion; Data mining; Sentiment analysis; sentiment analysis; transformer; Extraction; Uncertainty analysis; Transformer modeling; Edge extraction; emotion recognition Edge extraction; Emotion recognition edge extraction; rough set,,,emotion,No,Yes
scopus,Emotion Recognition based on fusion of multimodal physiological signals using LSTM and GRU,"Priyadarshini, N.; Aravinth, J.",2023,,,,10.1109/ICSCCC58608.2023.10176510,"Emotion recognition has become an important research topic to solve the practical problems faced by humans. The traditional method of Emotion recognition using facial expressions entails social issues such as privacy threats and reliability. The state of the person's real emotion can be reflected through physiological signals which are considered to be time series data. Emotion recognition using Multi-modal physiological signals gives better discriminative information when compared to information provided by the unimodal physiological signal. In this method, various physiological signals such as ECG, EEG, Respiration, and Temperature are segmented, fused and classified using Gated Recurrent Unit (GRU) and Long-Short Term Memory (LSTM). A multimodal fusion network is designed to fuse the features of four physiological signals. These features are classified into three classes namely sad, neutral and happy. The model designed is evaluated using three emotion datasets such as SEED, DREAMER and WESAD datasets respectively. From the results obtained it was observed that the proposed method achieves an average accuracy of 74% for multi-modal fusion using LSTM and 73% using GRU while 1DCNN acquired an accuracy of 61% for multi-model fusion.  © 2023 IEEE.",Long short-term memory; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Facial Expressions; Physiological signals; Physiology; Multi-modal fusion; Biomedical signal processing; EEG; LSTM; Electrocardiograms; Classifieds; ECG; Fusion; Gated recurrent unit; GRU; Practical problems; Research topics; Respiration; Temperature,,,emotion,No,Yes
scopus,Exploiting Modality-Invariant Feature for Robust Multimodal Emotion Recognition with Missing Modalities,"Zuo, H.; Liu, R.; Zhao, J.; Gao, G.; Li, H.",2023,,2023-June,,10.1109/ICASSP49357.2023.10095836,"Multimodal emotion recognition leverages complementary information across modalities to gain performance. However, we cannot guarantee that the data of all modalities are always present in practice. In the studies to predict the missing data across modalities, the inherent difference between heterogeneous modalities, namely the modality gap, presents a challenge. To address this, we propose to use invariant features for a missing modality imagination network (IF-MMIN) which includes two novel mechanisms: 1) an invariant feature learning strategy that is based on the central moment discrepancy (CMD) distance under the full-modality scenario; 2) an invariant feature based imagination module (IF-IM) to alleviate the modality gap during the missing modalities prediction, thus improving the robustness of multimodal joint representation. Comprehensive experiments on the benchmark dataset IEMOCAP demonstrate that the proposed model outperforms all baselines and invariantly improves the overall emotion recognition performance under uncertain missing-modality conditions. We release the code at: https://github.com/ZhuoYulang/IF-MMIN. © 2023 IEEE.",Emotion Recognition; Speech recognition; Multimodal emotion recognition; Learning systems; Benchmarking; Missing modality imagination; Learning strategy; Computer vision; Invariant feature; Invariant features; Feature learning; Feature-based; Missing data; Central moment discrepancy; Central moment discrepancy (CMD); Central moments; Gain performance,,,emotion,No,Yes
scopus,Leveraging Label Information for Multimodal Emotion Recognition,"Wang, P.; Zeng, S.; Chen, J.; Fan, L.; Chen, M.; Wu, Y.; He, X.",2023,,2023-August,,10.21437/Interspeech.2023-1732,"Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance. © 2023 International Speech Communication Association. All rights reserved.",Emotion Recognition; Embeddings; Speech recognition; Character recognition; Multimodal emotion recognition; Cross-attention; Learn+; Speech communication; cross-attention; label embedding; Label embedding; Label information; Speech information; Speech modality; Text information; Token frames,,,emotion,Yes,Yes
scopus,A Method of Multimodal Emotion Recognition in Video Learning Based on Knowledge Enhancement,"Ye, H.; Zhou, Y.; Tao, X.",2023,,47,,10.32604/csse.2023.039186,"With the popularity of online learning and due to the significant influence of emotion on the learning effect, more and more researches focus on emotion recognition in online learning. Most of the current research uses the comments of the learning platform or the learner's expression for emotion recognition. The research data on other modalities are scarce. Most of the studies also ignore the impact of instructional videos on learners and the guidance of knowledge on data. Because of the need for other modal research data, we construct a synchronous multimodal data set for analyzing learners' emotional states in online learning scenarios. The data set recorded the eye movement data and photoplethysmography (PPG) signals of 68 subjects and the instructional video they watched. For the problem of ignoring the instructional videos on learners and ignoring the knowledge, a multimodal emotion recognition method in video learning based on knowledge enhancement is proposed. This method uses the knowledge-based features extracted from instructional videos, such as brightness, hue, saturation, the videos' clickthrough rate, and emotion generation time, to guide the emotion recognition process of physiological signals. This method uses Convolutional Neural Networks (CNN) and Long Short-TermMemory (LSTM) networks to extract deeper emotional representation and spatiotemporal information from shallow features. The model uses multi-head attention (MHA) mechanism to obtain critical information in the extracted deep features. Then, Temporal Convolutional Network (TCN) is used to learn the information in the deep features and knowledge-based features. Knowledge-based features are used to supplement and enhance the deep features of physiological signals. Finally, the fully connected layer is used for emotion recognition, and the recognition accuracy reaches 97.51%. Compared with two recent researches, the accuracy improved by 8.57% and 2.11%, respectively. On the four public data sets, our proposed method also achieves better results compared with the two recent researches. The experiment results show that the proposed multimodal emotion recognition method based on knowledge enhancement has good performance and robustness. © 2023 CRL Publishing. All rights reserved.",Long short-term memory; Emotion Recognition; Speech recognition; CNN; deep learning; Emotion recognition; Physiological signals; Deep learning; Learning systems; Data mining; Convolutional neural networks; Eye movements; Convolution; Convolutional neural network; LSTM; Convolutional networks; E-learning; Knowledge based systems; physiological signal; Temporal convolutional network; Instructional videos; knowledge enhancement; Knowledge enhancement; Long short-termmemory; TCN; video learning; Video learning,,,emotion,No,Yes
scopus,Evaluating significant features in context-aware multimodal emotion recognition with XAI methods,"Khalane, A.; Makwana, R.; Shaikh, T.; Ullah, A.",2023,,,,10.1111/exsy.13403,"Expert systems are being extensively used to make critical decisions involving emotional analysis in affective computing. The evolution of deep learning algorithms has improved the potential for extracting value from multimodal emotional data. However, these black-box algorithms do not often explain the heuristics behind processing the input features for achieving certain outputs. This study focuses on the risks of using black-box deep learning models for critical tasks, such as emotion recognition, and describes how human understandable interpretations of the workings of these models are extremely important. This study utilizes one of the largest multimodal datasets available–CMU-MOSEI. Many researchers have used the pre-extracted features provided by the CMU Multimodal SDK with black-box deep learning models making it difficult to interpret the contribution of its individual features. This study describes the implications of significant features from various modalities (audio, video, text) identified using XAI in Multimodal Emotion Recognition. It describes the process of curating reduced feature models by using the Gradient SHAP XAI method. These reduced models with highly contributing features achieve comparable and at times even better results compared to their corresponding all-feature models as well as the baseline model GraphMFN. This study reveals that carefully selecting significant features for a model can help filter out irrelevant features, and attenuate the noise or bias caused by them, leading to an improved performance efficiency of the expert systems by making them transparent, easily interpretable, and trustworthy. © 2023 The Authors. Expert Systems published by John Wiley & Sons Ltd.",Emotion Recognition; Multi-modal; Speech recognition; BERT; Deep learning; Character recognition; Multimodal emotion recognition; Learning systems; multimodal emotion recognition; Learning models; Learning algorithms; Interpretability; interpretability; Black boxes; CMU-MOSEI; Expert systems; explainable artificial intelligence; Explainable artificial intelligence; Feature models; XAI,,,emotion,No,Yes
scopus,Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers,"Li, J.; Chen, Y.; Zhang, X.; Nie, J.; Li, Z.; Yu, Y.; Zhang, Y.; Hong, R.; Wang, M.",2023,,2023-June,,10.1109/CVPRW59228.2023.00620,"In this paper, we present our advanced solutions to the two sub-challenges of Affective Behavior Analysis in the wild (ABAW) 2023: the Emotional Reaction Intensity (ERI) Estimation Challenge and Expression (Expr) Classification Challenge. ABAW 2023 aims to tackle the challenge of affective behavior analysis in natural contexts, with the ultimate goal of creating intelligent machines and robots that possess the ability to comprehend human emotions, feelings, and behaviors. For the Expression Classification Challenge, we propose a streamlined approach that handles the challenges of classification effectively. However, our main contribution lies in our use of diverse models and tools to extract multimodal features such as audio and video cues from the Hume-Reaction dataset. By studying, analyzing, and combining these features, we significantly enhance the model's accuracy for sentiment prediction in a multimodal context. Furthermore, our method achieves outstanding results on the Emotional Reaction Intensity (ERI) Estimation Challenge, surpassing the baseline method by an impressive 84% increase, as measured by the Pearson Coefficient, on the validation dataset. © 2023 IEEE.",Modal analysis; Classification (of information); Human emotion; Multimodal feature fusions; Computer vision; Intelligent robots; Behavior analysis; Multimodal feature extractions; Affective behaviors; Emotional reactions; Expressions classifications; Human feelings; Intelligent machine; Intensity estimation,,,emotion,No,Yes
scopus,Abnormal Emotion Recognition Based on Audio-Visual Modality Fusion,"Jiang, Y.; Hirota, K.; Dai, Y.; Ji, Y.; Shao, S.",2023,,14267 LNAI,,10.1007/978-981-99-6483-3_15,"In indoor places, such as homes or offices, when abnormal events occur, the behavior and voice of individuals or groups will display abnormal signals. These signals can be both visual and auditory, and they interact and complement each other to jointly create a sense of emotional atmosphere within the scene. In order to achieve effective and accurate perception and response of abnormal emotion during the interaction in smart home, a model of abnormal emotion recognition based on audio-visual modality fusion is proposed. Human skeleton motion data and audio data are utilized to construct separate deep learning networks for action recognition and speech emotion recognition. The accuracy rate achieved on the G3D dataset is 100% and the accuracy rate achieved on the CASIA corpus is 90.83%. For decision-level multimodal fusion, the predicted results of actions and speech emotions are mapped to the “abnormal” axis through fuzzification and weighted average methods. In this process, considerations are taken into account for the varying contributions of different speech emotions and behaviors to the abnormal emotion, as well as the recognition recall rates of the unimodal emotion models. Then the two modalities are allowed to mutually modify each other and achieve quantitative analysis of abnormal emotion through weighted additive fusion. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Speech emotion recognition; Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Deep learning; Multi-modal fusion; Modality Fusion; Multimodal fusion; Visual modalities; Automation; Audio-visual; Action recognition; Abnormal emotion; Accuracy rate; Fuzzification; Fuzzifications,,,emotion,No,Yes
scopus,Speech Emotion Recognition Using Machine Learning,"Mergu, R.R.; Shelke, R.J.; Bagade, Y.; Walchale, P.; Yemul, H.",2023,,765 LNNS,,10.1007/978-981-99-5652-4_12,"This paper describes improved research on speech emotion recognition (SER) systems. The definition, classification of the state of emotions and the expressions of emotions are introduced theoretically. In this research article, a SER system based on the CNN classifier and MFCC feature extraction are developed. Mel Frequency Cepstral Coefficients (MFCC) are excerpted from audio signals that are accustomed to train various classifiers. All seven emotions were categorized using a convolutional neural network (CNN). Surrey Audio Visually Expressed Emotion (SAVEE), Ryerson Affective Speech and Song Audiovisual Database (RAVDESS), Toronto Affective Speech Set (TESS), Crowdsourced Affective Multimodal Actor Dataset (CREMA-D) databases were used as experimental datasets. This study shows all four datasets using the CNN classifier. With 1D-CNN, the overall emotion recognition accuracy is 43%, the gender recognition accuracy is 81%, and the gender-neutral recognition accuracy of emotions is 48%. Using 2D-CNN, the overall accuracy rate for emotion recognition is 67.58%, the accuracy rate for gender recognition is 98%, and the accuracy rate for non-gender recognition of emotions is 65%. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2023.",Emotions; Machine learning; Speech emotion recognition; Emotion Recognition; Speech recognition; CNN; Emotion; Classification (of information); Convolutional neural networks; Convolutional neural network; Accuracy; Gender; Recognition; Recognition accuracy; 1d; 1D; 2d; 2D; Gender recognition; SER,,,emotion,No,Yes
scopus,Heterogeneous Convolutional Neural Networks for Emotion Recognition Combined with Multimodal Factorised Bilinear Pooling and Mobile Application Recommendation,"Saisanthiya, D.; Supraja, P.",2023,,17,,10.3991/ijim.v17i16.42735,"The field of emotion recognition has garnered considerable interest due to its diverse applications in mental health, personalised advertising and enhancing user experiences. This research paper introduces a unique and innovative method for emotion recognition by integrating heterogeneous convolutional neural networks (CNNs) with multimodal factorised bilinear pooling. Furthermore, the paper also incorporates the integration of mobile application recommendations as part of the overall approach. The proposed method leverages the power of CNNs to extract high-level features from different modalities, including facial expressions, speech signals and physiological signals. By using heterogeneous CNNs, each modality is processed independently to capture modality-specific emotional cues effectively. To fuse the extracted features, multimodal factorised bilinear pooling is employed, which captures the complex interactions between different modalities while reducing the computational complexity. This pooling technique efficiently combines the modality-specific features, resulting in a compact and discriminative representation of the emotional state. In addition to emotion recognition, this paper also introduces the integration of mobile app recommendations. By leveraging the recognised emotion, the system recommends relevant mobile applications that are tailored to the user’s emotional state. This integration enhances user experience and facilitates emotion regulation through the utilisation of appropriate mobile apps. Experimental evaluations are conducted on benchmark emotion recognition datasets, including the DEAP and MAHNOB_HCI datasets. The findings of the study highlight the effectiveness of the proposed methodology in terms of accuracy and robustness, surpassing existing approaches in the field. Additionally, the integration of the mobile app recommendation system showcases encouraging outcomes by offering personalised recommendations tailored to the user’s emotional state. © 2023 by the authors of this article. Published under CC-BY.",multimodal data; bilinear pooling; heterogeneous CNN; mobile application; recommendation system,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition and Sentiment Analysis Using Masked Attention and Multimodal Interaction,"Voloshina, T.; Makhnytkina, O.",2023,,2023-May,,10.23919/FRUCT58615.2023.10143065,"People express emotions verbally (with the linguistic part) and non-verbally (with facial expressions and speech tone). For better emotion recognition it is expedient to use both types of expressions. In this paper, a multimodal approach for emotion recognition based on fusion of textual, audio, and video data with masked multimodal attention and multimodal interaction are suggested. Our models is built on top of the language representation model Bidirectional Encoder Representations from Transformers (BERT). BERT is mostly used to work with text data, while the approaches with the interaction of text and audio modalities with fine-tuning a pre-trained BERT model are less common. In this work, a new 3-Modal Cross-BERT model that utilizes BERT fine-tuning based on textual, audio, and video data using masked multimodal attention and the model with multimodal interaction are proposed. Our algorithms were evaluated on publicly available multimodal sentiment and emotion analysis datasets CMU-MOSI, CMU-MOSEI, IEMOCAP and MELD. Experimental results show significant improvements in the performance across all metrics compared to the previous state-of-the-art methods for chosen datasets.  © 2023 FRUCT.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Audio data; Multimodal emotion recognition; Sentiment analysis; Video recording; Multimodal Interaction; Video data; Audio and video; Fine tuning; Transformer modeling; Interactive computer systems,,,emotion,No,Yes
scopus,Cross-Modal Fusion Techniques for Utterance-Level Emotion Recognition from Text and Speech,"Luo, J.; Phan, H.; Reiss, J.",2023,,,,10.1109/ICASSP49357.2023.10096885,"Multimodal emotion recognition (MER) is a fundamental complex research problem due to the uncertainty of human emotional expression and the heterogeneity gap between different modalities. Audio and text modalities are particularly important for a human participant in understanding emotions. Although many successful attempts have been designed multimodal representations for MER, there still exist multiple challenges to be addressed: 1) bridging the heterogeneity gap between multimodal features and model inter- and intramodal interactions of multiple modalities; 2) effectively and efficiently modeling the contextual dynamics in the conversation sequence. In this paper, we propose Cross-Modal RoBERTa (CM-RoBERTa) model for emotion detection from spoken audio and corresponding transcripts. As the core unit of the CM-RoBERTa, parallel self- and cross- attention is designed to dynamically capture inter- and intra-modal interactions of audio and text. Specially, the mid-level fusion and residual module are employed to model longterm contextual dependencies and learn modality-specific patterns. We evaluate the approach on the MELD dataset and the experimental results show the proposed approach achieves the state-of-art performance on the dataset.  © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Deep learning; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Cross-modal; multimodal fusion; Uncertainty; Fusion techniques; speech emotion recognition; Emotional expressions; Research problems,,,emotion,No,Yes
scopus,Local domain generalization with low-rank constraint for EEG-based emotion recognition,"Tao, J.; Dan, Y.; Zhou, D.",2023,,17,,10.3389/fnins.2023.1213099,"As an important branch in the field of affective computing, emotion recognition based on electroencephalography (EEG) faces a long-standing challenge due to individual diversities. To conquer this challenge, domain adaptation (DA) or domain generalization (i.e., DA without target domain in the training stage) techniques have been introduced into EEG-based emotion recognition to eliminate the distribution discrepancy between different subjects. The preceding DA or domain generalization (DG) methods mainly focus on aligning the global distribution shift between source and target domains, yet without considering the correlations between the subdomains within the source domain and the target domain of interest. Since the ignorance of the fine-grained distribution information in the source may still bind the DG expectation on EEG datasets with multimodal structures, multiple patches (or subdomains) should be reconstructed from the source domain, on which multi-classifiers could be learned collaboratively. It is expected that accurately aligning relevant subdomains by excavating multiple distribution patterns within the source domain could further boost the learning performance of DG/DA. Therefore, we propose in this work a novel DG method for EEG-based emotion recognition, i.e., Local Domain Generalization with low-rank constraint (LDG). Specifically, the source domain is firstly partitioned into multiple local domains, each of which contains only one positive sample and its positive neighbors and k2 negative neighbors. Multiple subject-invariant classifiers on different subdomains are then co-learned in a unified framework by minimizing local regression loss with low-rank regularization for considering the shared knowledge among local domains. In the inference stage, the learned local classifiers are discriminatively selected according to their importance of adaptation. Extensive experiments are conducted on two benchmark databases (DEAP and SEED) under two cross-validation evaluation protocols, i.e., cross-subject within-dataset and cross-dataset within-session. The experimental results under the 5-fold cross-validation demonstrate the superiority of the proposed method compared with several state-of-the-art methods. Copyright © 2023 Tao, Dan and Zhou.",emotion; emotion recognition; human; male; learning; electroencephalography; benchmarking; electroencephalogram; classifier; domain adaptation; article; cross validation; adaptation; data base; local learning; local regression; subdomain generalization,,,emotion,No,Yes
scopus,Multimodal Cross-Attention Bayesian Network for Social News Emotion Recognition,"Wang, X.; Li, M.; Chang, Y.; Luo, X.; Yao, Y.; Li, Z.",2023,,2023-June,,10.1109/IJCNN54540.2023.10191298,"Multimodal emotion recognition comprehensively identifies the emotion contained in multimodal data by bridging the gaps between heterogeneous dataset. In recent years, multimodal emotion recognition methods have gained significant attention and been shown to surpass single-modal approaches. Most of the existing multimodal emotion analysis methods simply combine different modalities to improve the recognition capability of consistent emotion expressions across multimodal data. However, it remains challenging to recognize the right emotion when multiple modalities' contents carry inconsistent or even contradictory emotions. To solve this problem, we propose a novel image-text emotion recognition model named Multimodal Cross-Attention Bayesian Network(MCABN). The entire network exploits Bayesian theory to learn the distribution of its weight parameters, making optimization directions and results of parameters interpretable. What's more, the model leverages the consistency and complementarity between visual content and textual description to arrive at accurate decisions. Specifically, for each modality, multiple explainable features (color, texture, and shape feature in the image, while adjective, adverb, verb, noun, and negative feature in the text) and one unexplainable feature are fused as its feature representation to reinforce emotion-related features. Then two single-modal attention modules(Visual Attention Module and Textual Attention Module) capture the most discriminative features in a single image and text; Two cross-modal attention modules(Image-guided Text Attention Module and Text-guided Image Attention Module) extract the complementary and dominant features between two modalities by interactive learning. Finally, the outputs of four attention modules are integrated through intermediate fusion to predict the final emotion. The experimental results on the NVTD and MVSA-Multiple dataset indicate that the proposed MCABN outperforms state-of-the-art baselines by substantial margins. © 2023 IEEE.",Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Multi-modal data; Character recognition; Multimodal emotion recognition; Features fusions; Textures; Multimodal Emotion Recognition; Attention Mechanism; Neural networks; Feature Fusion; Single-modal; Bayesia n networks; Bayesian networks; Bayesian Neural Network; Bayesian neural networks; Social news,,,emotion,No,No
scopus,Topic and Style-aware Transformer for Multimodal Emotion Recognition,"Qiu, S.; Sekhar, N.; Singhal, P.",2023,,,,,"Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals. Also, we propose content-oriented features Topic and Speaking style on top of it to approach the subjectivity issues. Experiments conducted on the benchmark dataset MOSEI show our model can outperform SOTA results and effectively incorporate visual signals and handle subjectivity issues by serving as content ""normalization"". © 2023 Association for Computational Linguistics.",Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Visual languages; Multi-modality; Visual modalities; Human communications; Emotion expression; Computational linguistics; Visual signals; High dimensionality,,,emotion,No,Yes
scopus,MMOD-MEME: A Dataset for Multimodal Face Emotion Recognition on Code-Mixed Tamil Memes,"Kannan, R.R.; Ravikiran, M.; Rajalakshmi, R.",2023,,1802 CCIS,,10.1007/978-3-031-33231-9_24,"Multimodal Facial Emotion Recognition (MFER) for Low resourced Language like Tamil is handled with code-mixed text of Tamil and English. The newly created dataset addresses the multimodal approach on facial emotion recognition with the help of code-mixed memes. The dataset provides facial emotions for the memes and the code-mixed comments for the memes. The memes posted in websites and social media are collected to prepare the dataset. Overall dataset contains 4962 memes with annotated facial emotions and the code-mixed memes in it. Each are annotated by the 3 different annotators with single face emotion and double face emotions with code-mixed Tamil memes. Convolutional Neural Network (CNN) has been applied for detecting the emotions on this dataset containing single faces alone. The preliminary results on the single face emotion dataset has resulted in an accuracy of 0.3028. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Character recognition; Social media; Social networking (online); Convolutional neural networks; Convolutional neural network; Multi-modal approach; Facial emotions; Code-mixed tamil meme; Code-mixed Tamil Meme; Face emotion recognition; Facial Emotion; Social Media and Multimodal; Social medium and multimodal,,,emotion,Yes,Yes
scopus,Development of Facial Emotion Recognition System Using Unimodal and Multimodal Approach,"Taware, S.; Thakare, A.D.",2023,,1920,,10.1007/978-3-031-45121-8_22,"The lack of very precise emotion recognition in current human-computer interactions (HCI) is a technology gap. In their interactions with people, these systems are unable to adequately identify, express, and feel emotions. They continue to be less sensitive to human emotions. Facial emotion recognition attempts this gap by recognising emotions with help of different feature extraction techniques like local binary pattern, histogram of oriented gradients, Gabor filter and Scale-invariant feature transform. The feature technique has been evaluated on extended Cohn Kanade database (CK + 48). Nowadays deep learning used for facial emotion recognition due to increased size of dataset like FER 2013 and RAF-DB dataset. Convolution network has been evaluated on two mentioned datasets with highest accuracy. At last Convolution Neural Network (CNN) has applied on multimodal dataset i.e. Emotic to recognise emotion in context. It recognises almost categories of emotion. Compared to existing works the coverage of different emotions is increased by 5% in multimodal emotion recognition. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Deep learning; Human computer interaction; Graphic methods; Convolution; Histogram of oriented gradients; Recognition systems; Facial emotion recognition; Facial emotions; Local binary pattern; Context Awareness; Context- awareness; Facial Emotion recognition (FER); Gabor filter and scale-invariant feature transform; Gabor filter and Scale-invariant feature transform; Gabor filters; histogram of oriented gradients; Invariant feature transforms; local binary pattern; Local binary patterns; Scale invariant features,,,emotion,No,Yes
scopus,"Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation","Chen, F.; Shao, J.; Zhu, S.; Shen, H.T.",2023,,2023-June,,10.1109/CVPR52729.2023.01036,"Complex relationships of high arity across modality and context dimensions is a critical challenge in the Emotion Recognition in Conversation (ERC) task. Yet, previous works tend to encode multimodal and contextual relationships in a loosely-coupled manner, which may harm relationship modelling. Recently, Graph Neural Networks (GNN) which show advantages in capturing data relations, offer a new solution for ERC. However, existing GNN-based ERC models fail to address some general limits of GNNs, including assuming pairwise formulation and erasing high-frequency signals, which may be trivial for many applications but crucial for the ERC task. In this paper, we propose a GNN-based model that explores multivariate relationships and captures the varying importance of emotion discrepancy and commonality by valuing multi-frequency signals. We empower GNNs to better capture the inherent relationships among utterances and deliver more sufficient multimodal and contextual modelling. Experimental results show that our proposed method outperforms previous state-of-the-art works on two popular multimodal ERC datasets. © 2023 IEEE.",Multi-modal learning,,,emotion,No,No
scopus,A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition,"Zhang, X.; Li, Y.",2023,,2023-August,,10.21437/Interspeech.2023-523,"Multi-modal emotion recognition (MER) is an emerging research field in human-computer interactions. However, previous studies have explored several fusion methods to deal with the asynchronism and the heterogeneity of multimodal data but they mostly neglect the importance of discriminative unimodal information resulting in the ignorance of independence of uni-modality. Furthermore, the complementarity among different fusion strategies is seldom taken in consideration. To address these limitations, we propose a modality-collaborative fusion network (MCFN) consisting of three main components: a dual attention-based intra-modal learning module which is devoted to build the initial embedding spaces, a modality-collaborative learning approach is to reconcile the emotional information across modalities, and a two-stage fusion strategy to integrate multimodal features which are improved by a mutual adjustment approach. The proposed framework outperforms the state-of-the-art methods in overall experiments on two well-known public datasets. Our model will be available at https://github.com/zxiaohen/Speech-emotion-recognition-MCFN. © 2023 International Speech Communication Association. All rights reserved.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multi-modal data; Multimodal emotion recognition; Unimodal; Human computer interaction; Speech communication; Fusion methods; Fusion strategies; Research fields; Intra-modal; Modality-collaborative,,,emotion,No,No
scopus,Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition,"Kim, K.; Cho, N.",2023,,2023-August,,10.21437/Interspeech.2023-555,"Recognizing emotions in speech is essential for improving human-computer interactions, which require understanding and responding to the users' emotional states. Integrating multiple modalities, such as speech and text, enhances the performance of speech emotion recognition systems by providing a varied source of emotional information. In this context, we propose a model that enhances cross-modal transformer fusion by applying focus attention mechanisms to align and combine the salient features of two different modalities, namely, speech and text. The analysis of the disentanglement of the emotional representation various multiple embedding spaces using deep metric learning confirmed that our method shows enhanced emotion recognition performance. Furthermore, the proposed approach was evaluated on the IEMOCAP dataset. Experimental results demonstrated that our model achieves the best performance among other relevant multimodal speech emotion recognition systems. © 2023 International Speech Communication Association. All rights reserved.",Speech emotion recognition; Attention mechanisms; Emotion Recognition; Performance; Multi-modal; Speech recognition; Modal analysis; Deep learning; Character recognition; Multimodal emotion recognition; multimodal sentiment analysis; Human computer interaction; Sentiment analysis; multimodal emotion recognition; Cross-modal; Multimodal sentiment analyse; Speech communication; speech emotion recognition; focus-attention mechanism; Focus-attention mechanism; metric learning; Metric learning,,,emotion,No,Yes
scopus,Mandarin Emotion Recognition Based on Weighted Fusion Strategy,"Chen, C.; Long, X.",2023,,,,10.1109/ICETCI57876.2023.10176427,"In emotion recognition, different emotion features play different roles in emotion recognition. In order to improve the correct rate of emotion recognition, we modify the equivalent emotion feature weight in the traditional emotion recognition algorithm. Firstly, the weight value is set according to the contribution of each emotional feature to emotion recognition, then the weighting matrix of each modal feature is created, and finally a multimodal emotion recognition model based on feature level fusion and decision level weighted fusion is constructed and verified in CHEAVD data set. The results show that the multimodal emotion recognition model constructed in this paper improves the recognition rate by 5.43% compared with the emotion recognition model with only feature level fusion, and improves the recognition rate by 4.52%, 7.56% and 4.915% respectively compared with other mainstream emotion recognition models.  © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; multimodal; Recognition models; SVM; Feature-level fusions; Emotion feature; Mandarin emotion recognition; Mandarin Emotion Recognition; Weighted fusion; Weighted fusion of emotional feature; Weighted fusion of emotional features,,,emotion,No,No
scopus,A Publicly Available Multimodal Emotional Dataset Triggered by Both Videos and Odors,"Teng, W.; Lv, Z.; Zhang, X.",2023,,,,10.1109/ICMSP58539.2023.10170849,"This study investigates multimodal emotion signal recognition by using odors as a stimulus material to explore the influence of the odor on human emotions. It is conducted in the context of traditional electroencephalography and electrooculography fusion for emotion recognition. We designed an experimental paradigm involving visual, auditory and olfactory senses, adding odors to traditional audiovisual medias to enhance emotion elicitation, and built a multimodal emotion dataset. The dataset contains electroencephalography and electrooculography data obtained from 15 subjects in video-odor stimulation and video-only stimulation experiments. 90 video clips and 10 odors were selected as stimulus materials. Besides, the Transformer classifier was adopted to classify the three categories of emotion, and give the average classification accuracy for the different features. Under differential entropy features, the multimodal average classification accuracy under video-odor stimulation was 3% higher than that under video-only stimulation. The results indicated that olfactory stimuli can be combined with video stimuli to enhance multimodal emotion recognition ability. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multi-modal fusion; Classification (of information); electroencephalography; Electroencephalography; electrooculography; Electrophysiology; multimodal fusion; Classification accuracy; Human emotion; Auditory sense; emotio n recognition; Emotio n recognition; Odor-evoked dataset; odor-evoked datasets; Odors; Signal recognition; Stimulus materials,,,emotion,No,Yes
scopus,Research on Computer Interaction Design Based on User Emotion Recognition Algorithm,"He, H.",2023,,,,10.1109/NMITCON58196.2023.10276275,"Multimodal emotion recognition is a challenging and hot research direction in the field of artificial intelligence. Multimodal emotion recognition mainly improves emotion recognition performance by integrating multiple modal emotion information. The main difficulty lies in whether to learn more discriminative single modal emotional information and whether to fully explore multimodal complementary information through multimodal fusion methods. This article conducts relevant research on expression recognition, multimodal emotion recognition, and human-computer interaction in virtual environments, studies multimodal emotion recognition and its natural interaction in virtual environments, and proposes a model layer fusion method based on neural networks. After fusing speech emotion features and expression features, the complementary information between the fused features is learned through neural networks. Comparative experiments were conducted on the IEMOCAP dataset, and the recognition rate of the model layer fusion method reached 88%. Through comparative verification, it was shown that this method has certain effectiveness.  © 2023 IEEE.",Emotion Recognition; Speech recognition; deep learning; emotion recognition; Emotion recognition; Deep learning; Multi-modal fusion; Multimodal emotion recognition; Human computer interaction; multimodal fusion; Multilayer neural networks; Virtual reality; E-learning; Neural-networks; Fusion methods; Computer interaction; Interaction design; User emotions; virtual interaction; Virtual interactions,,,emotion,No,Yes
scopus,Privileged Knowledge Distillation for Dimensional Emotion Recognition in the Wild,"Aslam, M.H.; Osama Zeeshan, M.; Pedersoli, M.; Koerich, A.L.; Bacon, S.; Granger, E.",2023,,2023-June,,10.1109/CVPRW59228.2023.00336,"Automated emotion recognition (AER) has a growing number of applications, ranging from behavior analysis in assistive robotics and e-learning to depression and pain estimation healthcare. Systems for multimodal AER typically outperform unimodal approaches due to the complementary and redundant semantic information across modalities like visual, audio, language, physiological, etc. However, in practice, only a subset of these modalities is available at inference time, and using multiple modalities increases systems complexity. This paper focuses on video-based AER and aims to enhance the accuracy of unimodal systems by leveraging the Learning Under Privileged Information (LUPI) paradigm with information from multiple modalities. Without loss of generality, this study considers the audio modality as privileged information (only available during training), and introduces a new multimodal to unimodal privileged knowledge distillation (PKD). The teacher network is comprised of a multimodal AER architecture that can process audio-visual information and distills the learned knowledge to a unimodal visual student network. We validate our proposed multimodal PKD method on the challenging RECOLA and Affwild2 datasets for video-based AER, using weak and strong baseline AER architectures, as well as joint cross-attention fusion methods. The proposed method increases the absolute average concordance correlation coefficient accuracy by 8% on the RECOLA dataset, and by 2% on the arousal dimension of the Affwild2 dataset. The code available at multimodal-pkd. © 2023 IEEE.",Semantics; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Unimodal; Learning systems; Visual languages; Semantics Information; Network architecture; Multiple modalities; Distillation; Behavior analysis; E - learning; Assistive robotics; Robotic learning; Systems complexity,,,emotion,No,Yes
scopus,Research on emotion recognition based on multimodal fusion,"Zheng, W.; Su, L.",2023,,12645,,10.1117/12.2680821,"In order to solve the problem that the recognition accuracy of a single mode model depends on the emotion type, this paper proposes a multimodal emotion recognition model based on the bi-directional gated recurrent unit (BiGRU) network and attention mechanism. In this paper, BiGRU neural networks are used to extract high level features from the original features of text, audio and video, and then high level features are carried out. When high level features are fused, the attention mechanism is used to adjust the weight of emotional features, and finally emotional recognition is carried out to obtain emotional classification. The model is trained and tested on the CH-SIMS dataset, with an accuracy of 75.15%. Experimental results show that the proposed model has lower computational complexity and higher recognition accuracy. © 2023 SPIE.",attention mechanism; Attention mechanisms; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Deep learning; Multi-modal fusion; Multimodal emotion recognition; Bi-directional; Features fusions; feature fusion; High-level features; Recognition accuracy; bidirectional gating recurrent unit; Bidirectional gating recurrent unit,,,emotion,No,Yes
scopus,Emotional Irony Detection Based on Multi-Feature Fusion,"Li, S.; Dou, Q.",2023,,12709,,10.1117/12.2684932,"Irony is the use of counterwords, irony and other techniques to reflect the essential characteristics of people and things, and is a way to express emotions. Irony detection is an important research content in the field of sentiment analysis, and it is also one of the hot issues in the research of artificial intelligence in natural language processing. In this paper, a sentimental ironic detection model based on multi-feature fusion is proposed, which takes text features, image features and image attribute features as inputs, and image attribute features are represented by adjective-noun pairs abbreviated as ANPs. In this model, the neural network is first used to extract features from the three inputs, and then the text features and image features are fused through the shared fusion network, and the contrast fusion network fuses the text features and image attribute features, so as to express the contrast and difference of graphic and text information. Finally, satirical classification is carried out through the classification layer. Experimental results show that the model achieves good performance on the public multimodal irony detection dataset. © 2023 SPIE.",Image processing; Deep learning; Sentiment analysis; Image features; Image fusion; Feature extraction; Multi-feature fusion; Text feature; Contrast converged network; contrast converged networks; Converged networks; graphic fusion; Graphic fusion; Image attributes; Irony detection; Shared converged network; shared converged networks,,,emotion,No,Yes
scopus,DynamicMBFN: Dynamic Multimodal Bottleneck Fusion Network for Multimodal Emotion Recognition,"Sun, Y.; Cheng, D.; Chen, Y.; He, Z.",2023,,,,10.1109/ISCTIS58954.2023.10213035,"In the realm of multimodal emotion recognition, the processing of diverse data modalities such as audio, text, and video is a necessity. Yet, existing machine perception models predominantly aim at optimizing the handling of specific modalities, subsequently fusing the representations or predictions of each modality in later stages. These multimodal classification algorithms chiefly depend on the complementarity among different modalities to augment classification performance. However, they often grapple with challenges such as insufficient data and excessive computations while exploiting the complementary nature of multimodal information. To circumvent these issues, we introduce a multimodal fusion network, DynamicMBFN. This network implements dynamic evaluation strategies and sparse gating mechanisms to apprehend the information variations within each modality's features. Furthermore, we bring forward a bottleneck mechanism to compel the model to arrange and condense information within each modality, simultaneously sharing requisite information. Experimental findings on the IEMOCAP dataset substantiate that our algorithm not only ameliorates the performance of multimodal information fusion but also effectively mitigates computational costs. Thus, our model offers an efficacious solution for multimodal data processing and carries substantial practical implications for accomplishing dependable multimodal fusion.  © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Audio videos; Deep learning; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Information fusion; Multimodal Emotion Recognition; Multimodal information fusion; Dynamics; IEMOCAP; Late stage; Machine perception; Machine Perception; Multimodal Information Fusion; Perception model,,,emotion,No,Yes
scopus,Multi-Modal CNN Features Fusion for Emotion Recognition: A Modified Xception Model,"Shahzad, H.M.; Bhatti, S.M.; Jaffar, A.; Rashid, M.; Akram, S.",2023,,11,,10.1109/ACCESS.2023.3310428,"Facial expression recognition (FER) is advancing human-computer interaction, especially, today, where facial masks are commonly worn due to the COVID-19 pandemic. Traditional unimodal techniques for facial expression recognition may be ineffective under these circumstances. To address this challenge, multimodal approaches that incorporate data from various modalities, such as voice expressions, have emerged as a promising solution. This paper proposed a novel multimodal methodology based on deep learning to recognize facial expressions under masked conditions effectively. The approach utilized two standard datasets, M-LFW-F and CREMA-D, to capture facial and vocal emotional expressions. A multimodal neural network was then trained using fusion techniques, outperforming conventional unimodal methods in facial expression recognition. Experimental evaluations demonstrated that the proposed approach achieved an accuracy of 79.81%, a significant improvement over the 68.81% accuracy attained by the unimodal technique. These results highlight the superior performance of the proposed approach in facial expression recognition under masked conditions.  © 2013 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Multi-modal fusion; Human computer interaction; Features extraction; Neural networks; Neural-networks; neural network; Fusion techniques; Multimodal fusion technique; multimodal fusion techniques; facial expression under the mask; Facial expression under the mask; Mouth; Spectrograms,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition with Attention,"Cioroiu, G.; Radoi, A.",2023,,,,10.1109/ISSCS58449.2023.10190872,"Emotion recognition is one of main tasks in the human-computer interaction domain, and considering the recent advancement in Deep Learning, the topic greatly attracted the interest of the research community. The task is at the border between Affective Computing and Social Signal Processing, with applications that range from healthcare to robots, marketing and security. In this paper, we propose a novel approach for multimodal emotion recognition that introduces an attention mechanism which guides the extraction of visual features by means of the information extracted from the audio signal. The audio and visual information extraction is performed by convolutional neural networks applied to the Mel spectrogram of the audio signal and to equally-distanced video frames, respectively. The experimental results show that the insertion of the attention mechanism improve the overall accuracy of the emotion recognition system. The method is validated on a publicly available dataset, CREMA-D.  © 2023 IEEE.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Affective Computing; Human computer interaction; Convolutional neural networks; Signal processing; Audio signal; Research communities; Visual feature; Interaction domain; Main tasks; Social signal processing,,,emotion,No,Yes
scopus,Audio-Visual Emotion Recognition Using K-Means Clustering and Spatio-Temporal CNN,"Sharafi, M.; Yazdchi, M.; Rasti, J.",2023,,,,10.1109/IPRIA59240.2023.10147192,"Emotion recognition is a challenging task due to the emotional gap between subjective feeling and low-level audio-visual characteristics. Thus, the development of a feasible approach for high-performance emotion recognition might enhance human-computer interaction. Deep learning methods have enhanced the performance of emotion recognition systems in comparison to other current methods. In this paper, a multimodal deep convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) network are proposed, which fuses the audio and visual cues in a deep model. The spatial and temporal features extracted from video frames are fused with short-term Fourier transform (STFT) extracted from audio signals. Finally, a Softmax classifier is used to classify inputs into seven groups: anger, disgust, fear, happiness, sadness, surprise, and neutral mode. The proposed model is evaluated on Surrey Audio-Visual Expressed Emotion (SAVEE) database with an accuracy of 95.48%. Our experimental study reveals that the suggested method is more effective than existing algorithms in adapting to emotion recognition in this dataset. © 2023 IEEE.",Long short-term memory; Emotion Recognition; Performance; Deep neural networks; Speech recognition; Emotion recognition; Deep learning; Learning systems; Spatio-temporal; Human computer interaction; Convolutional neural networks; Brain; Convolution; Convolutional neural network; Audio-visual; K-means clustering; K-means++ clustering; Bidirectional long short-term memory; 3d-convolutional neural network; 3D-convolutional neural network; Short-term Fourier transform,,,emotion,No,Yes
scopus,QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition,"Li, Z.; Zhou, Y.; Liu, Y.; Zhu, F.; Yang, C.; Hu, S.",2023,,,,,"Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (i.e., textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent advances of quantum theory in modeling uncertainty, we make an initial attempt to design a quantum-inspired adaptive-priority-learning model (QAP) to address the challenges. Specifically, the quantum state is introduced to model modal features, which allows each modality to retain all emotional tendencies until the final classification. Additionally, we design Q-attention to orderly integrate three modalities, and then QAP learns modal priority adaptively so that modalities can provide different amounts of information based on priority. Experimental results on the IEMOCAP and MOSEI datasets show that QAP establishes new state-of-the-art results. © 2023 Association for Computational Linguistics.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; State of the art; Classification (of information); Learn+; Learning models; Uncertainty; Computational linguistics; Quantum state; Uncertainty analysis; Amount of information; Modeling uncertainties; Quantum theory,,,emotion,No,No
scopus,Deep Neural Networks for Comprehensive Multimodal Emotion Recognition,"Tiwari, A.; Kumar, S.; Mehrotra, T.; Singh, R.K.",2023,,,,10.1109/ICDT57929.2023.10150945,"Emotions may be expressed in many different ways, making automatic affect recognition challenging. Several industries may benefit from this technology, including audiovisual search and human- machine interface. Recently, neural networks have been developed to assess emotional states with unprecedented accuracy. We provide an approach to emotion identification that makes use of both visual and aural signals. It's crucial to isolate relevant features in order to accurately represent the nuanced emotions conveyed in a wide range of speech patterns. We do this by using a Convolutional Neural Network (CNN) to parse the audio track for feature extraction and a 50-layer deep ResNet to process the visual track. Machine learning algorithms, in addition to needing to extract the characteristics, should also be robust against outliers and reflective of their surroundings. To solve this problem, LSTM networks are used. We train the system from the ground up, using the RECOLA datasets from the AVEC 2016 emotion recognition research challenge, and we demonstrate that our method is superior to prior approaches that relied on manually constructed aural and visual cues for identifying genuine emotional states. It has been demonstrated that the visual modality predicts valence more accurately than arousal. The best results for the valence dimension from the RECOLA dataset are shown in Table III below.  © 2023 IEEE.",Long short-term memory; Emotion Recognition; Deep neural networks; Emotional state; Speech recognition; deep learning; Deep learning; Multimodal emotion recognition; Learning systems; Convolutional neural networks; Convolution; LSTM; Learning algorithms; Neural-networks; Affect recognition; End to end; Convolution neural network; Convolution Neural Network; Human Machine Interface; learning end to end; Learning end to end,,,emotion,No,Yes
scopus,Elastic Graph Transformer Networks for EEG-Based Emotion Recognition,"Jiang, W.-B.; Yan, X.; Zheng, W.-L.; Lu, B.-L.",2023,,,,10.1109/ICASSP49357.2023.10096511,"Electroencephalogram (EEG) has been applied in emotion recognition due to excellent temporal resolution with less competitive spatial resolution. This leads to the consequence that the majority of EEG-based emotion recognition models emphasize on exploiting temporal features while ignoring the efficient information provided by spatial resolution. To extract more informative representations, we propose an elastic Graph Transformer network for emotion recognition (EmoGT) inspired by the advantages of Transformer in time-series analysis and the superior performance of graph convolutional networks in topological analysis. Moreover, it is able to be flexibly expanded to cope with multimodal inputs by employing specially designed structures. Experimental results on 3 public datasets demonstrate that our models outperform the state-of-the-art results by 3% on average in both single and multimodal cases, indicating the effectiveness of utilizing temporal and spatial information simultaneously.  © 2023 IEEE.",Emotion Recognition; Performance; Speech recognition; emotion recognition; Emotion recognition; Electroencephalography; EEG; Eye movements; Recognition models; Convolutional networks; Time series analysis; eye movements; Temporal features; Elastic graphs; graph transformer; Graph transformer; Image resolution; Spatial resolution; Temporal resolution; Time-series analysis; Topology,,,emotion,No,Yes
scopus,MTGR: Improving Emotion and Sentiment Analysis with Gated Residual Networks,"Hajlaoui, R.; Bilodeau, G.-A.; Rockemann, J.",2023,,13643 LNCS,,10.1007/978-3-031-37660-3_11,"In this paper, we address the problem of emotion recognition and sentiment analysis. Implementing an end-to-end deep learning model for emotion recognition or sentiment analysis that uses different modalities of data has become an emerging research area. Numerous research studies have shown that multimodal transformers can efficiently combine and integrate different heterogeneous modalities of data, and improve the accuracy of emotion/sentiment prediction. Therefore, in this paper, we propose a new multimodal transformer for sentiment analysis and emotion recognition. Compared to previous work, we propose to integrate a gated residual network (GRN) into the multimodal transformer to better capitalize on the various signal modalities. Our method shows an improvement of the F1 score and the accuracy results on the CMU-MOSI and IEMOCAP datasets compared to the state-of-the-art results. © 2023, Springer Nature Switzerland AG.",Natural language processing; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Deep learning; Multi-modal fusion; Sentiment analysis; Emotion analysis; Multimodal fusion; Image enhancement; Language processing; Natural languages; Video signal processing; End to end; Image and video processing,,,emotion,No,Yes
scopus,User Emotion Status Recognition in MOOCs Study Environment Based on Eye Tracking and Video Feature Fusion,"Xing, B.; Wang, K.; Song, X.; Pan, Y.; Shi, Y.; Pang, S.",2023,,,,10.1109/IHMSC58761.2023.00029,"Emotion is an essential aspect that influences the learning outcome and MOOCs learning experience. This research investigates the possibility of employing the fusion characteristics of video and users' eye tracking features to assess MOOC learners' emotions. In the experiment, we gathered eye tracking data from MOOCs students. We applied OpenSmile and OpenCV to extract the video features. And a fusion data set with the eye-tracking data and video features was established in the experiment. Machine learning method is utilized to explore the optimal model using fused feature data sets, and the impacts of multimodal features on emotion recognition are discussed. Our findings demonstrate that the fusion feature can achieve an accuracy rate of 86.3%, which outperformed the single-modal feature sets. According to the study, this technique can be used to learn students' emotion status in their MOOC study and predict the students' affective perception on MOOCs videos.  © 2023 IEEE.",Emotion Recognition; Speech recognition; Eye tracking; Behavioral research; Learning systems; Affective Computing; Affective computing; Data set; Students; Video features; Eye-tracking; User emotions; MOOC study; MOOCs study; Status recognition; Tracking data; Tracking feature; Video analysis,,,emotion,No,Yes
scopus,Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition,"Yang, H.; Gao, X.; Wu, J.; Gan, T.; Ding, N.; Jiang, F.; Nie, L.",2023,,,,,"The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, i.e., long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we propose the Self-adaptive Context and Modal-interaction Modeling (SCMM) framework. We first design the context representation module, which consists of three submodules to model multiple contextual representations. Thereafter, we propose the modal-interaction module, including three interaction submodules to make full use of each modality. Finally, we come up with a self-adaptive path selection module to select an appropriate path in each module and integrate the features to obtain the final representation. Extensive experiments under four settings on three multimodal datasets, including IEMOCAP, MELD, and MOSEI, demonstrate that our proposed method outperforms the state-of-the-art approaches. © 2023 Association for Computational Linguistics.",Emotion Recognition; Speech recognition; Multimodal emotion recognition; Modal interactions; Multiple modalities; Interaction modules; Computational linguistics; Context representation; First designs; Interaction modeling; Modelling framework; Path selection; Submodules,,,emotion,No,No
scopus,Emotion Recognition Using Text and Speech Through Machine Learning,"Singla, C.; Singh, S.",2023,,1056 LNEE,,10.1007/978-981-99-3656-4_33,"Speech recognition is an important problem that is reflected in the process of applications which work in the field of education and communication that exists in human and computer. Emotion identification from speech is a nontrivial task refers to the ambiguous meaning of emotion itself. Although it has improved several times over the years, especially with the advent of deep neural networks (DNN). There are three main reasons through which the awareness of emotion recognition becomes a challenging task: (1) human emotions are inaccessible, which means it is difficult to distinguish; (2) in general, a person’s emotions can only be experienced from time to time during prolonged speech; (3) speech data with emotional labels are often limited. The emotions extracted in this study are happy, sad, neutral and angry. The wav files and its transcriptions are taken from the publically available dataset called IEMOCAP. Taking into account both the features extracted from the speech and text, and then, both the features are fused to get one feature file that is used further for the classification of the emotions using simple machine learning models. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Emotions; Machine learning; Speech emotion recognition; Emotion Recognition; Deep neural networks; Speech recognition; Emotion; Emotion recognition; Deep learning; Character recognition; Multimodal emotion recognition; Learning systems; Machine-learning; Speech communication; Speech emotion recognition (SER); Audio features; Text feature; MFCC; Audio and text features,,,emotion,Yes,No
scopus,SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition,"Zhao, Z.; Gao, T.; Wang, H.; Schuller, B.",2023,,2023-August,,10.21437/Interspeech.2023-413,"To achieve efficient feature fusion, existing research tends to employ cross-attention to control the contributions of different modalities in fusion. However, this inevitably causes high computational effort and introduces noise weights due to redundant computations. Therefore, this paper proposes sliding window attention (SliWa) to control the feature perception range and dynamically model the modality fusion at different granularities. In addition, we present a novel feature map classifier (FMC) based on high-response feature reuse (HRFR), which explicitly preserves the deep emotional feature structure, thus preventing the submersion of the crucial classification information after average flattening and the negative impacts of parameter flooding. We unify the mentioned modules in the SWRR framework, and the experimental results on the commonly used datasets IEMOCAP and CMU-MOSEI reveal the effectiveness of SWRR in improving the performance of emotion recognition. © 2023 International Speech Communication Association. All rights reserved.",Emotion Recognition; Speech recognition; Multimodal emotion recognition; Modality Fusion; Classification (of information); multimodal emotion recognition; Features fusions; classifier; feature fusion; Speech communication; Computational effort; Feature map; feature reuse; Feature reuse; High response; Redundant computation; Response features; Sliding Window,,,emotion,No,Yes
scopus,Speech Emotion Recognition Using Global-Aware Cross-Modal Feature Fusion Network,"Li, F.; Luo, J.",2023,,14087 LNCS,,10.1007/978-981-99-4742-3_17,"Speech emotion recognition (SER) facilitates better interpersonal communication. Emotion is normally present in conversation in many forms, such as speech and text. However, existing emotion recognition systems use only features of a single modality for emotion recognition, ignoring the interaction of multimodal information. Therefore, in our study, we propose a global-aware cross-modal feature fusion network for recognizing emotions in conversations. We introduce a residual cross-modal fusion attention module (ResCMFA) and a global-aware block to fuse information from multiple modalities and capture global information. More specifically, we first use transfer learning to extract wav2vec 2.0 features and text features that are fused by the ResCMFA module. Then, multimodal features are fed into the global-aware block to capture the most important emotional information on a global scale. Finally, extensive experiments on the IEMOCAP dataset have shown that our proposed algorithm has significant advantages over state-of-the-art methods. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Attention; Speech emotion recognition; Emotion Recognition; Speech recognition; Emotion recognition; attention; Cross-modal; Features fusions; Recognition systems; Speech communication; speech emotion recognition; Inter-personal communications; global-aware; Global-aware; System use; wav2vec 2.0; Wav2vec 2.0,,,emotion,No,No
scopus,A New Spatio-Temporal Neural Architecture with Bi-LSTM for Multimodal Emotion Recognition,"Jothimani, S.; Sangeethaa, S.N.; Premalatha, K.; Sathishkannan, R.",2023,,,,10.1109/ICCES57224.2023.10192713,"Emotional recognition uses implicit annotation to detect the user's emotional response to multimedia. This helps create efficient user-centric services. Researchers are increasingly using physiologically-based ways to convey emotions objectively. Traditional methods for addressing the problem of emotion recognition have mostly concentrated their efforts on the extraction of various sorts of manually-created characteristics. Nevertheless, features that are hand-crafted always require domain expertise for the individual job, and the process of building the appropriate features may take more time. As a result, determining the physiologically based temporal feature representation that is the most useful for emotion identification has become the primary focus of the majority of recent research. A variety of different applications require emotional identification in conversations as a first step. These applications include opinion mining over chat history, threads on social media, debates, argumentation mining, and interpreting customer feedback in live discussions, amongst others. Currently, systems do not adapt to the various individuals in a debate by considering each comment as if it were made by a distinct individual. Integrating information from multiple modalities requires careful selection and fusion of relevant features. This can be a complex task, as different modalities may have different temporal and spatial characteristics, and may provide redundant or conflicting information. This study's objective is to describe a unique strategy based on deep learning, Convolutional Neural Networks, and Bidirectional Long Short-Term Memory that keeps track of the independent party systems across a whole conversation in order to identify emotional responses using the data gathered from such provinces. For two separate datasets RAVDESS and SAVEE, our model achieves much better results 100% than the current state-of-the-art solution. © 2023 IEEE.",Long short-term memory; Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Physiology; Multimodal emotion recognition; Spatio-temporal; Features extraction; Multimodal; Convolutional neural networks; Extraction; Feature extraction; Feature Extraction; Emotional recognition; Emotional Recognition; Emotional response; Neural architectures; Conversation; Deep Neural Networks; User-centric service,,,emotion,Yes,Yes
scopus,Multi-Scale Receptive Field Graph Model for Emotion Recognition in Conversations,"Wei, J.; Hu, G.; Tuan, L.A.; Yang, X.; Zhu, W.",2023,,2023-June,,10.1109/ICASSP49357.2023.10094596,"Emotion recognition in conversations (ERC) has gained more attention, where contextual information modeling and multimodal fusion have been the focus and challenges in recent years. In this paper, we proposed a Multi-Scale Receptive Field Graph model (MSRFG) to tackle the challenges of ERC. Specifically, MSRFG constructs multi-scale perception graphs and learns contextual information via parallel multi-scale receptive field paths. To compensate for the deficiency of temporal information learning by the graph network, MSRFG injects temporal dependencies into the graph network to model the temporal relationships between utterances. Moreover, to achieve the effective fusion of multimodal information, MSRFG converges the multi-scale features of each modality separately and performs the learning of attention weights after the integration of converged features. We carried out experiments on IEMOCAP and MELD datasets to validate the effectiveness of the proposed method, and the results proved the superiority of our model over the existing SOTA methods. The code is available at https://github.com/Janie1996/MSRFG1 © 2023 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Learning systems; Graph networks; Multimodal fusion; Emotion recognition in conversation; Emotion recognition in conversations; Contextual information; Graph theory; Multi-scales; Graph model; Contextual modeling; Multi-scale features; Receptive fields,,,emotion,No,No
scopus,Enhancing Contextualized GNNs for Multimodal Emotion Recognition: Improving Accuracy and Robustness,"Shahbaz, M.H.; Mahboob, K.; Ali, F.",2023,,,,10.1109/IMTIC58887.2023.10178481,"Emotion recognition from facial expressions is an important research area in the field of artificial intelligence. In this study, a novel deep-learning model is proposed for emotion recognition from facial expressions. The model uses a combination of modifications in COGMEN (Contextualized GNN-based Multimodal Emotion Recognition) to extract features from facial images and other modalities and capture temporal dependencies between frames. The model was evaluated on the IEMOCAP dataset, achieving an accuracy of 87.8%, outperforming (mostly) existing state-of-the-art methods. Furthermore, the model has a lower computational complexity compared to other methods, making it more practical for real-time applications. A high F1 score was reported for each emotion category, indicating good performance across all classes. The results demonstrate the potential method for improving emotion recognition from multiple modalities (i.e.: Audio, Visual & Text) and have implications for a wide range of applications such as affective computing, human-computer interaction, and mental health monitoring.  © 2023 IEEE.",emotion; Emotion Recognition; Speech recognition; Emotion; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Character recognition; Multimodal emotion recognition; Human computer interaction; Learning models; model; Modality; Research areas; Graph; COGMEN; Contextualized GNN-based multimodal emotion recognition; graph; modalities; Model use,,,emotion,No,Yes
scopus,DialogueSMM: Emotion Recognition in Conversation with Speaker-Aware Multimodal Multi-head Attention,"Niu, C.; Xu, S.; Jia, Y.; Zan, H.",2023,,14303 LNAI,,10.1007/978-3-031-44696-2_40,"Emotion recognition in conversation (ERC) aims to automatically detect and track the emotional states of speakers in dialogue, which is essential for social dialogue system and decision-making. However, most existing ERC models only use textual information or fuse multimodal information in a simple way like concatenation. To fully leverage multimodal information, we propose a speaker-aware multimodal multi-head attention (DialogueSMM) model for ERC, which can effectively integrate textual, audio, and visual modalities, consider different speakers, and utilize emotion clues. Experimental results on both English and Chinese benchmark datasets show that DialogueSMM outperforms comparative state-of-the-art models. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",Emotion Recognition; Emotional state; Multi-modal; Speech recognition; Emotion recognition; Multi-modal information; Multimodal; Speech processing; Decision making; Emotion recognition in conversation; Dialogue systems; Speaker-aware; Multi-head attention; Decisions makings; Social dialogue,,,emotion,No,No
scopus,A novel feature fusion network for multimodal emotion recognition from EEG and eye movement signals,"Fu, B.; Gu, C.; Fu, M.; Xia, Y.; Liu, Y.",2023,,17,,10.3389/fnins.2023.1234162,"Emotion recognition is a challenging task, and the use of multimodal fusion methods for emotion recognition has become a trend. Fusion vectors can provide a more comprehensive representation of changes in the subject's emotional state, leading to more accurate emotion recognition results. Different fusion inputs or feature fusion methods have varying effects on the final fusion outcome. In this paper, we propose a novel Multimodal Feature Fusion Neural Network model (MFFNN) that effectively extracts complementary information from eye movement signals and performs feature fusion with EEG signals. We construct a dual-branch feature extraction module to extract features from both modalities while ensuring temporal alignment. A multi-scale feature fusion module is introduced, which utilizes cross-channel soft attention to adaptively select information from different spatial scales, enabling the acquisition of features at different spatial scales for effective fusion. We conduct experiments on the publicly available SEED-IV dataset, and our model achieves an accuracy of 87.32% in recognizing four emotions (happiness, sadness, fear, and neutrality). The results demonstrate that the proposed model can better explore complementary information from EEG and eye movement signals, thereby improving accuracy, and stability in emotion recognition. Copyright © 2023 Fu, Gu, Fu, Xia and Liu.",convolutional neural network; emotion; artificial neural network; human; multimodal emotion recognition; attention; electroencephalogram; eye movement; human experiment; plant seed; electroencephalogram (EEG); feature fusion; article; fear; feature extraction; Convolutional Neural Networks (CNN); happiness; multi-scale; sadness,,,emotion,No,Yes
scopus,Conversational Emotion Detection and Elicitation: A Preliminary Study,"Farooq, M.; De Silva, V.; Tibebu, H.; Shi, X.",2023,,,,10.1109/GlobConET56651.2023.10149922,"Emotion recognition in conversation is a challenging task as it requires an understanding of the contextual and linguistic aspects of a conversation. Emotion recognition in speech has been well studied, but in bi-directional or multi-directional conversations, emotions can be very complex, mixed, and embedded in context. To tackle this challenge, we propose a method that combines state-of-the-art RoBERTa model (robustly optimized BERT pretraining approach) with a Bidirectional long short-term memory (BiLSTM) network for contextualized emotion recognition. RoBERTa is a transformer-based language model, which is an advanced version of the well-known BERT. We use RoBERTa features as input to a BiLSTM model that learns to capture contextual dependencies and sequential patterns in the input text. The proposed model is trained and evaluated on a Multimodal EmotionLines Dataset (MELD) to recognize emotions in conversation. The textual modality of the dataset is utilized for the experimental evaluation, with the weighted average F1 score and accuracy used as performance metrics. The experimental results indicate that the incorporation of a pre-trained transformer-based language model with a BiLSTM network significantly enhances the recognition of emotions in contextualized conversational settings.  © 2023 IEEE.",Long short-term memory; Emotion Recognition; Speech recognition; Emotion recognition; Emotion elicitation; Bi-directional; Emotion detection; Brain; Emotion recognition in conversation; Memory network; Computational linguistics; Language model; Bidirectional long short-term memory  network; Bidirectional Long Short-Term Memory (BiLSTM) network; Emotion recognition in conversation (ERC); In contexts; Transformer-based language model,,,emotion,No,Yes
scopus,RW-MMDCG:Muti-modal via Rolling-Window Directed Graph Network for Conversational Emotion Recognition,"Wang, R.; Zha, D.; Zhao, Q.; He, Y.; Wang, X.",2023,,,,10.1109/CSCWD57460.2023.10152697,"Multimodal Conversational Emotion recognition (MMCER) aims to detect the muti-emotion label for each utterance from heterogeneous visual, text and audio modalities. In this paper, we focus on applying multi-modal graph data structures to conversational emotion recognition and use a novel and efficient graph - MMDCGs to better integrate multi-modal contextual information into conversations. MMDCG provides a new way of encoding intrinsic structural connectivity. Besides, inspired by time series analysis, we set a rolling time window as the receptive field, which can reduce the interference of remote information on the current utterances detection and achieve the purpose of data enhancement. We innovatively ensemble such graph structures with transformers, named rolling-windows MMDCGs (RW-MMDCG). Comprehensive experiments are performed on two representative multi-modal datasets, IEMOCAP and MELD, and we compare them with existing baselines, demonstrating the great advantages and effectiveness of RW-MMDCG.  © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Character recognition; Directed graphs; Graph networks; Graphic methods; Contextual information; Encodings; Multi-modelling; Time series analysis; Graph; graph; multimodel; Rolling window; Structural connectivity; Time series forecasting; timeseries forecasting,,,emotion,Yes,Yes
scopus,Knowledge-Aware Bayesian Co-Attention for Multimodal Emotion Recognition,"Zhao, Z.; Wang, Y.; Wang, Y.",2023,,2023-June,,10.1109/ICASSP49357.2023.10095798,"Multimodal emotion recognition is a challenging research area that aims to fuse different modalities to predict human emotion. However, most existing models that are based on attention mechanisms have difficulty in learning emotionally relevant parts on their own. To solve this problem, we propose to incorporate external emotion-related knowledge in the co-attention based fusion of pre-trained models. To effectively incorporate this knowledge, we enhance the co-attention model with a Bayesian attention module (BAM) where a prior distribution is estimated using the emotion-related knowledge. Experimental results on the IEMOCAP dataset show that the proposed approach can outperform several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA). © 2023 IEEE.",Attention mechanisms; Emotion Recognition; Speech recognition; Multimodal emotion recognition; multimodal emotion recognition; transfer learning; Transfer learning; Human emotion; Attention model; Bayesian; Research areas; Bayesian attention; knowledge injection; Knowledge injection; Prior distribution,,,emotion,No,Yes
scopus,Emotion Recognition from Electroencephalographic and Peripheral Physiological Signals Using Artificial Intelligence with Explicit Features,"Santana, M.A.; Gomes, J.C.; Torcate, A.S.; Fonseca, F.S.; Suarez, A.; Souza, G.M.; Moreno, G.M.M.; dos Santos, W.P.",2023,,,,,"In the same way that emotions influence human relationships, the recognition of emotions by artificial intelligence is fundamental for a better relationship between man and machine. This computational field has been growing rapidly with analyzes of peripheral and cerebral physiological signals, such as electroencephalography (EEG). Emerging as an alternative to facial expression recognition, these types of non-invasive approaches tend to be simpler and, in certain situations, more viable and cheaper. In this chapter, we present a multimodal approach for emotion recognition, manipulating EEG, electrocardiogram (ECG), temperature, respiration and galvanic skin response signals (GSR). To perform the experiments we used the MAHNOB-HCI database, from which we created four different datasets by extracting explicit numeric features from the signals (complete set; with feature selection; with resampling and with both feature selection and resampling). Each of these datasets were applied as input to different classifier models based on Random Forest, Support Vector Machines and Extreme Learning Machines. Among the four proposed approaches, those with instance reduction showed better results. All experiments were carried out with 30 repetitions each, in order to acquire statistically significant results. Of these, we highlight the Random Forest with accuracy values up to 100%, with high repeatability. Both SVM and ELM also had accuracies around 100% in some configurations. We also emphasize the importance of using new data for a deeper analysis of their behavior in real-life issues in addition to providing greater variability. © 2024 selection and editorial matter, Ganesh R. Naik and Wellington Pinheiro dos Santos; individual chapters, the contributors.",,,,emotion,No,Yes
scopus,Emotions in Spoken Language - Do we need acoustics?,"Probol, N.; Mieskes, M.",2023,,,,,"Work on emotion detection is often focused on textual data from i.e. Social Media. If multimodal data (i.e. speech) is analysed, the focus again is often placed on the transcription. This paper takes a closer look at how crucial acoustic information actually is for the recognition of emotions from multimodal data. To this end we use the IEMOCAP data, which is one of the larger data sets that provides transcriptions, audio recordings and manual emotion categorization. We build models for emotion classification using text-only, acoustics-only and combining both modalities in order to examine the influence of the various modalities on the final categorization. Our results indicate that using text-only models outperform acoustics-only models. But combining text-only and acoustic-only models improves the results. Additionally, we perform a qualitative analysis and find that a range of misclassifications are due to factors not related to the model, but to the data such as, recording quality, a challenging classification task and misclassifications that are unsurprising for humans. © 2023 Association for Computational Linguistics.",Multi-modal data; Social media; Emotion detection; Classification (of information); Large datasets; Emotion classification; Computational linguistics; Textual data; Text processing; Spoken languages; Acoustic information; Audio recordings; Recognition of emotion; Misclassifications,,,emotion,No,No
scopus,Exploring Attention Mechanisms for Multimodal Emotion Recognition in an Emergency Call Center Corpus,"Deschamps-Berger, T.; Lamel, L.; Devillers, L.",2023,,2023-June,,10.1109/ICASSP49357.2023.10096112,"The emotion detection technology to enhance human decision-making is an important research issue for real-world applications, but real-life emotion datasets are relatively rare and small. The experiments conducted in this paper use the CEMO, which was collected in a French emergency call center. Two pre-trained models based on speech and text were fine-tuned for speech emotion recognition. Using pre-trained Transformer encoders mitigates our data's limited and sparse nature. This paper explores the different fusion strategies of these modality-specific models. In particular, fusions with and without cross-attention mechanisms were tested to gather the most relevant information from both the speech and text encoders. We show that multimodal fusion brings an absolute gain of 4-9% with respect to either single modality and that the Symmetric multi-headed cross-attention mechanism performed better than late classical fusion approaches. Our experiments also suggest that for the real-life CEMO corpus, the audio component encodes more emotive information than the textual one. © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Speech recognition; Character recognition; Signal encoding; Decision making; Level fusion; Late fusion; Call centers; speech emotion recognition; Emotional corpora; cross-attention fusion; Cross-attention fusion; emergency call center; Emergency call center; Emergency calls; late fusion; model-level fusion; Model-level fusion; real-life emotional corpus; Real-life emotional corpus; Transformer-based en-coder; Transformer-based en-coders,,,emotion,No,Yes
scopus,An Adaptive Framework of Multimodal Emotion Recognition Based on Collaborative Discriminative Learning,"Wang, Y.; Guo, X.; Zhang, Y.; Ren, Y.; Huang, W.; Liu, Z.; Feng, Y.; Dai, X.; Zhang, W.; Che, H.",2023,,,,10.1109/ICACI58115.2023.10146184,"Multimodal emotion recognition has great application scenarios in the field of human-computer interaction, and the main task lies in how to capture and handle the consistency and complementarity of multimodal signals. However, in numerous scenarios, multimodal fusion is significantly challenged by variances in the strengths and weaknesses of different modality signals in carrying emotional information. This paper aims to learn the essential information of strong modalities and inter-modality correlations thus achieving better emotion recognition. We propose an adaptive multimodal emotion recognition framework based on collaborative discriminative learning (AMCDL), a framework for emotion recognition that engages in collaborative learning and adjusts to the information-processing capabilities of various emotion recognition models. AMCDL calculates the inter-modality correlation using Canonical Correlation Analysis (CCA) and uses it as an influencing factor to boost the multiplicative combination method to create a class loss function. When trained on several modalities, the class loss function can integrate complementary information to enable strong modal expression, enhancing emotion perception. On the benchmark dataset CMU-MOSI, AMCDL significantly improves classification accuracy when compared to the state-of-the-art models. The experiments showed the flexibility and generality of AMCDL by applying several emotion recognition models to it and producing better classification results than the original model. © 2023 IEEE.",Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Multimodal emotion recognition; Intermodality; Human computer interaction; Classification (of information); multimodal emotion recognition; Recognition models; Discriminative learning; Loss functions; adaptive; Adaptive; Adaptive framework; collaborative discriminative learning; Collaborative discriminative learning; modality correlation; Modality correlation,,,emotion,No,Yes
scopus,Semantic Representation is Superior to Syntactic Representation for Emotion Classification Using Graph Neural Networks,"Vashisht, N.",2023,,,,10.1109/ICDSNS58469.2023.10245002,"The vast volume of user-generated material available on popular social media platforms may be mined for insights about people's emotional states. There are several advantages to this, including learning how people react to the news and events. Short text length and the requirement to identify many emotions (a multi-label classification problem) make social media post emotion categorization challenging. Deep neural networks, including Convolutional neural network designs and recurrent neural network models, have been used in the majority of past work on emotion categorization. However, in order to categorize a broad variety of emotions, none of these systems have attempted to gather semantic and syntax information from text. In this research, we suggest using semantics and syntax aware graphs of attention to categorize emotions from unlabeled text. To finish off the multimodal emotion identification, a graph convolutional neural network (GCNN) is deployed. Extensive trials on two actual data sets, IEMOCAP and MELD, show that the GANNs model achieves greater average accuracy along with f1 scores than the current models for multimodal emotion detection, particularly for emotions like 'happiness' and 'anger.' As a result, the model's emotional intelligence may be improved with the help of dependent syntactic analysis and a self-attention mechanism. © 2023 IEEE.",Semantics; Deep neural networks; Embeddings; Multi-modal; Social media; Social networking (online); Sentiment analysis; Classification (of information); Convolutional neural networks; Convolution; Convolutional neural network; Recurrent neural networks; Emotion classification; Graph neural networks; Syntactics; Semantic Web; Attention based graph neural network and social medium; Attention based Graph Neural Networks and Social Media; Dependency; Emotion Classification; Network media; Semantic; Semantic representation; Syntactic; Syntactic representation,,,emotion,Yes,Yes
scopus,Bimodal Speech Emotion Recognition using Fused Intra and Cross Modality Features,"Kakuba, S.; Han, D.S.",2023,,2023-July,,10.1109/ICUFN57995.2023.10199790,"The interactive speech between two or more inter locutors involves the text and acoustic modalities. These modalities consist of intra and cross-modality relationships at different time intervals which if modeled well, can avail emotionally rich cues for robust and accurate prediction of emotion states. This necessitates models that take into consideration long short-term dependency between the current, previous, and future time steps using multimodal approaches. Moreover, it is important to contextualize the interactive speech in order to accurately infer the emotional state. A combination of recurrent and/or convolutional neural networks with attention mechanisms is often used by researchers. In this paper, we propose a deep learning-based bimodal speech emotion recognition (DLBER) model that uses multi-level fusion to learn intra and cross-modality feature representations. The proposed DLBER model uses the transformer encoder to model the intra-modality features that are combined at the first level fusion in the local feature learning block (LFLB). We also use self-attentive bidirectional LSTM layers to further extract intramodality features before the second level fusion for further progressive learning of the cross-modality features. The resultant feature representation is fed into another self-attentive bidirectional LSTM layer in the global feature learning block (GFLB). The interactive emotional dyadic motion capture (IEMOCAP) dataset was used to evaluate the performance of the proposed DLBER model. The proposed DLBER model achieves 72.93% and 74.05% of F1 score and accuracy respectively.  © 2023 IEEE.",Long short-term memory; Speech emotion recognition; Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Intermodality; Learning systems; Convolutional neural networks; Recognition models; Level fusion; Cross modality; Fusion; Feature representation; Feature learning; Inter modality feature; inter modality features; Intra modality feature; intra modality features,,,emotion,No,Yes
scopus,Exploring multimodal data analysis for emotion recognition in teachers’ teaching behavior based on LSTM and MSCNN,"Lu, Y.; Chen, Z.; Zheng, Q.; Zhu, Y.; Wang, M.",2023,,,,10.1007/s00500-023-08760-2,"The diverse range of emotions exhibited in instructional behavior exerts a profound influence on the effectiveness of teaching and the cognitive state of learners. By leveraging an emotion recognition model, we can analyze the invaluable feedback information derived from teaching behavior data, thereby facilitating the enhancement of pedagogical effectiveness. However, conventional emotion recognition models fall short in capturing the intricate emotional features and subtle nuances inherent in teaching behavior, thereby hindering the accuracy of emotion classification. In light of this, we propose a groundbreaking multimodal emotion recognition model for teaching behavior, founded upon the Long Short-Term Memory (LSTM) and Multi-Scale Convolutional Neural Network (MSCNN). Our approach involves extracting both low-level and high-level local features from text, audio, and image modalities, utilizing LSTM and MSCNN, respectively. Subsequently, a transformer encoder is employed to fuse the extracted features, which are then fed into a fully connected layer for emotion recognition. Experimental results affirm that our proposed model attains an accuracy rate of 84.5% and an F1 score of 84.1% on a self-curated dataset, surpassing other comparative models. These outcomes unequivocally establish the efficacy and superiority of our emotion recognition model. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Long short-term memory; Transformer; Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Multimodal emotion recognition; Convolutional neural networks; Convolutional neural network; Recognition models; LSTM; Teachers'; Multi-scales; MSCNN; Multi-scale convolutional neural network; Multimodal data analysis; Teaching behavior,,,emotion,No,Yes
scopus,Brain Network Features Differentiate Intentions from Different Emotional Expressions of the Same Text,"Li, Z.; Zhao, B.; Zhang, G.; Dang, J.",2023,,2023-June,,10.1109/ICASSP49357.2023.10095376,"Intent differentiation in speech communication relies not only on linguistic information but also on paralinguistic information. The same textual content, when pronounced with different prosodies and emotions, may express totally different intentions. The true intentions in this condition can be easily grasped by our brain. Therefore, combining text, speech, and electroencephalography (EEG) for intent discrimination on the same text may be an effective approach. Before fusing speech and text modalities, the current study focused on exploring effective EEG-based features for Chinese intent recognition as no previous research has utilized EEG signals for this purpose. To tackle this issue, we first created a Chinese multimodal spoken language intention understanding (CMSLIU) dataset, in which the same texts were pronounced with varying prosodies to express different intents. To identify effective brain features that were most relevant to intent recognition improvement, we compared the event-related spectral perturbation and effective brain connectivity patterns on two intent conditions (praise vs. irony). It was found that the praise expression tended to elicit stronger high-frequency brain activities while the irony expression involved a more suppressive network connection in the right hemisphere. These features were trained on the CMSLIU dataset and achieved an intention classification accuracy of 78.66%, which indicated a great potential of the EEG features in intent discrimination on the same text. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Character recognition; Human computer interaction; Classification (of information); Electroencephalography; EEG; Electrophysiology; Brain; Human-computer interaction; Intent recognition; Linguistics; Speech communication; Spoken languages; Condition; Brain network feature; brain network features; Brain networks; Intent understanding; Intention understanding; Network features; paralinguistic information; Paralinguistic information,,,emotion,No,Yes
scopus,Multi-Modal Transformer with Multi-Head Attention for Emotion Recognition,"Xu, C.; Gao, Y.",2023,,,,10.1109/ICSECE58870.2023.10263303,"Multimodal emotion recognition (MER) is a technology aimed at identifying and analyzing emotions from multiple sensory data. In modern life, people usually use not only one modality of information but also multiple modalities of data, such as images, audio, and text, at the same time during their interactions with the outside world. To address this issue, this paper proposes a sentiment recognition model called MTMAER based on Transformer architecture and a multi-head attention mechanism. Unlike existing models that only consider the unimodal feature of the text, MTMAER combines text, image, and audio data. However, existing methods use LSTM to extract semantic information in text features, which has poor long-distance modeling ability. Therefore, the paper uses Transformer to capture sequential contextual information, which can extract deeper semantic information. To achieve cross-modal feature fusion and capture complementary semantic information, the paper also employs a multi-head attention mechanism to effectively capture feature information of different modalities of data, achieving discrete emotion recognition. Experimental results show that MTMAER performs well in the task of multimodal emotion recognition, with an accuracy of 60.89% and a F1 value of 61.55% on the IEMOCAP dataset, demonstrating strong practicality and scalability and can be applied in various practical scenarios.  © 2023 IEEE.",Long short-term memory; Semantics; Transformer; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; multimodal emotion recognition; Semantics Information; Recognition models; human-computer interaction; Multiple modalities; multi-head attention mechanism; Multi-head attention mechanism; Information use; Sensory data,,,emotion,No,Yes
scopus,CMCF-SRNet: A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation,"Zhang, X.; Li, Y.",2023,,1,,,"Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To address these limitations, we propose a cross-modality context fusion and semantic refinement network (CMCF-SRNet). Specifically, we first design a cross-modal locality-constrained transformer to explore the multimodal interaction. Second, we investigate a graph-based semantic refinement transformer, which solves the limitation of insufficient semantic relationship information between utterances. Extensive experiments on two public benchmark datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, indicating its potential application in emotion recognition. Our model is available at https://github.com/zxiaohen/CMCF-SRNet. © 2023 Association for Computational Linguistics.",Semantics; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Speech processing; Benchmarking; Cross-modal; Graphic methods; Cross modality; Dialogue systems; Computational linguistics; Simple++; Semantic refinement; Semantic Web; Contextual feature; ITS applications; First designs,,,emotion,No,No
scopus,PhyMER: Physiological Dataset for Multimodal Emotion Recognition With Personality as a Context,"Pant, S.; Yang, H.-J.; Lim, E.; Kim, S.-H.; Yoo, S.-B.",2023,,11,,10.1109/ACCESS.2023.3320053,"Physiological signals are widely used in the recognition of affective status. Recording of such physiological signals involves elicitation of emotions through different stimuli including video-based stimulus. Considering that the same stimulus videos often induce different emotions in different individuals, emotion recognition in such a scenario requires consideration of the individual differences in the consumption of the stimulus content. With this as our goal, we present a Physiological dataset for Multimodal Emotion Recognition (PhyMER) for studying emotion through physiological response with personality as a context. The PhyMER dataset consists of electroencephalogram (EEG), electrodermal activity (EDA), blood volume pulse (BVP), and skin temperature along with the personality traits of 30 participants. We collected the video-based stimulus dataset for emotion elicitation and developed a web-based annotation tool for labeling felt emotions. We compared the stimulus labels and the self-annotation of felt emotions labeled during physiological data recording. Correlation among personalities was analyzed to study the impact of personality on the intensity of emotions in arousal and valence dimensions. Finally, we proposed a baseline model for the classification of emotions using physiological signals. The dataset is publicly available to the academic community for analysis of affective states and the development of emotion recognition models.  © 2013 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Classification (of information); Electroencephalography; Biomedical signal processing; EEG; Electrophysiology; Physiological models; Electrocardiography; Emotion classification; Video; Electrodermal activity; personality traits; Personality traits; emotion classification; Physiological response; Annotation; Felt; Individual Differences; Motion estimation,,,emotion,Yes,No
scopus,Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition,"Sun, J.; Han, S.; Ruan, Y.-P.; Zhang, X.; Liu, Y.; Huang, Y.; Zheng, S.-K.; Li, T.",2023,,1,,,"Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multimodal transformer model. The new dataset, CHinese Emotion Recognition dataset with Modality-wise Annotations, abbreviated as CHERMA, provides uni-modal labels for each individual modality, and multi-modal labels for all modalities jointly observed. The model consists of uni-modal transformer modules that learn representations for each modality, and a multi-modal transformer module that fuses all modalities. All the modules are supervised by their corresponding labels separately, and the forward information flow is uni-directionally from the uni-modal modules to the multimodal module. The supervision strategy and the model architecture guarantee each individual modality learns its representation independently, and meanwhile the multimodal module aggregates all information. Extensive empirical results demonstrate that our proposed scheme outperforms state-of-the-art alternatives, corroborating the importance of modality independence in multi-modal emotion recognition. The dataset and codes are availabel at https://github.com/sunjunaimer/LFMIM. © 2023 Association for Computational Linguistics.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Learn+; Modeling performance; Multiple modalities; Multi-modal learning; Computational linguistics; Learning approach; Independence models; Layer-wise; Modality independence,,,emotion,Yes,Yes
scopus,Multimodal Emotion Recognition Using Contextualized Audio Information and Ground Transcripts on Multiple Datasets,"Chauhan, K.; Sharma, K.K.; Varma, T.",2023,,,,10.1007/s13369-023-08395-3,"The widespread applications of emotion recognition (ER) in various fields have recently attracted much attention from researchers. Consequently, an array of advanced techniques has emerged, driven by enhancing the accuracy and robustness of these recognition systems. As emotional dialogue comprises sound and spoken content, the proposed model encodes the information from audio and text sequences using two separate channels and merges them for emotional classification. The two channels used inputs from audio and text modalities. The audio channel is encoded using a deep convolutional neural network with residual connections and further transformed using a self-attention-based multihead attention network called channel-wise global head pooling. Unlike the vanilla multihead attention network, an adaptive global pooling is used after concatenating all the heads. The text channel is encoded using a pre-trained BERT model. The proposed ER method is validated on four benchmark databases: Interactive Emotional Dyadic Motion Capture in English, the Berlin emotional speech dataset in the German language, Ryerson Audio-Visual Database of Emotional Speech and Song in North American English and Crowd-sourced Emotional Multimodal Actors Dataset in English. The classification accuracy on the above emotional corpora is 85.71%, 79.52%, 76.71% and 73.91%, respectively. Furthermore, cross-corpus analysis is presented to understand the variability of speech and text features and the robustness of the proposed architecture. © 2023, King Fahd University of Petroleum & Minerals.",Speech emotion recognition; CNN; Speech processing; MFCC; Emotional speech database; Multihead attention,,,emotion,No,Yes
scopus,Exploring Complementary Features in Multi-Modal Speech Emotion Recognition,"Wang, S.; Ma, Y.; Ding, Y.",2023,,,,10.1109/ICASSP49357.2023.10096709,"Speech emotion recognition (SER) is of great importance in human-computer interaction. Recent research has demonstrated that self-supervised learned acoustic and linguistic features are helpful in this task. However, few works have fully exploit the advantages of the pre-trained features in SER. The primary challenge is how to effectively extract the complementary emotional information implied in the pre-trained features of the respective modality. To tackle this challenge, we propose a novel modality-sensitive multimodal speech emotion recognition framework. In a nutshell, we aim to exploit the typical emotion features in each modality and then fuse the complementary emotional information for classification. Specifically, we first utilize the parallel uni-modal encoders to refine the emotion-related information from the pre-trained features of each modality. For better fusion of the multimodal features, we develop a group of learnable emotion query tokens to gather the emotional information from the refined acoustic and linguistic features with the cross-attention mechanism in the transformer decoder. Observing the modality bias problem in multimodal methods, we introduce the random modality masking training strategy to maximize the utilization of the emotional information in each modality and mitigate this problem. We evaluate our method on the widely used IEMOCAP dataset and achieve 1.1% and 0.9% improvements on the unweighted accuracy and weighted accuracy, respectively. Extensive experiments demonstrate the effectiveness of the proposed method.  © 2023 IEEE.",Speech emotion recognition; Transformer; Emotion Recognition; Multi-modal; Speech recognition; Multi-modal fusion; Human computer interaction; Classification (of information); Linguistics; Emotional information; Emotion feature; Multi-modal Fusion; Speech Emotion Recognition; Acoustic features; Complementary features; Linguistic features; Recent researches,,,emotion,No,Yes
scopus,Learning Emotion Representations from Verbal and Nonverbal Communication,"Zhang, S.; Pan, Y.; Wang, J.Z.",2023,,2023-June,,10.1109/CVPR52729.2023.01821,"Emotion understanding is an essential but highly challenging component of artificial general intelligence. The absence of extensive annotated datasets has significantly impeded advancements in this field. We present Emotion-CLIp, the first pre-training paradigm to extract visual emotion representations from verbal and nonverbal communication using only uncurated data. Compared to numerical labels or descriptions used in previous methods, communication naturally contains emotion information. Furthermore, acquiring emotion representations from communication is more congruent with the human learning process. We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emotion cues using sentiment-guided contrastive learning. Extensive experiments validates the effectiveness and transferability of Emotion Clip. Using merely linear-probe evaluation protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and rivals many multimodal approaches across various benchmarks. We anticipate that the advent of Emotion Clip will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related domains. The code and pretrained models are available at https://github.com/Xeaver/EmotionCLIP. © 2023 IEEE.",Multi-modal learning,,,emotion,Yes,No
scopus,"EngageDat-vL: A Multimodal Engagement Dataset Comprising of Emotional, Cognitive, and Behavioral Cues in Virtual Learning Environment","Akre, S.; Palandurkar, N.; Iyengar, A.; Chayande, G.; Kumar, P.",2023,,14301 LNCS,,10.1007/978-3-031-45170-6_28,"To assess student engagement in an e-learning environment, this work proposes a novel learning analytics dataset collection that combines emotional, cognitive, and behavioral data. The dataset includes facial expressions recorded by a webcam during e-learning sessions and log data such as play, pause, seek, course views, course material views, lecture views, quiz responses, etc. The procedure for gathering the data and the difficulties encountered are covered in the study. Combining emotional, cognitive, and behavioral cues for engagement detection in e-learning settings is made possible by this dataset, which represents a significant advancement in the field of learning analytics. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",Emotion Recognition; Multi-modal; Emotion recognition; Face recognition; Computer aided instruction; Facial emotion recognition; Facial emotions; E-learning; E - learning; Behavioral cues; Engagement detection; Learning analytic; Learning analytics; Student engagement; Virtual learning environments,,,emotion,No,No
scopus,A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations,"Zheng, W.; Yu, J.; Xia, R.; Wang, S.",2023,,1,,,"Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods may contain multiple people's faces, which will inevitably introduce noise to the emotion prediction of the real speaker. To tackle this issue, we propose a two-stage framework named Facial expression-aware Multimodal Multi-Task learning (FacialMMT). Specifically, a pipeline method is first designed to extract the face sequence of the real speaker of each utterance, which consists of multimodal face recognition, unsupervised face clustering, and face matching. With the extracted face sequences, we propose a multimodal facial expression-aware emotion recognition model, which leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Experiments demonstrate the effectiveness of the proposed FacialMMT framework on the benchmark MELD dataset. The source code is publicly released at https://github.com/NUSTM/FacialMMT. © 2023 Association for Computational Linguistics.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Multimodal emotion recognition; Learning systems; Multitask learning; Visual information; Learning frameworks; Visual scene; Face sequences; Multi-party conversations,,,emotion,No,No
scopus,Two-Stage Adaptation for Cross-Corpus Multimodal Emotion Recognition,"Huang, Z.; Zhao, J.; Jin, Q.",2023,,14303 LNAI,,10.1007/978-3-031-44696-2_34,"The development of multimodal emotion recognition is severely limited by time-consuming annotation costs. In this paper, we pay attention to the multimodal emotion recognition task in the cross-corpus setting, which can help adapt a trained model to an unlabeled target corpus. Inspired by the recent development of pre-trained models, we adopt a multimodal emotion pre-trained model to provide a better representation learning foundation for our task. However, we may face two domain gaps when applying a pre-trained model to the cross-corpus downstream task: the scenario gap between pre-trained and downstream corpora, and the distribution gap between different downstream sets. To bridge these two gaps, we propose a two-stage adaptation method. Specifically, we first adapt a pre-trained model to the task-related scenario through task-adaptive pre-training. We then fine-tune the model with a cluster-based loss to align the distribution of two downstream sets in a class-conditional manner. Additionally, we propose a ranking-based pseudo-label filtering strategy to obtain more balanced and high-quality samples from the target sets for calculating the cluster-based loss. We conduct extensive experiments on two emotion datasets, IEMOCAP and MSP-IMPROV. The results of our experiments demonstrate the effectiveness of our proposed two-stage adaptation method and the pseudo-label filtering strategy in cross-corpus settings. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multimodal; Transfer learning; Transfer Learning; Down-stream; Cross-corpus; Adaptation methods; Cluster-based; Filtering strategies; Two domains,,,emotion,No,No
scopus,Unimodalities Count as Perspectives in Multimodal Emotion Annotation,"Du, Q.; Labat, S.; Demeester, T.; Hoste, V.",2023,,3494,,,"Most datasets for multimodal emotion recognition only have one emotion annotation for all the modalities combined, which serves as a gold standard for single modalities. This procedure ignores, however, the fact that each modality constitutes a unique perspective that contains its own clues. Moreover, as in unimodal emotion analysis, the perspectives of annotators can also diverge in a multimodal setup. In this paper, we therefore propose to annotate each modality independently and to more closely investigate how perspectives between modalities and annotators diverge. Moreover, we also explore the role of annotator training on perspectivism. We find that for the different unimodal levels, the annotations made on text resemble most closely those of the multimodal setup. Furthermore, we see that annotator training has a positive influence on the annotator agreement in modalities with lower agreement scores, but it also reduces the variety of perspectives. We therefore suggest that a moderate training which still values the individual perspectives of annotators might be beneficial before starting annotations. Finally, we observe that negative sentiment and emotions tend to be annotated more inconsistently across the different modality setups. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Emotion Recognition; Multi-modal; Multimodal emotion recognition; Unimodal; Emotion analysis; Annotator agreement; Counts-as; Gold standards; Multimodal versus unimodal emotion annotation; Perspectivism in NLP; Unimodality,,,emotion,No,No
scopus,An Empirical Study and Improvement for Speech Emotion Recognition,"Wu, Z.; Lu, Y.; Dai, X.",2023,,2023-June,,10.1109/ICASSP49357.2023.10095042,"Multimodal speech emotion recognition aims to detect speakers' emotions from audio and text. Prior works mainly focus on exploiting advanced networks to model and fuse different modality information to facilitate performance, while neglecting the effect of different fusion strategies on emotion recognition. In this work, we consider a simple yet important problem: how to fuse audio and text modality information is more helpful for this multimodal task. Further, we propose a multimodal emotion recognition model improved by perspective loss. Empirical results show our method obtained new state-of-the-art results on the IEMOCAP dataset. The in-depth analysis explains why the improved model can achieve improvements and outperforms baselines. © 2023 IEEE.",Speech emotion recognition; Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Character recognition; Multi-modal fusion; Multimodal fusion; Fusion strategies; Advanced networks; Empirical studies; Perspective loss,,,emotion,No,Yes
scopus,Multimodal Video Emotional Analysis of Time Features Alignment and Information Auxiliary Learning,"Chang, Y.; Liu, T.; Xu, Q.; Zhao, X.; Xiao, X.; Wang, Z.",2023,,2023-June,,10.1109/IJCNN54540.2023.10191930,"With the rapid growth of various images and video data on the network platform, multimodal emotional analysis and recognition have become an increasingly popular research field. Inspired by the human emotional judgment and cross information between different characteristics, this paper proposed a multimodal video emotional analysis network of time features alignment and information auxiliary learning(METI). This network not only fully consider the time alignment between modals to obtain the preliminary information but also pay attention to the auxiliary information in the video and make emotional judgments. METI network is mainly composed of the following three parts: Time alignment network pays attention to time alignment information between different modes; Prepose feedback module uses the auxiliary information in the video to simulate the emotional judgment process; Multimodal gate control module pays attention to the emotional correlation of the context. The comparison of experimental results shows that the performance of the METI model on the dataset exceeds the current most advanced emotional analysis model. At the same time, it has been proved from ablation experiments that the modules and methods proposed in this paper can improve the accuracy of emotion recognition by nearly 4%, and has been respectively proved in the seven classification experiment of emotions and three classification experiment of sentiment. © 2023 IEEE.",Emotion Recognition; Multi-modal; Modal analysis; Sentiment analysis; sentiment analysis; Feature information; Emotional analysis; Feature alignment; Auxiliary information; information auxiliary learning; Information auxiliary learning; modal fusion; Modal fusion; time alignment; Time alignment; Time features,,,emotion,No,Yes
scopus,Dual Memory Fusion for Multimodal Speech Emotion Recognition,"Priyasad, D.; Fernando, T.; Sridharan, S.; Denman, S.; Fookes, C.",2023,,2023-August,,10.21437/Interspeech.2023-1090,"Deep learning has been widely used in multi-modal Speech Emotion Recognition (SER) to learn sentiment-related features by aggregating representations from multiple modes. However, most SOTA methods use attentive fusion or late fusion of data which ignores the possibility of long-term dependencies among data. In this study, we propose a transformer-based SER architecture that fuses modality representations through explicit memory modules, where the information from current inputs is integrated with historical information allowing the model to understand the relative importance of modes over time. We have used Wav2Vec2 and BERT models to extract audio and text features which are then fused together by aggregating features from individual modes with information stored in memory, followed by downstream classification. Following state-of-the-art methods, we evaluate our proposed method on the IEMOCAP dataset and results indicate that memory-based fusion can achieve substantial improvements. © 2023 International Speech Communication Association. All rights reserved.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Deep learning; Character recognition; Multi-modal fusion; Classification (of information); Learn+; multimodal fusion; Multitask learning; Memory architecture; Late fusion; Memory network; Speech communication; multi-task learning; Long-term dependencies; memory networks; Multiple modes,,,emotion,No,No
scopus,Text Augmentation-Based Model for Emotion Recognition Using Transformers,"Mohammad, F.; Khan, M.; Marwat, S.N.K.; Jan, N.; Gohar, N.; Bilal, M.; Al-Rasheed, A.",2023,,76,,10.32604/cmc.2023.040202,"Emotion Recognition in Conversations (ERC) is fundamental in creating emotionally intelligent machines. Graph-Based Network (GBN) models have gained popularity in detecting conversational contexts for ERC tasks. However, their limited ability to collect and acquire contextual information hinders their effectiveness. We propose a Text Augmentation-based computational model for recognizing emotions using transformers (TA-MERT) to address this. The proposed model uses the Multimodal Emotion Lines Dataset (MELD), which ensures a balanced representation for recognizing human emotions. The model used text augmentation techniques to produce more training data, improving the proposed model’s accuracy. Transformer encoders train the deep neural network (DNN) model, especially Bidirectional Encoder (BE) representations that capture both forward and backward contextual information. This integration improves the accuracy and robustness of the proposed model. Furthermore, we present a method for balancing the training dataset by creating enhanced samples from the original dataset. By balancing the dataset across all emotion categories, we can lessen the adverse effects of data imbalance on the accuracy of the proposed model. Experimental results on the MELD dataset show that TA-MERT outperforms earlier methods, achieving a weighted F1 score of 62.60% and an accuracy of 64.36%. Overall, the proposed TA-MERT model solves the GBN models’ weaknesses in obtaining contextual data for ERC. TA-MERT model recognizes human emotions more accurately by employing text augmentation and transformer-based encoding. The balanced dataset and the additional training samples also enhance its resilience. These findings highlight the significance of transformer-based approaches for special emotion recognition in conversations. © 2023 Tech Science Press. All rights reserved.",Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Network models; Character recognition; Graphic methods; Network coding; Emotion recognition in conversation; Contextual information; Balancing; bidirectional encoder representation for transformer; Bidirectional encoder representation for transformer; Graph-based; graph-based network; Graph-based network; Multimodal emotion line dataset; multimodal emotion lines dataset; text augmentation-based model; Text augmentation-based model,,,emotion,No,Yes
scopus,Multimodal Deep Learning Model for Subject-Independent EEG-based Emotion Recognition,"Dharia, S.Y.; Valderrama, C.E.; Camorlinga, S.G.",2023,,2023-September,,10.1109/CCECE58730.2023.10289007,"Regulating emotion is crucial for maintaining well-being and social relationships. However, as we age, the volume of the frontal lobes is reduced, which can cause difficulties in regulating emotions. Electroencephalography (EEG)-based emotion recognition has the potential to understand the complexity of human emotions and the atrophy of the frontal lobes that leads to cognitive impairment. In this study, we investigated a multimodal deep learning approach for subject-independent emotion recognition using EEG and eye movement data. To that end, we proposed an attention mechanism layer to fuse features extracted from the EEG and eye movement data. We tested our approach in two benchmarking emotion recognition datasets: SEED-IV and SEED-V. Our approach achieved an average accuracy of 67.3% and 72.3% for SEED-IV and SEED-V, respectively. Our results demonstrate the potential of multimodal deep learning models for subject-independent emotion recognition using EEG and eye movement data, which can have important implications for assessing emotional regulation in clinical and research settings. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Deep Learning; Learning systems; Electroencephalography; Electrophysiology; Brain; Eye movements; Learning models; Human emotion; Well being; Clinical research; Electroencephalography (EEG); Eye movement datum; Frontal lobes; Social relationships,,,emotion,No,Yes
scopus,TMFER: Multimodal Fusion Emotion Recognition Algorithm Based on Transformer,"Qin, Z.; Chen, X.; Li, G.; Cui, J.",2023,,,,10.1109/ICHCI58871.2023.10277906,"According to the problems of the existing emotion recognition algorithms, which are not rich in emotion information, weak in feature representation and not high in recognition accuracy, this paper proposes a multimodal fusion emotion recognition algorithm based on Transformer (TMFER), which fuses three modalities of text, speech and image information for emotion recognition. For the different characteristics of each modal information, Bert model pre-training processing, MFCC feature extraction and CNN feature extractor extraction methods are used to extract features for each modality respectively, to explore deeper features. To address the problem of unreasonable combination of multi-modal features, the Transformer Encode multi-headed attention mechanism is used to build a feature fusion module to extract and combine potential feature information in different modalities in parallel. The fused data are fed into the algorithm classification module for sentiment recognition classification, and a joint supervised loss function based on large margin learning is customized to solve the problem of unbalanced classification and feature confounding in the baseline model. Finally, based on the IEMOCAP and MELD multimodal datasets, the TMFER algorithm is experimentally compared with current algorithms in the field that are more effective in emotion recognition classification. The experimental results show that the TMFER algorithm outperforms other algorithms in all evaluation metrics. © 2023 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Character recognition; Multi-modal fusion; Classification (of information); Learning algorithms; Extraction; Feature representation; Recognition algorithm; Recognition accuracy; Text information; Joint supervision; Multimode fusion; Multimodes,,,emotion,No,Yes
scopus,MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations,"Shi, T.; Huang, S.-L.",2023,,1,,,"Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping relationships across different modalities. Furthermore, existing state-of-the-art ERC models have difficulty classifying minority and semantically similar emotion categories. To address these challenges, we propose a novel attention-based correlation-aware multimodal fusion framework named MultiEMO, which effectively integrates multimodal cues by capturing cross-modal mapping relationships across textual, audio and visual modalities based on bidirectional multi-head cross-attention layers. The difficulty of recognizing minority and semantically hard-to-distinguish emotion classes is alleviated by our proposed Sample-Weighted Focal Contrastive (SWFC) loss. Extensive experiments on two benchmark ERC datasets demonstrate that our MultiEMO framework consistently outperforms existing state-of-the-art approaches in all emotion categories on both datasets, the improvements in minority and semantically similar emotions are especially significant. © 2023 Association for Computational Linguistics.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal information; Multi-modal fusion; 'current; Emotion classification; Language processing; Natural languages; Contextual information; Computational linguistics; Mapping; Complex mapping; Mapping relationships,,,emotion,No,No
scopus,Residual Learning with Bi-LSTM and Multi-Head Attention for Multi-Modal Emotion Recognition,"Gao, Y.; Xu, C.",2023,,,,10.1109/ICIPCA59209.2023.10257779,"Multimodal emotion recognition has a wide range of applications in the fields of intelligent recommendation and human-computer interaction. In recent research on emotion recognition, the model using recurrent neural network could observe the context semantic information and infer the emotion label jointly according to the context information, but it lacked the capture of key information and did not solve the problem of network degradation. Therefore, this paper designs a model based on Bi-LSTM, multi-head attention mechanism and residual connection fusion (Att-BiLSTM). The Bi-LSTM structure realizes the context semantic inference function, the multi-head attention mechanism realizes the emphasis on key information, and the residual connection alleviates the network degradation caused by the overfitting problem. Att-BiLSTM achieves 62.1% precision and 61.8% recall on the IEMOCAP dataset, which is better than the existing comparison algorithms.  © 2023 IEEE.",Long short-term memory; Semantics; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; Residual network; Multi-head attention; multi-head attention; Multi-modal emotion recognition; Recent researches; Bi-LSTM; multi-modal emotion recognition; Network degradations; residual network,,,emotion,Yes,No
scopus,Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism,"Sousa, A.; Young, K.; d’Aquin, M.; Zarrouk, M.; Holloway, J.",2023,,14020 LNCS,,10.1007/978-3-031-35681-0_43,"Automatic Emotion Detection (ED) aims to build systems to identify users’ emotions automatically. This field has the potential to enhance HCI, creating an individualised experience for the user. However, ED systems tend to perform poorly on people with Autism Spectrum Disorder (ASD). Hence, the need to create ED systems tailored to how people with autism express emotions. Previous works have created ED systems tailored for children with ASD but did not share the resulting dataset. Sharing annotated datasets is essential to enable the development of more advanced computer models for ED within the research community. In this paper, we describe our experience establishing a process to create a multimodal annotated dataset featuring children with a level 1 diagnosis of autism. In addition, we introduce CALMED (Children, Autism, Multimodal, Emotion, Detection), the resulting multimodal emotion detection dataset featuring children with autism aged 8–12. CALMED includes audio and video features extracted from recording files of study sessions with participants, together with annotations provided by their parents into four target classes. The generated dataset includes a total of 57,012 examples, with each example representing a time window of 200 ms (0.2 s). Our experience and methods described here, together with the dataset shared, aim to contribute to future research applications of affective computing in ASD, which has the potential to create systems to improve the lives of people with ASD. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Emotion Recognition; Multi-modal; Artificial intelligence; Emotion detection; Affective Computing; Diseases; Multi-modal dataset; Multimodal Dataset; Autism spectrum disorders; Detection system; Autism; Annotated datasets; Children with autisms; Multimodal emotion detection; Multimodal Emotion Detection,,,emotion,Yes,No
scopus,Sentiment Analysis and Emotion Recognition from Speech Using Universal Speech Representations,"Atmaja, B.T.; Sasou, A.",2022,,22,,10.3390/s22176369,"The study of understanding sentiment and emotion in speech is a challenging task in human multimodal language. However, in certain cases, such as telephone calls, only audio data can be obtained. In this study, we independently evaluated sentiment analysis and emotion recognition from speech using recent self-supervised learning models—specifically, universal speech representations with speaker-aware pre-training models. Three different sizes of universal models were evaluated for three sentiment tasks and an emotion task. The evaluation revealed that the best results were obtained with two classes of sentiment analysis, based on both weighted and unweighted accuracy scores (81% and 73%). This binary classification with unimodal acoustic analysis also performed competitively compared to previous methods which used multimodal fusion. The models failed to make accurate predictionsin an emotion recognition task and in sentiment analysis tasks with higher numbers of classes. The unbalanced property of the datasets may also have contributed to the performance degradations observed in the six-class emotion, three-class sentiment, and seven-class sentiment tasks. © 2022 by the authors.",emotion; Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; human; Affective Computing; Sentiment analysis; controlled study; learning; speech; human experiment; sentiment analysis; article; binary classification; affective computing; speech emotion recognition; acoustic analysis; Emotion recognition from speech; Sentiment analyse and emotion recognition; sentiment analysis and emotion recognition; Telephone calls; universal speech representation; Universal speech representation,,,emotion,No,Yes
scopus,"The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress","Christ, L.; Amiriparian, S.; Baird, A.; Tzirakis, P.; Kathan, A.; Müller, N.; Stappen, L.; Meßner, E.-M.; König, A.; Cowen, A.; Cambria, E.; Schuller, B.W.",2022,,,,10.1145/3551876.3554817,"The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to multimodal sentiment and emotion recognition. For this year's challenge, we feature three datasets: (i) the Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset that contains audio-visual recordings of German football coaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in which reactions of individuals to emotional stimuli have been annotated with respect to seven emotional expression intensities, and (iii) the Ulm-Trier Social Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled with continuous emotion values (arousal and valence) of people in stressful dispositions. Using the introduced datasets, MuSe 2022 addresses three contemporary affective computing problems: in the Humor Detection Sub-Challenge (MuSe-Humor), spontaneous humour has to be recognised; in the Emotional Reactions Sub-Challenge (MuSe-Reaction), seven fine-grained 'in-the-wild' emotions have to be predicted; and in the Emotional Stress Sub-Challenge (MuSe-Stress), a continuous prediction of stressed emotion values is featured. The challenge is designed to attract different research communities, encouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the communities of audio-visual emotion recognition, health informatics, and symbolic sentiment analysis. This baseline paper describes the datasets as well as the feature sets extracted from them. A recurrent neural network with LSTM cells is used to set competitive baseline results on the test partitions for each sub-challenge. We report an Area under the Curve (AUC) of.8480 for MuSe-Humor;.2801 mean (from 7-classes) Pearson's Correlations Coefficient for MuSe-Reaction, as well as.4931 Concordance Correlation Coefficient (CCC) and.4761 for valence and arousal in MuSe-Stress, respectively.  © 2022 ACM.",Long short-term memory; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Modal analysis; Multi-modal fusion; multimodal sentiment analysis; Affective Computing; Sentiment analysis; multimodal fusion; Multimodal sentiment analyse; Statistical tests; Challenge; Humor detection; affective computing; benchmark; Benchmark; challenge; Sports; Emotional stress; humor detection; Medical informatics,,,emotion,Yes,Yes
scopus,RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition,"Yuan, L.; Huang, G.; Li, F.; Yuan, X.; Pun, C.-M.; Zhong, G.",2023,,31,,10.1109/TASLP.2023.3284509,"Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation module (BiAM). First, GGM constructs a novel graph to reduce the redundancy of target node information. Then, SCBM calculates the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the node. Meanwhile, BiAM is a novel aggregation method that can preserve the information of nodes during the aggregation process. This module can construct the interaction between different modalities and capture long-range contextual information based on similarity clusters. On both the IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a 2.17 ∼ 5.21% improvement over that of the most advanced method. © 2014 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Oral communication; Speech processing; multimodal fusion; Convolution; Convolutional neural network; Redundancy; Convolutional networks; Graph theory; Neural networks; Context models; Information filtering; Bandpass filters; context modeling; Information filter; similarity cluster; Similarity cluster,,,emotion,No,Yes
scopus,Dual-Sequence LSTM Multimodal Emotion Recognition Based on Attention Mechanism,"Dong, D.; Ji, R.; Mei, Y.",2023,,1770 CCIS,,10.1007/978-981-99-0301-6_12,"This paper presents a multimodal emotion recognition method based on dual-sequence Long Short-Term Memory (LSTM) network with attention mechanism. The ResNeXt50 network and coordinated attention mechanism are introduced to the video sequences to acquire the long-term dependence information in the location and space of video images. CNN with self-attention mechanism is utilized to catch the semantic features of audio sequences. In order to eliminate redundancy, a dual-sequence LSTM cross-modal network embedded with self-attention mechanism is applied for emotion feature fusion, and finally get the emotion output. The experimental results on RAVDESS dataset and eNTERFACE ’05 dataset prove that, compared with different baseline methods, the proposed algorithm is more attentive to the correlation between features and can significantly improve the recognition accuracy. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Long short-term memory; Semantics; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multimodal; Attention mechanism; LSTM; Video sequences; Memory network; Recognition methods; Long-term dependence; Semantic features; Video image,,,emotion,No,Yes
scopus,Detecting Music-Induced Emotion Based on Acoustic Analysis and Physiological Sensing: A Multimodal Approach,"Hu, X.; Li, F.; Liu, R.",2022,,12,,10.3390/app12189354,"The subjectivity of listeners’ emotional responses to music is at the crux of optimizing emotion-aware music recommendation. To address this challenge, we constructed a new multimodal dataset (“HKU956”) with aligned peripheral physiological signals (i.e., heart rate, skin conductance, blood volume pulse, skin temperature) and self-reported emotion collected from 30 participants, as well as original audio of 956 music pieces listened to by the participants. A comprehensive set of features was extracted from physiological signals using methods in physiological computing. This study then compared performances of three feature sets (i.e., acoustic, physiological, and combined) on the task of classifying music-induced emotion. Moreover, the classifiers were also trained on subgroups of users with different Big-Five personality traits for further customized modeling. The results reveal that (1) physiological features contribute to improving performance on valence classification with statistical significance; (2) classification models built for users in different personality groups could sometimes further improve arousal prediction; and (3) the multimodal classifier outperformed single-modality ones on valence classification for most user groups. This study contributes to designing music retrieval systems which incorporate user physiological data and model listeners’ emotional responses to music in a customized manner. © 2022 by the authors.",multimodal recognition; customization; music retrieval and generation; physiological measures; sound and music computing,,,emotion,No,Yes
scopus,Improving multimodal fusion with Main Modal Transformer for emotion recognition in conversation,"Zou, S.; Huang, X.; Shen, X.; Liu, H.",2022,,258,,10.1016/j.knosys.2022.109978,"Emotion recognition in conversation (ERC) is essential for developing empathic conversation systems. In conversation, emotions can exist in multiple modalities, i.e., audio, text, and visual. Due to the inherent characteristics of each modality, it is not easy for the model to use all modalities effectively when fusing modal information. However, existing approaches consider the same representation ability of each modality, resulting in unsatisfactory fusion across modalities. Therefore, we consider different modalities with different representation abilities, propose the concept of the main modal, i.e., the modal with stronger representation ability after feature extraction, and then propose the method of Main Modal Transformer (MMTr) to improve the effect of multimodal fusion. The method preserves the integrity of the main modal features and enhances the representation of weak modalities by using multihead attention to learn the information interactions between modalities. In addition, we designed a new emotional cue extractor that extracts emotional cues from two levels (the speaker's self-context and the contextual context in conversation) to enrich the conversation information obtained by each modal. Extensive experiments on two benchmark datasets validate the effectiveness and superiority of our model. © 2022 Elsevier B.V.",Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Multi-modal fusion; Emotion recognition in conversation; Multiple modalities; Multihead; Main modal; Multihead attention; Conversation systems; Emotional cue; Emotional cues,,,emotion,No,No
scopus,Towards enhancing emotion recognition via multimodal framework,"Akalya Devi, C.; Karthika Renuka, D.; Pooventhiran, G.; Harish, D.; Yadav, S.; Thirunarayan, K.",2023,,44,,10.3233/JIFS-220280,"Emotional AI is the next era of AI to play a major role in various fields such as entertainment, health care, self-paced online education, etc., considering clues from multiple sources. In this work, we propose a multimodal emotion recognition system extracting information from speech, motion capture, and text data. The main aim of this research is to improve the unimodal architectures to outperform the state-of-the-arts and combine them together to build a robust multi-modal fusion architecture. We developed 1D and 2D CNN-LSTM time-distributed models for speech, a hybrid CNN-LSTM model for motion capture data, and a BERT-based model for text data to achieve state-of-the-art results, and attempted both concatenation-based decision-level fusion and Deep CCA-based feature-level fusion schemes. The proposed speech and mocap models achieve emotion recognition accuracies of 65.08% and 67.51%, respectively, and the BERT-based text model achieves an accuracy of 72.60%. The decision-level fusion approach significantly improves the accuracy of detecting emotions on the IEMOCAP and MELD datasets. This approach achieves 80.20% accuracy on IEMOCAP which is 8.61% higher than the state-of-the-art methods, and 63.52% and 61.65% in 5-class and 7-class classification on the MELD dataset which are higher than the state-of-the-arts. © 2023 - IOS Press. All rights reserved.",Long short-term memory; Emotion Recognition; Speech recognition; BERT; Emotion recognition; Character recognition; State of the art; Classification (of information); Text data; Arts computing; CNN-LSTM; Motion capture; DCCA; Decision level fusion; Distributed models; Time-distributed model; time-distributed models,,,emotion,No,Yes
scopus,Multimodal emotion recognition based on feature fusion and residual connection,"Du, X.; Yang, J.; Xie, X.",2023,,,,10.1109/EEBDA56825.2023.10090537,"Multimodal emotion recognition is widely used in human-computer interaction, medical care, education, human resources, and other fields, and is one of the important technologies in current research. The existing research is based on single-modal emotion recognition, which has the problems of low accuracy of emotion recognition, key information missing, and gradient disappearing. Based on this situation, this paper proposes a multimodal emotion recognition model based on feature fusion and residual connection, which uses Bi-LSTM and Muti-head attention to extract key features in voice, text, and video, and combines residual connection mechanisms to prevent gradient from disappearing. The average accuracy of this model on the IEMOCAP dataset can reach 61.4%, and the F1 value can reach 61.7%. Compared with other single-modal emotion recognition models and some baseline models, the emotion classification effect of this model is better. © 2023 IEEE.",Long short-term memory; Emotion Recognition; Speech recognition; Emotion recognition; Character recognition; Multimodal emotion recognition; 'current; Human computer interaction; Feature fusion; Features fusions; Recognition models; Multi-head attention; Single-modal; Bi-LSTM; Information gradients; Information missing,,,emotion,No,Yes
scopus,KTFEv2: Multimodal Facial Emotion Database and its Analysis,"Nguyen, H.; Tran, N.; Nguyen, H.D.; Nguyen, L.; Kotani, K.",2023,,11,,10.1109/ACCESS.2023.3246047,"In recent years, the focus of human emotion analysis has gradually shifted towards not only using visible information, but also thermal infrared (IR) information. This requires a great deal of facial emotion data both in visible and thermal IR information. However, most existing databases contain either visible information or posed thermal IR information only. For these reasons, we propose and establish a multimodal facial emotion database including both natural spontaneous visible and thermal IR videos. Beside updating more thermal infrared information, the built dataset in this study also enhances the information of intensity emotions. In which, each emotion is classified into three levels (low, medium, and high). Seven spontaneous emotions from thirty subjects are recorded in the database. Audio and visual stimuli were used to elicit emotions during the experiment. After the standard procedure of collecting data finished, the database has been through the careful annotation and verification procedure. Furthermore, the built database is analyzed by using modern machine learning models such as CNN, ResNet50, YOLO, and using a combination of different models to analyze the dataset. The obtained results are feasible and show that this dataset is useful for use in practice. The results of thermal data analysis provide us with a promising idea for future research on estimating human emotion.  © 2013 IEEE.",Multi-modal; Facial expression; Facial Expressions; Emotion analysis; Facial emotions; Human emotion; Database systems; facial emotion; Infrared radiation; KTFE database; spontaneous database; Spontaneous database; thermal image; Thermal images; Thermal-infrared; visible image; Visible image,,,emotion,Yes,No
scopus,How Does a Social Robot Analyze Emotions?,"Buvet, P.-A.; Fache, B.; Fadel, W.; Rouam, A.",2023,,561 LNNS,,10.1007/978-3-031-18344-7_31,"We present a study on the multimodal analysis of emotions. It will be used to create an emotion detector that can be implemented in a social robot. First, we specify the nature of the emotions studied. Secondly, we detail the main stages of the research: constitution of the dataset; extraction of the dataset from an image stream, a sound stream, and a text stream; classification of facial expressions and prosody in image and sound streams; labeling of the emotions of the text flow with a predefined typology; labeling the emotions of the image and sound stream classes with the tagged text stream. Thirdly, we discuss the protocol used to evaluate the research work. Finally, we present the perspectives of the research. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Robotics; Machine learning; Emotion recognition,,,emotion,Yes,No
scopus,A bimodal network based on Audio–Text-Interactional-Attention with ArcFace loss for speech emotion recognition,"Tang, Y.; Hu, Y.; He, L.; Huang, H.",2022,,143,,10.1016/j.specom.2022.07.004,"Speech emotion recognition (SER) is an essential part of human–computer interaction. Meanwhile, the SER has widely utilized multimodal information in SER in recent years. This paper focuses on exploiting the acoustic and textual modalities for the SER task. We propose a bimodal network based on an Audio–Text-Interactional-Attention (ATIA) structure which can facilitate the interaction and fusion of the emotionally salient information within the acoustic and textual modalities. We also explored four different ATIA structures and verified their effectiveness. Finally, we selected one ATIA structure to build our bimodal network with the best performance. Furthermore, our SER model adopts an additive angular margin loss, named ArcFace loss, applied to the deep face recognition field. Compared with the widespread Softmax loss, our visualization results demonstrated the effectiveness of the ArcFace loss function. ArcFace loss can improve the discriminate power of features by focusing on the angles between the features and the weights. As we know, it is the first time to apply ArcFace loss in the field of SER. Finally, the results show that the bimodal network combined ArcFace loss achieved 72.8% of Weighted Accuracy (WA) and 62.5% of Unweighted Accuracy (UA) for the seven-class emotion classification, and 82.4% of WA and 80.6% of UA for the four-class emotion classification on the IEMOCAP dataset. © 2022 Elsevier B.V.",Speech emotion recognition; Emotion Recognition; Performance; Speech recognition; Face recognition; Multi-modal information; Character recognition; Network-based; Human computer interaction; Classification (of information); Recognition models; Emotion classification; Audio acoustics; Arcface loss; ArcFace loss; Audio–text-interactional-attention; Audio–Text-Interactional-Attention; Bimodal network; Visualization results,,,emotion,No,Yes
scopus,Real-Time Human-Music Emotional Interaction Based on Deep Learning and Multimodal Sentiment Analysis,"Jiang, T.; Deng, S.; Wu, P.; Jiang, H.",2023,,2023,,10.1155/2023/4939048,"Music, as an integral component of culture, holds a prominent position and is widely accessible. There has been growing interest in studying sentiment represented by music and its emotional effects on its audiences, however, much of the existing literature is subjective and overlooks the impact of music on the real-time expression of emotion. In this article, two labeled datasets for music sentiment classification and multimodal sentiment classification were developed. Deep learning is used to classify music sentiment, while decision-level fusion is used to classify the multimodal sentiment of real-time listeners. We combine sentiment analysis with a conventional online music playback system and propose an innovative human-music emotional interaction system based on multimodal sentiment analysis and deep learning. It has been demonstrated through individual observation and questionnaire studies that the interaction between human and musical sentiments has a positive impact on the negative emotions of listeners.  © 2023 Tianyue Jiang et al.",Multi-modal; Modal analysis; Deep learning; Sentiment analysis; Classification (of information); Music; Sentiment classification; Real- time; Labeled dataset; Decision level fusion; Emotional interactions; Integral components; Interaction systems; Online music,,,emotion,No,No
scopus,Multimodal Human Facial Emotion Recognition Using DenseNet-161 and Image Feature Stabilization Algorithm,"Angeline, R.; Nithya, A.A.",2022,,39,,10.18280/ts.390630,"Human Facial Emotion Recognition (FER) is the technology to predict listener's emotion of static images and videos to uncover data on one's enthusiastic states like happy, sad, frustration, anxiety, surprise, hate and neutral states. It's a part of the affective computing technology, which may be a collaborative area of research on listener's emotions. The ability of a computer to recognise and understand and frequently explain human emotions and emotional states which relies on computing (AI) technology. The most difficult part of the FER is to plan the various looks with the separate enthusiastic states. The standard steps of Facial Emotion Recognition are i) Face RoI identification ii) Feature Extraction and iii) Emotion Recognition. Convolutional Neural Network models are the most commonly utilised to detect listener emotions. In this paper, an Image Feature Stabilization Algorithm (IFSA) is proposed to improve the efficiency of facial emotion recognition by implementing Deep Convolutional Neural Network (DCNN) model using the Transfer Learning (TL) technique. The architecture entails employing a FER-compatible pre-trained Densenet-161 based DCNN model and then fine-tuning the model for face emotion data. Initially, the dense layer(s) is/are trained, followed by the fine-tuning of each of the pre-trained DCNN blocks, resulting in an improvement in FER accuracy, particularly for difficult front face views like partial view. Experiments when performed on the CK+ dataset by employing a 10-fold cross validation method using various pre-train models like VGG 16, VGG19, ResNet-18, 34, 50, 152, Inception-V3 and DenseNet-161 showed accuracy of 85.9%, 94.6%, 91%, 91%, 91.1%, 95.1%, 97.1% and 98.7% respectively. Thus, Facial Emotion Recognition performed using the finetuned DenseNet-161 architecture demonstrated exceptionally improved accuracy compared to other pretrained models, along with the proposed image feature stabilization algorithm. The proposed architecture using Densenet-161 showed improved accuracy of 98.78% and 97.52% in other challenging FER datasets like KDEF and JAFEE. © 2022 Lavoisier. All rights reserved.",Emotion Recognition; Deep neural networks; Speech recognition; Emotion recognition; Face recognition; Convolutional neural networks; Convolution; Convolutional neural network; Image enhancement; Network architecture; Image features; transfer learning; Transfer learning; Feature extraction; Facial emotion recognition; Facial emotions; convolutional neural network (CNN); Deep convolutional neural network; facial emotion recognition; CK+; deep CNN; Densenet-161; DenseNet-161; image feature stabilization algorithm; Image feature stabilization algorithm; Stabilization; Stabilization algorithms,,,emotion,No,Yes
scopus,Recognition of Emotions in Speech Using Convolutional Neural Networks on Different Datasets,"Zielonka, M.; Piastowski, A.; Czyżewski, A.; Nadachowski, P.; Operlejn, M.; Kaczor, K.",2022,,11,,10.3390/electronics11223831,"Artificial Neural Network (ANN) models, specifically Convolutional Neural Networks (CNN), were applied to extract emotions based on spectrograms and mel-spectrograms. This study uses spectrograms and mel-spectrograms to investigate which feature extraction method better represents emotions and how big the differences in efficiency are in this context. The conducted studies demonstrated that mel-spectrograms are a better-suited data type for training CNN-based speech emotion recognition (SER). The research experiments employed five popular datasets: Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D), Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), Surrey Audio-Visual Expressed Emotion (SAVEE), Toronto Emotional Speech Set (TESS), and The Interactive Emotional Dyadic Motion Capture (IEMOCAP). Six different classes of emotions were used: happiness, anger, sadness, fear, disgust, and neutral. However, some experiments were prepared to recognize just four emotions due to the characteristics of the IEMOCAP dataset. A comparison of classification efficiency on different datasets and an attempt to develop a universal model trained using all datasets were also performed. This approach brought an accuracy of 55.89% when recognizing four emotions. The most accurate model for six emotion recognition was trained and achieved 57.42% accuracy on a combination of four datasets (CREMA-D, RAVDESS, SAVEE, TESS). What is more, another study was developed that demonstrated that improper data division for training and test sets significantly influences the test accuracy of CNNs. Therefore, the problem of inappropriate data division between the training and test sets, which affected the results of studies known from the literature, was addressed extensively. The performed experiments employed the popular ResNet18 architecture to demonstrate the reliability of the research results and to show that these problems are not unique to the custom CNN architecture proposed in experiments. Subsequently, the label correctness of the CREMA-D dataset was studied through the employment of a prepared questionnaire. © 2022 by the authors.",artificial intelligence; machine learning; classification; speech emotion recognition; SER; convolutional neural networks,,,emotion,Yes,Yes
scopus,Graph Theoretical Analysis of EEG Functional Connectivity Patterns and Fusion with Physiological Signals for Emotion Recognition,"Xefteris, V.-R.; Tsanousa, A.; Georgakopoulou, N.; Diplaris, S.; Vrochidis, S.; Kompatsiaris, I.",2022,,22,,10.3390/s22218198,"Emotion recognition is a key attribute for realizing advances in human–computer interaction, especially when using non-intrusive physiological sensors, such as electroencephalograph (EEG) and electrocardiograph. Although functional connectivity of EEG has been utilized for emotion recognition, the graph theory analysis of EEG connectivity patterns has not been adequately explored. The exploitation of brain network characteristics could provide valuable information regarding emotions, while the combination of EEG and peripheral physiological signals can reveal correlation patterns of human internal state. In this work, a graph theoretical analysis of EEG functional connectivity patterns along with fusion between EEG and peripheral physiological signals for emotion recognition has been proposed. After extracting functional connectivity from EEG signals, both global and local graph theory features are extracted. Those features are concatenated with statistical features from peripheral physiological signals and fed to different classifiers and a Convolutional Neural Network (CNN) for emotion recognition. The average accuracy on the DEAP dataset using CNN was 55.62% and 57.38% for subject-independent valence and arousal classification, respectively, and 83.94% and 83.87% for subject-dependent classification. Those scores went up to 75.44% and 78.77% for subject-independent classification and 88.27% and 90.84% for subject-dependent classification using a feature selection algorithm, exceeding the current state-of-the-art results. © 2022 by the authors.","Humans; Emotions; emotion; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Physiology; human; Neural Networks, Computer; physiology; Multi-modal fusion; Human computer interaction; Classification (of information); electroencephalography; Electroencephalography; procedures; Biomedical signal processing; EEG; multimodal fusion; Convolutional neural network; algorithm; Algorithms; arousal; Multimodal physiological signal; Electroencephalograph; Graph theory; Neural networks; multimodal physiological signals; functional connectivity; Arousal; Functional connectivity; Functional connectivity patterns; Graph theoretical analysis; graph theory",,,emotion,No,Yes
scopus,Stocastic Multimodal Fusion Method for Classifying Emotions With Attention Mechanism Using Deep Learning,"Selvi, R.; Vijayakumaran, C.",2023,,,,10.1109/ICACCS57279.2023.10113124,"Deep multimodal Learning is a subfield of machine learning that seeks to train artificial intelligence models to handle and discover relationships between various modalities in order to minimize reader bias and is most likely to generate high-quality data due to the standardized training program's controlled and transparent setting, which includes interpretation methods and criterion as well as improving diagnostic accuracy compared to only one modality In order to accomplish multimodal emotion categorization, Using physiological data as input, propose an unique classification technique in this study called stochastic multimodal fusion with attention mechanism. To be more precise, this work utilizes an adversarial learning-based discriminator to drive the feature extraction technique to explicitly learn the linked information. Moreover, we developed a reconstruction method based on attention to motivate the feature transformation to master the feature maps associated with the emotional field. In this way, more perception information can be added to the features of each mode of operation. Our method considers both correlative and interrelated data from three modalities for multimodal fusion, in contrast to previous multi-modal approaches, which solely concentrate on learning complimentary features from temporal characteristics. For multimodal fusion, our technique considers both the correlations and the differences between the three modalities. This is unlike previous multimodal-based methodologies, which only focused on learning correlations from temporal characteristics. As part of our evaluation process, conduct extensive testing on the ASCERTAIN database, a publicly accessible multimodal dataset. The findings of the experiments show that our suggested strategy is preferable to recognize different emotions than the methods that are currently considered to be state-of-the-art and that it improves accuracy on average. © 2023 IEEE.",Attention; Attention mechanisms; Emotion Recognition; Multi-modal; deep learning; Behavioral research; Deep learning; Multi-modal fusion; Learning systems; Emotion detection; attention; Physiological models; Statistical tests; Multi-modal learning; Digital storage; Fusion methods; Quality control; fusion; Stochastic models; Stochastic systems; emotion detection; LSTMP; Temporal characteristics,,,emotion,No,Yes
scopus,Emotion recognition based on multi-modal physiological signals and transfer learning,"Fu, Z.; Zhang, B.; He, X.; Li, Y.; Wang, H.; Huang, J.",2022,,16,,10.3389/fnins.2022.1000716,"In emotion recognition based on physiological signals, collecting enough labeled data of a single subject for training is time-consuming and expensive. The physiological signals’ individual differences and the inherent noise will significantly affect emotion recognition accuracy. To overcome the difference in subject physiological signals, we propose a joint probability domain adaptation with the bi-projection matrix algorithm (JPDA-BPM). The bi-projection matrix method fully considers the source and target domain’s different feature distributions. It can better project the source and target domains into the feature space, thereby increasing the algorithm’s performance. We propose a substructure-based joint probability domain adaptation algorithm (SSJPDA) to overcome physiological signals’ noise effect. This method can avoid the shortcomings that the domain level matching is too rough and the sample level matching is susceptible to noise. In order to verify the effectiveness of the proposed transfer learning algorithm in emotion recognition based on physiological signals, we verified it on the database for emotion analysis using physiological signals (DEAP dataset). The experimental results show that the average recognition accuracy of the proposed SSJPDA-BPM algorithm in the multimodal fusion physiological data from the DEAP dataset is 63.6 and 64.4% in valence and arousal, respectively. Compared with joint probability domain adaptation (JPDA), the performance of valence and arousal recognition accuracy increased by 17.6 and 13.4%, respectively. Copyright © 2022 Fu, Zhang, He, Li, Wang and Huang.",emotion; emotion recognition; human; human experiment; multimodal fusion; domain adaptation; learning algorithm; article; transfer learning; arousal; noise; transfer of learning; physiological signal; individual difference; probability,,,emotion,Yes,Yes
scopus,A Multi-Level Circulant Cross-Modal Transformer for Multimodal Speech Emotion Recognition,"Gong, P.; Liu, J.; Wu, Z.; Han, B.; Ken Wang, Y.; He, H.",2023,,74,,10.32604/cmc.2023.028291,"Speech emotion recognition, as an important component of human-computer interaction technology, has received increasing attention. Recent studies have treated emotion recognition of speech signals as a multimodal task, due to its inclusion of the semantic features of two different modalities, i.e., audio and text. However, existing methods often fail in effectively represent features and capture correlations. This paper presents a multi-level circulant cross-modal Transformer (MLCCT) for multimodal speech emotion recognition. The proposed model can be divided into three steps, feature extraction, interaction and fusion. Self-supervised embedding models are introduced for feature extraction, which give a more powerful representation of the original data than those using spectrograms or audio features such as Mel-frequency cepstral coefficients (MFCCs) and low-level descriptors (LLDs). In particular, MLCCT contains two types of feature interaction processes, where a bidirectional Long Short-term Memory (Bi-LSTM) with circulant interaction mechanism is proposed for low-level features, while a two-stream residual cross-modal Transformer block is applied when high-level features are involved. Finally, we choose self-attention blocks for fusion and a fully connected layer to make predictions. To evaluate the performance of our proposed model, comprehensive experiments are conducted on three widely used benchmark datasets including IEMOCAP, MELD and CMU-MOSEI. The competitive results verify the effectiveness of our approach. © 2023 Tech Science Press. All rights reserved.",Long short-term memory; Semantics; Speech emotion recognition; Emotion Recognition; Embeddings; Multi-modal; Speech recognition; Character recognition; Human computer interaction; Features extraction; Benchmarking; Cross-modal; Extraction; Feature extraction; self-attention; Self-attention; Cross-modal transformer; Multilevels; cross-modal transformer; Circulants; self-supervised embedding model; Self-supervised embedding model,,,emotion,No,Yes
scopus,Multi-Channel Weight-Sharing Autoencoder Based on Cascade Multi-Head Attention for Multimodal Emotion Recognition,"Zheng, J.; Zhang, S.; Wang, Z.; Wang, X.; Zeng, Z.",2023,,25,,10.1109/TMM.2022.3144885,"Multimodal Emotion Recognition is challenging because of the heterogeneity gap among different modalities. Due to the powerful ability of feature abstraction, Deep Neural Networks (DNNs) have exhibited significant success in bridging the heterogeneity gap in cross-modal retrieval and generation tasks. In this work, a DNNs-based Multi-channel Weight-sharing Autoencoder with Cascade Multi-head Attention (MCWSA-CMHA) is proposed to generically address the affective heterogeneity gap in MER. Specifically, multimodal heterogeneity features are extracted by multiple independent encoders, and then a scalable heterogeneous feature fusion module (CMHA) is realized by connecting multiple multi-head attention modules in series. The core of the proposed algorithm is to reduce the heterogeneity between the output features of different encoders through the unsupervised training of MCWSA, and then to model the affective interactions between different modal features through the supervised training of CMHA. Experimental results demonstrate that the proposed MCWSA-CMHA achieves outperformance on two publicly available datasets compared with the state-of-the-art techniques. In addition, visualization experiments and approximation experiments are used to verify the effectiveness of each module in the proposed algorithm, and the experimental results show that the proposed MCWSA-CMHA can mine more emotion-related information among multimodal features compared with other fusion methods.  © 2022 IEEE.",Attention mechanisms; Emotion Recognition; Deep neural networks; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Signal encoding; Data mining; Job analysis; Task analysis; Features extraction; Auto encoders; autoencoder; Decoding; multi-head attention mechanism; Multi-head attention mechanism; Multi channel; Approximation algorithms; Multimodal emotion recognition (MER),,,emotion,No,No
scopus,"Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables","Saganowski, S.; Komoszyńska, J.; Behnke, M.; Perz, B.; Kunc, D.; Klich, B.; Kaczmarek, Ł.D.; Kazienko, P.",2022,,9,,10.1038/s41597-022-01262-0,"The Emognition dataset is dedicated to testing methods for emotion recognition (ER) from physiological responses and facial expressions. We collected data from 43 participants who watched short film clips eliciting nine discrete emotions: amusement, awe, enthusiasm, liking, surprise, anger, disgust, fear, and sadness. Three wearables were used to record physiological data: EEG, BVP (2x), HR, EDA, SKT, ACC (3x), and GYRO (2x); in parallel with the upper-body videos. After each film clip, participants completed two types of self-reports: (1) related to nine discrete emotions and (2) three affective dimensions: valence, arousal, and motivation. The obtained data facilitates various ER approaches, e.g., multimodal ER, EEG- vs. cardiovascular-based ER, discrete to dimensional representation transitions. The technical validation indicated that watching film clips elicited the targeted emotions. It also supported signals’ high quality. © 2022, The Author(s).",Humans; Emotions; Facial Expression; emotion; facial expression; human; physiology; anger; self report; Anger; psychology; sadness; Sadness; Self Report,,,emotion,No,No
scopus,Cross-modal distillation with audio–text fusion for fine-grained emotion classification using BERT and Wav2vec 2.0,"Kim, D.; Kang, P.",2022,,506,,10.1016/j.neucom.2022.07.035,"Fine-grained emotion classification for mood- and emotion-related physical-characteristics detection and its application to computer technology using biometric sensors has been extensively researched in the field of affective computing. Although text modality has achieved a considerably high performance from the perspective of sentiment analysis, which simply classifies a positive or negative label, fine-grained emotion classification requires additional information besides text. An audio feature can be adopted as the additional information as it is closely associated with text, and the characteristics of the changes in sound pulses can be employed in fine-grained emotion classification. However, the multimodal datasets related to fine-grained emotion are limited, and the scalability and efficiency are insufficient for multimodal training to be applied extensively via the self-supervised learning (Self-SL) approach, which can adequately represent modality. To address these limitations, we propose cross-modal distillation (CMD), which induces the feature spaces of student models with a few parameters while receiving those of the teacher models that can adequately express each modality based on Self-SL. The proposed CMD performs the mapping of a feature space between teacher-student models based on contrastive learning, while two attention mechanisms—cross-attention between audio and text features and self-attention for features in modality—are performed during knowledge distillation. Wav2vec 2.0 and BERT, which are already adequately trained for audio and text via Self-SL, were adopted as teacher models; audio–text transformer models were used as student models. Accordingly, the CMD-based representation learning applies a lightweight model for IEMOCAP, MELD, and CMU–MOSEI datasets with the task of multi-class emotion classification, while exhibiting better fine-grained emotion classification performance than benchmark models with a considerably low uncertainty for prediction. © 2022 Elsevier B.V.",emotion; Transformer; BERT; human; Learning systems; Sentiment analysis; Classification (of information); learning; attention; Benchmarking; Cross-modal; human experiment; article; Emotion classification; feature learning (machine learning); prediction; Contrastive learning; Audio acoustics; Distillation; uncertainty; Knowledge distillation; Student Modeling; Students; Fine grained; Wav2vec 2.0; distillation; Multi-class emotion classification; teacher; Wav2Vec 2.0,,,emotion,No,Yes
scopus,A Framework to Evaluate Fusion Methods for Multimodal Emotion Recognition,"Pena, D.; Aguilera, A.; Dongo, I.; Heredia, J.; Cardinale, Y.",2023,,11,,10.1109/ACCESS.2023.3240420,"Multimodal methods for emotion recognition consider several sources of data to predict emotions; thus, a fusion method is needed to aggregate the individual results. In the literature, there is a high variety of fusion methods to perform this task, but they are not suitable for all scenarios. In particular, there are two relevant aspects that can vary from one application to another: (i) in many scenarios, individual modalities can have different levels of data quality or even be absent, which demands fusion methods able to discriminate non-useful from relevant data; and (ii) in many applications, there are hardware restrictions that limit the use of complex fusion methods (e.g., a deep learning model), which could be quite computationally intensive. In this context, developers and researchers need metrics, guidelines, and a systematic process to evaluate and compare different fusion methods that can fit to their particular application scenarios. As a response to this need, this paper presents a framework that establishes a base to perform a comparative evaluation of fusion methods to demonstrate how they adapt to the quality differences of individual modalities and to evaluate their performance. The framework provides equivalent conditions to perform a fair assessment of fusion methods. Based on this framework, we evaluate several fusion methods for multimodal emotion recognition. Results demonstrate that for the architecture and dataset selected, the methods that best fit are: Self-Attention and Weighted methods for all available modalities, and Self-Attention and Embracenet+ when a modality is missing. Concerning the time, the best times correspond to Multilayer Perceptron (MLP) and Self-Attention models, due to their small number of operations. Thus, the proposed framework provides insights for researchers in this area to identify which fusion methods better fit their requirements, and thus to justify the selection.  © 2013 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Learning models; Multi-modality; fusion methods; Fusion methods; Quality control; multimodality; Complex fusion; Data quality; Hardware restrictions; Systematic process,,,emotion,No,Yes
scopus,A Multi-Modal Deep Learning Approach for Emotion Recognition,"Shahzad, H.M.; Bhatti, S.M.; Jaffar, A.; Rashid, M.",2023,,36,,10.32604/iasc.2023.032525,"In recent years, research on facial expression recognition (FER) under mask is trending. Wearing a mask for protection from Covid 19 has become a compulsion and it hides the facial expressions that is why FER under the mask is a difficult task. The prevailing unimodal techniques for facial recognition are not up to the mark in terms of good results for the masked face, however, a multimodal technique can be employed to generate better results. We proposed a multimodal methodology based on deep learning for facial recognition under a masked face using facial and vocal expressions. The multimodal has been trained on a facial and vocal dataset. We have used two standard datasets, M-LFW for the masked dataset and CREMA-D and TESS dataset for vocal expressions. The vocal expressions are in the form of audio while the faces data is in image form that is why the data is heterogenous. In order to make the data homogeneous, the voice data is converted into images by taking spectrogram. A spectrogram embeds important features of the voice and it converts the audio format into the images. Later, the dataset is passed to the multimodal for training. neural network and the experimental results demonstrate that the proposed multimodal algorithm outsets unimodal methods and other state-of-the-art deep neural network models. © Tech Science Press.",facial expression recognition; Deep learning; speech emotion recognition; covid-19; multi-model neural network; spectrogram,,,emotion,No,No
scopus,Real-Time Multimodal Emotion Recognition in Conversation for Multi-Party Interactions,"Rasendrasoa, S.; Pauchet, A.; Saunier, J.; Adam, S.",2022,,,,10.1145/3536221.3556601,"In order to improve multi-party social interaction with artificial companions such as robots or virtual agents, real-time Emotion Recognition in Conversation (ERC) is required. In this context, ERC is a challenging task which involves multiple challenges, such as processing multimodal data over time, taking into account the multi-party context with any number of participants, understanding implied relevant commonsense knowledge during interaction and taking into account each participant's emotional attitude. To deal with the aforementioned challenges, we design a multimodal off-the-shelf model that meets the requirements of real-life scenarios, specifically dyadic and multi-party interactions. We propose a Knowledge Aware Multi-Headed Network that integrates various sources including the dialog history and commonsense knowledge about the speaker and other participants. The weights of these pieces of information are modulated using a multi-head attention mechanism. The proposed model is learnt in a Multi-Task Learning framework which combines the ERC task with a Dialogue Act (DA) recognition task and an Emotion Shift (ES) detection task through a joint learning strategy. Our proposition obtains competitive and stable results on several benchmark datasets that vary in number of participants and length of conversations, and outperforms the state-of-the-art on one of these datasets. The importance of DA and ES prediction in determining the speaker's current emotional state is investigated.  © 2022 ACM.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Affective Computing; Affective computing; Data handling; Commonsense knowledge; Emotion recognition in conversation; Multimodal Interaction; Real- time; Social interactions; Interactive computer systems; Dialog acts; Emotion Recognition in Conversation; Multi-party Interaction; Multiparty interaction,,,emotion,No,No
scopus,Towards Machine Learning-Based Emotion Recognition from Multimodal Data,"Shahriar, M.F.; Arnab, M.S.A.; Khan, M.S.; Rahman, S.S.; Mahmud, M.; Kaiser, M.S.",2023,,519 LNNS,,10.1007/978-981-19-5191-6_9,"Understanding human emotion is vital to communicate effectively with others, monitor patients, analyse behaviour, and keep an eye on those who are vulnerable. Emotion recognition is essential to achieve a complete human-machine interoperability experience. Artificial intelligence, mainly machine learning (ML), have been used in recent years to improve the model for recognising emotions from a single type of data. A multimodal system has been proposed that uses text, facial expressions, and speech signals to identify emotions in this work. The MobileNet architecture is used to predict emotion from facial expressions, and different ML classifiers are used to predict emotion from text and speech signals in the proposed model. The Facial Expression Recognition 2013 (FER2013) dataset has been used to recognise emotion from facial expressions, whilst the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset was used for both text and speech emotion recognition. The proposed ensemble technique consisting of random forest, extreme gradient boosting, and multi-layer perceptron achieves an accuracy of 70.67%, which is better than the unimodal approaches used. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Machine learning; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Multi-modal data; Character recognition; Multimodal; Patient monitoring; Machine-learning; Forecasting; Speech signals; Human emotion; Speech communication; Mobilenet; MobileNet; Recognizing emotions; Human-machine; Prediction,,,emotion,No,Yes
scopus,Multi-Label Multimodal Emotion Recognition With Transformer-Based Fusion and Emotion-Level Representation Learning,"Le, H.-D.; Lee, G.-S.; Kim, S.-H.; Kim, S.; Yang, H.-J.",2023,,11,,10.1109/ACCESS.2023.3244390,"Emotion recognition has been an active research area for a long time. Recently, multimodal emotion recognition from video data has grown in importance with the explosion of video content due to the emergence of short video social media platforms. Effectively incorporating information from multiple modalities in video data to learn robust multimodal representation for improving recognition model performance is still the primary challenge for researchers. In this context, transformer architectures have been widely used and have significantly improved multimodal deep learning and representation learning. Inspired by this, we propose a transformer-based fusion and representation learning method to fuse and enrich multimodal features from raw videos for the task of multi-label video emotion recognition. Specifically, our method takes raw video frames, audio signals, and text subtitles as inputs and passes information from these multiple modalities through a unified transformer architecture for learning a joint multimodal representation. Moreover, we use the label-level representation approach to deal with the multi-label classification task and enhance the model performance. We conduct experiments on two benchmark datasets: Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Carnegie Mellon University Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) to evaluate our proposed method. The experimental results demonstrate that the proposed method outperforms other strong baselines and existing approaches for multi-label video emotion recognition.  © 2013 IEEE.",Transformer; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Multi-modal fusion; Multimodal emotion recognition; Learning systems; Classification (of information); Multimodal fusion; Video recording; Modeling performance; Multiple modalities; Video data; transformers; Multi-labels; multi-label video emotion recognition; Multi-label video emotion recognition,,,emotion,Yes,Yes
scopus,Multimodal emotion recognition with action features,"Sun, Y.-N.; Wen, Y.-H.; Shu, Y.-Z.; Liu, Y.-J.",2022,,43,,10.11996/JG.j.2095-302X.2022061159,"In recent years, using knowledge of computer science to realize emotion recognition based on multimodal data has become an important research direction in the fields of natural human-computer interaction and artificial intelligence. The emotion recognition research using visual modality information usually focuses on facial features, rarely considering action features or multimodal features fused with action features. Although action has a close relationship with emotion, it is difficult to extract valid action information from the visual modality. In this paper, we started with the relationship between action and emotion, and introduced action data extracted from visual modality to classic multimodal emotion recognition dataset, MELD. The body action features were extracted based on ST-GCN model, and the action features were applied to the LSTM model-based single-modal emotion recognition task. In addition, body action features were introduced to bi-modal emotion recognition in MELD dataset, improving the performance of the fusion model based on the LSTM network. The combination of body action features and text features enhanced the recognition accuracy of the context model with pre-trained memory compared with that only using the text features. The results of the experiment show that although the accuracy of body action features for emotion recognition is not higher than those of traditional text features and audio features, body action features play an important role in the process of multimodal emotion recognition. The experiments on emotion recognition based on single-modal and multimodal features validate that people use actions to convey their emotions, and that using body action features for emotion recognition has great potential. © 2022 Editorial of Board of Journal of Graphics. All rights reserved.",emotion recognition; multimodality; action and emotion; action features; visual modality,,,emotion,No,Yes
scopus,Multi-modal fusion network with complementarity and importance for emotion recognition,"Liu, S.; Gao, P.; Li, Y.; Fu, W.; Ding, W.",2023,,619,,10.1016/j.ins.2022.11.076,"Multimodal emotion recognition, that is, emotion recognition uses machine learning to generate multi-modal features on the basis of videos which has become a research hotspot in the field of artificial intelligence. Traditional multi-modal emotion recognition method only simply connects multiple modalities, and the interactive utilization rate of modal information is low, and it cannot reflect the real emotion under the conflict of modal features well. This article first proves that effective weighting can improve the discrimination between modalities. Therefore, this paper takes into account the importance differences between multiple modalities, and assigns weights to them through the importance attention network. At the same time, considering that there is a certain complementary relationship between the modalities, this paper constructs an attention network with complementary modalities. Finally, the reconstructed features are fused to obtain a multi-modal feature with good interaction. The method proposed in this paper is compared with traditional methods in public datasets. The test results show that our method is accurate in It performs well in both the rate and confusion matrix metrics. © 2022 Elsevier Inc.",Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Multi-modal fusion; Multimodal emotion recognition; Attention mechanism; Machine-learning; Multiple modalities; Hotspots; Recognition methods,,,emotion,No,No
scopus,SER30K: A Large-Scale Dataset for Sticker Emotion Recognition,"Liu, S.; Zhang, X.; Yang, J.",2022,,,,10.1145/3503161.3548407,"With the popularity of instant messaging applications, online chatting plays an essential role in our daily life. The prevailing use of stickers to express emotions in online chatting leads to the necessity of multimodal sticker emotion recognition. Considering the lack of sticker emotion data, we collect a large-scale sticker emotion recognition dataset named SER30K. It consists of a total of 1,887 sticker themes with total 30,739 sticker images. Some commonly used images, such as realistic images and facial expression images, have been well studied in the field of emotion analysis. However, it is still challenging to understand the emotion of sticker images. Since the characteristics in stickers from the same theme are similar, we can only accurately predict the emotion by capturing the local information (e.g., expressions, poses) and understanding the global information (e.g., relations among objects). To tackle this challenge, we propose a LOcal Re-Attention multimodal network (LORA) to learn sticker emotions in an end-to-end manner. Different from previous approaches using convolutional neural networks, LORA employs the vision transformer to extract visual features, leading to better capture the global relations. In addition, we design a local re-attention module to focus on important region information. Then a simple but efficient modal fusion module combines visual and language features. Extensive experiments are performed on the SER30K and other emotion recognition datasets, demonstrating the effectiveness of our proposed method. Our code, model and dataset are released on https://github.com/nku-shengzheliu/SER30K.  © 2022 ACM.",Emotion Recognition; Speech recognition; Emotion recognition; Social networking (online); Visual languages; Emotion analysis; Convolutional neural networks; Large dataset; Dataset; Multi-modal learning; multimodal learning; dataset; Visual feature; Multimodal network; Instant messaging; Large-scale datasets; Online chatting; Sticker emotion analyse; sticker emotion analysis,,,emotion,No,Yes
scopus,Empirical Comparison between Deep and Classical Classifiers for Speaker Verification in Emotional Talking Environments,"Nassif, A.B.; Shahin, I.; Lataifeh, M.; Elnagar, A.; Nemmour, N.",2022,,13,,10.3390/info13100456,"Speech signals carry various bits of information relevant to the speaker such as age, gender, accent, language, health, and emotions. Emotions are conveyed through modulations of facial and vocal expressions. This paper conducts an empirical comparison of performances between the classical classifiers: Gaussian Mixture Model (GMM), Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Artificial neural networks (ANN); and the deep learning classifiers, i.e., Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Gated Recurrent Unit (GRU) in addition to the ivector approach for a text-independent speaker verification task in neutral and emotional talking environments. The deep models undergo hyperparameter tuning using the Grid Search optimization algorithm. The models are trained and tested using a private Arabic Emirati Speech Database, Ryerson Audio–Visual Database of Emotional Speech and Song dataset (RAVDESS) database, and a public Crowd-Sourced Emotional Multimodal Actors (CREMA) database. Experimental results illustrate that deep architectures do not necessarily outperform classical classifiers. In fact, evaluation was carried out through Equal Error Rate (EER) along with Area Under the Curve (AUC) scores. The findings reveal that the GMM model yields the lowest EER values and the best AUC scores across all datasets, amongst classical classifiers. In addition, the ivector model surpasses all the fine-tuned deep models (CNN, LSTM, and GRU) based on both evaluation metrics in the neutral, as well as the emotional speech. In addition, the GMM outperforms the ivector using the Emirati and RAVDESS databases. © 2022 by the authors.",Long short-term memory; Emotion Recognition; Deep neural networks; Speech recognition; Classification (of information); Features extraction; Convolutional neural networks; Convolution; Convolutional neural network; Support vector machines; deep neural network; feature extraction; Database systems; Emirati; Emotional speech; Nearest neighbor search; Audio-visual database; Classical classifier; classical classifiers; emotional speech; Empirical comparison; Gaussian distribution; Gaussian Mixture Model; I vectors; speaker verification; Speaker verification,,,emotion,No,No
scopus,Cross-Modality Learning by Exploring Modality Interactions for Emotion Reasoning,"Tran, T.-D.; Ho, N.-H.; Pant, S.; Yang, H.-J.; Kim, S.-H.; Lee, G.",2023,,11,,10.1109/ACCESS.2023.3283597,"Even without hearing or seeing individuals, humans are able to determine subtle emotions from a range of indicators and surroundings. However, existing research on emotion recognition is mostly focused on recognizing the emotions of speakers across complete modalities. In real-world situations, emotion reasoning is an interesting field for inferring human emotions from a person's surroundings when neither the face nor voice can be observed. Therefore, in this paper, we propose a novel multimodal approach for predicting emotion from missing one or more modalities based on attention mechanisms. Specifically, we employ self-attention for each unimodal representation to extract the dominant features and utilize the compounded paired-modality attention (CPMA) among sets of modalities to identify the context of the considered individual, such as the interplay of modalities, and capture people's interactions in the video. The proposed model is trained on the Multimodal Emotion Reasoning (MEmoR) dataset, which includes multimedia inputs such as visual, audio, text, and personality. The proposed model achieves a weighted F1-score of 50.63% for the primary emotion group and 42.7% for the fine-grained one. According to the results, our proposed model outperforms the conventional approaches in terms of emotion reasoning.  © 2013 IEEE.",Cognition; Emotion Recognition; Speech recognition; Emotion recognition; Job analysis; Task analysis; Brain modeling; Computational modelling; Cross modality; self-attention; Self-attention; Cognitive systems; Audition; compounded paired-modality attention; Compounded paired-modality attention; cross-modality learning; Cross-modality learning; Emotion reasoning; modified SDPA; Modified SDPA,,,emotion,No,Yes
scopus,Multimodal transformer augmented fusion for speech emotion recognition,"Wang, Y.; Gu, Y.; Yin, Y.; Han, Y.; Zhang, H.; Wang, S.; Li, C.; Quan, D.",2023,,17,,10.3389/fnbot.2023.1181598,"Speech emotion recognition is challenging due to the subjectivity and ambiguity of emotion. In recent years, multimodal methods for speech emotion recognition have achieved promising results. However, due to the heterogeneity of data from different modalities, effectively integrating different modal information remains a difficulty and breakthrough point of the research. Moreover, in view of the limitations of feature-level fusion and decision-level fusion methods, capturing fine-grained modal interactions has often been neglected in previous studies. We propose a method named multimodal transformer augmented fusion that uses a hybrid fusion strategy, combing feature-level fusion and model-level fusion methods, to perform fine-grained information interaction within and between modalities. A Model-fusion module composed of three Cross-Transformer Encoders is proposed to generate multimodal emotional representation for modal guidance and information fusion. Specifically, the multimodal features obtained by feature-level fusion and text features are used to enhance speech features. Our proposed method outperforms existing state-of-the-art approaches on the IEMOCAP and MELD dataset. Copyright © 2023 Wang, Gu, Yin, Han, Zhang, Wang, Li and Quan.",emotion; Speech emotion recognition; Transformer; Emotion Recognition; Multi-modal; Speech recognition; human; Modal interactions; speech; human experiment; transformer; Information fusion; article; Hybrid fusions; Feature-level fusions; Fusion methods; Fine grained; speech emotion recognition; Breakthrough point; hybrid fusion; modal interaction; multimodal enhancement; Multimodal enhancements,,,emotion,No,No
scopus,Multimodal Emotional Classification Based on Meaningful Learning,"Filali, H.; Riffi, J.; Boulealam, C.; Mahraz, M.A.; Tairi, H.",2022,,6,,10.3390/bdcc6030095,"Emotion recognition has become one of the most researched subjects in the scientific community, especially in the human–computer interface field. Decades of scientific research have been conducted on unimodal emotion analysis, whereas recent contributions concentrate on multimodal emotion recognition. These efforts have achieved great success in terms of accuracy in diverse areas of Deep Learning applications. To achieve better performance for multimodal emotion recognition systems, we exploit Meaningful Neural Network Effectiveness to enable emotion prediction during a conversation. Using the text and the audio modalities, we proposed feature extraction methods based on Deep Learning. Then, the bimodal modality that is created following the fusion of the text and audio features is used. The feature vectors from these three modalities are assigned to feed a Meaningful Neural Network to separately learn each characteristic. Its architecture consists of a set of neurons for each component of the input vector before combining them all together in the last layer. Our model was evaluated on a multimodal and multiparty dataset for emotion recognition in conversation MELD. The proposed approach reached an accuracy of 86.69%, which significantly outperforms all current multimodal systems. To sum up, several evaluation techniques applied to our work demonstrate the robustness and superiority of our model over other state-of-the-art MELD models. © 2022 by the authors.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Classification (of information); Multilayer neural networks; Feature extraction; Neural-networks; multimodal emotion recognition (MER); deep learning (DL); Emotional classification; Meaningful neural network; meaningful neural network (MNN); Multimodal and multiparty dataset for emotion recognition in conversation (MELD); multimodal and multiparty dataset for emotion recognition in conversations (MELD); Scientific community,,,emotion,No,Yes
scopus,K-Means Clustering-Based Kernel Canonical Correlation Analysis for Multimodal Emotion Recognition in Human-Robot Interaction,"Chen, L.; Wang, K.; Li, M.; Wu, M.; Pedrycz, W.; Hirota, K.",2023,,70,,10.1109/TIE.2022.3150097,"In this article, K-meansclustering-based Kernel canonical correlation analysis algorithm is proposed for multimodal emotion recognition in human-robot interaction (HRI). The multimodal features (gray pixels; time and frequency domain) extracted from facial expression and speech are fused based on Kernel canonical correlation analysis. K-means clustering is used to select features from multiple modalities and reduce dimensionality. The proposed approach can improve the heterogenicity among different modalities and make multiple modalities complementary to promote multimodal emotion recognition. Experiments on two datasets, namely SAVEE and eNTERFACE'05, are conducted to evaluate the accuracy of the proposed method. The results show that the proposed method produces good recognition rates that are higher than the ones produced by the methods without K-means clustering; more specifically, they are 2.77% higher in SAVEE and 4.7% higher in eNTERFACE'05.  © 1982-2012 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; Human robot interaction; Correlation; multimodal emotion recognition; Features extraction; Feature fusion; Features fusions; Kernel; Frequency domain analysis; Time domain analysis; Feature extraction; Man machine systems; Correlation methods; K-means clustering; K-means++ clustering; Histogram; Kernel canonical correlation analyse; Kernel canonical correlation analysis; Kernel canonical correlation analysis (KCCA),,,emotion,No,Yes
scopus,The decadal perspective of facial emotion processing and Recognition: A survey,"Rathour, N.; Singh, R.; Gehlot, A.; Vaseem Akram, S.; Kumar Thakur, A.; Kumar, A.",2022,,75,,10.1016/j.displa.2022.102330,"Facial expression recognition (FER) is playing a crucial role in distinct psychological disorders, human–machine interaction, and a multitude of multimedia applications. The transformation of FER from lab to wild conditions and significant advancement in deep learning has led to the implementation of automatic FER. In this article, we provide a review of FER that includes Ekman's six basic emotions, the significance of FER with datasets, and deep learning algorithms. The article classified the fundamental procedure of FER into distinct for clear understanding. The significance of each procedure in FER including face detection& tracking, extracting facial features of dynamic & static images, and facial expression classification is addressed with algorithms in this article. The existing state of art deep neural networks including convolution neural network (CNN), deep belief network (DBN), the deep auto encoder (DAE), and recurrent neural network (RNN) for FER are also presented in this article. Finally, the article provides challenges and recommendations namely deficiency in datasets, biasness and inconsistency in data set, integration of robust models, multimodal for effective recognition. FER with technology for sustainable health, edge computing powered devices for FER implementation, adoption of FER-based human interaction robots, and customized portable device for FER. © 2022 Elsevier B.V.",Emotion Recognition; Deep neural networks; CNN; Emotion recognition; Face recognition; Facial expression; Facial Expressions; Deep learning; Human robot interaction; Facial expression recognition; Deep learning (DL); Learning algorithms; Recurrent neural networks; Facial emotions; Human machine interaction; FER; Convolution neural network; Emotion processing; Facial emotion dataset; Psychological disorders,,,emotion,No,No
scopus,Judging the emotional states of customer service staff in the workplace: A multimodal dataset analysis,"Liu, P.; Zhang, Y.; Xiong, Z.; Wang, Y.; Qing, L.",2022,,13,,10.3389/fpsyg.2022.1001885,"Background: Emotions play a decisive and central role in the workplace, especially in the service-oriented enterprises. Due to the highly participatory and interactive nature of the service process, employees’ emotions are usually highly volatile during the service delivery process, which can have a negative impact on business performance. Therefore, it is important to effectively judge the emotional states of customer service staff. Methods: We collected data on real-life work situations of call center employees in a large company. Three consecutive studies were conducted: first, the emotional states of 29 customer service staff were videotaped by wide-angle cameras. In Study 1, we constructed scoring criteria and auxiliary tools of picture-type scales through a free association test. In Study 2, two groups of experts were invited to evaluate the emotional states of customer service staff. In Study 3, based on the results in Study 2 and a multimodal emotional recognition method, a multimodal dataset was constructed to explore how each modality conveys the emotions of customer service staff in workplace. Results: Through the scoring by 2 groups of experts and 1 group of volunteers, we first developed a set of scoring criteria and picture-type scales with the combination of SAM scale for judging the emotional state of customer service staff. Then we constructed 99 (out of 297) sets of stable multimodal emotion datasets. Based on the comparison among the datasets, we found that voice conveys emotional valence in the workplace more significantly, and that facial expressions have more prominant connection with emotional arousal. Conclusion: Theoretically, this study enriches the way in which emotion data is collected and can provide a basis for the subsequent development of multimodal emotional datasets. Practically, it can provide guidance for the effective judgment of employee emotions in the workplace. Copyright © 2022 Liu, Zhang, Xiong, Wang and Qing.",emotion recognition; multimodal dataset; customer service staff; picture-type scale; workplace emotions,,,emotion,No,Yes
scopus,Modality encoded latent dataset for emotion recognition,"Mert, A.",2023,,79,,10.1016/j.bspc.2022.104140,"Variational autoencoder (VAE) is an unsupervised learning that represents high dimensional input data into normally distributed latent space. Multi-channel physiological signals, namely EEG and peripherals are mostly preferred for affective computing. The DEAP dataset is converted into multimodal latent dataset for emotion recognition in this study. 40-ch recordings of 32 participants are encoded to different modalities of peripherals and 32-ch EEG. First, short-time Fourier transform (STFT) is used to extract time–frequency (TF) distribution for training VAE. Thus, the localized components in the each channel of the modalities is converted to 100-dimensional space using VAE. The proposed method is applied to each participant's recordings to obtain new latent encoded dataset. Within and between subject classification results using latent dataset are compared to the original data for peripheral, 32ch EEG and peripheral with EEG modalities. Naive Bayes (NB) classifier is used to evaluate the encoding performance of the 100-dimensional modalities, and compared to original results. The error rates of leave-one participant-out cross-validation (LOPO CV) 0.3322 and 0.3327 are yielded for high/low arousal and valence states while the originals are 0.349 and 0.382. © 2022 Elsevier Ltd",emotion; Emotion Recognition; Speech recognition; Emotion recognition; Physiological signals; adult; female; human; male; Learning systems; Classification (of information); Auto encoders; autoencoder; Variational autoencoder; electroencephalogram; human experiment; classifier; article; Data fusion; arousal; cross validation; Bayesian learning; Multimodal data fusion; Multi channel; clinical article; Barium compounds; High-dimensional; Higher-dimensional; Input datas; Latent space; Normal distribution; short time Fourier transform; Sodium compounds; valence (chemistry),,,emotion,No,Yes
scopus,Content recommendation based on recognised Emotion,"Challa, S.; Gowri, M.V.; Roobini, M.S.; Nandhini, P.; Daphine Desona Clemency, A.C.",2023,,,,10.1109/RAEEUCCI57140.2023.10134505,"E-commerce, video websites, and social networking sites are just a few examples of where recommendation algorithms are currently being used. As the amount and variety of data used in recommendation systems grows, more and more of it will be unstructured multimodal data, including text, images, and videos. Yet, it is difficult to employ unstructured multimodal data to enhance the effectiveness of recommendation schemes because of the illustration gap between different modalities. In this paper, we present a new multimodal framework for user-driven ensemble modelling based on the recommendation of mood descriptions. In contrast to standard approaches, our video reference formulation makes use of not just the audio stream but also the face (emotion) and textual modalities that together make up a video document's elementary data. Audio data is used as input by algorithms like Gaussian Naïve Bayes (GNB), Logistic Regression (LR), and Convolutional Neural Network (CNN), whereas text data is used by methods like, LR, Long-short Term Memory (LSTM), and Naive Bayes. The user's current disposition is used to inform recommendations for appropriate music, quotes, etc. generated by these ensemble models. Comprehensive experiments on real-world datasets demonstrate that, across a wide range of assessment measures, the proposed ensemble model greatly outperforms the state-of-the-art baselines. © 2023 IEEE.",Long short-term memory; Emotion Recognition; Multi-modal data; Music; Convolutional neural networks; Brain; Recommender systems; Ensemble models; Neural-networks; Naive bayes; Logistics regressions; Content recommendations; Long-short term; Long-short Term; Neural Network; Recommendation systems; Short term; Unstructured multimodal data; User-based ensemble model,,,emotion,No,No
scopus,A Novel Long Short-Term Memory Network Model For Multimodal Music Emotion Analysis In Affective Computing,"Chen, W.",2023,,26,,10.6180/jase.202303_26(3).0008,"The emotion recognition of medium audio/video in affective computing has important application value for deep cognition in human-computer interaction (HCI)/brain-computer interaction (BCI) and other fields. Especially in the modern distance education, music emotion analysis can be used as one of the important techniques for real-time evaluation of teaching process. In complex dance scenes, the accuracy of music emotion analysis with traditional methods is not high. Therefore, this paper proposes a novel long short-term memory (LSTM) network model for multimodal music emotion analysis in affective computing. Dual-channel LSTM is used to simulate human auditory and visual processing pathways respectively to process the emotional information of music and facial expressions. Then, we train and test the model on an open bi-modal music dataset. Based on the LSTM model, the analytic hierarchy process (AHP) is introduced to fuse weighted feature at decision level. Finally, experiments show that the proposed method can effectively improve the recognition rate, and save a lot of training time. © The Author(’s).",Long short-term memory; Emotion Recognition; Multi-modal; Emotion recognition; Modal analysis; Audio videos; Network models; Affective Computing; Human computer interaction; Emotion analysis; Brain; LSTM; human-computer interaction; Statistical tests; Memory network; Cognitive systems; Music emotions; affective computing; Distance education; analytic hierarchy process; Brain computer interactions; Music emotion analyse; Music emotion analysis,,,emotion,No,Yes
scopus,An Ensemble Learning Method for Emotion Charting Using Multimodal Physiological Signals,"Awan, A.W.; Usman, S.M.; Khalid, S.; Anwar, A.; Alroobaea, R.; Hussain, S.; Almotiri, J.; Ullah, S.S.; Akram, M.U.",2022,,22,,10.3390/s22239480,"Emotion charting using multimodal signals has gained great demand for stroke-affected patients, for psychiatrists while examining patients, and for neuromarketing applications. Multimodal signals for emotion charting include electrocardiogram (ECG) signals, electroencephalogram (EEG) signals, and galvanic skin response (GSR) signals. EEG, ECG, and GSR are also known as physiological signals, which can be used for identification of human emotions. Due to the unbiased nature of physiological signals, this field has become a great motivation in recent research as physiological signals are generated autonomously from human central nervous system. Researchers have developed multiple methods for the classification of these signals for emotion detection. However, due to the non-linear nature of these signals and the inclusion of noise, while recording, accurate classification of physiological signals is a challenge for emotion charting. Valence and arousal are two important states for emotion detection; therefore, this paper presents a novel ensemble learning method based on deep learning for the classification of four different emotional states including high valence and high arousal (HVHA), low valence and low arousal (LVLA), high valence and low arousal (HVLA) and low valence high arousal (LVHA). In the proposed method, multimodal signals (EEG, ECG, and GSR) are preprocessed using bandpass filtering and independent components analysis (ICA) for noise removal in EEG signals followed by discrete wavelet transform for time domain to frequency domain conversion. Discrete wavelet transform results in spectrograms of the physiological signal and then features are extracted using stacked autoencoders from those spectrograms. A feature vector is obtained from the bottleneck layer of the autoencoder and is fed to three classifiers SVM (support vector machine), RF (random forest), and LSTM (long short-term memory) followed by majority voting as ensemble classification. The proposed system is trained and tested on the AMIGOS dataset with k-fold cross-validation. The proposed system obtained the highest accuracy of 94.5% and shows improved results of the proposed method compared with other state-of-the-art methods. © 2022 by the authors.",Long short-term memory; Humans; Emotions; emotion; Multi-modal; Physiological signals; human; physiology; Learning systems; Auto encoders; electroencephalography; Electroencephalography; procedures; Biomedical signal processing; Electrophysiology; Support vector machines; ECG signals; Electrocardiograms; Frequency domain analysis; Time domain analysis; physiological signals; arousal; EEG signals; Galvanic skin response; support vector machine; Electrocardiogram signal; Support Vector Machine; Electroencephalogram signals; Arousal; Discrete wavelet transforms; emotion charting; Emotion charting; ensemble classifier; Ensemble-classifier; ICA; Independent component analysis; Independent components analysis; Signal reconstruction; Spectrographs; stacked autoencoder; Stacked autoencoder; wavelet analysis; Wavelet Analysis,,,emotion,No,Yes
scopus,Multimodal Emotion Classification With Multi-Level Semantic Reasoning Network,"Zhu, T.; Li, L.; Yang, J.; Zhao, S.; Xiao, X.",2023,,25,,10.1109/TMM.2022.3214989,"Nowadays, people are accustomed to posting images and associated text for expressing their emotions on social networks. Accordingly, multimodal sentiment analysis has drawn increasingly more attention. Most of the existing image-text multimodal sentiment analysis methods simply predict the sentiment polarity. However, the same sentiment polarity may correspond to quite different emotions, such as happiness vs. excitement and disgust vs. sadness. Therefore, sentiment polarity is ambiguous and may not convey the accurate emotions that people want to express. Psychological research has shown that objects and words are emotional stimuli and that semantic concepts can affect the role of stimuli. Inspired by this observation, this paper presents a new MUlti-Level SEmantic Reasoning network (MULSER) for fine-grained image-text multimodal emotion classification, which not only investigates the semantic relationship among objects and words respectively, but also explores the semantic relationship between regional objects and global concepts. For image modality, we first build graphs to extract objects and global representation, and employ a graph attention module to perform bilevel semantic reasoning. Then, a joint visual graph is built to learn the regional-global semantic relations. For text modality, we build a word graph and further apply graph attention to reinforce the interdependencies among words in a sentence. Finally, a cross-modal attention fusion module is proposed to fuse semantic-enhanced visual and textual features, based on which informative multimodal representations are obtained for fine-grained emotion classification. The experimental results on public datasets demonstrate the superiority of the proposed model over the state-of-the-art methods.  © 2022 IEEE.",Semantics; Cognition; Multi-modal; Modal analysis; Social networking (online); Job analysis; Sentiment analysis; Task analysis; Classification (of information); Features extraction; Emotion classification; Multimodal emotion classification; Graph attention module; Semantic reasoning,,,emotion,No,No
scopus,Learning modality-fused representation based on transformer for emotion analysis,"Shi, P.; Hu, M.; Ren, F.; Shi, X.; Xu, L.",2022,,31,,10.1117/1.JEI.31.6.063032,"Modality-fused representation is an essential and challenging task in multimodal emotion analysis. Previous studies have already yielded remarkable achievements. However, there are two problems: insufficient feature interaction and rough data fusion. To investigate these two challenges more deeply, first, a hybrid architecture, which consists of convolution and a transformer, is proposed to extract local and global features. Second, for extracting more sufficient mutual features from multimodal datasets, our model is comprised of three parts: (1) the interior transformer encoder (TE) aims to extract the intramodality characteristics from the current monomodality; (2) the between TE aims to extract the intermodality feature between two different modalities; and (3) the enhance TE aims to extract the target modality enhance feature from multimodality. Finally, instead of directly fusing features by a linear function, we employ a popular and widely used multimodal factorized high-order pooling mechanism to obtain a more distinguishable feature representation. Extensive experiments on three multimodal sentiment datasets (CMU-MOSEI, CMU-MOSI, and IEMOCAP) demonstrate that our approach reaches the state-of-the-art in an unaligned version setting. Compared with the mainstream methods, our proposed method shows superiority in both word-aligned and unaligned settings. © 2022 SPIE and IS&T.",Transformer; Emotion Recognition; Multi-modal; Modal analysis; Learning modalities; Emotion analysis; Convolution; transformer; Feature interactions; Data fusion; Multimodal emotion analyse; convolution; Hybrid architectures; modality-fused; Modality-fused; multimodal emotion analysis; Representation; representations; Rough data,,,emotion,No,No
scopus,ViPER: Video-based Perceiver for Emotion Recognition,"Vaiani, L.; La Quatra, M.; Cagliero, L.; Garza, P.",2022,,,,10.1145/3551876.3554806,"Recognizing human emotions from videos requires a deep understanding of the underlying multimodal sources, including images, audio, and text. Since the input data sources are highly variable across different modality combinations, leveraging multiple modalities often requires ad hoc fusion networks. To predict the emotional arousal of a person reacting to a given video clip we present ViPER, a multimodal architecture leveraging a modality-agnostic transformer based model to combine video frames, audio recordings, and textual annotations. Specifically, it relies on a modality-agnostic late fusion network which makes ViPER easily adaptable to different modalities. The experiments carried out on the Hume-Reaction datasets of the MuSe-Reaction challenge confirm the effectiveness of the proposed approach.  © 2022 ACM.",Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Character recognition; Video recording; Multiple modalities; Multi-modal learning; Data-source; multimodal learning; Video processing; Video signal processing; Recognizing Human Emotion; Multimodal sources; video processing; Input datas; Agnostic learning; modality-agnostic learning; Modality-agnostic learning,,,emotion,No,No
scopus,Multimodal EEG Emotion Recognition Based on the Attention Recurrent Graph Convolutional Network,"Chen, J.; Liu, Y.; Xue, W.; Hu, K.; Lin, W.",2022,,13,,10.3390/info13110550,"EEG-based emotion recognition has become an important part of human–computer interaction. To solve the problem that single-modal features are not complete enough, in this paper, we propose a multimodal emotion recognition method based on the attention recurrent graph convolutional neural network, which is represented by Mul-AT-RGCN. The method explores the relationship between multiple-modal feature channels of EEG and peripheral physiological signals, converts one-dimensional sequence features into two-dimensional map features for modeling, and then extracts spatiotemporal and frequency–space features from the obtained multimodal features. These two types of features are input into a recurrent graph convolutional network with a convolutional block attention module for deep semantic feature extraction and sentiment classification. To reduce the differences between subjects, a domain adaptation module is also introduced to the cross-subject experimental verification. This proposed method performs feature learning in three dimensions of time, space, and frequency by excavating the complementary relationship of different modal data so that the learned deep emotion-related features are more discriminative. The proposed method was tested on the DEAP, a multimodal dataset, and the average classification accuracies of valence and arousal within subjects reached 93.19% and 91.82%, respectively, which were improved by 5.1% and 4.69%, respectively, compared with the only EEG modality and were also superior to the most-current methods. The cross-subject experiment also obtained better classification accuracies, which verifies the effectiveness of the proposed method in multimodal EEG emotion recognition. © 2022 by the authors.",Semantics; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; Human computer interaction; Classification (of information); multimodal; Convolutional neural networks; EEG; Convolution; Convolutional neural network; Recurrent neural networks; Classification accuracy; Convolutional networks; Recognition methods; Single-modal; Convolutional block attention module; convolutional block attention module; recurrent graph convolutional network; Recurrent graph convolutional network,,,emotion,No,Yes
scopus,MCPN: A Multiple Cross-Perception Network for Real-Time Emotion Recognition in Conversation,"Liu, W.; Sun, X.",2023,,1765 CCIS,,10.1007/978-981-99-2401-1_1,"Emotion recognition in conversation (ERC) is crucial for developing empathetic machines. Most of the recent related works generally model the speaker interaction and context information as a static process but ignore the temporal dynamics of the interaction and the semantics in the dialogue. At the same time, the misclassification of similar emotions is also a challenge to be solved. To solve the above problems, we propose a Multiple Cross-Perception Network, MCPN, for multimodal real-time conversation scenarios. We dynamically select speaker interaction intervals for each time step, so that the model can effectively capture the dynamics of interaction. Meanwhile, we introduce the multiple cross-perception process to perceive the context and speaker state information captured by the model alternately, so that the model can capture the semantics and interaction information specific to each time step more accurately. Furthermore, we propose an emotion triple recognition process to improve the model’s ability to recognize similar emotions. Experiments on multiple datasets demonstrate the effectiveness of the proposed method. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Natural language processing; Semantics; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal; Language processing; Natural languages; Natural Language Processing; Real-time emotion recognition; Interaction information; Real-time emotion recognition in conversation; Real-time Emotion Recognition in Conversation; Related works; Time step,,,emotion,No,No
scopus,An optimized multi-label TSK fuzzy system for emotion recognition of multimodal physiological signals∗,"Li, Y.; Fu, Z.; He, X.; Huang, J.",2023,,,,10.1109/CBS55922.2023.10115349,"Nowadays, as intelligent detection devices become reachable, how to integrate multimodal physiological signals to identify human emotions becomes a hot topic. However, the corresponding relationship between physiological signals and emotions is often difficult to be modeled mathematically, and too many signals will bring redundant features and reduce the accuracy and efficiency of emotion classification. In order to solve these problems, this paper adopts multi-label TSK fuzzy system to establish fuzzy rules for emotion recognition. Meanwhile, this paper proposes an optimization method of fusing subspace clustering and fuzzy C-means clustering to solve the problem of establishing fuzzy rules for high-dimensional features. We also consider uniform regularization to balance the trigger strength of different fuzzy rules. A dataset named DEAP is used to test the accuracy of our method. The results showed that Multi-label TSK fuzzy system with subspace fuzzy C-means and uniform regularization has a nearly 10% improvement in accuracy compared with the traditional SVM, and it's also helpful to explore the correlation of different labels such as arousal and valence, which can contribute to the further study of affective computing. © 2023 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Clustering algorithms; Biomedical signal processing; Support vector machines; Statistical tests; Human emotion; Multi-labels; Fuzzy inference; Regularisation; Detection device; Fuzzy rules; Hot topics; Intelligent detection; TSK fuzzy system,,,emotion,Yes,Yes
scopus,Multimodal Emotion Recognition From EEG Signals and Facial Expressions,"Wang, S.; Qu, J.; Zhang, Y.; Zhang, Y.",2023,,11,,10.1109/ACCESS.2023.3263670,"Emotion recognition has attracted attention in recent years. It is widely used in healthcare, teaching, human-computer interaction, and other fields. Human emotional features are often used to recognize different emotions. Currently, there is more and more research on multimodal emotion recognition based on the fusion of multiple features. This paper proposes a deep learning model for multimodal emotion recognition based on the fusion of electroencephalogram (EEG) signals and facial expressions to achieve an excellent classification effect. First, a pre-trained convolution neural network (CNN) is used to extract the facial features from the facial expressions. Next, the attention mechanism is introduced to extract more critical facial frame features. Then, we apply CNNs to extract spatial features from original EEG signals, which use a local convolution kernel and a global convolution kernel to learn the features of left and right hemispheres channels and all EEG channels. After feature-level fusion, the fusion features of the facial expression features and EEG features are fed into the classifier for emotion recognition. This paper conducted experiments on the DEAP and MAHNOB-HCI datasets to evaluate the performance of the proposed model. The accuracy of valence dimension classification is 96.63%, and arousal dimension classification is 97.15% on the DEAP dataset, while 96.69% and 96.26% on the MAHNOB-HCI dataset. The experimental results show that the proposed model can effectively recognize emotions.  © 2013 IEEE.",facial expressions; attention mechanism; Attention mechanisms; Emotion Recognition; Speech recognition; deep learning; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Multimodal emotion recognition; Human computer interaction; Classification (of information); Electroencephalography; Biomedical signal processing; EEG; Convolution; Learning models; Electroencephalogram signals; Convolution neural network; Convolution kernel; Multiple features,,,emotion,No,Yes
scopus,"MuSe 2022 Challenge: Multimodal Humour, Emotional Reactions, and Stress","Amiriparian, S.; Christ, L.; König, A.; Meßner, E.-M.; Cowen, A.; Cambria, E.; Schuller, B.W.",2022,,,,10.1145/3503161.3551792,"The 3rd Multimodal Sentiment Analysis Challenge (MuSe) focuses on multimodal affective computing. The workshop is held in conjunction with ACM Multimedia'22. Three datasets are provided as part of the challenge: (i) the Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset which contains humour-tagged audio-visual data of German football coaches, (ii) the Hume-Reaction dataset, which contains annotations on how people respond to emotional stimuli in terms of seven different emotional expression intensities, and (iii) the Ulm-Trier Social Stress Test (Ulm-TSST) dataset, which consists of audio-visual recordings labelled with continuous emotion values of individuals in stressful circumstances. Based on these datasets three affective computing challenges are defined: 1) Humor Detection Sub-Challenge (MuSe-Humor), for spontaneous humour recognition, 2) Emotional Reactions Sub-Challenge (MuSe-Reaction), for prediction of seven fine-grained in-the-wild' emotions, and 3) Emotional Stress Sub-Challenge (MuSe-Stress), for continuous prediction of stressed emotion values. In this summary, we describe the motivation behind the challenge, participation and its conditions, as well as the outcomes. The complete MuSe'22 workshop proceedings are available at: https://dl.acm.org/doi/proceedings/10.1145/3551876  © 2022 Owner/Author.",Emotion Recognition; Multi-modal; emotion recognition; Emotion recognition; Multi-modal fusion; multimodal sentiment analysis; Affective Computing; Sentiment analysis; multimodal fusion; Multimodal sentiment analyse; Statistical tests; Challenge; Humor detection; affective computing; challenge; Sports; summary paper; Summary paper; Emotional reactions; humor detection,,,emotion,Yes,No
scopus,Improving Mental Health Through Multimodal Emotion Detection from Speech and Text Data Using Long-Short Term Memory,"Bhagat, D.; Ray, A.; Sarda, A.; Dutta Roy, N.; Mahmud, M.; De, D.",2023,,519 LNNS,,10.1007/978-981-19-5191-6_2,"In today’s world of cut-throat competition, where everyone is running an invisible race, we often find ourselves alone amongst the crowd. The advancements in technology are making our lives easier, yet man being a social animal is losing touch with society. As a result, today a huge part of the population is suffering from psychological disorders. Inferiority complex, inability to fulfil dreams, loneliness, etc., are considered to be the common reasons to disturb mental stability, which may further lead to disorders like depression. In extreme cases, depression causes loss of precious lives when an individual decides to commit suicide. Assessing an individual’s mental health in an interactive way with the core help of machine learning is the primary focus of this work. To realize this objective, we have used the most suitable long-short term memory (LSTM) architecture. It is an artificial recurrent neural network (RNN) in the field of deep learning on Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and FastText datasets to get 86% accuracy when fed with model-patient conversational data. Further, we discussed the scope of enhancing cognitive control capabilities over the psychiatric disorders, which may even lead to severe level of depression and suicidal attacks. Here, the proposed system will help to determine the severity level of depression in a person and will help with the recovery process. The system comprises of a wrist-band to measure some biological parameters, a headband to analyse the mental health and a user-friendly website and mobile application which has an in-built chatbot. AI-based chatbot will talk to the patients and help them reveal their thoughts, which they are otherwise not able to communicate to their peers. A person can chat via text message, which is to be stored in the database for further analysis. The novelty of this work is in the sentiment analysis of voice chat, which therefore creates a comfortable environment for the user. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Long short-term memory; Emotion Recognition; Multi-modal; Learning systems; Emotion detection; Emotion analysis; Brain; Mental health; Chatbots; Neural-networks; Speech data; Sentiment; Neural network; AI-supported chatbot; Depression prediction,,,emotion,No,No
scopus,Efficient emotion recognition using hyperdimensional computing with combinatorial channel encoding and cellular automata,"Menon, A.; Natarajan, A.; Agashe, R.; Sun, D.; Aristio, M.; Liew, H.; Shao, Y.S.; Rabaey, J.M.",2022,,9,,10.1186/s40708-022-00162-8,"In this paper, a hardware-optimized approach to emotion recognition based on the efficient brain-inspired hyperdimensional computing (HDC) paradigm is proposed. Emotion recognition provides valuable information for human–computer interactions; however, the large number of input channels (> 200) and modalities (> 3) involved in emotion recognition are significantly expensive from a memory perspective. To address this, methods for memory reduction and optimization are proposed, including a novel approach that takes advantage of the combinatorial nature of the encoding process, and an elementary cellular automaton. HDC with early sensor fusion is implemented alongside the proposed techniques achieving two-class multi-modal classification accuracies of > 76% for valence and > 73% for arousal on the multi-modal AMIGOS and DEAP data sets, almost always better than state of the art. The required vector storage is seamlessly reduced by 98% and the frequency of vector requests by at least 1/5. The results demonstrate the potential of efficient hyperdimensional computing for low-power, multi-channeled emotion recognition tasks. © 2022, The Author(s).",emotion; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; human; Signal encoding; Human computer interaction; Classification (of information); Article; Brain; Digital storage; Encoding (symbols); Wearable sensors; Brain-inspired; Sensor fusion; implementation science; Wearable; Cellular automata; cellular automaton; computer analysis; Computing power; Hardware efficient; hyperdimensional computing; Hyperdimensional computing; memory; memory optimization; Memory optimization; Multi-modal sensor fusion; Multimodal sensor,,,emotion,No,Yes
scopus,Review of Eye Tracking Metrics Involved in Emotional and Cognitive Processes,"Skaramagkas, V.; Giannakakis, G.; Ktistakis, E.; Manousos, D.; Karatzanis, I.; Tachos, N.; Tripoliti, E.; Marias, K.; Fotiadis, D.I.; Tsiknakis, M.",2023,,16,,10.1109/RBME.2021.3066072,"Eye behaviour provides valuable information revealing one's higher cognitive functions and state of affect. Although eye tracking is gaining ground in the research community, it is not yet a popular approach for the detection of emotional and cognitive states. In this paper, we present a review of eye and pupil tracking related metrics (such as gaze, fixations, saccades, blinks, pupil size variation, etc.) utilized towards the detection of emotional and cognitive processes, focusing on visual attention, emotional arousal and cognitive workload. Besides, we investigate their involvement as well as the computational recognition methods employed for the reliable emotional and cognitive assessment. The publicly available datasets employed in relevant research efforts were collected and their specifications and other pertinent details are described. The multimodal approaches which combine eye-tracking features with other modalities (e.g. biosignals), along with artificial intelligence and machine learning techniques were also surveyed in terms of their recognition/classification accuracy. The limitations, current open research problems and prospective future research directions were discussed for the usage of eye-tracking as the primary sensor modality. This study aims to comprehensively present the most robust and significant eye/pupil metrics based on available literature towards the development of a robust emotional or cognitive computational model.  © 2008-2011 IEEE.",Humans; Artificial Intelligence; Cognition; emotion; Eye tracking; Behavioral research; human; videorecording; artificial intelligence; Learning systems; Artificial intelligence; Job analysis; Task analysis; machine learning; eye movement; Eye movements; algorithm; eyelid reflex; arousal; stress; cognition; Cognitive systems; Cognitive workloads; Review; Cognitive process; Eye-tracking; Blink; blinks; cognitive workload; Cognitive workload dataset heraklion; cognitive workload datasets; computer model; emotional arousal; Emotional arousal; Emotional arousal dataset; emotional arousal datasets; emotional stress; eye fixation; eye tracking; eye-tracking technology; Eye-Tracking Technology; Fixation; fixations; gaze; Gaze; Gaze-tracking; mental effort; microsaccade eye movement; pupil; Pupil; pupil diameter; saccades; saccadic eye movement; smooth pursuit; Smooth pursuit; smooth pursuit eye movement; visual attention; Visual Attention; visual system parameters; workload; Workload,,,emotion,No,Yes
scopus,On the Audio-Visual Emotion Recognition using Convolutional Neural Networks and Extreme Learning Machine,"Ashraf, A.; Gunawan, T.S.; Arifin, F.; Kartiwi, M.; Sophian, A.; Habaebi, M.H.",2022,,10,,10.52549/ijeei.v10i3.3879,"The advances in artificial intelligence and machine learning concerning emotion recognition have been enormous and in previously inconceivable ways. Inspired by the promising evolution in human-computer interaction, this paper is based on developing a multimodal emotion recognition system. This research encompasses two modalities as input, namely speech and video. In the proposed model, the input video samples are subjected to image pre-processing and image frames are obtained. The signal is pre-processed and transformed into the frequency domain for the audio input. The aim is to obtain Mel-spectrogram, which is processed further as images. Convolutional neural networks are used for training and feature extraction for both audio and video with different configurations. The fusion of outputs from two CNNs is done using two extreme learning machines. For classification, the proposed system incorporates a support vector machine. The model is evaluated using three databases, namely eNTERFACE, RML, and SAVEE. For the eNTERFACE dataset, the accuracy obtained without and with augmentation was 87.2% and 94.91%, respectively. The RML dataset yielded an accuracy of 98.5%, and for the SAVEE dataset, the accuracy reached 97.77%. Results achieved from this research are an illustration of the fruitful exploration and effectiveness of the proposed system. © 2022 Institute of Advanced Engineering and Science. All rights reserved.",Machine Learning; Artificial Intelligence; Emotion Recognition; Human-Computer Interaction; Convolutional Neural Networks,,,emotion,No,Yes
scopus,Multimodal Emotion Analysis Based on AttMISA,"Wang, X.; Mi, X.",2023,,,,10.1109/BDICN58493.2023.00052,"Multimodal affective analysis is a new research field. Combining multiple modal information to analyze users' videos is a challenging task. Past studies have tried to fuse information from various modes using complex methods, but the structure of different information has led to differences in data distribution patterns. To address this problem, in past studies, MISA proposed that by creating two different subspaces, within the first subspace, models learn the common features of multiple modal data and reduce their differences. In the second subspace, the model independently learns the characteristics of the data for each mode. On the basis of this research, AttMISA is proposed. On the basis of MISA, attention modules are added to different subspaces. Also summarizes the commonly used datasets of multimodal emotion analysis for social media; Finally, it discusses the difficulty of network public opinion analysis and the future research direction. © 2023 IEEE.",Emotion Recognition; Multi-modal; Modal analysis; Learn+; Emotion analysis; Multimodal; Social aspects; Research fields; AttMISA; Complex methods; Data distribution; Distribution patterns; Emotionalanalyse; EmotionalAnalysis; Subspace modeling,,,emotion,No,No
scopus,Fine-Grained Emotion Recognition Using Brain-Heart Interplay Measurements and eXplainable Convolutional Neural Networks,"Gagliardi, G.; Alfeo, A.L.; Catrambone, V.; Cimino, M.G.C.A.; De Vos, M.; Valenzal, G.",2023,,2023-April,,10.1109/NER52421.2023.10123758,"Emotion recognition from electro-physiological signals is an important research topic in multiple scientific domains. While a multimodal input may lead to additional information that increases emotion recognition performance, an optimal processing pipeline for such a vectorial input is yet undefined. Moreover, the algorithm performance often compromises between the ability to generalize over an emotional dimension and the explainability associated with its recognition accuracy. This study proposes a novel explainable artificial intelligence architecture for a 9-level valence recognition from electroencephalographic (EEG) and electrocardiographic (ECG) signals. Synchronous EEG-ECG information are combined to derive vectorial brain-heart interplay features, which are rearranged in a sparse matrix (image) and then classified through an explainable convolutional neural network. The proposed architecture is tested on the publicly available MAHNOB dataset also against the use of vectorial EEG input. Results, also expressed in terms of confusion matrices, outperform the current state of the art, especially in terms of recognition accuracy. In conclusion, we demonstrate the effectiveness of the proposed approach embedding multimodal brain-heart dynamics in an explainable fashion.  © 2023 IEEE.",Emotion Recognition; Performance; Speech recognition; Emotion recognition; Physiological signals; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Convolution; Convolutional neural network; Electrocardiography; Network architecture; Fine grained; Recognition accuracy; Research topics; Algorithm performance; Multimodal inputs; Optimal processing,,,emotion,No,Yes
scopus,Emotion Recognition in Conversations Using Brain and Physiological Signals,"Saffaryazdi, N.; Goonesekera, Y.; Saffaryazdi, N.; Hailemariam, N.D.; Temesgen, E.G.; Nanayakkara, S.; Broadbent, E.; Billinghurst, M.",2022,,,,10.1145/3490099.3511148,"Emotions are complicated psycho-physiological processes that are related to numerous external and internal changes in the body. They play an essential role in human-human interaction and can be important for human-machine interfaces. Automatically recognizing emotions in conversation could be applied in many application domains like health-care, education, social interactions, entertainment, and more. Facial expressions, speech, and body gestures are primary cues that have been widely used for recognizing emotions in conversation. However, these cues can be ineffective as they cannot reveal underlying emotions when people involuntarily or deliberately conceal their emotions. Researchers have shown that analyzing brain activity and physiological signals can lead to more reliable emotion recognition since they generally cannot be controlled. However, these body responses in emotional situations have been rarely explored in interactive tasks like conversations. This paper explores and discusses the performance and challenges of using brain activity and other physiological signals in recognizing emotions in a face-to-face conversation. We present an experimental setup for stimulating spontaneous emotions using a face-to-face conversation and creating a dataset of the brain and physiological activity. We then describe our analysis strategies for recognizing emotions using Electroencephalography (EEG), Photoplethysmography (PPG), and Galvanic Skin Response (GSR) signals in subject-dependent and subject-independent approaches. Finally, we describe new directions for future research in conversational emotion recognition and the limitations and challenges of our approach.  © 2022 ACM.",Long short-term memory; Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Human computer interaction; Electroencephalography; Biomedical signal processing; Electrophysiology; Brain; Human-computer interaction; Multimodal fusion; Multimodal Emotion Recognition; Photoplethysmography; Photoplethysmography (PPG); Galvanic skin response; Neurophysiology; Conversational emotion recognition; Recognizing emotions; Electroencephalography (EEG); Galvanic Skin Response (GSR); Human-Computer interaction (HCI),,,emotion,No,Yes
scopus,A personality-guided affective brain—computer interface for implementation of emotional intelligence in machines,"Li, S.; Li, W.; Xing, Z.; Yuan, W.; Wei, X.; Zhang, X.; Hu, B.",2022,,23,,10.1631/FITEE.2100489,"Affective brain—computer interfaces have become an increasingly important topic to achieve emotional intelligence in human—machine collaboration. However, due to the complexity of electroencephalogram (EEG) signals and the individual differences in emotional response, it is still a great challenge to design a reliable and effective model. Considering the influence of personality traits on emotional response, it would be helpful to integrate personality information and EEG signals for emotion recognition. This study proposes a personality-guided attention neural network that can use personality information to learn effective EEG representations for emotion recognition. Specifically, we first use a convolutional neural network to extract rich temporal and regional representations of EEG signals, and a special convolution kernel is designed to learn inter- and intra-regional correlations simultaneously. Second, inspired by the fact that electrodes within distinct brain scalp regions play different roles in emotion recognition, a personality-guided regional-attention mechanism is proposed to further explore the contributions of electrodes within a region and between regions. Finally, attention-based long short-term memory is designed to explore the temporal dynamics of EEG signals. Experiments on the AMIGOS dataset, which is a dataset for multimodal research for affect, personality traits, and mood on individuals and groups, show that the proposed method can significantly improve the performance of subject-independent emotion recognition and outperform state-of-the-art methods. © 2022, Zhejiang University Press.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Learn+; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Attention mechanism; Brain; Electroencephalogram; Convolution; Electroencephalogram (EEG); Electrodes; Emotional intelligence; Emotional response; Personality traits; Electroencephalogram signals; Human-machine collaboration; Tp39; TP39,,,emotion,No,Yes
scopus,Mandarin Emotion Recognition Based on Weighted Fusion Strategegy,"Chen, C.; Long, X.; Hu, T.; Zhong, S.",2022,,,,10.1109/ICITBS55627.2022.00040,"In emotion recognition, different emotion features play different roles in emotion recognition. In order to improve the correct rate of emotion recognition, we modify the equivalent emotion feature weight in the traditional emotion recognition algorithm. Firstly, the weight value is set according to the contribution of each emotional feature to emotion recognition, then the weighting matrix of each modal feature is created, and finally a multimodal emotion recognition model based on feature level fusion and decision level weighted fusion is constructed and verified in CHEAVD data set. The results show that the multimodal emotion recognition model constructed in this paper improves the recognition rate by 5.43% compared with the emotion recognition model with only feature level fusion, and improves the recognition rate by 4.52%, 7.56% and 4.915% respectively compared with other mainstream emotion recognition models.  © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; multimodal; Recognition models; SVM; Feature-level fusions; Emotion feature; Mandarin emotion recognition; Mandarin Emotion Recognition; Weighted fusion; Weighted fusion of emotional feature; Weighted fusion of emotional features,,,emotion,No,No
scopus,Survey of deep learning based multimodal emotion recognition,"Zhao, X.; Yang, Y.; Zhang, S.",2022,,16,,10.3778/j.issn.1673-9418.2112081,"Multimodal emotion recognition aims to recognize human emotional states through different modalities related to human emotion expression such as audio, vision, text, etc. This topic is of great importance in the fields of human-computer interaction, artificial intelligence, affective computing, etc., and has attracted much attention. In view of the great success of deep learning methods developed in recent years in various tasks, a variety of deep neural networks have been used to learn high-level emotional feature representations for multimodal emotion recognition. In order to systematically summarize the research advance of deep learning methods in the field of multi-modal emotion recognition, this paper aims to present comprehensive analysis and summarization on recent multi-modal emotion recognition literatures based on deep learning. First, the general framework of multimodal emotion recognition is given, and the commonly used multimodal emotional dataset is introduced. Then, the principle of representative deep learning techniques and its advance in recent years are briefly reviewed. Subsequently, this paper focuses on the advance of two key steps in multimodal emotion recognition: Emotional feature extraction methods related to audio, vision, text, etc., including hand-crafted feature extraction and deep feature extraction; multi-modal information fusion strategies integrating different modalities. Finally, the challenges and opportunities in this field are analyzed, and the future development direction is pointed out. © 2022, Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",Emotion recognition; Deep learning; Multimodal; Fusion; Deep feature; Hand-crafted feature,,,emotion,No,No
scopus,Investigating EEG-based functional connectivity patterns for multimodal emotion recognition,"Wu, X.; Zheng, W.-L.; Li, Z.; Lu, B.-L.",2022,,19,,10.1088/1741-2552/ac49a7,"Objective. Previous studies on emotion recognition from electroencephalography (EEG) mainly rely on single-channel-based feature extraction methods, which ignore the functional connectivity between brain regions. Hence, in this paper, we propose a novel emotion-relevant critical subnetwork selection algorithm and investigate three EEG functional connectivity network features: strength, clustering coefficient, and eigenvector centrality. Approach. After constructing the brain networks by the correlations between pairs of EEG signals, we calculated critical subnetworks through the average of brain network matrices with the same emotion label to eliminate the weak associations. Then, three network features were conveyed to a multimodal emotion recognition model using deep canonical correlation analysis along with eye movement features. The discrimination ability of the EEG connectivity features in emotion recognition is evaluated on three public datasets: SEED, SEED-V, and DEAP. Main results. The experimental results reveal that the strength feature outperforms the state-of-the-art features based on single-channel analysis. The classification accuracies of multimodal emotion recognition are 95.08±6.42% on the SEED dataset, 84.51±5.11% on the SEED-V dataset, and 85.34±2.90% and 86.61±3.76% for arousal and valence on the DEAP dataset, respectively, which all achieved the best performance. In addition, the brain networks constructed with 18 channels achieve comparable performance with that of the 62-channel network and enable easier setups in real scenarios. Significance. The EEG functional connectivity networks combined with emotion-relevant critical subnetworks selection algorithm we proposed is a successful exploration to excavate the information between channels.  © 2022 IOP Publishing Ltd.","Emotions; emotion; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; adult; Deep learning; female; human; male; Neural Networks, Computer; Multimodal emotion recognition; Classification (of information); multimodal emotion recognition; Article; controlled study; electroencephalography; Electroencephalography; Biomedical signal processing; correlation analysis; EEG; electroencephalogram; Electrophysiology; eye movement; Brain; Eye movements; nerve cell network; arousal; Feature extraction; artifact; brain; functional connectivity; multimodal deep learning; Multimodal deep learning; computer vision; Brain computer interface; Arousal; clinical article; Brain networks; eye tracking; affective brain-computer interface; Affective brain-computer interface; artifact reduction; brain functional connectivity network; Brain functional connectivity network; eigenvector centrality; electrooculogram; Eysenck Personality Questionnaire; Functional connectivity networks; Selection algorithm; speech discrimination; Subnetworks; visual stimulation",,,emotion,Yes,Yes
scopus,Aspect-Based Emotion Analysis and Multimodal Coreference: A Case Study of Customer Comments on Adidas Instagram Posts,"De Bruyne, L.; Karimi, A.; De Clercq, O.; Prati, A.; Hoste, V.",2022,,,,,"While aspect-based sentiment analysis of user-generated content has received a lot of attention in the past years, emotion detection at the aspect level has been relatively unexplored. Moreover, given the rise of more visual content on social media platforms, we want to meet the ever-growing share of multimodal content. In this paper, we present a multimodal dataset for Aspect-Based Emotion Analysis (ABEA). Additionally, we take the first steps in investigating the utility of multimodal coreference resolution in an ABEA framework. The presented dataset consists of 4,900 comments on 175 images and is annotated with aspect and emotion categories and the emotional dimensions of valence and arousal. Our preliminary experiments suggest that ABEA does not benefit from multimodal coreference resolution, and that aspect and emotion classification only requires textual information. However, when more specific information about the aspects is desired, image recognition could be essential. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Multi-modal; Modal analysis; Emotion detection; Sentiment analysis; Classification (of information); Emotion analysis; Case-studies; Sentiment Analysis; Emotion Detection; Image recognition; ABEA; ABSA; Aspect-based emotion analyse; Coreference; Coreference resolution; Multimodal coreference; Multimodal Coreference,,,emotion,Yes,No
scopus,Multi-Attention Module for Dynamic Facial Emotion Recognition,"Zhi, J.; Song, T.; Yu, K.; Yuan, F.; Wang, H.; Hu, G.; Yang, H.",2022,,13,,10.3390/info13050207,"Video-based dynamic facial emotion recognition (FER) is a challenging task, as one must capture and distinguish tiny facial movements representing emotional changes while ignoring the facial differences of different objects. Recent state-of-the-art studies have usually adopted more complex methods to solve this task, such as large-scale deep learning models or multimodal analysis with reference to multiple sub-models. According to the characteristics of the FER task and the shortcomings of existing methods, in this paper we propose a lightweight method and design three attention modules that can be flexibly inserted into the backbone network. The key information for the three dimensions of space, channel, and time is extracted by means of convolution layer, pooling layer, multi-layer perception (MLP), and other approaches, and attention weights are generated. By sharing parameters at the same level, the three modules do not add too many network parameters while enhancing the focus on specific areas of the face, effective feature information of static images, and key frames. The experimental results on CK+ and eNTERFACE’05 datasets show that this method can achieve higher accuracy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",lightweight; Speech recognition; Emotion recognition; Face recognition; Modal analysis; Deep learning; State of the art; Image enhancement; Facial emotion recognition; Facial emotions; facial emotion recognition; Complex methods; attention module; Attention module; Emotional change; Facial movements; Lightweight; Recent state,,,emotion,No,Yes
scopus,Fusion with Hierarchical Graphs for Multimodal Emotion Recognition,"Tang, S.; Luo, Z.; Nan, G.; Baba, J.; Yoshikawa, Y.; Ishiguro, H.",2022,,,,10.23919/APSIPAASC55919.2022.9979932,"Automatic emotion recognition (AER) based on enriched multimodal inputs, including text, speech, and visual clues, is crucial in the development of emotionally intelligent machines. Although complex modality relationships have been proven effective for AER, they are still largely underexplored because previous works predominantly relied on various fusion mechanisms with simply concatenated features to learn mul-timodal representations for emotion classification. This paper proposes a novel hierarchical fusion graph convolutional net-work (HFGCN) model that learns more informative multimodal representations by considering the modality dependencies during the feature fusion procedure. Specifically, the proposed model fuses multimodality inputs using a two-stage graph construction approach and encodes the modality dependencies into the con-versation representation. We verified the interpretable capabilities of the proposed method by projecting the emotional states to a 2D valence-arousal (VA) subspace. Extensive experiments showed the effectiveness of our proposed model for more accurate AER, which yielded state-of-the-art results on two public datasets, IEMOCAP and MELD.  © 2022 Asia-Pacific of Signal and Information Processing Association (APSIPA).",Emotion Recognition; Speech recognition; Character recognition; Multimodal emotion recognition; Learn+; Emotion classification; Fusion mechanism; Automatic emotion recognition; Hierarchical fusions; Intelligent machine; Multimodal inputs; Hierarchical graphs; Visual clues,,,emotion,No,No
scopus,M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database,"Zhao, J.; Zhang, T.; Hu, J.; Liu, Y.; Jin, Q.; Wang, X.; Li, H.",2022,,1,,,"The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M3ED is annotated with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral) at utterance level, and encompasses acoustic, visual, and textual modalities. To the best of our knowledge, M3ED is the first multimodal emotional dialogue dataset in Chinese. It is valuable for cross-culture emotion analysis and recognition. We apply several state-of-the-art methods on the M3ED dataset to verify the validity and quality of the dataset. We also propose a general Multimodal Dialogue-aware Interaction framework, MDI, to model the dialogue context for emotion recognition, which achieves comparable performance to the state-of-the-art methods on the M3ED. The full dataset and codes are available. © 2022 Association for Computational Linguistics.",Emotion Recognition; Emotional state; Multi-modal; Emotion recognition; Modal analysis; Emotion analysis; Multimodal dialogue; Multi-labels; State-of-the-art methods; Cross culture; Data resources; Dialogue database,,,emotion,Yes,Yes
scopus,Multi-head attention fusion networks for multi-modal speech emotion recognition,"Zhang, J.; Xing, L.; Tan, Z.; Wang, H.; Wang, K.",2022,,168,,10.1016/j.cie.2022.108078,"Multi-modal speech emotion recognition is a study to predict emotion categories by combining speech data with other types of data, such as video, speech text transcription, body action, or facial expression when speaking, which will involve the fusion of multiple features. Most of the early studies, however, directly spliced multi-modal features in the fusion layer after single-modal modeling, resulting in ignoring the connection between speech and other modal features. As a result, we propose a novel multi-modal speech emotion recognition model based on multi-head attention fusion networks, which employs transcribed text and motion capture (MoCap) data involving facial expression, head rotation, and hand action to supplement speech data and perform emotion recognition. In unimodal, we use a two-layer Transformer's encoder combination model to extract speech and text features separately, and MoCap is modeled using a deep residual shrinkage network. Simultaneously, We innovated by changing the input of the Transformer encoder to learn the similarities between speech and text, speech and MoCap, and then output text and MoCap features that are more similar to speech features, and finally, predict the emotion category using combined features. In the IEMOCAP dataset, our model outperformed earlier research with a recognition accuracy of 75.6%. © 2022",Speech emotion recognition; Multi-modal; Speech recognition; Face recognition; Facial Expressions; Character recognition; Signal encoding; Multimodal; Speech; Features fusions; Speech features; Feature Fusion; Multi-head attention; Speech data; Speech Emotion Recognition; Motion capture; Multiple features; Action expressions; Multi-Head Attention,,,emotion,No,Yes
scopus,Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts,"Agarwal, H.; Bansal, K.; Joshi, A.; Modi, A.",2022,,,,,"Emotion Recognition in Conversations (ERC) is an important and active research area. Recent work has shown the benefits of using multiple modalities (e.g., text, audio, and video) for the ERC task. In a conversation, participants tend to maintain a particular emotional state unless some stimuli evokes a change. There is a continuous ebb and flow of emotions in a conversation. Inspired by this observation, we propose a multimodal ERC model and augment it with an emotion-shift component that improves performance. The proposed emotion-shift component is modular and can be added to any existing multimodal ERC model (with a few modifications). We experiment with different variants of the model, and results show that the inclusion of emotion shift signal helps the model to outperform existing models for ERC on MOSEI and IEMOCAP datasets. © 2022 MMMPIE. All Rights Reserved.",Emotion Recognition; Emotional state; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multiple modalities; Audio and video; Research areas; Improve performance; Ebb-and-flow; Modulars,,,emotion,No,Yes
scopus,Temporal Relation Inference Network for Multimodal Speech Emotion Recognition,"Dong, G.-N.; Pun, C.-M.; Zhang, Z.",2022,,32,,10.1109/TCSVT.2022.3163445,"Speech emotion recognition (SER) is a non-trivial task for humans, while it remains challenging for automatic SER due to the linguistic complexity and contextual distortion. Notably, previous automatic SER systems always regarded multi-modal information and temporal relations of speech as two independent tasks, ignoring their association. We argue that the valid semantic features and temporal relations of speech are both meaningful event relationships. This paper proposes a novel temporal relation inference network (TRIN) to help tackle multi-modal SER, which fully considers the underlying hierarchy of phonetic structure and its associations between various modalities under the sequential temporal guidance. Mainly, we design a temporal reasoning calibration module to imitate real and abundant contextual conditions. Unlike the previous works, which assume all multiple modalities are related, it infers the dependency relationship between the semantic information from the temporal level and learns to handle the multi-modal interaction sequence with a flexible order. To enhance the feature representation, an innovative temporal attentive fusion unit is developed to magnify the details embedded in a single modality from semantic level. Meanwhile, it aggregates the feature representation from both the temporal and semantic levels to maximize the integrity of feature representation by an adaptive feature fusion mechanism to selectively collect the implicit complementary information to strengthen the dependencies between different information subspaces. Extensive experiments conducted on two benchmark datasets demonstrate the superiority of our TRIN method against some state-of-the-art SER methods.  © 1991-2012 IEEE.",Semantics; Cognition; Speech emotion recognition; Speech recognition; Emotion recognition; Correlation; Job analysis; Task analysis; Features extraction; Speech; Multi-modal learning; Hidden Markov models; Hidden-Markov models; multi-modal learning; Inference network; relation inference network; Relation inference network; temporal learning; Temporal learning,,,emotion,No,No
scopus,STERM: A Multimodal Speech Emotion Recognition Model in Filipino Gaming Settings,"Magno, G.A.G.; Cuchapin, L.J.V.; Estrada, J.E.",2022,,,,10.1109/HNICEM57413.2022.10109472,"Gaming is highly connected to emotion. Unfortunately, most game experience research has little or no connection to the emotion literature, which makes the emotion in games poorly understood. As technology and understanding of emotion are progressing, the researchers would like to take the opportunity to unfold discoveries that relate to recognizing the underlying emotions while playing Valorant, one of the trending online games nowadays. To recognize emotions, a model for speech emotion recognition must be developed. For emotion recognition in human speech, one can either extract emotion-related attributes from speech data or translate the speech dataset into its text equivalence prior to analyzing the data using natural language processing. Furthermore, emotion detection will benefit from the use of an audio-textual multimodal set-up, but it is not easily possible to devise a system that can learn from multimodality. It is either one can independently construct models for two input sources and aggregate them at the decision level. Inspired by this idea, the researchers in this paper proposed a speech emotion recognition model utilizing two modalities: speech and text. This study aims to discover the performance of a natural speech database consisting of in-game audio communications of Filipino gamers in multimodal emotion recognition and also, to detect the profane words uttered using audio and textual features. Employing deep learning algorithms like Convolutional Neural Networks (CNN) for speech and Natural Language Processing for recognizing emotions from the text as well as detecting the profane words that existed, results were evaluated in accordance with its statistical measures and then combined in order to evaluate the results and show the proposed approach achieves the state-of-the-art performance on the natural speech database. © 2022 IEEE.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Deep learning; Deep Learning; Character recognition; Classification (of information); Multimodal; Convolutional neural networks; Learning algorithms; Emotion classification; NLP; Database systems; Speech Emotion Recognition; MFCC; Experiments; Filipino gaming context; Filipino Gaming Context; Key word matching; Keyword Matching; Natural database; Natural Database; Profane word detection; Profane Words Detection; Text emotion classification; Text Emotion Classification,,,emotion,No,Yes
scopus,Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis,"Chu, I.-H.; Chen, Z.; Yu, X.; Han, M.; Xiao, J.; Chang, P.",2022,,,,,"Multimodal speech emotion recognition (SER) and sentiment analysis (SA) are important techniques for human-computer interaction. Most existing multimodal approaches utilize either shallow cross-modal fusion of pretrained features, or deep cross-modal fusion with raw features. Recently, attempts have been made to fuse pretrained feature representations in a deep fusion manner during fine-tuning stage. However, those approaches have not led to improved results, partially due to their relatively simple fusion mechanisms and lack of proper cross-modal pretraining. In this work, leveraging single-modal pretrained models (RoBERTa and HuBERT), we propose a novel deeply-fused audio-text bi-modal transformer with carefully designed cross-modal fusion mechanism and a stage-wise cross-modal pretraining scheme to fully facilitate the cross-modal learning. Our experiment results show that the proposed method achieves state-of-the-art results on the public IEMOCAP emotion and CMU-MOSEI sentiment datasets, exceeding the previous benchmarks by a large margin. © 2022 Association for Computational Linguistics.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Modal analysis; Human computer interaction; Sentiment analysis; Cross-modal; Multi-modal approach; Large dataset; Computational linguistics; Simple++; Pre-training; Fusion mechanism; Fine tuning; Feature representation,,,emotion,No,No
scopus,A Sentiment and Emotion aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting,"Chauhan, D.S.; Singh, G.V.; Arora, A.; Ekbal, A.; Bhattacharyya, P.",2022,,29,,,"In this paper, we hypothesize that humor is closely related to sentiment and emotions. Also, due to the tremendous growth in multilingual content, there is a great demand for building models and systems that support multilingual information access. To this end, we first extend the recently released Multimodal Multiparty Hindi Humor (M2H2) dataset by adding parallel English utterances corresponding to Hindi utterances and then annotating each utterance with sentiment and emotion classes. We name it Sentiment, Humor, and Emotion aware Multilingual Multimodal Multiparty Dataset (SHEMuD). Therefore, we propose a multitask framework wherein the primary task is humor detection, and the auxiliary tasks are sentiment and emotion identification. We design a multitasking framework wherein we first propose a Context Transformer to capture the deep contextual relationships with the input utterances. We then propose a Sentiment and Emotion aware Embedding (SE-Embedding) to get the overall representation of a particular emotion and sentiment w.r.t. the specific humor situation. Experimental results on the SHEMuD show the efficacy of our approach and shows that multitask learning offers an improvement over the single-task framework for both monolingual (4.86 points ↑ in Hindi and 5.9 points ↑ in English in F1-score) and multilingual (5.17 points ↑ in F1-score) setting. © 2022 Proceedings - International Conference on Computational Linguistics, COLING. All rights reserved.",Emotion Recognition; Embeddings; Multi-modal; F1 scores; Multitask learning; Computational linguistics; Emotion identifications; Building model; Building systems; Contextual relationships; Information access; Primary task,,,emotion,Yes,Yes
scopus,Emotion-Based Reinforcement Attention Network for Depression Detection on Social Media: Algorithm Development and Validation,"Cui, B.; Wang, J.; Lin, H.; Zhang, Y.; Yang, L.; Xu, B.",2022,,10,,10.2196/37818,"Background: Depression detection has recently received attention in the field of natural language processing. The task aims to detect users with depression based on their historical posts on social media. However, existing studies in this area use the entire historical posts of the users and select depression indicator posts. Moreover, these methods fail to effectively extract deep emotional semantic features or simply concatenate emotional representation. To solve this problem, we propose a model to extract deep emotional semantic features and select depression indicator posts based on the emotional states. Objective: This study aims to develop an emotion-based reinforcement attention network for depression detection of users on social media. Methods: The proposed model is composed of 2 components: the emotion extraction network, which is used to capture deep emotional semantic information, and the reinforcement learning (RL) attention network, which is used to select depression indicator posts based on the emotional states. Finally, we concatenated the output of these 2 parts and send them to the classification layer for depression detection. Results: Experimental results of our model on the multimodal depression data set outperform the state-of-the-art baselines. Specifically, the proposed model achieved accuracy, precision, recall, and F1-score of 90.6%, 91.2%, 89.7%, and 90.4%, respectively. Conclusions: The proposed model utilizes historical posts of users to effectively identify users' depression tendencies. The experimental results show that the emotion extraction network and the RL selection layer based on emotional states can effectively improve the accuracy of detection. In addition, sentence-level attention layer can capture core posts. ©Bin Cui, Jian Wang, Hongfei Lin, Yijia Zhang, Liang Yang, Bo Xu. Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 09.08.2022.",social media; depression detection; emotion-based reinforcement; emotional semantic features; sentence-level attention,,,emotion,No,Yes
scopus,Speech Emotion Recognition based on Long Short Term Memory network,"Al-Hussain, M.M.; Alouane, M.T.-H.",2022,,,,10.1109/ITSIS56166.2022.10118390,"Nowadays emotion recognition systems have regained importance in the field of Artificial Intelligent. In recent publications, emotion is extracted from several types of signals such as human facial expression, speech, electroencephalographs, gestures, etc. Recently, speech recognition systems are gaining interest in building monomodal systems or multimodal ones. In the current research paper, we introduce a speech emotion recognition system. The proposed system uses a Long-short Term Memory (LSTM) Network to extract emotion from speech features extracted from speech recordings after preprocessing. The RML dataset is used to experiment with the performance of the proposed system. Compared to the state of the art, the proposed system has very low complexity and significant emotion detection accuracy. In particular, an efficient choice of few speech features, a bidirectional LSTM network with a maximum of ten timestamps achieved an emotion detection accuracy of 100% for the six considered emotions. Due to its low complexity and significant performance, the proposed speech emotion recognition system can be deployed for real-time purposes.  © 2022 IEEE.",Long short-term memory; Speech emotion recognition; Emotion Recognition; Performance; Speech recognition; Emotion recognition; Emotion detection; Speech; Electroencephalography; Brain; Speech features; Accuracy; Complex networks; Memory network; Detection accuracy; Speech emotion recognition systems; Long-short term memory network; LSTM network; Speech Features,,,emotion,No,Yes
scopus,An Empirical Experiment on Feature Extractions Based for Speech Emotion Recognition,"Duong, B.V.; Ha, C.N.; Nguyen, T.T.; Nguyen, P.; Do, T.-H.",2022,,13758 LNAI,,10.1007/978-3-031-21967-2_15,"In recent years, the virtual assistant has become an essential part of many applications on smart devices. In these applications, users talk to virtual assistants in order to give commands. This makes speech emotion recognition to be a serious problem in improving the service and the quality of virtual assistants. However, speech emotion recognition is not a straightforward task as emotion can be expressed through various features. Having a deep understanding of these features is crucial to achieving a good result in speech emotion recognition. To this end, this paper conducts empirical experiments on three kinds of speech features: Mel-spectrogram, Mel-frequency cepstral coefficients, Tempogram, and their variants for the task of speech emotion recognition. Convolutional Neural Networks, Long Short-Term Memory, Multi-layer Perceptron Classifier, and Light Gradient Boosting Machine are used to build classification models used for the emotion classification task based on the three speech features. Two popular datasets: The Ryerson Audio-Visual Database of Emotional Speech and Song, and The Crowd-Sourced Emotional Multimodal Actors Dataset are used to train these models. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Speech emotion recognition; Emotion Recognition; Speech recognition; Features extraction; Convolutional neural networks; Multilayer neural networks; Speech features; Speech emotions; Virtual assistants; MFCC; Spectrograms; Spectrographs; Empirical experiments; Mel-spectrogram; MFCCs; Speech emotion; Tempogram,,,emotion,No,Yes
scopus,Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis,"Shou, Y.; Meng, T.; Ai, W.; Yang, S.; Li, K.",2022,,501,,10.1016/j.neucom.2022.06.072,"Multimodal Emotion Recognition for Conversation (ERC) is a challenging multi-class classification task that requires recognizing multiple speakers’ emotions in text, audio, video, and other modalities. ERC has received considerable attention from researchers due to its potential applications in opinion mining, advertising, and healthcare. However, the syntactic structure characteristics of the text itself have not been considered in this study. Taking into account this, this paper proposes a conversational affective analysis model (DSAGCN) combining dependent syntactic analysis and graph convolutional neural networks. Since words that reflect emotional polarity are usually concentrated exclusively in limited regions, the DSAGCN model first employs a self-attention mechanism to capture the most effective words in the dialogue context and obtain a more accurate vector representation of the emotional semantics. Then, based on speaker relationships and dependent syntactic relationships, the multimodal sentiment relationship graphs are constructed. Finally, a graph convolutional neural network is used to complete the recognition of multimodal emotion. In extensive experiments on two real datasets, IEMOCAP and MELD, the DSAGCN model outperforms the existing models in terms of average accuracy and f1 values for multimodal emotion recognition, especially for emotions such as “happiness” and “anger”. Thus, dependent syntactic analysis and self-attention mechanism can enhance the model's ability to understand emotions. © 2022 Elsevier B.V.",convolutional neural network; Semantics; emotion; Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Modal analysis; human; Character recognition; Multimodal emotion recognition; controlled study; Convolutional neural networks; attention; Convolution; Convolutional neural network; semantics; article; anger; Self attention mechanism; Syntactics; Convolution neural network; happiness; Dependency parsing; Dialog emotion recognition; Dialogue emotion recognition; Graph convolution neural network; Syntactic analysis,,,emotion,No,Yes
scopus,"A Multitask Framework for Sentiment, Emotion and Sarcasm aware Cyberbullying Detection from Multi-modal Code-Mixed Memes","Maity, K.; Jha, P.; Saha, S.; Bhattacharyya, P.",2022,,,,10.1145/3477495.3531925,"Detecting cyberbullying from memes is highly challenging, because of the presence of the implicit affective content which is also often sarcastic, and multi-modality (image + text). The current work is the first attempt, to the best of our knowledge, in investigating the role of sentiment, emotion and sarcasm in identifying cyberbullying from multi-modal memes in a code-mixed language setting. As a contribution, we have created a benchmark multi-modal meme dataset called MultiBully annotated with bully, sentiment, emotion and sarcasm labels collected from open-source Twitter and Reddit platforms. Moreover, the severity of the cyberbullying posts is also investigated by adding a harmfulness score to each of the memes. The created dataset consists of two modalities, text and image. Most of the texts in our dataset are in code-mixed form, which captures the seamless transitions between languages for multilingual users. Two different multimodal multitask frameworks (BERT+ResNET-Feedback and CLIP-CentralNet) have been proposed for cyberbullying detection (CD), the three auxiliary tasks being sentiment analysis (SA), emotion recognition (ER) and sarcasm detection (SAR). Experimental results indicate that compared to uni-modal and single-task variants, the proposed frameworks improve the performance of the main task, i.e., CD, by 3.18% and 3.10% in terms of accuracy and F1 score, respectively. © 2022 ACM.",emotion; Emotion Recognition; Multi-modal; Emotion; 'current; Meme; memes; Image texts; Sentiment; Computer crime; Cyber bullying; cyberbullying; sentiment; Multi modality image; multitask; Multitask; sarcasm; Sarcasm,,,emotion,Yes,Yes
scopus,LGCCT: A Light Gated and Crossed Complementation Transformer for Multimodal Speech Emotion Recognition,"Liu, F.; Shen, S.-Y.; Fu, Z.-W.; Wang, H.-Y.; Zhou, A.-M.; Qi, J.-Y.",2022,,24,,10.3390/e24071010,"Semantic-rich speech emotion recognition has a high degree of popularity in a range of areas. Speech emotion recognition aims to recognize human emotional states from utterances containing both acoustic and linguistic information. Since both textual and audio patterns play essential roles in speech emotion recognition (SER) tasks, various works have proposed novel modality fusing methods to exploit text and audio signals effectively. However, most of the high performance of existing models is dependent on a great number of learnable parameters, and they can only work well on data with fixed length. Therefore, minimizing computational overhead and improving generalization to unseen data with various lengths while maintaining a certain level of recognition accuracy is an urgent application problem. In this paper, we propose LGCCT, a light gated and crossed complementation transformer for multimodal speech emotion recognition. First, our model is capable of fusing modality information efficiently. Specifically, the acoustic features are extracted by CNN-BiLSTM while the textual features are extracted by BiLSTM. The modality-fused representation is then generated by the cross-attention module. We apply the gate-control mechanism to achieve the balanced integration of the original modality representation and the modality-fused representation. Second, the degree of attention focus can be considered, as the uncertainty and the entropy of the same token should converge to the same value independent of the length. To improve the generalization of the model to various testing-sequence lengths, we adopt the length-scaled dot product to calculate the attention score, which can be interpreted from a theoretical view of entropy. The operation of the length-scaled dot product is cheap but effective. Experiments are conducted on the benchmark dataset CMU-MOSEI. Compared to the baseline models, our model achieves an 81.0% F1 score with only 0.432 M parameters, showing an improvement in the balance between performance and the number of parameters. Moreover, the ablation study signifies the effectiveness of our model and its scalability to various input-sequence lengths, wherein the relative improvement is almost 20% of the baseline without a length-scaled dot product. © 2022 by the authors.",computational affection; cross-attention; lightweight model; entropy invariance; gate control; multimodal speech emotion recognition,,,emotion,No,Yes
scopus,Hybrid Model-Based Emotion Contextual Recognition for Cognitive Assistance Services,"Ayari, N.; Abdelkawy, H.; Chibani, A.; Amirat, Y.",2022,,52,,10.1109/TCYB.2020.3013112,"Endowing ubiquitous robots with cognitive capabilities for recognizing emotions, sentiments, affects, and moods of humans in their context is an important challenge, which requires sophisticated and novel approaches of emotion recognition. Most studies explore data-driven pattern recognition techniques that are generally highly dependent on learning data and insufficiently effective for emotion contextual recognition. In this article, a hybrid model-based emotion contextual recognition approach for cognitive assistance services in ubiquitous environments is proposed. This model is based on: 1) a hybrid-level fusion exploiting a multilayer perceptron (MLP) neural-network model and the possibilistic logic and 2) an expressive emotional knowledge representation and reasoning model to recognize nondirectly observable emotions; this model exploits jointly the emotion upper ontology (EmUO) and the n-ary ontology of events HTemp supported by the NKRL language. For validation purposes of the proposed approach, experiments were carried out using a YouTube dataset, and in a real-world scenario dedicated to the cognitive assistance of visitors in a smart devices showroom. Results demonstrated that the proposed multimodal emotion recognition model outperforms all baseline models. The real-world scenario corroborates the effectiveness of the proposed approach in terms of emotion contextual recognition and management and in the creation of emotion-based assistance services.  © 2013 IEEE.","Humans; Emotions; Cognition; emotion; Multi-modal; Speech recognition; human; Neural Networks, Computer; learning; Learning; Multilayer neural networks; cognition; Affect recognition; Cognitive assistance; Intelligent assistance service; Intelligent assistance services; Intelligent assistances; Knowledge representation; multimodal emotion/affect recognition; Multimodal emotion/affect recognition; ontologies; Ontology; Ontology's; Symbolic modeling; symbolic modeling and reasoning; Symbolic reasoning; ubiquitous robotics; Ubiquitous robotics",,,emotion,No,No
scopus,Comparative Deep Network Analysis of Speech Emotion Recognition Models using Data Augmentation,"Prasanna, Y.L.; Tarakaram, Y.; Mounika, Y.; Palaniswamy, S.; Vekkot, S.",2022,,,,10.1109/CENTCON56610.2022.10051557,"Speech Emotion Recognition (SER) is tasked with detecting emotion in speech regardless of semantic information. In this paper we have implemented and compared five different models for SER namely-CNN, BiLSTM with attention, CNN + BiLSTM with attention, Time Distributed CNN with LSTM and Time Distributed CNN with BiLSTM. The dataset used in this paper is Multimodal Emotion Lines Dataset (MELD). Since the dataset is unbalanced, data augmentation is performed to make it balanced. Some of the feature extraction techniques like MFCC, ZCR, mel scale are used to identify the differences in the audio of different emotions. Time distributed CNN with BiLSTM model has performed better than the other models with an accuracy, precision, recall and F1 score of 92%, 92%, 93% and 91% respectively. Despite using an unbalanced dataset we have achieved superior results compared to the state-of-the-art models which are trained on balanced datasets. This shows the prominence of data augmentation and time distributed layers. © 2022 IEEE.",Long short-term memory; Semantics; Speech emotion recognition; Emotion Recognition; Speech recognition; LSTM; Mel frequency cepstral co-efficient; Zero crossing rate; Data augmentation; BiLSTM; Mel Frequency Cepstral Co-efficients (MFCC); Speech Emotion Recognition (SER); Time distributed; Time Distributed; Zero Crossing Rate (ZCR),,,emotion,No,Yes
scopus,Emotion recognition using heterogeneous convolutional neural networks combined with multimodal factorized bilinear pooling,"Zhang, Y.; Cheng, C.; Wang, S.; Xia, T.",2022,,77,,10.1016/j.bspc.2022.103877,"Multimodal emotion recognition is one of the challenging topics in the field of knowledge-based systems and many methods have been studied successfully. Nevertheless, multimodal emotion recognition needs effective fusion representations of multimodal domains, and available methods still have problems on this challenging task. In view of this, this paper proposes a new deep learning model for emotion recognition based on heterogeneous convolutional neural networks (HCNNs) and multimodal factorized bilinear pooling (MFB). In the proposed model, firstly, we select the channels of electroencephalogram (EEG) signals to reduce the interference caused by the redundant channels. Secondly, the HCNNs extract the convolutional features of each modality, and then the MFB method fuses the deep convolution features of the different modalities. Finally, the ensembled strategy is used to verify the model proposed in this paper and explore the influence of various bands on the experiment. The proposed method allows all elements of each component to effectively contact with each other to express the complex internal relationship of each component modality. The experimental results show that the best average result of our proposed method achieves the accuracy of 91.84% on DEAP dataset and 90.17% on MAHNOB-HCI dataset, which proves that the proposed method can improve the performance of multimodal emotion recognition and significantly outperform the state-of-the-art. © 2022 Elsevier Ltd",convolutional neural network; emotion; Emotion Recognition; Multi-modal; Speech recognition; deep learning; Emotion recognition; Deep learning; human; Multimodal emotion recognition; controlled study; Electroencephalography; Convolutional neural networks; electroencephalogram; human experiment; Convolution; Convolutional neural network; Learning models; article; Fusion strategies; Knowledge based systems; Fusion strategy; Heterogeneous convolutional neural network; Knowledge-based systems; Multimodal domains; Multimodal factorized bilinear pooling,,,emotion,No,Yes
scopus,Using Facial Micro-Expressions in Combination With EEG and Physiological Signals for Emotion Recognition,"Saffaryazdi, N.; Wasim, S.T.; Dileep, K.; Nia, A.F.; Nanayakkara, S.; Broadbent, E.; Billinghurst, M.",2022,,13,,10.3389/fpsyg.2022.864047,"Emotions are multimodal processes that play a crucial role in our everyday lives. Recognizing emotions is becoming more critical in a wide range of application domains such as healthcare, education, human-computer interaction, Virtual Reality, intelligent agents, entertainment, and more. Facial macro-expressions or intense facial expressions are the most common modalities in recognizing emotional states. However, since facial expressions can be voluntarily controlled, they may not accurately represent emotional states. Earlier studies have shown that facial micro-expressions are more reliable than facial macro-expressions for revealing emotions. They are subtle, involuntary movements responding to external stimuli that cannot be controlled. This paper proposes using facial micro-expressions combined with brain and physiological signals to more reliably detect underlying emotions. We describe our models for measuring arousal and valence levels from a combination of facial micro-expressions, Electroencephalography (EEG) signals, galvanic skin responses (GSR), and Photoplethysmography (PPG) signals. We then evaluate our model using the DEAP dataset and our own dataset based on a subject-independent approach. Lastly, we discuss our results, the limitations of our work, and how these limitations could be overcome. We also discuss future directions for using facial micro-expressions and physiological signals in emotion recognition. Copyright © 2022 Saffaryazdi, Wasim, Dileep, Nia, Nanayakkara, Broadbent and Billinghurst.",emotion recognition; decision fusion; physiological signals; neural networks; electroencephalography (EEG); facial micro-expressions; OpenBCI,,,emotion,No,No
scopus,Multimodal Analysis of Physiological Signals for Wearable-Based Emotion Recognition Using Machine Learning,"Alskafi, F.A.; Khandoker, A.H.; Lee, U.; Park, C.Y.; Jelinek, H.F.",2022,,2022-September,,10.22489/CinC.2022.328,"Recent advancements in wearable technology and machine learning have led to an increased research interest in the use of peripheral physiological signals to recognize emotion granularity. In healthcare, the ability to create an algorithm that classifies emotion content can aid in the development of treatment protocols for psychopathology and chronic disease. The non-invasive nature of peripheral physiological signals however is usually of low quality due to low sampling rates. As a result, single-mode physiological signal-based emotion recognition shows low performance. In this research, we explore the use of multi-modal wearable-based emotion recognition using the K-EmoCon dataset. Physiological signals in addition to self-reported arousal and valence records were analyzed with a battery of datamining algorithms including decision trees, support vector machines, k-nearest neighbors, and ensembles. Performance was evaluated using accuracy, true positive rate, and area under the receiver operating characteristic curve. Results support the assumption with 83% average accuracy when using an ensemble bagged tree algorithm compared to single heart rate-based emotion accuracy of 56.1%. Emotion granularity can be identified by wearables with multi-modal signal recording capabilities that improve diagnostics and possibly treatment efficacy. © 2022 Creative Commons.",Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Physiological signals; Physiology; Learning systems; Biomedical signal processing; Support vector machines; Machine-learning; Multimodal analysis; Diseases; Wearable technology; Diagnosis; Decision trees; Nearest neighbor search; Research interests; Chronic disease; Low qualities; Signal analysis; Technology learning,,,emotion,No,Yes
scopus,Enterprise Strategic Management From the Perspective of Business Ecosystem Construction Based on Multimodal Emotion Recognition,"Bi, W.; Xie, Y.; Dong, Z.; Li, H.",2022,,13,,10.3389/fpsyg.2022.857891,"Emotion recognition (ER) is an important part of building an intelligent human-computer interaction system and plays an important role in human-computer interaction. Often, people express their feelings through a variety of symbols, such as words and facial expressions. A business ecosystem is an economic community based on interacting organizations and individuals. Over time, they develop their capabilities and roles together and tend to develop themselves in the direction of one or more central enterprises. This paper aims to study a multimodal ER method based on attention mechanism. It analyzes the current emotional state of consumers and the development direction of enterprises through multi-modal ER of human emotions and analysis of market trends, so as to provide the most appropriate response or plan. This paper firstly describes the related methods of multimodal ER and deep learning in detail, and briefly outlines the meaning of enterprise strategy in the business ecosystem. Then, two datasets, CMU-MOSI and CMU-MOSEI, are selected to design the scheme for multimodal ER based on self-attention mechanism. Through the comparative analysis of the accuracy of single-modal and multi-modal ER, the self-attention mechanism is applied in the experiment. The experimental results show that the average recognition accuracy of happy under multimodal ER reaches 91.5%. Copyright © 2022 Bi, Xie, Dong and Li.",attention mechanism; deep learning; multimodal emotion recognition; business ecosystem; enterprise strategic management,,,emotion,No,Yes
scopus,Human emotion recognition based on time–frequency analysis of multivariate EEG signal,"Padhmashree, V.; Bhattacharyya, A.",2022,,238,,10.1016/j.knosys.2021.107867,"Understanding the expression of human emotional states plays a prominent role in interactive multimodal interfaces, affective computing, and the healthcare sector. Emotion recognition through electroencephalogram (EEG) signals is a simple, inexpensive, compact, and precise solution. This paper proposes a novel four-stage method for human emotion recognition using multivariate EEG signals. In the first stage, multivariate variational mode decomposition (MVMD) is employed to extract an ensemble of multivariate modulated oscillations (MMOs) from multichannel EEG signals. In the second stage, multivariate time–frequency (TF) images are generated using joint instantaneous amplitude (JIA), and joint instantaneous frequency (JIF) functions computed from the extracted MMOs. In the next stage, deep residual convolutional neural network ResNet-18 is customized to extract hidden features from the TF images. Finally, the classification is performed by the softmax layer. To further evaluate the performance of the model, various machine learning (ML) classifiers are employed. The feasibility and validity of the proposed method are verified using two different public emotion EEG datasets. The experimental results demonstrate that the proposed method outperforms the state-of-the-art emotion recognition methods with the best accuracy of 99.03, 97.59, and 97.75 percent for classifying arousal, dominance, and valence emotions, respectively. Our study reveals that TF-based multivariate EEG signal analysis using a deep residual network achieves superior performance in human emotion recognition. © 2021 Elsevier B.V.",Image processing; Speech recognition; Emotion recognition; Deep learning; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Human emotion recognition; Residual network; Interface states; Time-frequency Analysis; Brain computer interface; Electroencephalogram signals; Signal analysis; Mode decomposition; Multivariant analysis; Multivariate EEG; Multivariate electroencephalogram; Multivariate variational mode decomposition; MVMD; Time-frequency images; Time–frequency analysis,,,emotion,No,Yes
scopus,Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition,"Liu, W.; Qiu, J.-L.; Zheng, W.-L.; Lu, B.-L.",2022,,14,,10.1109/TCDS.2021.3071170,"Multimodal signals are powerful for emotion recognition since they can represent emotions comprehensively. In this article, we compare the recognition performance and robustness of two multimodal emotion recognition models: 1) deep canonical correlation analysis (DCCA) and 2) bimodal deep autoencoder (BDAE). The contributions of this article are threefold: 1) we propose two methods for extending the original DCCA model for multimodal fusion: a) weighted sum fusion and b) attention-based fusion; 2) we systemically compare the performance of DCCA, BDAE, and traditional approaches on five multimodal data sets; and 3) we investigate the robustness of DCCA, BDAE, and traditional approaches on SEED-V and DREAMER data sets under two conditions: 1) adding noises to multimodal features and 2) replacing electroencephalography features with noises. Our experimental results demonstrate that DCCA achieves state-of-the-art recognition results on all five data sets: 1) 94.6% on the SEED data set; 2) 87.5% on the SEED-IV data set; 3) 84.3% and 85.6% on the DEAP data set; 4) 85.3% on the SEED-V data set; and 5) 89.0%, 90.6%, and 90.7% on the DREAMER data set. Meanwhile, DCCA has greater robustness when adding various amounts of noises to the SEED-V and DREAMER data sets. By visualizing features before and after DCCA transformation on the SEED-V data set, we find that the transformed features are more homogeneous and discriminative across emotions.  © 2016 IEEE.",Speech recognition; Emotion recognition; Deep learning; Multi-modal fusion; Multimodal emotion recognition; Learning systems; State of the art; multimodal emotion recognition; eye movement; Multimodal features; multimodal deep learning; Traditional approaches; electroencephalography (EEG); Bimodal deep autoencoder (BDAE); Canonical correlation analysis; deep canonical correlation analysis (DCCA); Multimodal datasets; robustness,,,emotion,No,Yes
scopus,Emotion Recognition on Multimodal with Deep Learning and Ensemble,"Dharma, D.A.; Zahra, A.",2022,,13,,10.14569/IJACSA.2022.0131278,"Emotion Recognition on multimodal dataset is a difficult task, which is one of the most important tasks in topics like Human Computer Interaction (HCI). This paper presents a multimodal approach for emotion recognition on dataset MELD. The dataset contains three modalities, audio, text, and facial features. In this research, only audio and text features will be experimented on. For audio data, the raw audio is converted into MFCC as an input to a bidirectional LSTM, which will be built to perform emotion classification. On the other hand, BERT will be used to tokenize the text data as an input to the text model. To classify the emotion in text data, a Bidirectional LSTM will be built. And finally, the voting ensemble method will be implemented to combine the result from two modalities. The model will be evaluated using F1-score and confusion matrix. The unimodal audio model achieved 41.69% of F1-score, while the unimodal text model achieved 47.29% of F1-score, and the voting ensemble model achieved 47.47% of F1-score. To conclude this research, this paper also discussed future works, which involved how to build and improve deep learning models and combine them with ensemble model for better performance in emotion recognition tasks in multimodal dataset © 2022, International Journal of Advanced Computer Science and Applications.All Rights Reserved.",Long short-term memory; Natural language processing; Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Learning systems; Human computer interaction; F1 scores; Language processing; Natural languages; Multi-modal dataset; Audio features; Ensemble methods; Ensemble method,,,emotion,No,Yes
scopus,Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions,"Tahir, M.; Halim, Z.; Rahman, A.U.; Waqas, M.; Tu, S.; Chen, S.; Han, Z.",2022,,18,,10.1145/3480968,"The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human-computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.  © 2022 Association for Computing Machinery.",Machine learning; Character recognition; Affective Computing; Human computer interaction; Decision making; Affective computing; Support vector machines; Text data; Affective state; Pattern recognition; Text feature; Learning methods; Decisions makings; Affective states; Computing applications; Data driven decision; Data-driven decision-making; Pattern recognition method,,,emotion,No,Yes
scopus,A Distributed Ensemble Machine Learning Technique for Emotion Classification from Vocal Cues,"Vijayan, B.; Soman, G.; Vivek, M.V.; Judy, M.V.",2022,,13773 LNCS,,10.1007/978-3-031-24094-2_9,"Human-computer interaction and the creation of humanoid robots both depend heavily on emotions. By integrating the concept of emotion understanding, intelligent software systems become more effective and intuitive in resembling human-human interactions. Typically, we combine factors like intonation (speech), facial expression (visual modality), and word content (text). All possible multimodal combinations must be taken into consideration to process emotions appropriately. Among multimodal approaches, the use of human audio samples for emotion processing is given more weight than the use of facial expressions. To accomplish accurate categorization, analyzing massive volumes of real-time data has become more necessary. Machine Learning (ML) models that operate in a distributed fashion are crucial, given the size and complexity of the problem under study. In this respect, we propose a distributed ensemble model for vocal cue-based emotion classification. Three base ML models that work in a distributed manner were used. According to the findings, the ensemble model proposed differentiates between the seven fundamental emotions with reasonable accuracy. The proposed distributed ensemble model performed better than existing ML models on TESS, SAVEE, and RAVDESS, achieving 86% accuracy on the unified dataset. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Machine learning; Facial Expressions; Human computer interaction; Emotion classification; Machine learning models; Ensemble models; Machine learning algorithms; Anthropomorphic robots; Humanoid robot; Machine learning techniques; Distributed machine learning; Distributed machine learning algorithm; Distributed ML algorithms; Vocal cue; Vocal cues,,,emotion,No,Yes
scopus,Human Emotion Detection with Electroencephalography Signals and Accuracy Analysis Using Feature Fusion Techniques and a Multimodal Approach for Multiclass Classification,"Kimmatkar, N.V.; Babu, B.V.",2022,,12,,10.48084/etasr.5073,"Biological brain signals may be used to identify emotions in a variety of ways, with accuracy depended on the methods used for signal processing, feature extraction, feature selection, and classification. The major goal of the current work was to use an adaptive channel selection and classification strategy to improve the effectiveness of emotion detection utilizing brain signals. Using different features picked by feature fusion approaches, the accuracy of existing classification models' emotion detection is assessed. Statistical modeling is used to determine time-domain and frequency-domain properties. Multiclass classification accuracy is examined using Neural Networks (NNs), Lasso regression, k-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF). After performing hyperparameter tuning, a remarkable increase in accuracy is achieved using Lasso regression, while RF performed well for all the feature sets. 78.02% and 76.77% accuracy were achieved for a small and noisy 24 feature dataset by Lasso regression and RF respectively whereas 76.54% accuracy is achieved by Lasso regression with the backward elimination wrapper method. © 2022, Dr D. Pylarinos. All rights reserved.",feature fusion; DNN; Lasso regression,,,emotion,No,Yes
scopus,Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes,"Mishra, S.; Suryavardan, S.; Chakraborty, M.; Patwa, P.; Rani, A.; Chadha, A.; Reganti, A.; Das, A.; Sheth, A.; Chinnakotla, M.; Ekbal, A.; Kumar, S.",2022,,3555,,,"Analyzing memes on the internet has emerged as a crucial endeavor due to the impact this multi-modal form of content wields in shaping online discourse. Memes have become a powerful tool for expressing emotions and sentiments, possibly even spreading hate and misinformation, through humor and sarcasm. In this paper, we present the overview of the Memotion 3 shared task, as part of the DeFactify 2 workshop at AAAI-23. The task released an annotated dataset of Hindi-English code-mixed memes based on their Sentiment (Task A), Emotion (Task B), and Emotion intensity (Task C). Each of these is defined as an individual task and the participants are ranked separately for each task. Over 50 teams registered for the shared task and 5 made final submissions to the test set of the Memotion 3 dataset. CLIP, BERT modifications, ViT etc. were the most popular models among the participants along with approaches such as Student-Teacher model, Fusion, and Ensembling. The best final F1 score for Task A is 34.41, Task B is 79.77 and Task C is 59.82. © 2023 Copyright for this paper by its authors.",Emotion Recognition; Multi-modal; Sentiment analysis; multimodal; Emotion analysis; Statistical tests; Meme; Memes; Annotated datasets; Emotion intensity; C (programming language); codemixed; Codemixed; Hindi-english; Hindi-English; Student teachers; Test sets,,,emotion,Yes,No
scopus,Mobile Emotion Recognition via Multiple Physiological Signals using Convolution-augmented Transformer,"Yang, K.; Tag, B.; Gu, Y.; Wang, C.; Dingler, T.; Wadley, G.; Goncalves, J.",2022,,,,10.1145/3512527.3531385,"Recognising and monitoring emotional states play a crucial role in mental health and well-being management. Importantly, with the widespread adoption of smart mobile and wearable devices, it has become easier to collect long-term and granular emotion-related physiological data passively, continuously, and remotely. This creates new opportunities to help individuals manage their emotions and well-being in a less intrusive manner using off-the-shelf low-cost devices. Pervasive emotion recognition based on physiological signals is, however, still challenging due to the difficulty to efficiently extract high-order correlations between physiological signals and users' emotional states. In this paper, we propose a novel end-to-end emotion recognition system based on a convolution-augmented transformer architecture. Specifically, it can recognise users' emotions on the dimensions of arousal and valence by learning both the global and local fine-grained associations and dependencies within and across multimodal physiological data (including blood volume pulse, electrodermal activity, heart rate, and skin temperature). We extensively evaluated the performance of our model using the K-EmoCon dataset, which is acquired in naturalistic conversations using off-the-shelf devices and contains spontaneous emotion data. Our results demonstrate that our approach outperforms the baselines and achieves state-of-the-art or competitive performance. We also demonstrate the effectiveness and generalizability of our system on another affective dataset which used affect inducement and commercial physiological sensors. © 2022 ACM.",Emotion Recognition; Emotional state; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Physiology; Biomedical signal processing; Convolution; physiological signals; Mental health; Well being; Physiological data; Wearable devices; convolution-augmented transformer; Convolution-augmented transformer; Low-cost devices; Off-the-shelf mobile device; off-the-shelf mobile devices,,,emotion,No,Yes
scopus,End-to-End Modeling and Transfer Learning for Audiovisual Emotion Recognition in-the-Wild,"Dresvyanskiy, D.; Ryumina, E.; Kaya, H.; Markitantov, M.; Karpov, A.; Minker, W.",2022,,6,,10.3390/mti6020011,"As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely “in-the-wild” data. This work investigates audiovisual deep learning approaches to emotion recognition in in-the-wild problem. Inspired by the outstanding performance of end-to-end and transfer learning techniques, we explored the effectiveness of architectures in which a modality-specific Convolutional Neural Network (CNN) is followed by a Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) using the AffWild2 dataset under the Affective Behavior Analysis in-the-Wild (ABAW) challenge protocol. We deployed unimodal end-to-end and transfer learning approaches within a multimodal fusion system, which generated final predictions using a weighted score fusion scheme. Exploiting the proposed deep-learning-based multimodal system, we reached a test set challenge performance measure of 48.1% on the ABAW 2020 Facial Expressions challenge, which advances the first-runner-up performance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Emotion recognition; Affective computing; Multimodal fusion; Deep learning architectures; Face processing; Multimodal representations,,,emotion,No,Yes
scopus,Multimodal Architecture for Emotion Prediction in Videos Using Ensemble Learning,"Venkatraman, S.; Saha, P.",2022,,,,,"Systems for the collection of user-generated videos and automatic analysis of these are expanding rapidly day by day in smart industries. In this chapter, we are going to predict emotions carried by the videos, for example, joy, sadness, etc. We begin by presenting a carefully designed dataset with manual annotations that was gathered from a major video-sharing website and can be used as a foundation for new study. This dataset provides a wide number of features, ranging from audio characteristics to images to high-level semantic properties. Convolution neural networks (CNNs) are used for emotion recognition in an image and Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP) are being used to extract audio data. Both the emotion outputs from frames of video and audio are combined together by using ensemble learning. The primary focus is on the investigation of multimodal architecture for emotion transaction analysis in smart industries, which can predict emotions in a video. © 2023 selection and editorial matter, P Suresh, T Poongodi, B Balamurugan, and Meenakshi Sharma; individual chapters, the contributors.",,,,emotion,No,No
scopus,Hierarchical Weighted Framework for Emotional Distress Detection using Personalized Affective Cues,"Jadhay, N.N.; Sugandhi, R.",2022,,10,,10.52547/jist.16499.10.38.89,"Emotional distress detection has become a hot topic of research in recent years due to concerns related to mental health and complex nature distress identification. One of the challenging tasks is to use non-invasive technology to understand and detect emotional distress in humans. Personalized affective cues provide a non-invasive approach considering visual, vocal, and verbal cues to recognize the affective state. In this paper, we are proposing a multimodal hierarchical weighted framework to recognize emotional distress. We are utilizing negative emotions to detect the unapparent behavior of the person. To capture facial cues, we have employed hybrid models consisting of a transfer learned residual network and CNN models. Extracted facial cue features are processed and fused at decision using a weighted approach. For audio cues, we employed two different models exploiting the LSTM and CNN capabilities fusing the results at the decision level. For textual cues, we used a BERT transformer to learn extracted features. We have proposed a novel decision level adaptive hierarchical weighted algorithm to fuse the results of the different modalities. The proposed algorithm has been used to detect the emotional distress of a person. Hence, we have proposed a novel algorithm for the detection of emotional distress based on visual, verbal, and vocal cues. Experiments on multiple datasets like FER2013, JAFFE, CK+, RAVDESS, TESS, ISEAR, Emotion Stimulus dataset, and Daily-Dialog dataset demonstrates the effectiveness and usability of the proposed architecture. Experiments on the enterface'05 dataset for distress detection has demonstrated significant results. © 2022. All Rights Reserved.",Transformers; Convolution Neural Network; Distress Detection; Hierarchical Fusion; Long Short-Term Memory,,,emotion,No,No
scopus,Analytical Review and Study on Emotion Recognition Strategies Using Multimodal Signals,"Vala, J.M.; Jaliya, U.K.",2022,,1759 CCIS,,10.1007/978-3-031-23092-9_21,"Emotion is very important in the field of decision-making, human recognition, and the social intercourse. Multimodal emotion recognition is the promising research area of computing as well as sentiment analysis. Here, the information is carried out by the signals with various natures for making the emotion recognition systems accurately. Nowadays, the several robust emotion recognitions were developed for handling various languages and cultures. Hence, this has been complex because of potential applicability of the emotion recognizers over wide range of various scenarios. This work present survey of 50 papers based on emotion recognition strategies. In addition, thorough investigation is done based on the year of publication, adapted methodology, implementation tool, employed datasets, evaluation metrics, and values of evaluation metrics. On the other hand, the analysis of the methods with respect to the merits and demerits of the methods are presented. Finally, the issues of existing methods considering conventional emotion recognition strategies are elaborated to obtain improved contribution in devising significant emotion recognition strategy. Moreover, the probable future research directions in attaining efficient emotion recognition are elaborated. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Deep learning; Decision making; Classification accuracy; Analytical reviews; Analytical studies; Evaluation metrics; Fusion-based technique; Fusion-based techniques; Multimodal signal; Multimodal signals; Recognition strategies,,,emotion,No,No
scopus,Audio-Visual Fusion Network Based on Conformer for Multimodal Emotion Recognition,"Guo, P.; Chen, Z.; Li, Y.; Liu, H.",2022,,13605 LNAI,,10.1007/978-3-031-20500-2_26,"Audio-visual emotion recognition aims to integrate audio and visual information for accurate emotion prediction, which is widely used in real application scenarios. However, most existing methods lack fully exploiting complementary information within modalities to obtain rich feature representations related to emotions. Recently, Transformer and CNN-based models achieve remarkable results in the field of automatic speech recognition. Motivated by this, we propose a novel audio-visual fusion network based on 3D-CNN and Convolution-augmented Transformer (Conformer) for multimodal emotion recognition. Firstly, the 3D-CNN is employed to process face sequences extracted from the video, and the 1D-CNN is used to process MFCC features of audio signals. Secondly, the visual and audio features are fed into a feature fusion module, which contains a set of convolutional layers for extracting local features and the self-attention mechanism for capturing global interactions of multimodal information. Finally, the fused features are input into linear layers to obtain the prediction results. To verify the effectiveness of the proposed method, experiments are performed on RAVDESS and a newly collected dataset named PKU-ER. The experimental results show that the proposed model achieves state-of-the-art performance in audio-only, video-only, and audio-visual fusion experiments. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Transformer; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Network-based; Convolutional neural networks; Convolution; Convolutional neural network; Audio-visual fusion; Audio-visual; Emotion predictions; Convolutional Neural Network; Audio and visual information; Real applications,,,emotion,No,Yes
scopus,Real-Time Video Emotion Recognition Based on Reinforcement Learning and Domain Knowledge,"Zhang, K.; Li, Y.; Wang, J.; Cambria, E.; Li, X.",2022,,32,,10.1109/TCSVT.2021.3072412,"Multimodal emotion recognition in conversational videos (ERC) develops rapidly in recent years. To fully extract the relative context from video clips, most studies build their models on the entire dialogues which make them lack of real-time ERC ability. Different from related researches, a novel multimodal emotion recognition model for conversational videos based on reinforcement learning and domain knowledge (ERLDK) is proposed in this paper. In ERLDK, the reinforcement learning algorithm is introduced to conduct real-time ERC with the occurrence of conversations. The collection of history utterances is composed as an emotion-pair which represents the multimodal context of the following utterance to be recognized. Dueling deep-Q-network (DDQN) based on gated recurrent unit (GRU) layers is designed to learn the correct action from the alternative emotion categories. Domain knowledge is extracted from public dataset based on the former information of emotion-pairs. The extracted domain knowledge is used to revise the results from the RL module and is transformed into other dataset to examine the rationality. The experimental results on datasets show that ERLDK achieves the state-of-the-art results on weighted average and most of the specific emotion categories.  © 1991-2012 IEEE.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Data mining; State of the art; Learning algorithms; Reinforcement learning; Recurrent neural networks; reinforcement learning; domain knowledge; Domain knowledge; Public dataset; Real time videos; real-time video conversation; Video clips; Weighted averages,,,emotion,No,No
scopus,Multimodal Emotion Recognition Using Heterogeneous Ensemble Techniques,"Esfar-E-Alam, A.M.; Hossain, M.; Gomes, M.; Islam, R.; Raihana, R.",2022,,,,10.1109/ICCIT57492.2022.10054720,"Emotion recognition and sentiment analysis serve several purposes, from analyzing human behavior under specific conditions to the enhancement of customer experience for various services. In this paper, a multimodal approach is used to identify 4 classes of emotions by combining both speech and text features to improve classification accuracy. The methodology involves the implementation of six models for both audio and text domains combined using four different heterogeneous ensemble techniques - hard voting, soft voting, blending and stacking. The effects of each ensemble method on the accuracy for the multimodal classification task are also investigated. The results of this study show that the usage of ensemble learning to combine modalities greatly improves classification, with stacking being the best-performing ensemble technique for the selected collection of models. The proposed model outperforms several existing methods for 4-class emotion detection on the IEMOCAP dataset, obtaining a weighted accuracy of 81.2%.  © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Behavioral research; Emotion recognition; Multimodal emotion recognition; Learning systems; Sentiment analysis; multimodal; Stackings; Ensemble learning; IEMOCAP; Human behaviors; Blending; ensemble learning; Ensemble techniques; Heterogeneous ensembles; stacking,,,emotion,No,Yes
scopus,Multimodal Data Collection System for Driver Emotion Recognition Based on Self-Reporting in Real-World Driving,"Oh, G.; Jeong, E.; Kim, R.C.; Yang, J.H.; Hwang, S.; Lee, S.; Lim, S.",2022,,22,,10.3390/s22124402,"As vehicles provide various services to drivers, research on driver emotion recognition has been expanding. However, current driver emotion datasets are limited by inconsistencies in collected data and inferred emotional state annotations by others. To overcome this limitation, we propose a data collection system that collects multimodal datasets during real-world driving. The proposed system includes a self-reportable HMI application into which a driver directly inputs their current emotion state. Data collection was completed without any accidents for over 122 h of real-world driving using the system, which also considers the minimization of behavioral and cognitive disturbances. To demonstrate the validity of our collected dataset, we also provide case studies for statistical analysis, driver face detection, and personalized driver emotion recognition. The proposed data collection system enables the construction of reliable large-scale datasets on real-world driving and facilitates research on driver emotion recognition. The proposed system is avaliable on GitHub. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Emotions; emotion; Emotional state; Multi-modal; Speech recognition; Driver emotion recognition; Emotion recognition; Face recognition; Multi-modal data; multimodal; information processing; Large dataset; Data acquisition; driver emotion recognition; Accidents, Traffic; Automobile Driving; car driving; psychology; traffic accident; Current drivers; Data Collection; Data collection system; prevention and control; real-world driving; Real-world drivings; self-report; Self-report; Self-reporting",,,emotion,No,No
scopus,A multiple feature fusion framework for video emotion recognition in the wild,"Samadiani, N.; Huang, G.; Luo, W.; Chi, C.-H.; Shu, Y.; Wang, R.; Kocaturk, T.",2022,,34,,10.1002/cpe.5764,"Human emotions can be recognized from facial expressions captured in videos. It is a growing research area in which many have attempted to improve video emotion detection in both lab-controlled and unconstrained environments. While existing methods show a decent recognition accuracy on lab-controlled datasets, they deliver much lower accuracy in a real-world uncontrolled environment, where a variety of challenges need to be addressed such as variations in illumination, head pose, and individual appearance. Moreover, automatically identifying the key frames consisting of the expression from real-world videos is another challenge. In this article, to overcome these challenges, we provide a video emotion recognition via multiple feature fusion method. First, a uniform local binary pattern (LBP) and the scale-invariant feature transform features are extracted from each frame in the video sequences. By applying a random forest classifier, all of the static frames are then labelled by the related emotion class. In this way, the key frames can be automatically identified, including neutral and other expressions. Furthermore, from the key frames, a new geometric feature vector and the LBP from three orthogonal planes are extracted. To further improve robustness, audio features are extracted from the video sequences as an additional dimension to augmenting visual facial expression analysis. The audio and visual features are fused through a kernel multimodal sparse representation. Finally, the corresponding emotion labels to the video sequences can be assigned when a multimodal quality measure specifies the quality of each modality and its role in the decision. The results on both acted facial expressions in the Wild and MMI datasets demonstrate that the proposed method outperforms several counterpart video emotion recognition methods. © 2020 John Wiley & Sons, Ltd.",Speech recognition; emotion recognition; Emotion recognition; Video recording; data fusion; Data fusion; Decision trees; Sparse representation; Random forests; random forest; datasets in the wild; Facial expression analysis; LBP; Scale invariant feature transforms; SIFT; sparse representation; Unconstrained environments; Uniform local binary patterns,,,emotion,No,Yes
scopus,Recognition of Human Inner Emotion Based on Two-Stage FCA-ReliefF Feature Optimization,"Pan, L.; Wang, S.; Yin, Z.; Song, A.",2022,,51,,10.5755/j01.itc.51.1.29430,"Currently, there is a growing interesting in emotion recognition. Representation of emotional states is a very challenging issue. Considering the calculation cost and generalization capability for practical appli¬cation, a series of features which contain common time and frequency domain are extracted from physio¬logical signals to represent different emotional states. To reduce feature dimensionality and improve the emotion recognition accuracy, a two-stage feature optimization method based on feature correlation anal¬ysis (FCA) and ReliefF algorithm is proposed to select critical features. Firstly, FCA is employed to analyze the redundancy between features, then ReliefF is adopted to analyze the correlation between features and categories, and the optimal feature subset is obtained using the two-stage FCA-ReliefF feature optimiza¬tion method. Support vector machine is employed as the classifier to evaluate classification performance in this investigation. The effectiveness of the method which is proposed is validated by testing on two public¬ly available multimodal emotion datasets, Augsburg Biosignal Toolbox (AuBT) and Database for Emotion Analysis Using Physiological Signals (DEAP). Compared with recent similar reported studies, the method developed in this research for emotion recognition is stable and competitive, and its accuracy reaches to 98.40% (AuBT) and 92.34% (DEAP). © 2022, Kauno Technologijos Universitetas. All rights reserved.",Emotion recognition; Physiological signals; Feature fusion; FCA-ReliefF; Feature optimization,,,emotion,No,Yes
scopus,A Multitask Multimodal Ensemble Model for Sentiment- and Emotion-Aided Tweet Act Classification,"Saha, T.; Upadhyaya, A.; Saha, S.; Bhattacharyya, P.",2022,,9,,10.1109/TCSS.2021.3088714,"Speech act classification determining the communicative intent of an utterance has been studied widely over the years as an independent task. This holds true for discussion in any for a, including social media platforms such as Twitter. However, the tweeter's emotional state has a huge impact on its pragmatic content because communication is fundamentally characterized and mediated by direct emotions. Sentiment as a human behavior often has a strong relation to emotion, and one helps to understand the other better. We hypothesize that the association between emotion and sentiment will provide a clearer understanding of the tweeter's state of mind, aiding the identification of tweet acts (speech acts in Twitter, TAs). As the first step, we create a new multimodal, emotion-TA, EmoTA dataset collected from the open-source Twitter dataset. To incorporate these multiple aspects, we propose a multitask ensemble adversarial learning framework for multimodal TA classification (TAC). In addition, we also incorporate a joint embedding network, with bidirectional constraints to capture and efficiently integrate the shared semantic relationships across modalities and learn generalized features across multiple tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, TAC, by benefiting from the two secondary tasks, i.e., sentiment and emotion analyses compared to its unimodal and single-task TAC variants. © 2014 IEEE.",Semantics; Emotion; Behavioral research; Social networking (online); Emotion analysis; Semantic relationships; Social media platforms; Adversarial learning; Twitter; sentiment; multitask; Communicative intent; Embedding network; Ensemble modeling; Independent tasks; speech acts,,,emotion,No,Yes
scopus,Emotional Analysis Through Deep Learning Models,"Lakshmanan, M.; Mala, G.S.A.",2022,,,,10.1109/ICSTCEE56972.2022.10100188,"Physical fatigue is a major contributor to emotional suffering. Recently, scholars have been interested in emotion analysis. The first stage involves researching and pre-processing the content of emotional fatigue. The findings of a survey of novel theories, methods, and tools for speech emotion analysis are presented in this work. It introduces the idea of emotional exhaustion and categories it. Then, utilising multimodal data, the system is configured to identify emotional tiredness. The approach for recognising emotions in different voices is suggested in this study and is based on deep learning. Using many channels of a recurrent neural network, the system extracts audio data components for emotional recognition. Finally, the multi-modal data components are merged for emotion prediction. The speech analysis techniques, datasets, and resources are compiled. These results could shed light on applications of emotional contact as network nationalism, social networking, and human-computer interaction. © 2022 IEEE.",Long short-term memory; Emotion Recognition; Modal analysis; Multi-modal data; Human computer interaction; Emotion analysis; LSTM; Learning models; Speech emotions; Emotional analysis; RNN; Pre-processing; Data components; Emotion Analysis; Physical fatigues,,,emotion,No,No
scopus,Hybrid Approach for Human Emotion Recognition from Speech,"Singh, S.P.; Kumar, S.; Verma, S.; Kaur, I.",2022,,,,10.1109/ICAC3N56670.2022.10074492,"Even though emotions don't have much to do with the content of the speech, it has a major impact on human communication by providing much more positive feedback. Therefore, Speech emotion recognition (SER) and multimodal emotion recognition systems have been a hot area of research owing to their range of applications in several domains, such as social robots, virtual reality, and human-machine interaction applications. This paper compares two models by choosing multi-dimension CNN models and features for SER on the RAVDESS dataset. © 2022 IEEE.",Artificial Intelligence; Speech emotion recognition; Emotion Recognition; Speech recognition; CNN; Multimodal emotion recognition; Human robot interaction; Human emotion recognition; Recognition systems; Human communications; Virtual reality; Speech communication; Emotional recognition; Hybrid approach; Emotion recognition from speech; Emotional recognition from speech; Emotional Recognition from Speech; MFCC feature; MFCC Features,,,emotion,No,No
scopus,Multimodal emotion recognition based on manifold learning and convolution neural network,"Zhang, Y.; Cheng, C.; Zhang, Y.D.",2022,,81,,10.1007/s11042-022-13149-8,"Multimodal emotion recognition task based on physiological signals is becoming a research hotspot. Traditional methods need to design and extract a series of features from single-channel or multi-channel physiological signals on the basis of extensive domain knowledge. These methods cannot make full use of the relevant information among channels, and the emotion recognition of a single modality cannot fully express the emotional state. This paper proposes a multimodal emotion recognition model based on manifold learning and a convolutional neural network (CNN). The electroencephalograph (EEG) signals are combined with peripheral physiological signals and eye movement signals respectively, the multivariate synchrosqueezing transform (MSST) is used to simulate the joint oscillation structure of multi-channel signals, and then the related feature parameters are extracted and fused into feature vectors. The proposed method finds the corresponding low dimensional embedding features for given high-dimensional features by an improved manifold learning method, which feeds into the deep convolutional neural network (DCNN) model for emotion recognition. We perform extensive four-category experiments on the dimensions of arousal and valence. Results indicate that our proposed model achieves average accuracies of 90.05% and 88.17% on the DEAP and MAHNOB-HCI datasets respectively, which both receive better performances than most of the compared studies, verifying the effectiveness of the model. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Multimodal; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Physiological models; Eye movements; Convolution; Hotspots; Multi channel; Domain Knowledge; Convolution neural network; Task-based; Learning neural networks; Manifold learning,,,emotion,No,Yes
scopus,MMM: An Emotion and Novelty-aware Approach for Multilingual Multimodal Misinformation Detection,"Gupta, V.; Kumari, R.; Ashok, N.; Ghosal, T.; Ekbal, A.",2022,,,,,"The growth of multilingual web content in lowresource languages is becoming an emergingchallenge to detect misinformation. One particular hindrance to research on this problemis the non-availability of resources and tools.Majority of the earlier works in misinformation detection are based on English contentwhich confines the applicability of the researchto a specific language only. Increasing presence of multimedia content on the web haspromoted misinformation in which real multimedia content (images, videos) are used indifferent but related contexts with manipulatedtexts to mislead the readers. Detecting thiscategory of misleading information is almostimpossible without any prior knowledge. Studies say that emotion-invoking and highly novelcontent accelerates the dissemination of falseinformation. To counter this problem, here inthis paper, we first introduce a novel multilingual multimodal misinformation dataset thatincludes background knowledge (from authentic sources) of the misleading articles. Second,we propose an effective neural model leveraging novelty detection and emotion recognitionto detect fabricated information. We performextensive experiments to justify that our proposed model outperforms the state-of-the-art(SOTA) on the concerned task1. © AACL-IJCNLP 2022.All rights reserved",Multi-modal; State of the art; Multimedia contents; Novelty detection; Background knowledge; Misleading informations; Multilingual web content; Neural modelling; Prior-knowledge; Specific languages,,,emotion,No,No
scopus,Semantic-based visual emotion recognition in videos-a transfer learning approach,"Sekar, V.; Jawaharlalnehru, A.",2022,,12,,10.11591/ijece.v12i4.pp3674-3683,"Automatic emotion recognition is active research in analyzing human’s emotional state over the past decades. It is still a challenging task in computer vision and artificial intelligence due to its high intra-class variation. The main advantage of emotion recognition is that a person’s emotion can be recognized even if he is extreme away from the surveillance monitoring since the camera is far away from the human; it is challenging to identify the emotion with facial expression alone. This scenario works better by adding visual body clues (facial actions, hand posture, body gestures). The body posture can powerfully convey the emotional state of a person in this scenario. This paper analyses the frontal view of human body movements, visual expressions, and body gestures to identify the various emotions. Initially, we extract the motion information of the body gesture using dense optical flow models. Later the high-level motion feature frames are transferred to the pre-trained convolutional neural network (CNN) models to recognize the 17 various emotions in Geneva multimodal emotion portrayals (GEMEP) dataset. In the experimental results, AlexNet exhibits the architecture's effectiveness with an overall accuracy rate of 96.63% for the GEMEP dataset is better than raw frames and 94% for visual geometry group-19 VGG-19, and 93.35% for VGG-16 respectively. This shows that the dense optical flow method performs well using transfer learning for recognizing emotions. © 2022 Institute of Advanced Engineering and Science. All rights reserved.",Convolutional neural network; Transfer learning; AlexNet; Dense optical flow; Human motion analysis; VGG-16; VGG-19,,,emotion,No,Yes
scopus,Music emotion recognition based on segment-level two-stage learning,"He, N.; Ferguson, S.",2022,,11,,10.1007/s13735-022-00230-z,"In most Music Emotion Recognition (MER) tasks, researchers tend to use supervised learning models based on music features and corresponding annotation. However, few researchers have considered applying unsupervised learning approaches to labeled data except for feature representation. In this paper, we propose a segment-based two-stage model combining unsupervised learning and supervised learning. In the first stage, we split each music excerpt into contiguous segments and then utilize an autoencoder to generate segment-level feature representation. In the second stage, we feed these time-series music segments to a bidirectional long short-term memory deep learning model to achieve the final music emotion classification. Compared with the whole music excerpts, segments as model inputs could be the proper granularity for model training and augment the scale of training samples to reduce the risk of overfitting during deep learning. Apart from that, we also apply frequency and time masking to segment-level inputs in the unsupervised learning part to enhance training performance. We evaluate our model on two datasets. The results show that our model outperforms state-of-the-art models, some of which even use multimodal architectures. And the performance comparison also evidences the effectiveness of audio segmentation and the autoencoder with masking in an unsupervised way. © 2022, The Author(s).",Autoencoder; Music emotion recognition; Unsupervised learning; Segment-level representation,,,emotion,Yes,Yes
scopus,Multimodal Emotion Recognition Using Deep Learning Techniques,"Jerald James, S.; Jacob, L.",2022,,,,10.1109/ICAC3N56670.2022.10074512,"Humans have the ability to perceive and depict a wide range of emotions. There are various models that can recognize seven primary emotions from facial expressions (joyful, gloomy, annoyed, dreadful, wonder, antipathy, and impartial). This can be accomplished by observing various activities such as facial muscle movements, speech, hand gestures, and so forth. Automatic emotion recognition is a significant issue that has been a hotly debated research topic in recent years. At the moment, several research people have taken a component in inheriting or extra multimodal for higher understanding. This paper indicates a method for emotion recognition that makes use of 3 modalities: facial images, audio indicators, and text detection from FER and CK+, RAVDESS, and Twitter tweets datasets, respectively. The CNN model achieved 66.67 percent on the FER-2013 dataset of labeled headshots while on the CK+ dataset, 98.4 percent accuracy was obtained. Finally, diverse fusion strategies had been approached, and each of those fusion techniques gave distinctive results. This project is a step towards the sense of interaction between human emotional aspects and the growing technology that is the future of development in today's world. © 2022 IEEE.",Emotions; Emotion Recognition; Speech recognition; Emotion; Behavioral research; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Deep Learning; Character recognition; Multimodal emotion recognition; Learning techniques; Recognition; Hand gesture; Facial muscles; Muscle movement,,,emotion,Yes,Yes
scopus,Novel dual-channel long short-term memory compressed capsule networks for emotion recognition,"Shahin, I.; Hindawi, N.; Nassif, A.B.; Alhudhaif, A.; Polat, K.",2022,,188,,10.1016/j.eswa.2021.116080,"Recent analysis on speech emotion recognition (SER) has made considerable advances with the use of MFCC's spectrogram features and the implementation of neural network approaches such as convolutional neural networks (CNNs). The fundamental issue of CNNs is that the spatial information is not recorded in spectrograms. Capsule networks (CapsNet) have gained gratitude as alternatives to CNNs with their larger capacities for hierarchical representation. However, the concealed issue of CapsNet is the compression method that is employed in CNNs cannot be directly utilized in CapsNet. To address these issues, this research introduces a text-independent and speaker-independent SER novel architecture, where a dual-channel long short-term memory compressed-CapsNet (DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features of CapsNet. Our proposed novel classifier can ensure the energy efficiency of the model and adequate compression method in speech emotion recognition, which is not delivered through the original structure of a CapsNet. Moreover, the grid search (GS) approach is used to attain optimal solutions. Results witnessed an improved performance and reduction in the training and testing running time. The speech datasets used to evaluate our algorithm are: Arabic Emirati-accented corpus, English “speech under simulated and actual stress (SUSAS)” corpus, English Ryerson audio-visual database of emotional speech and song (RAVDESS) corpus, and crowd-sourced emotional multimodal actors dataset (CREMA-D). This work reveals that the optimum feature extraction method compared to other known methods is MFCCs delta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM COMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers, CNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our results demonstrate that the proposed work yields average emotion recognition accuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and 31.9% based on CapsNet, CNN, support vector machine (SVM), multi-layer perceptron (MLP), k-nearest neighbor (KNN), radial basis function (RBF), and naïve Bayes (NB), respectively. © 2021 Elsevier Ltd",Long short-term memory; Speech emotion recognition; Deep neural networks; Speech recognition; Emotion recognition; Speech; Brain; Convolution; Convolutional neural network; LSTM; Support vector machines; Feature extraction; Emirati; Deep neural network; Capsule network; Nearest neighbor search; Image segmentation; Spectrograms; Spectrographs; Capsule networks; Compression methods; Delta Delta; Dual channel; Dual-channel; Energy efficiency,,,emotion,No,Yes
scopus,A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals,"Yin, G.; Sun, S.; Yu, D.; Li, D.; Zhang, K.",2022,,18,,10.1145/3490686,"Considerable attention has been paid to physiological signal-based emotion recognition in the field of affective computing. For reliability and user-friendly acquisition, electrodermal activity (EDA) has a great advantage in practical applications. However, EDA-based emotion recognition with large-scale subjects is still a tough problem. The traditional well-designed classifiers with hand-crafted features produce poorer results because of their limited representation abilities. And the deep learning models with auto feature extraction suffer the overfitting drop-off because of large-scale individual differences. Since music has a strong correlation with human emotion, static music can be involved as the external benchmark to constrain various dynamic EDA signals. In this article, we make an attempt by fusing the subject's individual EDA features and the external evoked music features. And we propose an end-to-end multimodal framework, the one-dimensional residual temporal and channel attention network (RTCAN-1D). For EDA features, the channel-temporal attention mechanism for EDA-based emotion recognition is first involved in mine the temporal and channel-wise dynamic and steady features. The comparisons with single EDA-based SOTA models on DEAP and AMIGOS datasets prove the effectiveness of RTCAN-1D to mine EDA features. For music features, we simply process the music signal with the open-source toolkit openSMILE to obtain external feature vectors. We conducted systematic and extensive evaluations. The experiments on the current largest music emotion dataset PMEmo validate that the fusion of EDA and music is a reliable and efficient solution for large-scale emotion recognition.  © 2022 Association for Computing Machinery.",Attention mechanisms; Speech recognition; Emotion recognition; Physiological signals; Deep learning; Multi-modal fusion; Affective Computing; Attention mechanism; Multimodal fusion; Dynamics; Electrodermal activity; Electrodes; Multimodal frameworks; Large-scales; Activity-based; Large-scale emotion recognition,,,emotion,No,No
scopus,Multimodal Music Emotion Recognition based on WLDNN_GAN,"Yin, L.; Tang, J.; Yu, J.",2022,,,,10.1109/ISAIEE57420.2022.00114,"In order to solve the current concern of music emotion recognition, this paper proposes the WLDNN_GAN algorithm, the abstract obtained music features are MFCC features, GTF features, midi music information features, through these three features for music emotion recognition and classification. Using the same dataset, the MSE, RMSE and R2 of some currently popular model models are compared horizontally for evaluation, and the experimental results show that the model proposed in this paper can achieve excellent performance in analysing music emotion information.  © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; 'current; Classification (of information); Multimodal information fusion; multimodal information fusion; Music emotions; Pre-processing; Information feature; Music information; music pre-processing; Music pre-processing; WLDNN_GAN,,,emotion,No,Yes
scopus,MEmoR: A Multimodal Emotion Recognition using affective biomarkers for smart prediction of emotional health for people analytics in smart industries,"Kumar, A.; Sharma, K.; Sharma, A.",2022,,123,,10.1016/j.imavis.2022.104483,"The intersection of people, data and intelligent machines has a far-reaching impact on the productivity, efficiency and operations of a smart industry. Internet-of-things (IoT) offers a great potential for workplace gains using the “quantified self” and the computer vision strategies. Their goal is to focus on productivity, fitness, wellness, and improvement of the work environment. Recognizing and regulating human emotion is vital to people analytics as it plays an important role in workplace productivity. Within the smart industry setting, various non-invasive IoT devices can be used to recognize emotions and study the behavioral outcomes in various situations. This research puts forward a deep learning model for detection of human emotional state in real-time using multimodal data from the Emotional Internet-of-things (E-IoT). The proposed multimodal emotion recognition model, MEmoR makes use of two data modalities: visual and psychophysiological. The video signals are sampled to obtain image frames and a ResNet50 model pre-trained for face recognition is fine-tuned for emotion classification. Simultaneously, CNN is trained on the psychophysiological signals and the results of the two modality networks are combined using decision-level weighted fusion. The model is tested on the benchmark Bio Vid Emo DB multimodal dataset and compared to the state-of-the-art. © 2022 Elsevier B.V.",Speech recognition; Emotion recognition; Face recognition; Deep learning; Multimodal emotion recognition; Affective Computing; Affective computing; Human emotion; Internet of things; Multi-modelling; Intelligent machine; Facial expression analysis; E-IoT; Emotional internet-of-thing; Facial expressions analysis; Multi-model; Productivity; Visual analysis; Work environments,,,emotion,No,No
scopus,Objectivity meets subjectivity: A subjective and objective feature fused neural network for emotion recognition,"Zhou, S.; Huang, D.; Liu, C.; Jiang, D.",2022,,122,,10.1016/j.asoc.2022.108889,"Using multimodal fusion method to deal with emotion recognition task has become a trend. The fusion vector can more comprehensively reflect the subject's emotional change state, so as to obtain a more accurate emotion recognition effect. However, different fusion input or feature fusion methods have different effects on the final fusion results. In this paper, we propose a subjective and objective feature fused neural network model (SOFNN) for emotion recognition, which can effectively learn spatial–temporal information from EEG signals and dynamically integrate EEG signals with eye movement signals. Specifically, we extract more abundant spatial and temporal information from the original EEG signal through a series of 1-D convolution kernels of different sizes and we verify the effectiveness of the extracted features through experiments. The size of the 1-D convolution kernels is determined by the characteristics (such as sampling rate and number of channels) of the original EEG signal. Then, we design a subjective and objective feature fusion framework to adjust the proportion of the two features through the dynamic learning of the weight vector, so as to fully exploit their respective advantages. We evaluate the performance of our model on the SEED-IV dataset, which is a common dataset. For the recognition task of four emotions (happy, sad, fear and neutral), our model achieves 86.27% accuracy and 10.16% standard deviation, which are better than the existing methods. In addition, we design a variety of ablation experiments to verify the effectiveness of each module in our model. The experiment results show that our model can make better use of the complementary relationship between subjective and objective features, which can achieve better emotion recognition effect. © 2022 Elsevier B.V.",Speech recognition; Emotion recognition; Multi-modal fusion; Feature fusion; Features fusions; Eye movements; Convolution; EEG signals; Neural-networks; Fusion methods; Convolution kernel; Emotional change; 1-D convolution kernel; 1-D convolution kernels; Temporal information,,,emotion,No,Yes
scopus,A multiturn complementary generative framework for conversational emotion recognition,"Wang, L.; Li, R.; Wu, Y.; Jiang, Z.",2022,,37,,10.1002/int.22805,"Conversational emotion recognition (CER) is a significant task due to its application in human–computer interaction. Existing work treats CER as an utterance-level classification task without considering that empathic response also reflects contextual emotion understanding. Previous work has proven that accurate recognition of emotions in the dialogue history is helpful to generate high-fit responses. In this paper, we investigate whether this conclusion is a sufficient and necessary condition. Specifically, we define an auxiliary empathic multiturn dialogue generation (MDG) task to enhance emotion understanding. Correspondingly, we present a Sequence-to-Sequence oriented framework that combines CER and MDG in a multitask learning manner to verify the complementarity between the two tasks. First, we use alternate recurrent neural networks to encode the content of historical utterances and represent the states of multiparty emotions, which are used for emotion classification. Second, since most MDG methods ignore the emotional coherence of the dialogue context itself, we use affine transformation to fuse hidden states of content and emotions to initialize the decoder. Finally, at each step of generation, an attention mechanism is used to fuse information from the dialogue history to ensure emotional coherence. The CER results of our models outperform the state-of-the-art on three prevalent emotional dialogue data sets. Further analysis demonstrates the mutual promotion and empathy interpretability between MDG and CER. Furthermore, our framework is scalable for different coding strategies and multimodal fusion. To the best of our knowledge, this is the first work to explore CER from the perspective of empathy through multitask learning with dialogue generation. © 2022 Wiley Periodicals LLC.",Speech recognition; deep learning; Emotion recognition; Human computer interaction; Recurrent neural networks; Emotion classification; Emotion understanding; Classification tasks; conversational emotion recognition; ITS applications; Recognition of emotion; affine transformation; Dialogue generations; Generation method; Multi-turn; multiturn dialogue generation; Sufficient and necessary condition,,,emotion,No,No
scopus,Deep learning based multimodal emotion recognition using model-level fusion of audio–visual modalities,"Middya, A.I.; Nag, B.; Roy, S.",2022,,244,,10.1016/j.knosys.2022.108580,"Emotion identification based on multimodal data (e.g., audio, video, text, etc.) is one of the most demanding and important research fields, with various uses. In this context, this research work has conducted a rigorous exploration of model-level fusion to find out the optimal multimodal model for emotion recognition using audio and video modalities. More specifically, separate novel feature extractor networks for audio and video data are proposed. After that, an optimal multimodal emotion recognition model is created by fusing audio and video features at the model level. The performances of the proposed models are assessed based on two benchmark multimodal datasets namely Ryerson Audio–Visual Database of Emotional Speech and Song (RAVDESS) and Surrey Audio–Visual Expressed Emotion (SAVEE) using various performance metrics. The proposed models achieve high predictive accuracies of 99% and 86% on the SAVEE and RAVDESS datasets, respectively. The effectiveness of the models are also verified by comparing their performances with the existing emotion recognition models. Some case studies are also conducted to explore the model's ability to capture the variability of emotional states of the speakers in publicly available real-world audio–visual media. © 2022 Elsevier B.V.",Performance; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Classification (of information); Benchmarking; Recognition models; Level fusion; Audio-visual; Audio and video; Classification; Audio features; Video features,,,emotion,No,Yes
scopus,Gaze-enhanced Crossmodal Embeddings for Emotion Recognition,"Abdou, A.; Sood, E.; Müller, P.; Bulling, A.",2022,,6,,10.1145/3530879,"Emotional expressions are inherently multimodal - integrating facial behavior, speech, and gaze - but their automatic recognition is often limited to a single modality, e.g. speech during a phone call. While previous work proposed crossmodal emotion embeddings to improve monomodal recognition performance, despite its importance, an explicit representation of gaze was not included. We propose a new approach to emotion recognition that incorporates an explicit representation of gaze in a crossmodal emotion embedding framework. We show that our method outperforms the previous state of the art for both audio-only and video-only emotion classification on the popular One-Minute Gradual Emotion Recognition dataset. Furthermore, we report extensive ablation experiments and provide detailed insights into the performance of different state-of-the-art gaze representations and integration strategies. Our results not only underline the importance of gaze for emotion recognition but also demonstrate a practical and highly effective approach to leveraging gaze information for this task.  © 2022 ACM.",Performance; Embeddings; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; State of the art; Classification (of information); Cross-modal; Multi-modality; Emotional expressions; gaze; Gaze; multi-modality; Explicit representation,,,emotion,No,Yes
scopus,Automatic Emotion Recognition in Children with Autism: A Systematic Literature Review,"Landowska, A.; Karpus, A.; Zawadzka, T.; Robins, B.; Barkana, D.E.; Kose, H.; Zorcec, T.; Cummins, N.",2022,,22,,10.3390/s22041649,"The automatic emotion recognition domain brings new methods and technologies that might be used to enhance therapy of children with autism. The paper aims at the exploration of methods and tools used to recognize emotions in children. It presents a literature review study that was performed using a systematic approach and PRISMA methodology for reporting quantitative and qualitative results. Diverse observation channels and modalities are used in the analyzed studies, including facial expressions, prosody of speech, and physiological signals. Regarding representation models, the basic emotions are the most frequently recognized, especially happiness, fear, and sadness. Both single-channel and multichannel approaches are applied, with a preference for the first one. For multimodal recognition, early fusion was the most frequently applied. SVM and neural networks were the most popular for building classifiers. Qualitative analysis revealed important clues on participant group construction and the most common combinations of modalities and methods. All channels are reported to be prone to some disturbance, and as a result, information on a specific symptoms of emotions might be temporarily or permanently unavailable. The challenges of proper stimuli, labelling methods, and the creation of open datasets were also identified. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Systematic literature review; Humans; Emotions; Facial Expression; emotion; facial expression; Speech recognition; Emotion recognition; Facial Expressions; Physiological signals; human; physiology; Affective Computing; Recognition, Psychology; speech; Speech; Affective computing; Support vector machines; Diseases; Speech signals; Autism spectrum disorder; Autism spectrum disorders; Automatic emotion recognition; Autism; autism; Autistic Disorder; child; Child; Children with autisms; Literature reviews",,,emotion,No,Yes
scopus,A novel spatio-temporal convolutional neural framework for multimodal emotion recognition,"Sharafi, M.; Yazdchi, M.; Rasti, R.; Nasimi, F.",2022,,78,,10.1016/j.bspc.2022.103970,"Proposing a practical method for high-performance emotion recognition could facilitate human–computer interaction. Among existing methods, deep learning techniques have improved the performance of emotion recognition systems. In this work, a new multimodal neural design is presented wherein audio and visual data are combined as the input to a hybrid network comprised of a bidirectional long short term memory (BiLSTM) network and two convolutional neural networks (CNNs). The spatial and temporal features extracted from video frames are fused with Mel-Frequency Cepstral Coefficients (MFCCs) and energy features extracted from audio signals and BiLSTM network outputs. Finally, a Softmax classifier is used to classify inputs into the set of target categories. The proposed model is evaluated on Surrey Audio–Visual Expressed Emotion (SAVEE), Ryerson Audio–Visual Database of Emotional Speech and Song (RAVDESS), and Ryerson Multimedia research Lab (RML) databases. Experimental results on these datasets prove the effectiveness of the proposed model where it achieves the accuracy of 99.75%, 94.99%, and 99.23% for the SAVEE, RAVDESS, and RML databases, respectively. Our experimental study reveals that the suggested method is more effective than existing algorithms in adapting to emotion recognition in these datasets. © 2022 Elsevier Ltd",Long short-term memory; convolutional neural network; emotion; Emotion Recognition; Performance; Speech recognition; deep learning; emotion recognition; Emotion recognition; Deep learning; human; videorecording; Learning systems; Human computer interaction; Article; information processing; Convolutional neural networks; Brain; Convolution; Convolutional neural network; algorithm; spatiotemporal analysis; long short term memory network; Memory network; cross validation; Audio-visual; entropy; Database systems; Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficient; Mel-frequency cepstral coefficients; Audio-visual database; architecture; audiovisual recording; Bidirectional long short term memory; constants and coefficients,,,emotion,No,Yes
scopus,UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition,"Hu, G.; Lin, T.-E.; Zhao, Y.; Lu, G.; Wu, Y.; Li, Y.",2022,,,,,"Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods. © 2022 Association for Computational Linguistics.",Semantics; Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Modality Fusion; Sentiment analysis; Human behaviors; Research topics; Knowledge-sharing; Semantic levels; Short periods; Work study,,,emotion,No,No
scopus,Emotion classification with multichannel physiological signals using hybrid feature and adaptive decision fusion,"Yan, M.; Deng, Z.; He, B.; Zou, C.; Wu, J.; Zhu, Z.",2022,,71,,10.1016/j.bspc.2021.103235,"An effective classification of emotions promotes efficient human-computer and human-human interaction. The diversity of emotion expression and different modal characteristics of multichannel physiological signals render emotion classification based on multichannel physiological signals challenging. This paper presents a multimodal emotion classification framework that uses multichannel physiological signals and introduces two key techniques – hybrid feature extraction and adaptive decision fusion. Firstly, a hybrid feature extraction method is proposed to extract statistical-related and event-related features from physiological signals. The extracted features capture the signal's statistical and context-dependent characteristics. An adaptive decision fusion method is then proposed to integrate signal modalities for emotion classification. Three different weights are designed by analyzing the physiological signals’ modal and cross-modal information. Finally, the presented framework is evaluated on the Wearable Stress and Affect Detection dataset through a comparative analysis. The results demonstrate the necessity of using event-related features and highlight the importance of developing adaptive decision fusion strategies for emotion classification. © 2021 Elsevier Ltd",emotion; Physiological signals; Physiology; human; Human computer interaction; Classification (of information); Features extraction; Biomedical signal processing; human experiment; Hybrid features; article; Emotion classification; Extraction; feature extraction; Feature extraction; physiological stress; Multi channel; Decisions fusion; Decision fusion; Classification of emotions; Hybrid-feature extraction; Computer Human Interaction; Multichannel physiological signal; Multichannel physiological signals,,,emotion,No,No
scopus,Multimodal Attentive Learning for Real-time Explainable Emotion Recognition in Conversations,"Arumugam, B.; Bhattacharjee, S.D.; Yuan, J.",2022,,2022-May,,10.1109/ISCAS48785.2022.9938005,"Human emotion recognition plays a pivotal role in building an intelligent conversational agent for providing real-time automated support service in various problem settings. Recent research works have explored the temporal patterns in conversations to enable a comprehensive understanding of the content and context of conversations from a video clip, which does not fully leverage the multi-modal (facial expressions of the participants, speech tone, content, and context of the discussion) information and their temporal evolution. To address this, we propose a multimodal attentive learning framework that keeps track of spatio-temporal states of the participants and their conversation dynamics. By designing a novel contrastive loss-based optimization framework, the proposed method exhibits promise in identifying the emotion state of the individual speaker in real-time and can identify top-k words in the conversation that influence emotion recognition. The consistent superior performance over other state-of-the-art works in two large-scale datasets, MELD and IEMOCAP, demonstrate the feasibility of our approach. © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multimodal; Cross-modal; Human emotion recognition; Large dataset; Real- time; Cross-modal attention; Spatiotemporal feature; Computer vision; Feature representation; Cross-modal Attention; Explainable decision visualization; Explainable Decision Visualization; Spatiotemporal feature representation; SpatioTemporal Feature Representation,,,emotion,No,Yes
scopus,Audio-Video Fusion with Double Attention for Multimodal Emotion Recognition,"Mocanu, B.; Tapu, R.",2022,,,,10.1109/IVMSP54334.2022.9816349,"Recently, the multimodal emotion recognition has become a hot topic of research, within the affective computing community, due to its robust performances. In this paper, we propose to analyze emotions in an end-to-end manner based on various convolutional neural networks (CNN) architectures and attention mechanisms. Specifically, we develop a new framework that integrates the spatial and temporal attention into a visual 3D-CNN and temporal attention into an audio 2D-CNN in order to capture the intra-modal features characteristics. Further, the system is extended with an audio-video cross-attention fusion approach that effectively exploits the relationship across the two modalities. The proposed method achieves 87.89% of accuracy on RAVDESS dataset. When compared with state-of-the art methods our system demonstrates accuracy gains of more than 1.89%.  © 2022 IEEE.",Emotion Recognition; Speech recognition; emotion recognition; Emotion recognition; Audio videos; Multimodal emotion recognition; Affective Computing; Convolutional neural networks; Convolutional neural network; Computer vision; Temporal attention; Spatial attention; Hot topics; cross-fusion; Cross-fusion; spatial attention; temporal attention; Video fusion,,,emotion,No,Yes
scopus,Context-aware Multimodal Fusion for Emotion Recognition,"Li, J.; Wang, S.; Chao, Y.; Liu, X.; Meng, H.",2022,,2022-September,,10.21437/Interspeech.2022-10592,"Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings. Copyright © 2022 ISCA.",Emotion Recognition; Performance; Emotional state; Speech recognition; deep learning; Emotion recognition; Deep learning; Multi-modal fusion; Unimodal; Classification (of information); Multi-modality; transfer learning; Transfer learning; Speech communication; Automatic emotion recognition; multimodality; Context-Aware,,,emotion,No,Yes
scopus,Branch-Fusion-Net for Multi-Modal Continuous Dimensional Emotion Recognition,"Li, C.; Xie, L.; Pan, H.",2022,,29,,10.1109/LSP.2022.3160373,"Regression modeling is a significant aspect of multi-modal continuous dimensional emotion recognition. Despite the developments of this domain, one of the limitations that severely impede the application of emotion recognition is that most methods utilized for regression modeling merely capture the temporal information. Motivated by this, we propose a new branch feature fusion framework BF-Net, whose core idea is to deeply combine local features captured by convolutional neural networks and temporal dependencies captured by long-short-term memory recurrent neural networks. The framework consists of two main components: two-branch structure and fusion of branches. The former captures local features and temporal dependencies respectively in an effective way. Besides, the latter utilizes an attention mechanism to fuse the features on a deep level. In this way, the framework achieves full utilization of the local features and temporal dependencies. The experiments on ULM-TSST dataset show that the proposed method is competitive or superior to the state-of-the-art works.  © 2022 IEEE.",Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Features extraction; Features fusions; Convolution; Convolutional neural network; feature fusion; Recurrent neural networks; Feature extraction; Regression analysis; Context models; Data structures; Deep-levels; Local feature; regression modeling; Regression modelling,,,emotion,No,No
scopus,Audio and Video-based Emotion Recognition using Multimodal Transformers,"John, V.; Kawanishi, Y.",2022,,2022-August,,10.1109/ICPR56361.2022.9956730,"Emotion recognition, an important research problem in human-robot interactions, is primarily achieved by extracting human emotions from audio and visual data. State-of-the-art performance is reported by audio-visual sensor fusion algorithms using deep learning models such as CNN, RNN, and LSTM. However, the RNN and LSTM are shown to be limited in handling the long-term dependencies over the entire input sequence. In this work, we propose to improve the performance of audio-visual emotion recognition using a novel transformer-based model, containing three transformer branches, named multimodal transformers. The three transformer branches, in our work, compute the audio self-attention, the video self-attention, and the audio-video cross attention. The self-attention branches identify the most relevant information in the audio and video input, while the cross-attention branch identifies the most relevant audio-video interactive information. The relevant information from these three branches report the best performance in our ablation study. We also propose a novel temporal embedding scheme, termed block embedding, to add the temporal information to the visual feature, derived from the multiple frames in the video. The proposed architecture is validated using the RAVDESS, CREMA-D, and SAVEE audio-visual public datasets. A detailed ablation study and comparative analysis with baseline models is performed. The results show that the proposed multi-modal transformer framework is better than the baseline methods. © 2022 IEEE.",Long short-term memory; Emotion Recognition; Performance; Embeddings; Multi-modal; Speech recognition; Emotion recognition; Audio videos; Human robot interaction; Audio-visual; Human emotion; Audio and video; Humans-robot interactions; Research problems; Ablation,,,emotion,No,Yes
scopus,Visual Emotion Representation Learning via Emotion-Aware Pre-training,"Zhang, Y.; Ding, W.; Xu, R.; Hu, X.",2022,,,,,"Despite recent progress in deep learning, visual emotion recognition remains a challenging problem due to the ambiguity of emotion perception, diverse concepts related to visual emotion, and lack of large-scale annotated datasets. In this paper, we present a large-scale multimodal pre-training method to learn visual emotion representation by aligning emotion, object, and attribute triplet with a contrastive loss. We conduct our pre-training on a large web dataset with noisy tags and fine-tune on smaller visual emotion classification datasets with class label supervision. Our method achieves state-of-the-art performance for visual emotion classification. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.",Emotion Recognition; Multi-modal; Behavioral research; Emotion recognition; Deep learning; Classification (of information); Learn+; Large dataset; Emotion classification; Pre-training; Large-scales; Emotion representation; Annotated datasets; Recent progress; Training methods,,,emotion,Yes,Yes
scopus,SPEECH EMOTION RECOGNITION USING SELF-SUPERVISED FEATURES,"Morais, E.; Hoory, R.; Zhu, W.; Gat, I.; Damasceno, M.; Aronowitz, H.",2022,,2022-May,,10.1109/ICASSP43922.2022.9747870,"Self-supervised pre-trained features have consistently delivered state-of-art results in the field of natural language processing (NLP); however, their merits in the field of speech emotion recognition (SER) still need further investigation. In this paper we introduce a modular End-to-End (E2E) SER system based on an Upstream + Downstream architecture paradigm, which allows easy use/integration of a large variety of self-supervised features. Several SER experiments for predicting categorical emotion classes from the IEMOCAP dataset are performed. These experiments investigate interactions among fine-tuning of self-supervised feature models, aggregation of frame-level features into utterance-level features and back-end classification networks. The proposed monomodal speech-only based system not only achieves SOTA results, but also brings light to the possibility of powerful and well fine-tuned self-supervised acoustic features that reach results similar to the results achieved by SOTA multimodal systems using both Speech and Text modalities. © 2022 IEEE",Speech emotion recognition; Emotion Recognition; Speech recognition; Language processing; Natural languages; End to end; Down-stream; Fine tuning; Speech emotion recognition systems; Modulars; end-to-end systems; End-to-end systems; Self-supervised feature; self-supervised features,,,emotion,No,Yes
scopus,Multimodal Music Emotion Recognition with Hierarchical Cross-Modal Attention Network,"Zhao, J.; Ru, G.; Yu, Y.; Wu, Y.; Li, D.; Li, W.",2022,,2022-July,,10.1109/ICME52920.2022.9859812,"Computational music emotion recognition is to recognize the emotional content in music tracks. In computational music emotion recognition studies, researchers have paid close attention to the audio content of the music tracks. Although lyrics content and music context contribute greatly to the perceived emotion, these kinds of emotional information are usually ignored. Based on this finding, we propose a multimodal music emotion recognition method jointly predicting the valence and arousal values by combining the audio, lyrics, track name, and artist of a given track. Audio features, lyrics features and context features are extracted separately and fused by a cross-modal attention mechanism, forming a hierarchical structure. Our proposed model outperforms two baselines by a large margin and achieves state-of-the-art performance on two public datasets.  © 2022 IEEE.",Natural language processing; Emotion Recognition; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Deep learning; Deep Learning; Music; Machine-learning; Learning algorithms; Large dataset; Audio acoustics; Language processing; Natural languages; Natural Language Processing; Multimodal machine learning; Music emotions; Multimodal Machine Learning; Music emotion recognition; Music Emotion Recognition,,,emotion,No,Yes
scopus,Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks,"Goncalves, L.; Busso, C.",2022,,2022-September,,10.21437/Interspeech.2022-11012,"Speech emotion recognition (SER) is a challenging task due to the limited availability of real-world labeled datasets. Since it is easier to find unlabeled data, the use of self-supervised learning (SSL) has become an attractive alternative. This study proposes new pre-text tasks for SSL to improve SER. While our target application is SER, the proposed pre-text tasks include audiovisual formulations, leveraging the relationship between acoustic and facial features. Our proposed approach introduces three new unimodal and multimodal pre-text tasks that are carefully designed to learn better representations for predicting emotional cues from speech. Task 1 predicts energy variations (high or low) from a speech sequence. Task 2 uses speech features to predict facial activation (high or low) based on facial landmark movements. Task 3 performs a multi-class emotion recognition task on emotional labels obtained from combinations of action units (AUs) detected across a video sequence. We pre-train a network with 60.92 hours of unlabeled data, fine-tuning the model for the downstream SER task. The results on the CREMA-D dataset show that the model pre-trained on the proposed domain-specific pre-text tasks significantly improves the precision (up to 5.1%), recall (up to 4.5%), and F1-scores (up to 4.9%) of our SER system. Copyright © 2022 ISCA.",Speech emotion recognition; Emotion Recognition; Speech recognition; Self-supervised learning; Supervised learning; Speech communication; Facial feature; self-supervised learning; speech emotion recognition; Labeled dataset; Real-world; Acoustic features; Audiovisual task; audiovisual tasks; Domain specific; Target application; Unlabeled data,,,emotion,Yes,Yes
scopus,Efficiency Analysis of Pre-trained CNN Models as Feature Extractors for Video Emotion Recognition,"Mehta, D.; Joshi, J.; Bisht, A.; Badoni, P.",2022,,289,,10.1007/978-981-19-0011-2_54,"Emotion recognition is a complex task that involves understanding and scrutinizing the information depicted by the human body in the form of various physiological signals. Earlier works have used these signals for manually extracting features, such as by using facial landmarks or by using Mel frequency cepstrum coefficient (MFCCs), to classify emotions. But with increasing computational power, neural networks are now widely being used to automate this task. Many state-of-the-art convolutional neural network (CNN) models can be used to extract features and classify emotions. In this research, a quantitative comparison has been done between five state-of-the-art CNN models—DenseNet, ResNet, Inception, VGG16, and Xception along with their variants, as feature extractors. Two popular audio-visual datasets—RAVDESS and SAVEE—have been preprocessed and the resultant data is used as input for a multimodal model for emotion classification. This model is built by first fine-tuning pre-trained CNN models using preprocessed audio data and preprocessed video data and then combining the fine-tuned models in a fusion network. The results obtained through this research will prove to help select the appropriate CNN model in future works that make use of CNN models as feature extractors for emotion recognition. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Speech recognition; CNN; Emotion recognition; State of the art; Classification (of information); Convolutional neural networks; Convolutional neural network; Transfer learning; Signal processing; Multimodal models; Neural network model; Feature extractor; Complex task; Efficiency analysis; Multimodal model; Video emotion recognition,,,emotion,No,No
scopus,"Emotion Recognition with Audio, Video, EEG, and EMG: A Dataset and Baseline Approaches","Chen, J.; Ro, T.; Zhu, Z.",2022,,10,,10.1109/ACCESS.2022.3146729,"This paper describes a new posed multimodal emotional dataset and compares human emotion classification based on four different modalities - audio, video, electromyography (EMG), and electroencephalography (EEG). The results are reported with several baseline approaches using various feature extraction techniques and machine-learning algorithms. First, we collected a dataset from 11 human subjects expressing six basic emotions and one neutral emotion. We then extracted features from each modality using principal component analysis, autoencoder, convolution network, and mel-frequency cepstral coefficient (MFCC), some unique to individual modalities. A number of baseline models have been applied to compare the classification performance in emotion recognition, including k-nearest neighbors (KNN), support vector machines (SVM), random forest, multilayer perceptron (MLP), long short-term memory (LSTM) model, and convolutional neural network (CNN). Our results show that bootstrapping the biosensor signals (i.e., EMG and EEG) can greatly increase emotion classification performance by reducing noise. In contrast, the best classification results were obtained by a traditional KNN, whereas audio and image sequences of human emotions could be better classified using LSTM.  © 2013 IEEE.",Long short-term memory; Speech recognition; Emotion recognition; Audio videos; Classification (of information); Features extraction; electroencephalography; Electroencephalography; Biomedical signal processing; Electrophysiology; Convolution; Support vector machines; Support vectors machine; Learning algorithms; Emotion classification; electromyography; Electromyography; Data collection; Audio acoustics; Extraction; Video; Feature extraction; Human emotion; Decision trees; Principal component analysis; Nearest neighbor search; data collection; Classification performance,,,emotion,No,Yes
scopus,A Deep Residual-based Model on Multi-Branch Aggregation for Stress and Emotion Recognition through Biosignals,"Mekruksavanich, S.; Hnoohom, N.; Jitpattanakul, A.",2022,,,,10.1109/ECTI-CON54298.2022.9795449,"Stress and emotion recognition (SER) is a rapidly growing field of study that has applications in various areas, including psychological wellbeing, rehabilitative services, athletic training, and human-computer interaction. Biological information such as the electrocardiogram (ECG), electromyography (EMG), and electrodermal activity (EDA) has been frequently utilized for the SER for learning-based approaches. This study introduces a convolutional neural network motivated by ResNeXt to facilitate multimodal awareness. The proposed model, named StressNeXt, can extract high-level insights from raw bio-signal signals and classify emotional expressions effectively. We undertake a series of investigations using a publicly released standard dataset (WESAD) to determine the optimal implementation of the proposed solution for recognizing stress and emotion. After incorporating preliminary fusion events, we examined deep learning models using 5-fold cross-validation. Our study demonstrates that the suggested technique can comprehend robust multimodal representations with an accuracy of 87.73% utilizing EDA. Additionally, the identification was designed to provide better to 99.92% by fusing with accelerometer sensor data. © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; deep learning; Emotion recognition; Deep learning; Human computer interaction; Convolutional neural networks; Biomedical signal processing; Electrocardiography; Electrodermal activity; Wearable sensors; Biosignals; Stress recognition; Athletic trainings; Biological information; biosignal; Learning-based approach; Psychological well-being; stress and emotion recognition; wearable sensors,,,emotion,No,Yes
scopus,Emotion Recognition with Pre-Trained Transformers Using Multimodal Signals,"Vazquez-Rodriguez, J.; Lefebvre, G.; Cumin, J.; Crowley, J.L.",2022,,,,10.1109/ACII55700.2022.9953852,"In this paper, we address the problem of multimodal emotion recognition from multiple physiological signals. We demonstrate that a Transformer-based approach is suitable for this task. In addition, we present how such models may be pre-trained in a multimodal scenario to improve emotion recognition performances. We evaluate the benefits of using multimodal inputs and pre-training with our approach on a state-of-the-art dataset.  © 2022 IEEE.",Machine Learning; Machine learning; Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Affective Computing; State of the art; Machine-learning; Multimodal Emotion Recognition; Pre-training; Multimodal inputs,,,emotion,No,No
scopus,Recognition of Advertisement Emotions with Application to Computational Advertising,"Shukla, A.; Gullapuram, S.S.; Katti, H.; Kankanhalli, M.; Winkler, S.; Subramanian, R.",2022,,13,,10.1109/TAFFC.2020.2964549,"Advertisements (ads) often contain strong emotions to capture audience attention and convey an effective message. Still, little work has focused on affect recognition (AR) from ads employing audiovisual or user cues. This work (1) compiles an affective video ad dataset which evokes coherent emotions across users; (2) explores the efficacy of content-centric convolutional neural network (CNN) features for ad AR vis-ã-vis handcrafted audio-visual descriptors; (3) examines user-centric ad AR from Electroencephalogram (EEG) signals, and (4) demonstrates how better affect predictions facilitate effective computational advertising via a study involving 18 users. Experiments reveal that (a) CNN features outperform handcrafted audiovisual descriptors for content-centric AR; (b) EEG features encode ad-induced emotions better than content-based features; (c) Multi-task learning achieves optimal ad AR among a slew of classifiers and (d) Pursuant to (b), EEG features enable optimized ad insertion onto streamed video compared to content-based or manual insertion, maximizing ad recall and viewing experience. © 2010-2012 IEEE.",multimodal; Electroencephalography; Convolutional neural networks; EEG; Multi-task learning; Affect recognition; Audiovisual; Electroencephalogram signals; multi-task learning; convolutional neural networks; Marketing; C (programming language); ad insertion; Ad insertions; advertisements; Computational advertisings; Content centric; Content-based; Content-based features; content-centric features; perception; Streamed video,,,emotion,No,No
scopus,A Multimodal Convolutional Neural Network Model for the Analysis of Music Genre on Children's Emotions Influence Intelligence,"Chen, W.; Wu, G.",2022,,2022,,10.1155/2022/5611456,"This paper designs a multimodal convolutional neural network model for the intelligent analysis of the influence of music genres on children's emotions by constructing a multimodal convolutional neural network model and profoundly analyzing the impact of music genres on children's feelings. Considering the diversity of music genre features in the audio power spectrogram, the Mel filtering method is used in the feature extraction stage to ensure the effective retention of the genre feature attributes of the audio signal by dimensional reduction of the Mel filtered signal, deepening the differences of the extracted features between different genres, and to reduce the input size and expand the model training scale in the model input stage, the audio power spectrogram obtained by feature extraction is cut the MSCN-LSTM consists of two modules: multiscale convolutional kernel convolutional neural network and long and short term memory network. The MSCNN network is used to extract the EEG signal features, the LSTM network is used to remove the temporal characteristics of the eye-movement signal, and the feature fusion is done by feature-level fusion. The multimodal signal has a higher emotion classification accuracy than the unimodal signal, and the average accuracy of emotion quadruple classification based on a 6-channel EEG signal, and children's multimodal signal reaches 97.94%. After pretraining with the MSD (Million Song Dataset) dataset in this paper, the model effect was further improved significantly. The accuracy of the Dense Inception network improved to 91.0% and 89.91% on the GTZAN dataset and ISMIR2004 dataset, respectively, proving that the Dense Inception network's effectiveness and advancedness of the Dense Inception network were demonstrated. © 2022 Wei Chen and Guobin Wu.","music; Long short-term memory; Humans; Emotions; emotion; Multi-modal; Modal analysis; human; Neural Networks, Computer; Classification (of information); Features extraction; Music; Convolutional neural networks; Convolution; Convolutional neural network; algorithm; Algorithms; Audio acoustics; Extraction; Feature extraction; EEG signals; Neural network model; Intelligence; Spectrograms; child; Child; Spectrographs; Audio power; Genre features; intelligence; Intelligent analysis; Music genre; Neural network models",,,emotion,No,Yes
scopus,Multimodal Neurophysiological Transformer for Emotion Recognition,"Koorathota, S.; Khan, Z.; Lapborisuth, P.; Sajda, P.",2022,,2022-July,,10.1109/EMBC48229.2022.9871421,"Understanding neural function often requires multiple modalities of data, including electrophysiogical data, imaging techniques, and demographic surveys. In this paper, we introduce a novel neurophysiological model to tackle major challenges in modeling multimodal data. First, we avoid non-alignment issues between raw signals and extracted, frequency-domain features by addressing the issue of variable sampling rates. Second, we encode modalities through 'cross-attention' with other modalities. Lastly, we utilize properties of our parent transformer architecture to model long-range dependencies between segments across modalities and assess intermediary weights to better understand how source signals affect prediction. We apply our Multimodal Neurophysiological Transformer (MNT) to predict valence and arousal in an existing open-source dataset. Experiments on non-aligned multimodal time-series show that our model performs similarly and, in some cases, outperforms existing methods in classification tasks. In addition, qualitative analysis suggests that MNT is able to model neural influences on autonomic activity in predicting arousal. Our architecture has the potential to be fine-tuned to a variety of downstream tasks, including for BCI systems. © 2022 IEEE.",Attention; Emotions; emotion; Emotion Recognition; Multi-modal; Emotion recognition; Multi-modal data; physiology; attention; Frequency domain analysis; Frequency domains; Forecasting; Multiple modalities; arousal; Computer vision; Neurophysiology; Arousal; neurophysiology; Data imaging; Domain feature; endoscopy; Endoscopy; Neural functions; Neurophysiological model; Raw signals,,,emotion,No,No
scopus,MM-DFN: MULTIMODAL DYNAMIC FUSION NETWORK FOR EMOTION RECOGNITION IN CONVERSATIONS,"Hu, D.; Hou, X.; Wei, L.; Jiang, L.; Mo, Y.",2022,,2022-May,,10.1109/ICASSP43922.2022.9747397,"Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal context features in a conversation. The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of the proposed model. © 2022 IEEE",Semantics; Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Speech processing; Graphic methods; multimodal fusion; Emotion recognition in conversation; Dialogue systems; Graph-based; dialogue systems; Dynamic fusion; emotion recognition in conversations; Multimodal dynamics,,,emotion,No,No
scopus,Multimodal Emotion Recognition in Response to Oil Paintings,"Luo, S.; Lan, Y.-T.; Peng, D.; Li, Z.; Zheng, W.-L.; Lu, B.-L.",2022,,2022-July,,10.1109/EMBC48229.2022.9871630,"Most previous affective studies use facial expression pictures, music or movie clips as emotional stimuli, which are either too simplified without contexts or too dynamic for emotion annotations. In this work, we evaluate the effectiveness of oil paintings as stimuli. We develop an emotion stimuli dataset with 114 oil paintings selected from subject ratings to evoke three emotional states (i.e., negative, neutral and positive), and acquire both EEG and eye tracking data from 20 subjects while watching the oil paintings. Furthermore, we propose a novel affective model for multimodal emotion recognition by 1) extracting informative features of EEG signals from both the time domain and the frequency domain, 2) exploring topological information embedded in EEG channels with graph neural networks (GNNs), and 3) combining EEG and eye tracking data with a deep autoencoder neural network. From the exper-iments, our model obtains an averaged classification accuracy of 94.72 % ± 1.47 %, which demonstrates the feasibility of using oil paintings as emotion elicitation material. © 2022 IEEE.","music; Humans; Emotions; emotion; Emotion Recognition; Deep neural networks; Emotional state; Speech recognition; Eye tracking; Facial Expressions; human; Neural Networks, Computer; Multimodal emotion recognition; Music; Frequency domain analysis; EEG signals; Computer vision; Eye-tracking; Topology; Tracking data; Affective modeling; Movie clips; Music clips; Oil paintings; painting; Painting; Paintings",,,emotion,No,Yes
scopus,WiFi and Vision enabled Multimodal Emotion Recognition,"Hou, Y.; Zhang, X.; Gu, Y.; Li, W.",2022,,2022-May,,10.1109/ICC45855.2022.9838315,"Emotion recognition plays a vital role in current research on human-computer interaction, and human emotion expressions are multi-modal. In this paper, we propose a passive multi-modal emotion recognition system based on facial expression and gesture. To achieve the system design, two major challenges must be addressed, namely, how to capture facial expression and gesture without disturbing the subject, and how to use the correlation between the two modalities to better recognize emotions. For the former, we use WiFi and vision for the passive gesture and facial expression capture, respectively. For the latter, we design a Multi-Source Learning method inspired by Multi-Task Learning to efficiently exploit the correlation between modalities for better emotion recognition. Finally, to evaluate the effectiveness of our system, we use low-cost vision and WiFi devices to prototype the system and build a WiFi-Vision emotion dataset for related research, and we verify the effectiveness of our system in emotion recognition and the superiority of multi-modality over single-modality through conduct extensive experiments. © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Facial Expressions; Multimodal emotion recognition; Learning systems; 'current; Human computer interaction; Dataset; Facial gestures; dataset; Multi-Sources; channel state information; Channel state information; Channel-state information; Wireless local area networks (WLAN); vision; multi-source learning; Multi-source learning,,,emotion,No,No
scopus,Human Emotion Recognition by Integrating Facial and Speech Features: An Implementation of Multimodal Framework using CNN,"Srinivas, P.V.V.S.; Mishra, P.",2022,,13,,10.14569/IJACSA.2022.0130172,"This Emotion recognition plays a prominent role in today's intelligent system applications. Human computer interface, health care, law, and entertainment are a few of the applications where emotion recognition is used. Humans convey their emotions in the form of text, voice, and facial expressions, thus developing a multimodal emotional recognition system playing a crucial role in human-computer or intelligent system communication. The majority of established emotional recognition algorithms only identify emotions in unique data, such as text, audio, or image data. A multimodal system uses information from a variety of sources and fuses the information by using fusion techniques and categories to improve recognition accuracy. In this paper, a multimodal system to recognise emotions was presented that fuses the features from information obtained from heterogenous modalities like audio and video. For audio feature extraction energy, zero crossing rate and Mel-Frequency Cepstral Coefficients (MFCC) techniques are considered. Of these, MFCC produced promising results. For video feature extraction, first the videos are converted to frames and stored in a linear scale space by using a spatial temporal Gaussian Kernel. The features from the images are further extracted by applying a Gaussian weighted function to the second momentum matrix of linear scale space data. The Marginal Fisher Analysis (MFA) fusion method is used to fuse both the audio and video features, and the resulted features are given to the FERCNN model for evaluation. For experimentation, the RAVDESS and CREMAD datasets, which contain audio and video data, are used. Accuracy levels of 95.56, 96.28, and 95.07 on the RAVDESS dataset and accuracies of 80.50, 97.88, and 69.66 on the CREMAD dataset in audio, video, and multimodal modalities are achieved, whose performance is better than the existing multimodal systems © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.",Multi-modal; Speech recognition; Emotion recognition; Face recognition; Character recognition; Multimodal; Audio systems; Extraction; Feature extraction; Audio and video; Intelligent systems; Fusion; Emotional recognition; Multimodal system; Information use; Cremad; Fercnn; Mfa; Mfcc; Ravdes; Ravdess,,,emotion,No,Yes
scopus,A Multimodal Non-Intrusive Stress Monitoring from the Pleasure-Arousal Emotional Dimensions,"Dahmane, M.; Alam, J.; St-Charles, P.-L.; Lalonde, M.; Heffner, K.; Foucher, S.",2022,,13,,10.1109/TAFFC.2020.2988455,"With the increasing development of advanced unmanned aerial vehicles (UAVs), communication between operators and these intelligent systems is becoming more stressful. For the safety of UAV flights, automatic psychological stress detection is becoming a key research topic for successful missions. Stress can be reliably estimated via some biological markers which are not appropriate in many cases of human-machine-interaction setups. In this article, we propose a non-intrusive deep learning-based stress level estimation approach. The goal is to identify the region where the operator's emotional state projects in the space defined by the latent dimensional emotions of arousal and valence since the stress region is well delimited in this space. The proposed multimodal approach uses sequential temporal CNN and LSTM with an Attention Weighted Average layer in the vision modality. As a second modality, we investigate local and global descriptors such as Mel-frequency cepstral coefficients, i-vector embeddings as well as Fisher-vector encodings. The multimodal-fusion approach uses a strategy referred to as 'late-fusion' that involves the combination of unimodal model outputs as inputs of the decision engine. Since we have to deal with more naturalistic behavior in operator-machine interaction contexts, the One minute Gradual Emotion Challenge dataset was used for predictive model validation. © 2010-2012 IEEE.",Long short-term memory; Multi-modal; deep learning; Deep learning; Emotion analysis; Intelligent systems; emotion analysis; Feature representation; Aircraft detection; Antennas; Continuous emotion; continuous emotions; face analysis; Face analysis; feature representation; Non-intrusive; Operator state monitoring; State monitoring; stress monitoring; Stress monitoring; Unmanned aerial vehicles (UAV),,,emotion,No,No
scopus,Multimodal Emotion Recognition Based on Feature Fusion,"Xu, Y.; Wu, X.; Su, H.; Liu, X.",2022,,,,10.1109/ICARM54641.2022.9959098,"In the field of human-computer interaction, human emotion recognition is a challenging problem, and it is also a key link to achieve barrier-free communication between human and machine. At present, most of the emotion recognition algorithms are constructed based on single modal social information, and the recognition results are one-sided and easily disturbed. The recognition accuracy is often difficult to meet the practical requirements after being separated from specific social environment conditions. Based on the above situation and problems, this paper adopts multimodal input and simultaneously includes three modal information of audio, text and facial expression to recognition emotion. Three single modal emotion recognition models are proposed based on three different input information, and the multimodal emotion recognition model are constructed by different feature fusion methods. The experimental results showed that the accuracy of multimodal model on the CH-SIMS dataset was 93.92%. In addition, compared with other emotion recognition models, the effectiveness of the proposed method is verified.  © 2022 IEEE.",Emotion Recognition; Speech recognition; Character recognition; Multimodal emotion recognition; Human computer interaction; Features fusions; Human emotion recognition; Recognition models; Single-modal; Recognition algorithm; Barrier-free; Free communications; Key links,,,emotion,No,Yes
scopus,Multimodal Feature Evaluation and Fusion for Emotional Well-Being Monitorization,"Zubiaga, I.; Justo, R.",2022,,13256 LNCS,,10.1007/978-3-031-04881-4_20,"Mental health is a global issue that plays an important roll in the overall well-being of a person. Because of this, it is important to preserve it, and conversational systems have proven to be helpful in this task. This research is framed in the MENHIR project, which aims at developing a conversational system for emotional well-being monitorization. As a first step for achieving this purpose, the goal of this paper is to select the features that can be helpful for training a model that aims to detect if a patient suffers from a mental illness. For that, we will use transcriptions extracted from conversational information gathered from people with different mental health conditions to create a data set. After the feature selection, the constructed data set will be fed to supervised learning algorithms and their performance will be evaluated. Concretely we will work with random forests, neural networks and BERT. © 2022, Springer Nature Switzerland AG.",Machine learning; Features fusions; Learning algorithms; Feature selection; Features selection; Diseases; Multimodal features; Feature extraction; Mental health; Decision trees; Well being; Data set; Petroleum reservoir evaluation; Conversational systems; Global issues; Feature evaluation; Monitorization,,,emotion,No,No
scopus,Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss,"Franceschini, R.; Fini, E.; Beyan, C.; Conti, A.; Arrigoni, F.; Ricci, E.",2022,,2022-August,,10.1109/ICPR56361.2022.9956589,"Emotion recognition is involved in several real-world applications. With an increase in available modalities, automatic understanding of emotions is being performed more accurately. The success in Multimodal Emotion Recognition (MER), primarily relies on the supervised learning paradigm. However, data annotation is expensive, time-consuming, and as emotion expression and perception depends on several factors (e.g., age, gender, culture) obtaining labels with a high reliability is hard. Motivated by these, we focus on unsupervised feature learning for MER. We consider discrete emotions, and as modalities text, audio and vision are used. Our method, as being based on contrastive loss between pairwise modalities, is the first attempt in MER literature. Our end-to-end feature learning approach has several differences (and advantages) compared to existing MER methods: i) it is unsupervised, so the learning is lack of data labelling cost; ii) it does not require data spatial augmentation, modality alignment, large number of batch size or epochs; iii) it applies data fusion only at inference; and iv) it does not require backbones pre-trained on emotion recognition task. The experiments on benchmark datasets show that our method outperforms several baseline approaches and unsupervised learning methods applied in MER. Particularly, it even surpasses a few supervised MER state-of-the-art. © 2022 IEEE.",Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Multimodal emotion recognition; Learning systems; Data fusion; Learning paradigms; Emotion expression; End to end; Real-world; Unsupervised learning; Automatic understanding; Data annotation; High reliability; Unsupervised feature learning,,,emotion,Yes,Yes
scopus,Subject independent emotion recognition using EEG and physiological signals – a comparative study,"Arthanarisamy Ramaswamy, M.P.; Palaniswamy, S.",2022,,,,10.1108/ACI-03-2022-0080,"Purpose: The aim of this study is to investigate subject independent emotion recognition capabilities of EEG and peripheral physiological signals namely: electroocoulogram (EOG), electromyography (EMG), electrodermal activity (EDA), temperature, plethysmograph and respiration. The experiments are conducted on both modalities independently and in combination. This study arranges the physiological signals in order based on the prediction accuracy obtained on test data using time and frequency domain features. Design/methodology/approach: DEAP dataset is used in this experiment. Time and frequency domain features of EEG and physiological signals are extracted, followed by correlation-based feature selection. Classifiers namely – Naïve Bayes, logistic regression, linear discriminant analysis, quadratic discriminant analysis, logit boost and stacking are trained on the selected features. Based on the performance of the classifiers on the test set, the best modality for each dimension of emotion is identified. Findings: The experimental results with EEG as one modality and all physiological signals as another modality indicate that EEG signals are better at arousal prediction compared to physiological signals by 7.18%, while physiological signals are better at valence prediction compared to EEG signals by 3.51%. The valence prediction accuracy of EOG is superior to zygomaticus electromyography (zEMG) and EDA by 1.75% at the cost of higher number of electrodes. This paper concludes that valence can be measured from the eyes (EOG) while arousal can be measured from the changes in blood volume (plethysmograph). The sorted order of physiological signals based on arousal prediction accuracy is plethysmograph, EOG (hEOG + vEOG), vEOG, hEOG, zEMG, tEMG, temperature, EMG (tEMG + zEMG), respiration, EDA, while based on valence prediction accuracy the sorted order is EOG (hEOG + vEOG), EDA, zEMG, hEOG, respiration, tEMG, vEOG, EMG (tEMG + zEMG), temperature and plethysmograph. Originality/value: Many of the emotion recognition studies in literature are subject dependent and the limited subject independent emotion recognition studies in the literature report an average of leave one subject out (LOSO) validation result as accuracy. The work reported in this paper sets the baseline for subject independent emotion recognition using DEAP dataset by clearly specifying the subjects used in training and test set. In addition, this work specifies the cut-off score used to classify the scale as low or high in arousal and valence dimensions. Generally, statistical features are used for emotion recognition using physiological signals as a modality, whereas in this work, time and frequency domain features of physiological signals and EEG are used. This paper concludes that valence can be identified from EOG while arousal can be predicted from plethysmograph. © 2022, Manju Priya Arthanarisamy Ramaswamy and Suja Palaniswamy.",Emotion recognition; Multimodal emotion recognition; EEG; EDA; Arousal; Valence; Respiration; Temperature; EMG; EOG; Plethysmograph; Subject independent,,,emotion,No,Yes
scopus,Cognitive Computing for Multimodal Sentiment Sensing and Emotion Recognition Fusion Based on Machine Learning Techniques Implemented by Computer Interface System,"Marandi, A.K.; Jethava, G.; Rajesh, A.; Gupta, S.; Sagar, S.; Sharma, S.",2022,,14,,10.17762/ijcnis.v14i2.5462,"In the last two decades, human emotion recognition and analysis have inspired a lot of interest, garnering a lot of research in neuroscience, psychology, cognitive science, and computer science. Multimodal emotion identification has gotten a lot of interest in field of affective computing, thanks to continued development of portable non-invasive human sensor technologies like BCI (brain–computer interfaces). This paper presents the cognitive computing based multimodal sentimental sensing and emotion recognition using machine learning architectures. Here the data has been collected based on the online review of the social media and twitter datasets. The collected dataset has been preprocessed and segmented to remove noise and unwanted numerical data. Then features have been extracted based on FFT (fast Fourier transform) based convolutional neural network (FFT-CNN) which extracts both numerical data and image data extraction. The extracted features have been classified based on transfer learning based SVM classification (TL-SVM). The experimental analysis has been carried out based on various datasets in terms of accuracy of 98%, precision of 88%, recall of 80%, F-1 score of 78% and mean square error of 45% for proposed FFT-CNN_TL-SVM. © 2022 Kohat University of Science and Technology. All rights reserved.",multimodal emotion recognition; BCI; FFT-CNN; sentimental sensing; TL-SVM,,,emotion,No,Yes
scopus,Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,"Zhao, Z.; Wang, Y.; Wang, Y.",2022,,2022-September,,10.21437/Interspeech.2022-10230,"The research and applications of multimodal emotion recognition have become increasingly popular recently. However, multimodal emotion recognition faces the challenge of lack of data. To solve this problem, we propose to use transfer learning which leverages state-of-the-art pre-trained models including wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including coattention-based early fusion and late fusion with the models trained on both embeddings are explored. Also, a multi-granularity framework which extracts not only frame-level speech embeddings but also segment-level embeddings including phone, syllable and word-level speech embeddings is proposed to further boost the performance. By combining our coattention-based early fusion model and late fusion model with the multi-granularity feature extraction framework, we obtain result that outperforms best baseline approaches by 1.3% unweighted accuracy (UA) on the IEMOCAP dataset. Copyright © 2022 ISCA.",Emotion Recognition; Embeddings; Speech recognition; Multimodal emotion recognition; multimodal emotion recognition; Fusion model; transfer learning; Transfer learning; Late fusion; Speech communication; Early fusion; Multi-granularity; Multi level fusion; multi-granularity framework; Multi-granularity framework; Research and application,,,emotion,No,Yes
scopus,SPEECH EMOTION RECOGNITION WITH CO-ATTENTION BASED MULTI-LEVEL ACOUSTIC INFORMATION,"Zou, H.; Si, Y.; Chen, C.; Rajan, D.; Chng, E.S.",2022,,2022-May,,10.1109/ICASSP43922.2022.9747095,"Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio information. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiLSTM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the proposed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub. © 2022 IEEE",Speech emotion recognition; Attention mechanisms; Emotion Recognition; Speech recognition; Multi-modal fusion; Multimodal fusion; Multilevels; End to end; Acoustic information; Speech emotion recognition systems; Audio information; Co-attention mechanism; Multi-level acoustic information,,,emotion,No,Yes
scopus,ER-MRL: Emotion Recognition based on Multimodal Representation Learning,"Guo, X.; Wang, Y.; Miao, Z.; Yang, X.; Guo, J.; Hou, X.; Zao, F.",2022,,,,10.1109/ICIST55546.2022.9926848,"In recent years, emotion recognition technology has been widely used in emotion change perception and mental illness diagnosis. Previous methods are mainly based on single-task learning strategies, which are unable to fuse multimodal features and remove redundant information. This paper proposes an emotion recognition model ER-MRL, which is based on multimodal representation learning. ER-MRL vectorizes the multimodal emotion data through encoders based on neural networks. The gate mechanism is used for multimodal feature selection. On this basis, ER-MRL calculates the modality specific and modality invariant representation for each emotion category. The Transformer model and multihead self-attention layer are applied to multimodal feature fusion. ER-MRL figures out the prediction result through the tower layer based on fully connected neural networks. Experimental results on the CMU-MOSI dataset show that ER-MRL has better performance on emotion recognition than previous methods.  © 2022 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Learning systems; Multimodal representation; Features fusions; feature fusion; Multilayer neural networks; Diseases; Multimodal features; Feature extraction; Learning strategy; multimodal representation; Mental illness; Change perceptions; gate mechanism; Gate mechanism; Single task learning,,,emotion,No,Yes
scopus,A Study on Discrete Action Sequences Using Deep Emotional Intelligence,"Santhoshkumar, R.; Geetha, M.K.",2022,,91,,,"Recognition of emotions from human plays a vital role in our day to day life and is essential for social communication. Automatic emotion recognition is becoming recent research focus on artificial intelligence. A facet of human intelligence is the ability to recognize emotion that is regarded as one of the attribute of emotional intelligence. Although research based on facial expressions or speech is seen in thrive, recognizing emotions from body gestures remains a less explored topic. This chapter proposes a machine learning approach and discussed with deep learning model to achieve emotional intelligence. The block based intensity value (BBIV) feature and the different bin level HoG feature (DBLHoG) are extracted from human body movements and are fed to a supervised learning algorithm. Support vector machine (SVM), k-nearest neighbor (KNN) and random forest classifiers are the supervised learning algorithm used in this chapter. Finally, the pre-trained deep convolutional neural network (DCNN) model is used. The experiment is conducted using Geneva multimodal emotion portrayals (GEMEP) corpus dataset. In this dataset, human body movement expressing the five archetypical emotions likes (anger, fear, joy, pride and sad). In this emotions recognition system, The random forest classifier outperformed better than the SVM and kNN classifier. Finally DCNN model achieve better recognition than random forest classifier. This chapter gives a brief study on achieving emotional intelligence with a DCNN Model. © 2022, Springer Nature Switzerland AG.",Emotion Recognition; Deep neural networks; Speech recognition; Convolutional neural networks; Convolutional neural network; Support vector machines; Support vectors machine; Learning algorithms; Decision trees; Emotional intelligence; Automatic emotion recognition; Neural network model; Nearest neighbor search; Recognition of emotion; Neural network models; Action sequences; Human body movement; Random forest classifier; Social communications,,,emotion,No,No
scopus,A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition,"Praveen, R.G.; De Melo, W.C.; Ullah, N.; Aslam, H.; Zeeshan, O.; Denorme, T.; Pedersoli, M.; Koerich, A.L.; Bacon, S.; Cardinal, P.; Granger, E.",2022,,2022-June,,10.1109/CVPRW56347.2022.00278,"Multimodal emotion recognition has recently gained much attention since it can leverage diverse and complementary modalities, such as audio, visual, and biosignals. However, most state-of-the- art audio-visual (A-V) fusion methods rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. This paper focuses on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos. We propose a joint cross-attention fusion model that can effectively exploit the complementary inter-modal relationships, allowing for an accurate prediction of valence and arousal. In particular, this model computes cross-attention weights based on the correlation between joint feature representations and individual modalities. By deploying a joint A-V feature representation into the cross-attention module, the performance of our fusion model improves significantly over the vanilla cross-attention module. Experimental results1 on the AffWild2 dataset highlight the robustness of our proposed A-V fusion model. It has achieved a concordance correlation coefficient (CCC) of 0.374 (0.663) and 0.363 (0.584) for valence and arousal, respectively, on the test set (validation set). This represents a significant improvement over the baseline for the third challenge of Affective Behavior Analysis in-the-Wild 2022 (ABAW3) competition, with a CCC of 0.180 (0.310) and 0.170 (0.170). © 2022 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; State of the art; Fusion model; Recurrent neural networks; Audio-visual fusion; Audio-visual; Computer vision; Feature representation; Attention model; Correlation coefficient; Biosignals,,,emotion,No,Yes
scopus,A Multimodal Corpus for Emotion Recognition in Sarcasm,"Ray, A.; Mishra, S.; Nunna, A.; Bhattacharyya, P.",2022,,,,,"While sentiment and emotion analysis have been studied extensively, the relationship between sarcasm and emotion has largely remained unexplored. A sarcastic expression may have a variety of underlying emotions. For example, “I love being ignored” belies sadness, while “my mobile is fabulous with a battery backup of only 15 minutes!” expresses frustration. Detecting the emotion behind a sarcastic expression is non-trivial yet an important task. We undertake the task of detecting the emotion in a sarcastic statement, which to the best of our knowledge, is hitherto unexplored. We start with the recently released multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions. We identify and correct 343 incorrect emotion labels (out of 690). We double the size of the dataset, label it with emotions along with valence and arousal which are important indicators of emotional intensity. Finally, we label each sarcastic utterance with one of the four sarcasm types-Propositional, Embedded, Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection research. Exhaustive experimentation with multimodal (text, audio, and video) fusion models establishes a benchmark for exact emotion recognition in sarcasm and outperforms the state-of-art sarcasm detection. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Character recognition; Sentiment analysis; multimodal; Emotion analysis; Audio and video; Emotion understanding; sarcasm; Sarcasm; Battery backup; Non-trivial; valence-arousal; Valence-arousal,,,emotion,Yes,No
scopus,Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition,"Dissanayake, V.; Seneviratne, S.; Suriyaarachchi, H.; Wen, E.; Nanayakkara, S.",2022,,2022-September,,10.21437/Interspeech.2022-11258,"Even with modern-day advanced machine learning techniques, Speech Emotion Recognition (SER) is a challenging task. Speech signals alone might not provide enough information to build robust emotion recognition models. The widespread usage of wearable devices provides multiple signal streams containing physiological and contextual cues, which could be incredibly beneficial to improving an SER system. However, research around multimodal emotion recognition with wearable and speech signals is limited. Also, the scarcity of annotated data for such scenarios limits the applicability of deep learning techniques. This paper presents a self-supervised fusion method for speech and wearable signals and evaluates its usage in the SER context. We further discuss three different fusion techniques in the context of multimodal emotion recognition. Our evaluations show that pretraining in the fusion stage significantly impacts the downstream emotion recognition task. Our method was able to achieve F1 Scores of 82.59% (arousal), 83.05% (valence) and 72.95% (emotion categories) for K-EmoCon dataset. Copyright © 2022 ISCA.",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; multimodal; Learning algorithms; Self-supervised learning; Wearable technology; Speech signals; Speech communication; self-supervised learning; speech emotion recognition; Signal fusions; Machine learning techniques; Paralinguistic; Computational paralinguistic; computational paralinguistics; Signal fusion,,,emotion,Yes,Yes
scopus,Emotion Recognition with Multimodal Transformer Fusion Framework Based on Acoustic and Lexical Information,"Guo, L.; Wang, L.; Dang, J.; Fu, Y.; Liu, J.; Ding, S.",2022,,29,,10.1109/MMUL.2022.3161411,"People usually express emotions through paralinguistic and linguistic information in speech. How to effectively integrate linguistic and paralinguistic information for emotion recognition is a challenge. Previous studies have adopted the bidirectional long short-term memory (BLSTM) network to extract acoustic and lexical representations followed by a concatenate layer, and this has become a common method. However, the interaction and influence between different modalities are difficult to promote using simple feature fusion for each sentence. In this article, we propose an implicitly aligned multimodal transformer fusion (IA-MMTF) framework based on acoustic features and text information. This model enables the two modalities to guide and complement each other when learning emotional representations. Thereafter, the weighed fusion is used to control the contributions of different modalities. Thus, we can obtain more complementary emotional representations. Experiments on the interactive emotional dyadic motion capture (IEMOCAP) database and multimodal emotionlines dataset (MELD) show that the proposed method outperforms the baseline BLSTM-based method.  © 2022 IEEE.",Transformer; Multi-modal; Speech recognition; Emotion recognition; Data mining; Features extraction; Linguistics; Feature extraction; Emotional representations; Acoustic information; Express emotions; Paralinguistic information; Linguistic information; Lexical information,,,emotion,No,No
scopus,Summarization of Videos from Online Events Based on Multimodal Emotion Recognition,"Abdrahimov, A.; Savchenko, A.V.",2022,,,,10.1109/RusAutoCon54946.2022.9896386,"In this paper, we propose a novel video summarization technique for automatic affect analysis of participants of an online event. At first, face verification neural network is used to cluster facial regions that correspond to each participant. Next, emotional features are extracted from each face by using EfficientNet model obtained in the previous paper of the author. The features of several consecutive frames are combined into a single descriptor that is used to classify emotions. In addition, audio features are extracted using wav2vec, and an ensemble of audio and video classifiers predicts emotions for each face. Finally, dependence of these emotions on time is visualized in special color charts. In the experimental study with the AFEW dataset it was demonstrated that the proposed approach makes it possible to obtain the best-known validation accuracy 67.88%. The models were optimized using OpenVINO and gained reasonable performance even if Nvidia GPUs are unavailable. The models and source code are publicly available at https://github.com/amirabdrahimov/multimodal-emotion-recognition. © 2022 IEEE.",facial expression recognition; Emotion Recognition; Speech recognition; Face recognition; Facial Expressions; Multimodal emotion recognition; multimodal emotion recognition; Facial expression recognition; Video recording; Neural-networks; Face Verification; Acted facial expression in the wild; AFEW (Acted Facial Expressions In The Wild); Affect analysis; Event-based; Facial regions; Program processors; video summarization; Video summarization,,,emotion,No,Yes
scopus,Real-time Human-Music Emotional Interaction Based on Multimodal Analysis,"Jiang, T.; Deng, S.; Wu, P.; Jiang, H.",2022,,,,10.1109/CoST57098.2022.00020,"Music, as an important part of the culture, occupies a significant position and can be easily accessed. The research on the sentiment represented by music and its effect on the listener's emotion is increasing gradually, but the existing research is often subjective and neglects the real-time expression of emotion. In this article, two labeled datasets are established. The deep learning method is used to classify music sentiment while the decision-level fusion method is used for real-time listener multimodal sentiment. We combine the sentiment analysis with a traditional online music playback system and propose innovatively a human-music emotional interaction system, using multimodal sentiment analysis based on the deep learning method. By means of individual observation and questionnaire survey, the interaction between human-music sentiments is proved to have a positive influence on listeners' negative emotions.  © 2022 IEEE.",Multi-modal; Modal analysis; Deep learning; Learning systems; Sentiment analysis; Music; Multimodal sentiment analyse; Multimodal sentiment analysis; Sentiment classification; Multimodal analysis; Real- time; Fusion; Surveys; Labeled dataset; Learning methods; Emotional interactions; Emotional Interaction; Music sentiment classification,,,emotion,No,No
scopus,Contextual and Cross-Modal Interaction for Multi-Modal Speech Emotion Recognition,"Yang, D.; Huang, S.; Liu, Y.; Zhang, L.",2022,,29,,10.1109/LSP.2022.3210836,"Speech emotion recognition combining linguistic content and audio signals in the dialog is a challenging task. Nevertheless, previous approaches have failed to explore emotion cues in contextual interactions and ignored the long-range dependencies between elements from different modalities. To tackle the above issues, this letter proposes a multimodal speech emotion recognition method using audio and text data. We first present a contextual transformer module to introduce contextual information via embedding the previous utterances between interlocutors, which enhances the emotion representation of the current utterance. Then, the proposed cross-modal transformer module focuses on the interactions between text and audio modalities, adaptively promoting the fusion from one modality to another. Furthermore, we construct associative topological relation over mini-batch and learn the association between deep fused features with graph convolutional network. Experimental results on the IEMOCAP and MELD datasets show that our method outperforms current state-of-the-art methods.  © 1994-2012 IEEE.",Speech emotion recognition; Transformer; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Character recognition; 'current; Cross-modal interaction; Convolution; Association reactions; Stackings; Convolutional networks; Graph convolutional network; speech emotion recognition; cross-modal interaction; graph convolutional network; Topology; Contextual interaction,,,emotion,No,No
scopus,Emotion Recognition for Multiple Context Awareness,"Yang, D.; Huang, S.; Wang, S.; Liu, Y.; Zhai, P.; Su, L.; Li, M.; Zhang, L.",2022,,13697 LNCS,,10.1007/978-3-031-19836-6_9,"Understanding emotion in context is a rising hotspot in the computer vision community. Existing methods lack reliable context semantics to mitigate uncertainty in expressing emotions and fail to model multiple context representations complementarily. To alleviate these issues, we present a context-aware emotion recognition framework that combines four complementary contexts. The first context is multimodal emotion recognition based on facial expression, facial landmarks, gesture and gait. Secondly, we adopt the channel and spatial attention modules to obtain the emotion semantics of the scene context. Inspired by sociology theory, we explore the emotion transmission between agents by constructing relationship graphs in the third context. Meanwhile, we propose a novel agent-object context, which aggregates emotion cues from the interactions between surrounding agents and objects in the scene to mitigate the ambiguity of prediction. Finally, we introduce an adaptive relevance fusion module for learning the shared representations among multiple contexts. Extensive experiments show that our approach outperforms the state-of-the-art methods on both EMOTIC and GroupWalk datasets. We also release a dataset annotated with diverse emotion labels, Human Emotion in Context (HECO). In practice, we compare with the existing methods on the HECO, and our approach obtains a higher classification average precision of 50.65% and a lower regression mean error rate of 0.7. The project is available at https://heco2022.github.io/. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Semantics; Emotion Recognition; Speech recognition; Emotion recognition; Hotspots; Uncertainty; Human emotion; Computer vision; Context- awareness; Context representation; Context understanding; In contexts; Multiple contexts; Vision communities,,,emotion,Yes,No
scopus,Depressive Emotion Tendency Detection for Users on Social Platform Based on Fusion of Graph and Text,"Yan, J.; Shu, X.; Shu, J.",2022,,,,10.1109/ICAIBD55127.2022.9820498,"Depression is mainly manifested in negative emotions, physical fatigue, and often suicidal ideation. In recent years, more and more patients with depression have posted on social platform to express their emotions. Based on this, a method is proposed to detect depressive emotion tendency for users on social platform based on fusion of graphic and text features (FGTF-SP-DDET). In the method, the Weibo User Depression Detection Dataset (WU3D) was simplified and preprocessed to fit the model. The main body of the image feature learning module is the improved VGG-16 model, and the main body of the text feature learning module is BERT, BiLSTM combined with Attention. In order to make full use of the emotion feature of different modalities, the extracted features were fused by the method of early fusion. The comprehensive depressive emotion tendency detection was carried out by classifier. Experimental results show that FGTF-SP-DDET outperforms single-feature based detection methods in accuracy, precision, recall and F1. Compared with the single-feature method, the image-text feature fusion method can effectively detect the depressive emotion tendency of social platform users.  © 2022 IEEE.",Learning systems; Image enhancement; Image features; Feature extraction; Multimodal feature fusions; Learning modules; Feature learning; Text feature; Main bodies; Physical fatigues; depressive tendency test; Depressive tendency test; multimodal feature fusion; social platform; Social platform; Suicidal ideation,,,emotion,No,No
scopus,Detecting emotions from human speech: role of gender information,"Gupta, M.; Patel, T.; Mankad, S.H.; Vyas, T.",2022,,,,10.1109/TENSYMP54529.2022.9864557,"For a human being, emotion is an expressive gesture through which we communicate and understand each other. Emotions constitute a major part in our day-to-day communication and other activities. In this work, we implement speech-based emotion recognition (SER) systems to study the role of gender information and augmentation on system performance. Experiments are conducted on a dataset prepared by combination of four publicly available datasets, Toronto Emotional Speech Set (TESS), Surrey Audio-Visual Expressed Emotion (SAVEE), Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), and Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D). Results indicate the efficacy of LSTM based approach specifically on female gender on augmented dataset.  © 2022 IEEE.",Long short-term memory; Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Audio-visual; Emotional speech; Mel frequency cepstral co-efficient; Mel-frequency cepstral coefficient; Mel-frequency cepstral coefficients; Toronto; Convolution neural network; Audio-visual database; CREMA-D; RAVDESS; Ryerson audio-visual database of emotional speech and song; Speech Emotion Recognition (SER); Convolution neural network (CNN); Crowd-sourced emotional multimodal actor dataset; Long short-term memory (LSTM); Mel-Frequency Cepstral Coefficients (MFCC); SAVEE; Surrey audio-visual expressed emotion; TESS; Toronto emotional speech set,,,emotion,No,Yes
scopus,Can We Exploit All Datasets? Multimodal Emotion Recognition Using Cross-Modal Translation,"Yoon, Y.C.",2022,,10,,10.1109/ACCESS.2022.3183587,"The use of sufficiently large datasets is important for most deep learning tasks, and emotion recognition tasks are no exception. Multimodal emotion recognition is the task of considering multiple types of modalities simultaneously to improve accuracy and robustness, typically utilizing three modalities: visual, audio, and text. Similar to other deep learning tasks, large datasets are required. Various heterogeneous datasets exist, including unimodal datasets constructed for traditional unimodal recognition and bimodal or trimodal datasets for multi-modal emotion recognition. A trimodal emotion recognition model shows high performance and robustness by comprehensively considering multiple modalities. However, the use of unimodal or bimodal datasets in this case is problematic. In this study, we propose a novel method to improve the performance of emotion recognition based on a cross-modal translator that can translate between the three modalities. The proposed method can train a multimodal model based on three modalities with different types of heterogeneous datasets, and the dataset does not require alignment between modalities: visual, audio, and text. We achieved a high performance exceeding the baseline in CMU-MOSEI and IEMOCAP, which are representative multimodal datasets, by adding unimodal and bimodal datasets to the trimodal dataset.  © 2013 IEEE.",Semantics; Transformer; Emotion Recognition; Performance; Speech recognition; emotion recognition; Emotion recognition; Deep learning; Character recognition; Multimodal emotion recognition; Unimodal; Learning systems; Job analysis; Task analysis; multimodal emotion recognition; machine learning; Cross-modal; Machine-learning; Generative adversarial networks; generative adversarial networks,,,emotion,No,Yes
scopus,MT-TCCT: Multi-task Learning for Multimodal Emotion Recognition,"Wang, Y.; Chen, Z.; Chen, S.; Zhu, Y.",2022,,13531 LNCS,,10.1007/978-3-031-15934-3_36,"Multimodal emotion recognition is an emerging research field, which aims to capture affective information from multimodal data, such as natural language, facial expression, and voice intonation. However, most existing methods focus more on modality-common information, and the modality-specific features are neglected. How to learn the two kinds of modal features effectively is a challenging problem. In this paper, we introduce unimodal sub-tasks and present a multi-task framework to provide subspaces to learn modality-private and modality-shared features respectively. These represen-tations provide a holistic view for emotion recognition. Besides, modal fragment-absence is also a challenging problem, as text modality has been proven to have more affective features, a text-centered feature reconstruction module employing cross-modal attention mechanism is designed to adapt semantics from one modality to another. To evaluate the performance of the proposed model, the experiments are conducted on CH-SIMS and CMU-MOSI datasets. The results validate the efficiency and generalization of the multi-task training-based framework. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Semantics; Emotion Recognition; Speech recognition; Multi-modal data; Multimodal emotion recognition; Learning systems; Multi tasks; Learn+; Cross-modal; Multitask learning; Cross-modal attention; Research fields; Learning network; Multi-task learning network,,,emotion,No,Yes
scopus,Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition,"Yang, W.; Fukayama, S.; Heracleous, P.; Ogata, J.",2022,,2022-September,,10.21437/Interspeech.2022-10354,"Speech-based multimodal affective computing has recently attracted significant research attention. Previous experimental results have shown that the audio-only approach exhibits inferior performance than the text-only approach in sentiment analysis and emotion recognition tasks. In this paper, we propose a new strategy to improve the performance of uni-modal and bi-modal affective computing systems via fine-tuning of two pre-trained self-supervised learning models (Text-RoBERTa and Speech-RoBERTa). We fine-tune the models on sentiment analysis and emotion recognition tasks using a shallow architecture, and apply crossmodal attention fusion to the models for further learning and final prediction or classification. We evaluate our proposed method on the CMU-MOSI, CMU-MOSEI and IEMOCAP datasets. The experimental results demonstrate that our approach exhibits superior performance for all benchmarks compared to existing state-of-the-art results, establishing the effectiveness of the proposed method. Copyright © 2022 ISCA.",Speech emotion recognition; Emotion Recognition; Performance; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Character recognition; Multimodal emotion recognition; multimodal sentiment analysis; Learning systems; Sentiment analysis; multimodal emotion recognition; Benchmarking; Cross-modal; Multimodal sentiment analyse; Self-supervised learning; Supervised learning; Speech communication; self-supervised learning; crossmodal transformer; Crossmodal transformer,,,emotion,No,Yes
scopus,Context-Aware Multimodal Emotion Recognition,"Khalane, A.; Shaikh, T.",2022,,350,,10.1007/978-981-16-7618-5_5,"Making human–computer interaction more organic and personalized for users essentially demands advancement in human emotion recognition. Emotions are perceived by humans considering multiple factors such as facial expressions, voice tonality, and information context. Although significant research has been conducted in the area of unimodal/multimodal emotion recognition in videos using acoustic/visual features, few papers have explored the potential of textual information obtained from the video utterances. Humans experience emotions through their audio-visual and linguistic senses, making it quintessential to take the latter into account. This paper outlines two different algorithms for recognizing multimodal emotional expressions in online videos. In addition to acoustic (speech), visual (facial), and textual (utterances) feature extraction using BERT, we utilize bidirectional LSTMs to capture the context between utterances. To obtain richer sequential information, we also implement a multi-head self-attention mechanism. Our analysis utilizes the benchmarking CMU multimodal opinion sentiment and emotion intensity (CMU-MOSEI) dataset, which is the largest dataset for sentiment analysis and emotion recognition to date. Our experiments result in improved F1 scores in comparison to the baseline models. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",BERT; Emotion; Multimodal; Multi-head attention; Recognition; CMU-MOSEI; Context-aware,,,emotion,No,Yes
scopus,MEMOBERT: PRE-TRAINING MODEL WITH PROMPT-BASED LEARNING FOR MULTIMODAL EMOTION RECOGNITION,"Zhao, J.; Li, R.; Jin, Q.; Wang, X.; Li, H.",2022,,2022-May,,10.1109/ICASSP43922.2022.9746910,"Multimodal emotion recognition study is hindered by the lack of labelled corpora in terms of scale and diversity, due to the high annotation cost and label ambiguity. In this paper, we propose a multimodal pre-training model MEmoBERT for multimodal emotion recognition, which learns multimodal joint representations through self-supervised learning from a self-collected large-scale unlabeled video data that come in sheer volume. Furthermore, unlike the conventional “pre-train, finetune” paradigm, we propose a prompt-based method that reformulates the downstream emotion classification task as a masked text prediction one, bringing the downstream task closer to the pre-training. Extensive experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, show that our proposed MEmoBERT significantly enhances emotion recognition performance. © 2022 IEEE",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Character recognition; Multimodal emotion recognition; Learning systems; Learn+; Multimodal; Benchmarking; Video data; Pre-training; Down-stream; Large-scales; Training model; Prompt,,,emotion,Yes,Yes
scopus,A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset,"Luna-Jiménez, C.; Kleinlein, R.; Griol, D.; Callejas, Z.; Montero, J.M.; Fernández-Martínez, F.",2022,,12,,10.3390/app12010327,"Emotion recognition is attracting the attention of the research community due to its multiple applications in different fields, such as medicine or autonomous driving. In this paper, we proposed an automatic emotion recognizer system that consisted of a speech emotion recognizer (SER) and a facial emotion recognizer (FER). For the SER, we evaluated a pre-trained xlsr-Wav2Vec2.0 transformer using two transfer-learning techniques: embedding extraction and fine-tuning. The best accuracy results were achieved when we fine-tuned the whole model by appending a multilayer perceptron on top of it, confirming that the training was more robust when it did not start from scratch and the previous knowledge of the network was similar to the task to adapt. Regarding the facial emotion recognizer, we extracted the Action Units of the videos and compared the performance between employing static models against sequential models. Results showed that sequential models beat static models by a narrow difference. Error analysis reported that the visual systems could improve with a detector of high-emotional load frames, which opened a new line of research to discover new ways to learn from videos. Finally, combining these two modalities with a late fusion strategy, we achieved 86.70% accuracy on the RAVDESS dataset on a subject-wise 5-CV evaluation, classifying eight emotions. Results demonstrated that these modalities carried relevant information to detect users’ emotional state and their combination allowed to improve the final system performance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Speech emotion recognition; Transformer; Human–computer interaction; Transfer learning; Facial emotion recognition; Audio-visual emotion recognition; RAVDESS; Action Units; Computational paralin-guistics; Xlsr-Wav2Vec2.0 transformer,,,emotion,No,Yes
scopus,Recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition,"Ahn, C.-S.; Kasun, L.L.C.; Sivadas, S.; Rajapakse, J.C.",2022,,2022-September,,10.21437/Interspeech.2022-888,"To infer emotions accurately from speech, fusion of audio and text is essential as words carry most information about semantics and emotions. Attention mechanism is essential component in multimodal fusion architecture as it dynamically pairs different regions within multimodal sequences. However, existing architecture lacks explicit structure to model dynamics between fused representations. Thus we propose recurrent multi-head attention in a fusion architecture, which selects salient fused representations and learns dynamics between them. Multiple 2-D attention layers select salient pairs among all possible pairs of audio and text representations, which are combined with fusion operation. Lastly, multiple fused representations are fed into recurrent unit to learn dynamics between fused representations. Our method outperforms existing approaches for fusion of audio and text for speech emotion recognition and achieves state-of-the-art accuracies on benchmark IEMOCAP dataset. Copyright © 2022 ISCA.",Semantics; Speech emotion recognition; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Character recognition; Multi-modal fusion; Human computer interaction; Learn+; multimodal fusion; human-computer interaction; Network architecture; Speech communication; Dynamics; speech emotion recognition; Paralinguistic; Computational paralinguistic; computational paralinguistics; Existing architectures; Fusion architecture; fusion of audio and text; Fusion of audio and text,,,emotion,No,Yes
scopus,EMOTIONFLOW: CAPTURE THE DIALOGUE LEVEL EMOTION TRANSITIONS,"Song, X.; Zang, L.; Zhang, R.; Hu, S.; Huang, L.",2022,,2022-May,,10.1109/ICASSP43922.2022.9746464,"Emotion recognition in conversations (ERC) has attracted increasing interests in recent years, due to its wide range of applications, such as customer service analysis, healthcare consultation, etc. One key challenge of ERC is that users' emotions would change due to the impact of others' emotions. That is, the emotions within the conversation can spread among the communication participants. However, the spread impact of emotions in a conversation is rarely addressed in existing researches. To this end, we propose EmotionFlow for ERC with the consideration of the spread of participants' emotions during a conversation. EmotionFlow first encodes users' utterance by concatenating the context with an auxiliary question, which helps to learn user-specific features. Then, conditional random field is applied to capture the sequential information at emotional level. We conduct extensive experiments on a public dataset Multimodal EmotionLines Dataset (MELD), and the results demonstrate the effectiveness of our proposed model. © 2022 IEEE",Natural language processing; Emotion Recognition; Speech recognition; Emotion recognition; Learn+; Speech processing; Language processing; Natural languages; Natural Language Processing; Dialogue systems; Customer-service; User emotions; Dialogue System; Random fields; Service analysis,,,emotion,No,No
scopus,CNN-Based Emotional Stress Classification using Smart Learning Dataset,"Andreas, A.; Mavromoustakis, C.X.; Song, H.; Batalla, J.M.",2022,,,,10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics55523.2022.00107,"Smart learning analytics aims to support researchers investigating mental health by improving the interpretation of the datasets acquired from physiological biomarkers. The key enabler for emotional stress classification are Machine Learning (ML) methods in conjunction with Online Transfer Learning (OTL). The knowledge of high-level characteristics at the top layers is obtained through an optimized Convolutional Neural Network (CNN)-based on emotional stress datasets. Nevertheless, the lack of performance in a real-time environment and the temporal patterns of data acquisition complications and their interpretation motivated us to contribute by tackling these concerns. Therefore, we propose an innovative procedure based on the aforementioned orientation through our research work. Considering mining data streams with concept drifts, we enable the ensemble classifiers. For evaluation, we compare the proposed classification, the LIBrary for Large LINEAR classification (LIBLINEAR) and the Deep Belief Network with Transfer Learning (DBNTL) model. Furthermore, we utilized a multimodal dataset of physical and biological characteristics obtained by fifteen individuals during a lab study. Finally, our framework based on the extracted results has presented more accuracy in classifying an individual's sense of stress. Hence, the proposed method achieves higher efficiency than the state-of-the-art models.  © 2022 IEEE.",emotion; CNN; Emotion; Deep learning; Network-based; Learning systems; Classification (of information); Convolutional neural networks; Convolutional neural network; Multilayer neural networks; Data acquisition; Transfer learning; stress; E-learning; classification; Stress classifications; Emotional stress; Learning dataset; online transfer learning; Online transfer learning; OTL,,,emotion,No,Yes
scopus,A Retrospective CNN-LSVM Hybrid Approach for Multimodal Emotion Recognition,"Gill, R.; Singh, J.; Modgill, A.",2022,,,,10.1109/DASA54658.2022.9765173,"Emotions play a vital role in a person's life and it affects their physically. Therefore, emotions can be manipulated by measuring customer perception of a place. People use facial expressions as a tool to express emotional states. Face recognition is always an exciting and challenging area for research on computer perspective. In this study, seven emotions such as anger, disgust, fear, happy, sad, surprise and neutral states have been classified through facial expression images. This paper presents a novel CNN-LSVM hybrid approach for multimodal emotion recognition. The proposed approach uses two datasets namely CK+ and FER-2013 images for emotion recognition. During CNN experimentation, several texture features have been extracting which have been used for classification purposes. The classification of texture features is achieved through the linear support vector machine technique. The LSVM model is responsible to classify the texture features. Throughout CNN, different hyperparameters such as batch size, epochs, and momentum have been used. With help of the CNN-LSVM approach, 92.3% average classification accuracy for different emotions has been achieved. © 2022 IEEE.",Machine learning; Facial expressions; Speech recognition; Behavioral research; Face recognition; Facial Expressions; Deep learning; Multimodal emotion recognition; Classification (of information); Convolutional neural networks; Convolutional neural network; Support vector machines; Support vectors machine; Textures; Deep learning (DL); Machine learning (ML); Hybrid approach; Support vector machine; Convolutional neural networks(CNN); Customer perceptions; Support vector machine (SVM); Texture features,,,emotion,No,Yes
scopus,Data Augmentation for Audio–Visual Emotion Recognition with an Efficient Multimodal Conditional GAN,"Ma, F.; Li, Y.; Ni, S.; Huang, S.; Zhang, L.",2022,,12,,10.3390/app12010527,"Audio–visual emotion recognition is the research of identifying human emotional states by combining the audio modality and the visual modality simultaneously, which plays an important role in intelligent human–machine interactions. With the help of deep learning, previous works have made great progress for audio–visual emotion recognition. However, these deep learning methods often require a large amount of data for training. In reality, data acquisition is difficult and expensive, especially for the multimodal data with different modalities. As a result, the training data may be in the low-data regime, which cannot be effectively used for deep learning. In addition, class imbalance may occur in the emotional data, which can further degrade the performance of audio–visual emotion recognition. To address these problems, we propose an efficient data augmentation framework by designing a multimodal conditional generative adversarial network (GAN) for audio–visual emotion recognition. Specifically, we design generators and discriminators for audio and visual modalities. The category information is used as their shared input to make sure our GAN can generate fake data of different categories. In addition, the high dependence between the audio modality and the visual modality in the generated multimodal data is modeled based on Hirschfeld–Gebelein–Rényi (HGR) maximal correlation. In this way, we relate different modalities in the generated data to approximate the real data. Then, the generated data are used to augment our data manifold. We further apply our approach to deal with the problem of class imbalance. To the best of our knowledge, this is the first work to propose a data augmentation strategy with a multimodal conditional GAN for audio–visual emotion recognition. We conduct a series of experiments on three public multimodal datasets, including eNTERFACE’05, RAVDESS, and CMEW. The results indicate that our multimodal conditional GAN has high effectiveness for data augmentation of audio–visual emotion recognition. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Audio; Data augmentation; Hirschfeld-Gebelein-Rényi (HGR) maximal correlation; Multimodal conditional generative adversarial network (GAN); Visual emotion recognition,,,emotion,No,Yes
scopus,ReferEmo: A Referential Quasi-multimodal Model for Multilabel Emotion Classification,"Esperanca, A.; Luo, X.",2022,,13426 LNCS,,10.1007/978-3-031-12423-5_27,"Textual emotion classification is a task in affective AI that branches from sentiment analysis and focuses on identifying emotions expressed in a given text excerpt. It has a wide variety of applications that improve human-computer interactions, particularly to empower computers to understand subjective human language better. Significant research has been done on this task, but very little of that research leverages one of the most emotion-bearing symbols we have used in modern communication: Emojis. In this research, we propose ReferEmo, a model that processes Emojis as textual inputs and leverages DeepMoji to generate affective feature vectors used as reference when aggregating different modalities of text encoding. To evaluate ReferEmo, we experimented on two benchmark datasets: SemEval’18 and GoEmotions for emotion classification, and achieved competitive performance compared to state-of-the-art models tested on these datasets. Notably, our model performs better on the underrepresented classes of each dataset. The source code of ReferEmo is available on Github (https://github.com/alvarosness/ReferEmo ). © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Natural language processing; Human computer interaction; Sentiment analysis; Classification (of information); Benchmarking; Emotion classification; Language processing; Natural languages; AI; Multimodal models; Multi-labels; Features vector; Multimodal model; Human language; Multilabel,,,emotion,No,Yes
scopus,REPRESENTATION LEARNING THROUGH CROSS-MODAL CONDITIONAL TEACHER-STUDENT TRAINING FOR SPEECH EMOTION RECOGNITION,"Srinivasan, S.; Huang, Z.; Kirchhoff, K.",2022,,2022-May,,10.1109/ICASSP43922.2022.9747754,"Generic pre-trained speech and text representations promise to reduce the need for large labeled datasets on specific speech and language tasks. However, it is not clear how to effectively adapt these representations for speech emotion recognition. Recent public benchmarks show the efficacy of several popular self-supervised speech representations for emotion classification. In this study, we show that the primary difference between the top-performing representations is in predicting valence while the differences in predicting activation and dominance dimensions are less pronounced. However, we show that even the best-performing HuBERT representation underperforms on valence prediction compared to a multimodal model that also incorporates text representation. We address this shortcoming by injecting lexical information into the speech representation using the multimodal model as a teacher. To improve the efficacy of our approach, we propose a novel estimate of the quality of the emotion predictions, to condition teacher-student training. We report new audio-only state-of-the-art concordance correlation coefficient (CCC) values of 0.757, 0.627, 0.671 for activation, valence and dominance predictions, respectively, on the MSP-Podcast corpus, and also state-of-the-art values of 0.667, 0.582, 0.545 on the IEMOCAP corpus. © 2022 IEEE",Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; Character recognition; State of the art; Cross-modal; Large dataset; Forecasting; Teachers'; Text representation; Personnel training; Representation learning; Multimodal models; multi-modal; speech emotion recognition; Labeled dataset; Chemical activation; Student training,,,emotion,No,Yes
scopus,Method for Conversational Emotion Recognition Using Hierarchical R-GCN,"Lai, H.; Li, L.; Hu, W.; Yan, X.",2022,,48,,10.19678/j.issn.1000-3428.0060346,"Conversational emotion recognition should consider not only the emotions of the speakers themselves, but also the emotions passing between speakers.This paper proposes an emotion recognition method based on Hierarchical Relational Graph Convolutional Network(HRGCN), which considers both two types of emotions to improve the recognition performance.The method employs a Basic Neural Network(BNN) to optimize the feature data of the conversational sequence, and divides the sequence into two different conversational subsequences according to the speaker.Two local Relational Graph Convolutional Networks(R-GCN) are used for local modelling of these two subsequences respectively, and the two locally modeled subsequences are reconcatenated in chronological order of the conversation.Furthermore, the global R-GCN is used to model the reconcatenated sequence globally.through hierarchical modeling of the input multimodal feature data, HRGCN can capture more contextual information.The experimental results on the IEMOCAP dataset show that HRGCN displays an accuracy of 84.48% and a F1 score of 84.40%, higher than LSTM, GRU and other mainstream recurrent neural networks. © 2022, Editorial Office of Computer Engineering. All rights reserved.",Emotion recognition; Artificial intelligence; Conversation; Basic Neural Network(BNN); Relational Graph Convolutional Network(R-GCN),,,emotion,No,Yes
scopus,Research on Music Emotional Expression Based on Reinforcement Learning and Multimodal Information,"Zhang, L.; Tian, Z.",2022,,2022,,10.1155/2022/2616220,"With the continuous development of the research in the field of emotion analysis, music, as a common multimodal information carrier in people's daily life, often transmits emotion through lyrics and melody, so it has been gradually incorporated into the research category of emotion analysis. The fusion classification model based on CNN-LSTM proposed in this paper effectively improves the accuracy of emotional classification of audio and lyrics. At the same time, in view of the problem that the traditional decision-level fusion method ignores the correlation between modes and the limitations of dataset, this paper further improves the existing Thayer dimension emotional decision fusion method, takes the audio energy axis data as the main discrimination basis, and improves the accuracy of decision fusion classification. Based on the results of music emotion analysis, this paper further carries out the task of music generation. Based on the feature that there is often consistent emotional expression between music words and songs, a dual Seq2Seq framework based on reinforcement learning is constructed. By introducing the reward value of emotional consistency and content fidelity, the output melody has the same emotion with the input lyrics and good results are achieved. Compared with the ordinary Seq2Seq, the accuracy of our proposed model is improved by about 1.1%. This shows that the accuracy of the model can be effectively improved by using reinforcement learning.  © 2022 Lige Zhang and Zhen Tian.",Long short-term memory; Emotion Recognition; Multi-modal information; Classification (of information); Emotion analysis; Music; Reinforcement learning; Audio acoustics; Reinforcement learnings; Daily lives; Emotional expressions; Model-based OPC; Classification models; Continuous development; Fusion classification; Information carriers,,,emotion,No,Yes
scopus,Analyzing the Role of Emotional Intelligence on the Performance of Small and Medium Enterprises (SMEs) Using AI-Based Convolutional Neural Networks (CNNs),"Serbaya, S.H.",2022,,2022,,10.1155/2022/7951676,"Human emotion detection is necessary for social interaction and plays an important role in our daily lives. Artificial intelligence research is rising, focusing on automated emotion detection. The capability to identify the emotion, which is considered one of the traits of emotional intelligence, is a component of human intelligence. Although the study is limited dependent on facial expressions or voice is flourishing, it is identifying emotions via body movements, a less researched issue. To attain emotional intelligence, this study suggests a deep learning approach. Here initially the video can be converted into image frames after the converted image frames can be preprocessed using the Glitter bandpass butter worth filter and contrast stretch histogram equalization. Then from the enhanced image, the features can be clustered using the hybrid Gaussian BIRCH algorithm. Then the specialized features are retrieved from the body of human gestures using the AdaDelta bacteria foraging optimization algorithm, and the selected features are fed to a supervised Kernel Boosting LENET deep-learning algorithm. The experiment is conducted using Geneva multimodal emotion portrayals (GEMEPs) corpus data set. This data set includes, human body gestures portraying the archetypes of five emotions, such as anger, fear, joy, pride, and sad. In these emotion detection techniques, the suggested Kernel Boosting LENET classifier achieves 98.5% accuracy, 94% precision, 95% sensitivity, and F-Score 93% outperformed better than the other existing classifiers. As a result, emotional acknowledgment may help small and medium enterprises (SMEs) to improve their performance and entrepreneurial orientation. The correlation coefficient of 188 and the significance coefficient of 0.00 show that emotional intelligence and SMEs performance have a significant and positive association. © 2022 Suhail H. Serbaya.",Emotion Recognition; Performance; Deep learning; Emotion detection; Convolutional neural networks; Convolution; Convolutional neural network; Image enhancement; Human emotion; Emotional intelligence; Data set; Social interactions; Daily lives; Image frames; Small-and-medium enterprise,,,emotion,No,No
scopus,A Multitask Multimodal Framework for Sentiment and Emotion-Aided Cyberbullying Detection,"Maity, K.; Kumar, A.; Saha, S.",2022,,26,,10.1109/MIC.2022.3158583,"Cyberbullying has become more widespread, especially among teens with the growth of the digital sphere and advancement of technology. This article is the first attempt in investigating the role of sentiment and emotion information for identifying cyberbullying in the Indian scenario. From Twitter, a benchmark Hind-English code-mixed corpus called BullySentEmo has been developed as there was no dataset available labeled with bully, sentiment, and emotion. The developed dataset consists of both the modalities, tweet- text and emoji. In India, the majority of communication on different social media platforms is based on Hindi and English, and language switching is a common practice in digital communication. A multitask multimodal framework called MT-MM-Bert+VecMap based on BERT and VecMap embedding schemes with emoji modality, has been developed. Our proposed multitask-multimodal framework outperforms all the single task and unimodal baselines with the highest accuracy values of 82.05(+/- 1.36)%, 77.87(+/- 1.93)%, and 58.05(+/-2.78)% for the cyberbully detection task, sentiment analysis task, and emotion recognition task, respectively.  © 1997-2012 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Data mining; Sentiment analysis; Task analysis; Multi-modality; Bit error rate; Bit-error rate; Sentiment Analysis; Blogs; Codes (symbols); Annotation; Computer crime; Cyber bullying; Cyberbullying; Code; Code-mixed; Code-Mixed; Multi tasking; Multi-tasking,,,emotion,Yes,Yes
scopus,Deep Residual Adaptive Neural Network Based Feature Extraction for Cognitive Computing with Multimodal Sentiment Sensing and Emotion Recognition Process,"Arora, G.; Sabharwal, M.; Kapila, P.; Paikaray, D.; Vekariya, V.; Narmadha, T.",2022,,14,,10.17762/ijcnis.v14i2.5507,"For the healthcare framework, automatic recognition of patients' emotions is considered to be a good facilitator. Feedback about the status of patients and satisfaction levels can be provided automatically to the stakeholders of the healthcare industry. Multimodal sentiment analysis of human is considered as the attractive and hot topic of research in artificial intelligence (AI) and is the much finer classification issue which differs from other classification issues. In cognitive science, as emotional processing procedure has inspired more, the abilities of both binary and multi-classification tasks are enhanced by splitting complex issues to simpler ones which can be handled more easily. This article proposes an automated audio-visual emotional recognition model for a healthcare industry. The model uses Deep Residual Adaptive Neural Network (DeepResANNet) for feature extraction where the scores are computed based on the differences between feature and class values of adjacent instances. Based on the output of feature extraction, positive and negative sub-nets are trained separately by the fusion module thereby improving accuracy. The proposed method is extensively evaluated using eNTERFACE'05, BAUM-2 and MOSI databases by comparing with three standard methods in terms of various parameters. As a result, DeepResANNet method achieves 97.9% of accuracy, 51.5% of RMSE, 42.5% of RAE and 44.9%of MAE in 78.9sec for eNTERFACE'05 dataset. For BAUM-2 dataset, this model achieves 94.5% of accuracy, 46.9% of RMSE, 42.9%of RAE and 30.2% MAE in 78.9 sec. By utilizing MOSI dataset, this model achieves 82.9% of accuracy, 51.2% of RMSE, 40.1% of RAE and 37.6% of MAE in 69.2sec. By analysing all these three databases, eNTERFACE'05 is best in terms of accuracy achieving 97.9%. BAUM-2 is best in terms of error rate as it achieved 30.2 % of MAE and 46.9% of RMSE. Finally MOSI is best in terms of RAE and minimal response time by achieving 40.1% of RAE in 69.2 sec. © 2022 Kohat University of Science and Technology. All rights reserved.",emotion recognition; feature extraction; neural network; healthcare; sentimental sensing; cognitive computing,,,emotion,No,Yes
scopus,1D Convolutional Autoencoder-Based PPG and GSR Signals for Real-Time Emotion Classification,"Kang, D.-H.; Kim, D.-H.",2022,,10,,10.1109/ACCESS.2022.3201342,"To apply emotion recognition and classification technology to the field of human-robot interaction, it is necessary to implement fast data processing and model weight reduction. This paper proposes a new photoplethysmogram (PPG) and galvanic skin response (GSR) signals-based labeling method using Asian multimodal data, a real-time emotion classification method, a 1d convolutional neural network autoencoder model, and a lightweight model obtained using knowledge distillation. In addition, the model performance was verified using the public DEAP dataset and the Asian multi-modal dataset 'MERTI-Apps'. For emotion classification, bio-signal data were window-sliced in 1-pulse units, and the label was reset to reflect the characteristics of the PPG and GSR signals. Simple data pre-processing, such as the prevention of loss and waveform duplication, was performed without using handcrafted features. The experiment showed that the accuracy of the proposed model using MERTI-Apps was 79.18% and 74.84% in the case of arousal and valence, respectively, for 3-class criteria, and the accuracy of the proposed model using DEAP was 81.33% and 80.25% in the case of arousal and valence, respectively, for 2-class criteria. The accuracy of the lightweight model was 77.87% and 73.49% in the case of arousal and valence, respectively, for 3-class criteria and its calculation time was reduced by more than 80% compared to the proposed 1d convolutional autoencoder model. We also confirmed that the proposed model improved computational time and accuracy compared to previous studies using MERTI-Apps and the lightweight model used in limited hardware environments enabled fast computation and real-time emotion classification.  © 2013 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Learning systems; Signal encoding; Human robot interaction; Classification (of information); Features extraction; Auto encoders; Electroencephalography; Electrophysiology; Brain modeling; Computational modelling; Convolution; Data handling; Knowledge management; PPG; Distillation; Real- time; Feature extraction; Galvanic skin response; GSR; Neural networks; Encodings; Knowledge distillation; Real time systems; Interactive computer systems; knowledge distillation; 1d convolutional autoencoder; 1D convolutional autoencoder; Biological system modeling; Biological systems; Photoplethysmogram; Real - Time system; real-time,,,emotion,Yes,Yes
scopus,Deep Auto-Encoders With Sequential Learning for Multimodal Dimensional Emotion Recognition,"Nguyen, D.; Nguyen, D.T.; Zeng, R.; Nguyen, T.T.; Tran, S.N.; Nguyen, T.; Sridharan, S.; Fookes, C.",2022,,24,,10.1109/TMM.2021.3063612,"Multimodal dimensional emotion recognition has drawn a great attention from the affective computing community and numerous schemes have been extensively investigated, making a significant progress in this area. However, several questions still remain unanswered for most of existing approaches including: (i) how to simultaneously learn compact yet representative features from multimodal data, (ii) how to effectively capture complementary features from multimodal streams, and (iii) how to perform all the tasks in an end-to-end manner. To address these challenges, in this paper, we propose a novel deep neural network architecture consisting of a two-stream auto-encoder and a long short term memory for effectively integrating visual and audio signal streams for emotion recognition. To validate the robustness of our proposed architecture, we carry out extensive experiments on the multimodal emotion in the wild dataset: RECOLA. Experimental results show that the proposed method achieves state-of-the-art recognition performance. © 1999-2012 IEEE.",Deep neural networks; Speech recognition; Emotion recognition; Deep learning; Multi-modal data; Learning systems; Signal encoding; Affective Computing; State of the art; multimodal emotion recognition; Auto encoders; Network architecture; Complementary features; Auto-encoder; Data streams; dimensional emotion recognition; long short term memory; Proposed architectures; Sequential learning,,,emotion,No,Yes
scopus,Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised Speech and Text Pre-Trained Embeddings,"Girish, K.V.V.; Konjeti, S.; Vepa, J.",2022,,2022-September,,10.21437/Interspeech.2022-10685,"Speech emotion recognition (SER) is useful in many applications and is approached using signal processing techniques in the past and deep learning techniques recently. Human emotions are complex in nature and can vary widely within an utterance. The SER accuracy has improved using various multimodal techniques but there is still some gap in understanding the model behaviour and expressing these complex emotions in a human interpretable form. In this work, we propose and define interpretability measures represented as a Human Level Indicator Matrix for an utterance and showcase it's effectiveness in both qualitative and quantitative terms. A word level interpretability is presented using an attention based sequence modelling of self-supervised speech and text pre-trained embeddings. Prosody features are also combined with the proposed model to see the efficacy at the word and utterance level. We provide insights into sub-utterance level emotion predictions for complex utterances where the emotion classes change within the utterance. We evaluate the model and provide the interpretations on the publicly available IEMOCAP dataset. Copyright © 2022 ISCA.",Speech emotion recognition; Emotion Recognition; Embeddings; Speech recognition; emotion recognition; Emotion recognition; Deep learning; Character recognition; Human computer interaction; human-computer interaction; Interpretability; Human emotion; Speech communication; Signal processing; Learning techniques; interpretability; Recognition accuracy; Paralinguistic; Computational paralinguistic; computational paralinguistics; Signal processing technique,,,emotion,No,Yes
scopus,A Novel Approach to Analyse Speech Emotion using CNN and Multilayer Perceptron,"Mishra, E.; Sharma, A.K.; Bhalotia, M.; Katiyar, S.",2022,,,,10.1109/ICACITE53722.2022.9823781,"With an increase in the need for real-time systems for analysing speech emotion and sentiment analysis systems for emotions in the human-computer interface, the field of SER has turned into the most studied area. For this paper, we tried to find a better way to analyse emotion from speech signals by taking gender regardless of the context of speech. The audio data used for training, testing, and classification is a combination of various databases like (CREMA-D) which stands for Crowd Sourced Emotional Multimodal Actors Dataset. Another one is Berlin Database of Emotional Speech which is a short-form of (EMO-DB) which is in German language with average of 3 sec, (SAVEE) or the Surrey Audio-Visual Expressed Emotion Database, Toronto Emotional Speech Set (TESS), Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). For this paper, we have used a total of four models, out of which two are ConvNet (CNN) and the other two are from multilayer perceptron (MLP). With calculated MFCCs and passed to gender classifier and then to the respective emotion class classifier for both MLP and CNN classifier. Eventually, we introduced the essential distinction in exactness detailed from MLP and CNN classifiers for recognising speech emotion. The acoustic features of time, frequency, and spectral have been taken into use. The so trained model classifies the gender of the speaker with one of the emotional states from the speech signal. © 2022 IEEE.",Speech emotion recognition; Emotion Recognition; Speech recognition; Classification (of information); Visual languages; Convolution; Statistical tests; Speech communication; Database systems; Emotional speech; Real time systems; Toronto; speech emotion recognition; Interactive computer systems; Audio-visual database; CREMA-D; RAVDESS; Ryerson audio-visual database of emotional speech and song; SAVEE; TESS; Toronto emotional speech set; Emo-DB; Multilayers,,,emotion,No,No
scopus,Deep Learning Based Data Fusion Methods for Multimodal Emotion Recognition,"Njoku, J.N.; Caliwag, A.C.; Lim, W.; Kim, S.; Hwang, H.-J.; Jeong, J.-W.",2022,,47,,10.7840/kics.2022.47.1.79,"Multimodal emotion recognition is a robust and reliable method as it utilizes multimodal data for more comprehensive representation of emotions. Data fusion is a key step in multimodal emotion recognition, because the accuracy of the recognition model mostly depends on how the different modalities are combined. The goal of this paper is to compare the performances of deep learning (DL) based models for the task of data fusion and multimodal emotion recognition. The contributions of this paper are two folds: 1) We introduce three DL models for multimodal fusion and classification: early fusion, hybrid fusion, and multi-task learning. 2) We systematically compare the performance of these models on three multimodal datasets. Our experimental results demonstrate that multi-task learning achieves the best results across all modalities; 75.41%, 68.33%, and 78.75% for classification of three emotional states using the combinations of audio-visual, EEG-audio, and EEG-visual data, respectively. © 2022, Korean Institute of Communications and Information Sciences. All rights reserved.",deep learning; emotion recognition; multimodal; EEG; Data-fusion,,,emotion,No,Yes
scopus,Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition,"Takashima, A.; Masumura, R.; Ando, A.; Yamazaki, Y.; Uchida, M.; Orihashi, S.",2022,,2022-September,,10.21437/Interspeech.2022-11307,"This paper proposes a novel modeling method for audio-visual emotion recognition. Since human emotions are expressed multi-modally, jointly capturing audio and visual cues is a potentially promising approach. In conventional multi-modal modeling methods, a recognition model was trained from an audio-visual paired dataset so as to only enhance audio-visual emotion recognition performance. However, it fails to estimate emotions from single-modal inputs, which indicates they are degraded by overfitting the combinations of the individual modal features. Our supposition is that the ideal form of the emotion recognition is to accurately perform both audio-visual multimodal processing and single-modal processing with a single model. This is expected to promote utilization of individual modal knowledge for improving audio-visual emotion recognition. Therefore, our proposed method employs a cross-modal transformer model that enables different types of inputs to be handled. In addition, we introduce a novel training method named interactive co-learning; it allows the model to learn knowledge from both and either of the modals. Experiments on a multi-label emotion recognition task demonstrate the effectiveness of the proposed method. Copyright © 2022 ISCA.",Emotion Recognition; Speech recognition; Emotion recognition; Learning systems; Cross-modal; Audio-visual; Human emotion; Speech communication; Cross-modal transformer; Single-modal; Audio-visual emotion recognition; cross-modal transformer; audio-visual emotion recognition; Co-learning; interactive co-learning; Interactive co-learning; Model method,,,emotion,Yes,Yes
scopus,Adapted Dynamic Memory Network for Emotion Recognition in Conversation,"Xing, S.; Mai, S.; Hu, H.",2022,,13,,10.1109/TAFFC.2020.3005660,"In this article, we address Emotion Recognition in Conversation (ERC) where conversational data are presented in a multimodal setting. Psychological evidence shows that self and inter-speaker influence are two central factors to emotion dynamics in conversation. State-of-the-art models do not effectively synthesise these two factors. Therefore, we propose an Adapted Dynamic Memory Network (A-DMN) where self and inter-speaker influences are modelled individually and further synthesised oriented towards the current utterance. Specifically, we model the dependency of the constituent utterances in a dialogue video using a global RNN to capture inter-speaker influence. Likewise, each speaker is assigned an RNN to capture their self influence. Afterwards, an Episodic Memory Module is devised to extract contexts for self and inter-speaker influence and synthesise them to update the memory. This process repeats itself for multiple passes until a refined representation is obtained and used for final prediction. Additionally, we explore cross-modal fusion in the context of multimodal ERC, and propose a convolution-based method which proves effective in extracting local interactions and computationally efficient. Extensive experiments demonstrate that A-DMN outperforms the state-of-the-art models on benchmark datasets.  © 2010-2012 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; State of the art; Emotion recognition in conversation; Memory network; Multimodal feature fusions; multimodal feature fusion; adapted dynamic memory network; Adapted dynamic memory network; ART model; Dynamic memory; Synthesised,,,emotion,No,No
scopus,Multimodal Emotion Recognition using Deep Learning Architectures,"Hina, I.; Shaukat, A.; Akram, M.U.",2022,,,,10.1109/ICoDT255437.2022.9787437,"Emotions are an essential part of immaculate communication. The purpose of this research work is to classify six basic emotions of humans namely anger, disgust, fear, happiness, sadness and surprise. In proposed method a sequential deep convolutional neural network is proposed for audio and visual modality. Audio classification is performed via fine-tuning of a pre-trained AlexNet model whereas, visual classification is performed with a hybrid deep network containing CNN and LSTM. Decision level and score level fusion have been implemented for multimodalities. SVM, random forest, K-NN, and logistic regression classifiers were being used for classifying emotion for fused audio-visual data. Experiments have been performed on RML and BAUM-1s dataset with LOSO and LOSGO cross validation techniques respectively. Recognition rates were extremely positive which shows the validity of the proposed methodology.  © 2022 IEEE.",Long short-term memory; Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Deep Learning; Multimodal emotion recognition; Convolutional neural networks; Convolution; Support vector machines; Audio-visual; Decision trees; Learning architectures; Convolution neural network; Audio-visual emotion recognition; CNN-LSTM; Long Short-Term Memory; Audio-Visual Emotion Recognition; Deep convolution neural network; Deep Convolution Neural Network; Recurrent Neural Network,,,emotion,No,Yes
scopus,COGMEN: COntextualized GNN based Multimodal Emotion recognitioN,"Joshi, A.; Bhat, A.; Jain, A.; Singh, A.V.; Modi, A.",2022,,,,,"Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multimodal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the-art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels. © 2022 Association for Computational Linguistics.",Emotion Recognition; Global informations; Emotional state; Speech recognition; Multimodal emotion recognition; Network-based; Human emotion; Graph neural networks; Computational linguistics; Humaninteraction; AI systems; Information contexts; Local information,,,emotion,No,No
scopus,A Deep Learning Technique for Emotion Recognition Using Face and Voice Features,"Kaur, S.; Kulkarni, N.",2021,,,,10.1109/PuneCon52575.2021.9686510,"Automatic emotion recognition is an emerging application in the field of artificial intelligence. Where the use of technology helps to curb various mental illnesses like anxiety, depression, schizophrenia etc. So, there is a need to create efficient artificial intelligent devices. In this experimental project, we developed a multimodal system using neural networks (CNN) for recognizing emotions from facial expressions and voice. We used an RGB images dataset for facial expression recognition. For voice emotion recognition, an audio recorded files dataset was used. We presented phases of automatic emotion recognition in the form of data acquisition, data analysis and data visualization. Comparative analysis among recent papers, features affecting the performance and proposed model in terms of accuracy have also been given in tabular form. The proposed model shows accuracy of 73% and 90.91% for face and voice features respectively. It's observed that variations in the factors of the feature-training phase of neural networks like epochs, batch sizes, training datasets, also affect the results.  © 2021 IEEE.",Speech recognition; Emotion recognition; Face recognition; Deep learning; Human computer interaction; Convolutional neural networks; Convolutional neural network; Automated emotion recognition; Data acquisition; Diseases; Data visualization; Learning techniques; Overfitting; Automatic emotion recognition; Data augmentation; Mental illness; Data overfitting; Emerging applications,,,emotion,No,Yes
scopus,Attentive Cross-modal Connections for Deep Multimodal Wearable-based Emotion Recognition,"Bhatti, A.; Behinaein, B.; Rodenburg, D.; Hungler, P.; Etemad, A.",2021,,,,10.1109/ACIIW52867.2021.9666360,"Classification of human emotions can play an essential role in the design and improvement of human-machine systems. While individual biological signals such as Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely used for emotion recognition with machine learning methods, multimodal approaches generally fuse extracted features or final classification/regression results to boost performance. To enhance multimodal learning, we present a novel attentive cross-modal connection to share information between convolutional neural networks responsible for learning individual modalities. Specifically, these connections improve emotion classification by sharing intermediate representations among EDA and ECG and apply attention weights to the shared information, thus learning more effective multimodal embeddings. We perform experiments on the WESAD dataset to identify the best configuration of the proposed method for emotion classification. Our experiments show that the proposed approach is capable of learning strong multimodal representations and outperforms a number of baselines methods.  © 2021 IEEE.",Attention mechanisms; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Affective Computing; Classification (of information); Convolutional neural networks; Biomedical signal processing; Cross-modal; Electrocardiography; Emotion classification; Wearable technology; Attention Mechanism; Human emotion; Human-machine systems; Electrodermal activity; Multimodal representation learning; Multimodal Representation Learning,,,emotion,No,Yes
scopus,Speech Expression Multimodal Emotion Recognition Based on Deep Belief Network,"Liu, D.; Chen, L.; Wang, Z.; Diao, G.",2021,,19,,10.1007/s10723-021-09564-0,"Aiming at the problems of insufficient information and poor recognition rate in single-mode emotion recognition, a multi-mode emotion recognition method based on deep belief network is proposed. Firstly, speech and expression signals are preprocessed and feature extracted to obtain high-level features of single-mode signals. Then, the high-level speech features and expression features are fused by using the bimodal deep belief network (BDBN), and the multimodal fusion features for classification are obtained, and the redundant information between modes is removed. Finally, the multi-modal fusion features are classified by LIBSVM to realize the final emotion recognition. Based on the Friends data set, the proposed model is demonstrated experimentally. The experimental results show that the recognition accuracy of multimodal fusion feature is the best, which is 90.89%, and the unweighted recognition accuracy of the proposed model is 86.17%, which is better than other comparison methods, and has certain research value and practicability. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.",Speech recognition; Emotion recognition; Multi-modal fusion; Multimodal emotion recognition; Classification (of information); Speech features; High-level features; Recognition accuracy; Bimodal deep belief network; Comparison methods; Deep belief networks; Expression signal; LIBSVM; Speech signal,,,emotion,No,Yes
scopus,"MuSe 2021 Challenge: Multimodal Emotion, Sentiment, Physiological-Emotion, and Stress Detection","Stappen, L.; Meßner, E.-M.; Cambria, E.; Zhao, G.; Schuller, B.W.",2021,,,,10.1145/3474085.3478582,"The 2nd Multimodal Sentiment Analysis (MuSe) 2021 Challenge-based Workshop is held in conjunction with ACM Multimedia'21. Two datasets are provided as part of the challenge. Firstly, the MuSe-CaR dataset, which focuses on user-generated, emotional vehicle reviews from YouTube, and secondly, the novel Ulm-Trier Social Stress (Ulm-TSST) dataset, which shows people in stressful circumstances. Participants are faced with four sub-challenges: predicting arousal and valence in a time- and value-continuous manner on a) MuSe-CaR (MuSe-Wilder) and b) Ulm-TSST (MuSe-Stress); c) predicting unsupervised created emotion classes on MuSe-CaR (MuSe-Sent); d) predicting a fusion of human-annotated arousal and measured galvanic skin response also as a continuous target on Ulm-TSST (MuSe-Physio). In this summary, we describe the motivation, the sub-challenges, the challenge conditions, the participation, and the most successful approaches. © 2021 Owner/Author.",Multi-modal; emotion recognition; Emotion recognition; Modal analysis; multimodal sentiment analysis; Emotion detection; Affective Computing; Sentiment analysis; Electrophysiology; Multimodal sentiment analyse; Forecasting; Challenge; Stress detection; affective computing; challenge; Stress recognition; stress recognition; muse; Muse,,,emotion,Yes,No
scopus,Multi-Task Learning for Jointly Detecting Depression and Emotion,"Zhang, Y.; Li, X.; Rong, L.; Tiwari, P.",2021,,,,10.1109/BIBM52615.2021.9669546,"Depression is a typical mood disease that makes people a persistent feeling of sadness and loss of interest and pleasure. Emotion thus comes into sight and is tightly entangled with depression in that one helps the understanding of the other. Depression and emotion detection has been a new research task. The central challenges in this task are multi-modal interaction and multi-task correlation. The existing approaches treat them as two separate tasks, and fail to model the relationships between them. In this paper, we propose an attentive multi-modal multitask learning framework, called AMM, to generically address such issues. The core modules are two attention mechanisms, viz. inter-modal (I {mathrm{e}}) and inter-task (I {t}) attentions. The main motivation of I {mathrm{e}} attention is to learn multi-modal fused representation. In contrast, It attention is proposed to learn the relationship between depression detection and emotion recognition. Extensive experiments are conducted on two large scale datasets, i.e., DAIC and multi-modal Getty Image depression (MGID). The results show the effectiveness of the proposed AMM framework, and also shows that AMM obtains better performance for the main task, i.e., depression detection with the help of the secondary emotion recognition task.  © 2021 IEEE.",Attention mechanisms; Multi-modal; Speech recognition; deep learning; emotion recognition; Emotion recognition; Deep learning; artificial intelligence; Emotion detection; Multi tasks; Learn+; Multitask learning; Large dataset; Multimodal Interaction; multi-task learning; multi-modal depression detection; Multi-modal depression detection,,,emotion,No,Yes
scopus,Multi-modal emotion recognition using speech features and text-embedding,"Byun, S.-W.; Kim, J.-H.; Lee, S.-P.",2021,,11,,10.3390/app11177967,"Recently, intelligent personal assistants, chat-bots and AI speakers are being utilized more broadly as communication interfaces and the demands for more natural interaction measures have increased as well. Humans can express emotions in various ways, such as using voice tones or facial expressions; therefore, multimodal approaches to recognize human emotions have been studied. In this paper, we propose an emotion recognition method to deliver more accuracy by using speech and text data. The strengths of the data are also utilized in this method. We conducted 43 feature vectors such as spectral features, harmonic features and MFCC from speech datasets. In addition, 256 embedding vectors from transcripts using pre-trained Tacotron encoder were extracted. The acoustic feature vectors and embedding vectors were fed into each deep learning model which produced a probability for the predicted output classes. The results show that the proposed model exhibited more accurate performance than in previous research. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Speech emotion recognition; Emotion recognition; Multi-modal emotion recognition,,,emotion,No,Yes
scopus,Zero-shot Video Emotion Recognition via Multimodal Protagonist-aware Transformer Network,"Qi, F.; Yang, X.; Xu, C.",2021,,,,10.1145/3474085.3475647,"Recognizing human emotions from videos has attracted significant attention in numerous computer vision and multimedia applications, such as human-computer interaction and health care. It aims to understand the emotional response of humans, where candidate emotion categories are generally defined by specific psychological theories. However, with the development of psychological theories, emotion categories become increasingly diverse and fine-grained, samples are also increasingly difficult to collect. In this paper, we investigate a new task of zero-shot video emotion recognition, which aims to recognize rare unseen emotions. Specifically, we propose a novel multimodal protagonist-aware transformer network, which is composed of two branches: one is equipped with a novel dynamic emotional attention mechanism and a visual transformer to learn better visual representations; the other is an acoustic transformer for learning discriminative acoustic representations. We manage to align the visual and acoustic representations with semantic embeddings of fine-grained emotion labels through jointly mapping them into a common space under a noise contrastive estimation objective. Extensive experimental results on three datasets demonstrate the effectiveness of the proposed method. © 2021 ACM.",Semantics; Health care; Multi-modal; Speech recognition; Emotion recognition; Affective Computing; Human computer interaction; multimodal; Computation theory; Zero-shot learning; Emotional response; affective computing; zero-shot learning; Fine grained; Recognizing Human Emotion; Computer vision applications; Medical computing; Multimedia applications; Psychological theory,,,emotion,No,No
scopus,CONSK-GCN: CONVERSATIONAL SEMANTIC- AND KNOWLEDGE-ORIENTED GRAPH CONVOLUTIONAL NETWORK FOR MULTIMODAL EMOTION RECOGNITION,"Fu, Y.; Okada, S.; Wang, L.; Guo, L.; Song, Y.; Liu, J.; Dang, J.",2021,,,,10.1109/ICME51207.2021.9428438,"Emotion recognition in conversations (ERC) has received significant attention in recent years due to its widespread applications in diverse areas, such as social media, health care, and artificial intelligence interactions. However, different from nonconversational text, it is particularly challenging to model the effective context-aware dependence for the task of ERC. To address this problem, we propose a new Conversational Semantic- and Knowledge-oriented Graph Convolutional Network (ConSK-GCN) approach that leverages both semantic dependence and commonsense knowledge. First, we construct the contextual inter-interaction and intradependence of the interlocutors via a conversational graph-based convolutional network based on multimodal representations. Second, we incorporate commonsense knowledge to guide ConSK-GCN to model the semantic-sensitive and knowledge-sensitive contextual dependence. The results of extensive experiments show that the proposed method outperforms the current state of the art on the IEMOCAP dataset. © 2021 IEEE",Semantics; Knowledge graph; Speech recognition; Emotion recognition; Multimodal emotion recognition; Social media; Graphic methods; Convolution; Commonsense knowledge; Convolutional networks; Graph convolutional network; Semantic Web; commonsense knowledge; conversational multimodal emotion recognition; Conversational multimodal emotion recognition; Conversational semantics; Knowledge-oriented; Oriented graph,,,emotion,No,No
scopus,HYBRID FUSION BASED APPROACH FOR MULTIMODAL EMOTION RECOGNITION WITH INSUFFICIENT LABELED DATA,"Kumar, P.; Khokher, V.; Gupta, Y.; Raman, B.",2021,,2021-September,,10.1109/ICIP42928.2021.9506714,"In this paper, a deep learning based fusion approach has been proposed to classify the emotions portrayed by image and corresponding text into discrete emotion classes. The proposed method first implements intermediate fusion on image and text inputs and then applies late fusion on image, text, and intermediate fusion’s output. We have also come up with a way to handle the unavailability of labeled multimodal emotional data. We have prepared a new dataset built on Balanced Twitter for Sentiment Analysis dataset (B-T4SA) dataset containing an image, text, and emotion labels, i.e., ‘happy,’ ‘sad,’ ‘hate’ and ‘anger.’ The emotion recognition accuracy of 90.20% has been achieved by the proposed method. Along with multi-class emotion recognition, we’ve also compared the sentiment classification results and found the proposed method to perform better than the benchmark approaches. © 2021 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Character recognition; Multimodal emotion recognition; Information fusion; Hybrid fusions; Image fusion; Late fusion; Image texts; Late Fusion; Image and text emotion; Image and Text Emotion; Image inputs; Information Fusion; Intermediate; Labeled data; Text input,,,emotion,Yes,Yes
scopus,Multi-modal residual perceptron network for audio–video emotion recognition,"Chang, X.; Skarbek, W.",2021,,21,,10.3390/s21165452,"Emotion recognition is an important research field for human–computer interaction. Audio– video emotion recognition is now attacked with deep neural network modeling tools. In published papers, as a rule, the authors show only cases of the superiority in multi-modality over audio-only or video-only modality. However, there are cases of superiority in uni-modality that can be found. In our research, we hypothesize that for fuzzy categories of emotional events, the within-modal and inter-modal noisy information represented indirectly in the parameters of the modeling neural network impedes better performance in the existing late fusion and end-to-end multi-modal network training strategies. To take advantage of and overcome the deficiencies in both solutions, we define a multi-modal residual perceptron network which performs end-to-end learning from multi-modal network branches, generalizing better multi-modal feature representation. For the proposed multi-modal residual perceptron network and the novel time augmentation for streaming digital movies, the state-of-the-art average recognition rate was improved to 91.4% for the Ryerson Audio–Visual Database of Emotional Speech and Song dataset and to 83.15% for the Crowd-Sourced Emotional Multi Modal Actors dataset. Moreover, the multi-modal residual perceptron network concept shows its potential for multi-modal applications dealing with signal sources not only of optical and acoustical types. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Humans; Emotions; emotion; Deep neural networks; Speech recognition; Emotion recognition; human; Neural Networks, Computer; Human computer interaction; State of the art; speech; Speech; Neural networks; Emotional speech; Feature representation; Deep neural network; Neural network model; Multimodal network; Computer interaction; Audio sensor; Deep features fusion; Motion Pictures; movie; Multi-modal classifier; Perceptron network; Video sensor",,,emotion,No,Yes
scopus,Multimodal emotion recognition on RAVDESS dataset using transfer learning,"Luna-Jiménez, C.; Griol, D.; Callejas, Z.; Kleinlein, R.; Montero, J.M.; Fernández-Martínez, F.",2021,,21,,10.3390/s21227665,"Emotion Recognition is attracting the attention of the research community due to the multiple areas where it can be applied, such as in healthcare or in road safety systems. In this paper, we propose a multimodal emotion recognition system that relies on speech and facial information. For the speech-based modality, we evaluated several transfer-learning techniques, more specifically, embedding extraction and Fine-Tuning. The best accuracy results were achieved when we fine-tuned the CNN-14 of the PANNs framework, confirming that the training was more robust when it did not start from scratch and the tasks were similar. Regarding the facial emotion recognizers, we propose a framework that consists of a pre-trained Spatial Transformer Network on saliency maps and facial images followed by a bi-LSTM with an attention mechanism. The error analysis reported that the frame-based systems could present some problems when they were used directly to solve a video-based task despite the domain adaptation, which opens a new line of research to discover new ways to correct this mismatch and take advantage of the embedded knowledge of these pre-trained models. Finally, from the combination of these two modalities with a late fusion strategy, we achieved 80.08% accuracy on the RAVDESS dataset on a subject-wise 5-CV evaluation, classifying eight emotions. The results revealed that these modalities carry relevant information to detect users’ emotional state and their combination enables improvement of system performance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Long short-term memory; Machine Learning; Emotions; emotion; Speech emotion recognition; Speech recognition; Emotion recognition; Face recognition; Neural Networks, Computer; Human computer interaction; Classification (of information); learning; Learning; machine learning; speech; Speech; Human–computer interaction; Transfer learning; Facial emotion recognition; Facial emotions; Audio-visual; Paralinguistic; Embedded systems; Computational paralinguistic; Audio–visual emotion recognition; Computational paralinguistics; Motor transportation; Spatial transformer; Spatial transformers",,,emotion,No,Yes
scopus,Multi-Modal Fusion Emotion Recognition Method of Speech Expression Based on Deep Learning,"Liu, D.; Wang, Z.; Wang, L.; Chen, L.",2021,,15,,10.3389/fnbot.2021.697634,"The redundant information, noise data generated in the process of single-modal feature extraction, and traditional learning algorithms are difficult to obtain ideal recognition performance. A multi-modal fusion emotion recognition method for speech expressions based on deep learning is proposed. Firstly, the corresponding feature extraction methods are set up for different single modalities. Among them, the voice uses the convolutional neural network-long and short term memory (CNN-LSTM) network, and the facial expression in the video uses the Inception-Res Net-v2 network to extract the feature data. Then, long and short term memory (LSTM) is used to capture the correlation between different modalities and within the modalities. After the feature selection process of the chi-square test, the single modalities are spliced to obtain a unified fusion feature. Finally, the fusion data features output by LSTM are used as the input of the classifier LIBSVM to realize the final emotion recognition. The experimental results show that the recognition accuracy of the proposed method on the MOSI and MELD datasets are 87.56 and 90.06%, respectively, which are better than other comparison methods. It has laid a certain theoretical foundation for the application of multimodal fusion in emotion recognition. © Copyright © 2021 Liu, Wang, Wang and Chen.",Long short-term memory; convolutional neural network; emotion; facial expression; Speech recognition; deep learning; emotion recognition; Emotion recognition; Facial Expressions; Deep learning; human; videorecording; Multi-modal fusion; Learning systems; Data mining; speech; Convolutional neural networks; human experiment; Brain; multimodal fusion; classifier; learning algorithm; Learning algorithms; article; short term memory; Extraction; Statistical tests; feature extraction; Feature extraction; voice; intermethod comparison; feature selection; noise; Feature extraction methods; Long and short term memory; Recognition accuracy; long short-term memory; expression; LibSVM classifier; Theoretical foundations; theoretical study; Traditional learning,,,emotion,No,Yes
scopus,Transformer Based Multimodal Speech Emotion Recognition with Improved Neural Networks,"Patamia, R.A.; Jin, W.; Acheampong, K.N.; Sarpong, K.; Tenagyei, E.K.",2021,,,,10.1109/PRML52754.2021.9520692,"With the procession of technology, the human-machine interaction research field is in growing need of robust automatic emotion recognition systems. Building machines that interact with humans by comprehending emotions paves the way for developing systems equipped with human-like intelligence. Previous architecture in this field often considers RNN models. However, these models are unable to learn in-depth contextual features intuitively. This paper proposes a transformer-based model that utilizes speech data instituted by previous works, alongside text and mocap data, to optimize our emotional recognition system's performance. Our experimental result shows that the proposed model outperforms the previous state-of-the-art. The IEMOCAP dataset supported the entire experiment.  © 2021 IEEE.",Speech emotion recognition; Multi-modal; Speech recognition; Deep learning; Deep Learning; Character recognition; Multimodal; Recognition systems; Neural-networks; Human machine interaction; Automatic emotion recognition; Research fields; Speech Emotion Recognition; Feature; Building machines; Features; Mocap,,,emotion,No,Yes
scopus,Investigations on audiovisual emotion recognition in noisy conditions,"Neumann, M.; Vu, N.T.",2021,,,,10.1109/SLT48900.2021.9383588,"In this paper we explore audiovisual emotion recognition under noisy acoustic conditions with a focus on speech features. We attempt to answer the following research questions: (i) How does speech emotion recognition perform on noisy data? and (ii) To what extend does a multimodal approach improve the accuracy and compensate for potential performance degradation at different noise levels?We present an analytical investigation on two emotion datasets with superimposed noise at different signal-to-noise ratios, comparing three types of acoustic features. Visual features are incorporated with a hybrid fusion approach: The first neural network layers are separate modality-specific ones, followed by at least one shared layer before the final prediction. The results show a significant performance decrease when a model trained on clean audio is applied to noisy data and that the addition of visual features alleviates this effect. © 2021 IEEE.",Speech emotion recognition; Speech recognition; Research questions; multimodal; Multi-modal approach; audiovisual; Multilayer neural networks; Audiovisual emotion recognition; Network layers; Audiovisual; speech emotion recognition; Acoustic conditions; Acoustic noise; Analytical investigations; data augmentation; noisy conditions; Performance degradation; Signal to noise ratio; Superimposed noise,,,emotion,No,Yes
scopus,Multimodal emotion recognition from art using sequential co-attention,"Tashu, T.M.; Hajiyeva, S.; Horvath, T.",2021,,7,,10.3390/jimaging7080157,"In this study, we present a multimodal emotion recognition architecture that uses both feature-level attention (sequential co-attention) and modality attention (weighted modality fusion) to classify emotion in art. The proposed architecture helps the model to focus on learning informative and refined representations for both feature extraction and modality fusion. The resulting system can be used to categorize artworks according to the emotions they evoke; recommend paintings that accentuate or balance a particular mood; search for paintings of a particular style or genre that represents custom content in a custom state of impact. Experimental results on the WikiArt emotion dataset showed the efficiency of the approach proposed and the usefulness of three modalities in emotion recognition. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Attention; Emotions; Modality fusion; Emotion analysis; Multimodal; Art,,,emotion,No,No
scopus,Music video emotion classification using slow–fast audio–video network and unsupervised feature representation,"Pandeya, Y.R.; Bhattarai, B.; Lee, J.",2021,,11,,10.1038/s41598-021-98856-2,"Affective computing has suffered by the precise annotation because the emotions are highly subjective and vague. The music video emotion is complex due to the diverse textual, acoustic, and visual information which can take the form of lyrics, singer voice, sounds from the different instruments, and visual representations. This can be one reason why there has been a limited study in this domain and no standard dataset has been produced before now. In this study, we proposed an unsupervised method for music video emotion analysis using music video contents on the Internet. We also produced a labelled dataset and compared the supervised and unsupervised methods for emotion classification. The music and video information are processed through a multimodal architecture with audio–video information exchange and boosting method. The general 2D and 3D convolution networks compared with the slow–fast network with filter and channel separable convolution in multimodal architecture. Several supervised and unsupervised networks were trained in an end-to-end manner and results were evaluated using various evaluation metrics. The proposed method used a large dataset for unsupervised emotion classification and interpreted the results quantitatively and qualitatively in the music video that had never been applied in the past. The result shows a large increment in classification score using unsupervised features and information sharing techniques on audio and video network. Our best classifier attained 77% accuracy, an f1-score of 0.77, and an area under the curve score of 0.94 with minimum computational cost. © 2021, The Author(s).","music; Machine Learning; Emotions; emotion; Video Recording; videorecording; Music; machine learning; classification; receiver operating characteristic; Databases, Factual; factual database; Models, Theoretical; ROC Curve; theoretical model",,,emotion,Yes,Yes
scopus,Hybrid mutimodal fusion for dimensional emotion recognition,"Ma, Z.; Ma, F.; Sun, B.; Li, S.",2021,,,,10.1145/3475957.3484457,"In this paper, we extensively present our solutions for the MuSe-Stress sub-challenge and the MuSe-Physio sub-challenge of Multimodal Sentiment Challenge (MuSe) 2021. The goal of MuSe-Stress sub-challenge is to predict the level of emotional arousal and valence in a time-continuous manner from audio-visual recordings and the goal of MuSe-Physio sub-challenge is to predict the level of psycho-physiological arousal from a) human annotations fused with b) galvanic skin response (also known as Electrodermal Activity (EDA)) signals from the stressed people. The Ulm-TSST dataset which is a novel subset of the audio-visual textual Ulm-Trier Social Stress dataset that features German speakers in a Trier Social Stress Test (TSST) induced stress situation is used in both sub-challenges. For the MuSe-Stress sub-challenge, we highlight our solutions in three aspects: 1) the audio-visual features and the bio-signal features are used for emotional state recognition. 2) the Long Short-Term Memory (LSTM) with the self-attention mechanism is utilized to capture complex temporal dependencies within the feature sequences. 3) the late fusion strategy is adopted to further boost the model's recognition performance by exploiting complementary information scattered across multimodal sequences. Our proposed model achieves CCC of 0.6159 and 0.4609 for valence and arousal respectively on the test set, which both rank in the top 3. For the MuSe-Physio sub-challenge, we first extract the audio-visual features and the bio-signal features from multiple modalities. Then, the LSTM module with the self-attention mechanism, and the Gated Convolutional Neural Networks (GCNN) as well as the LSTM network are utilized for modeling the complex temporal dependencies in the sequence. Finally, the late fusion strategy is used. Our proposed method also achieves CCC of 0.5412 on the test set, which ranks in the top 3. © 2021 ACM.",Long short-term memory; Multi-modal; Speech recognition; Emotion recognition; Multi-modal fusion; Convolutional neural networks; Electrophysiology; Physiological models; Brain; Convolution; Convolutional neural network; Audio-visual features; Continuous emotion recognition; Statistical tests; Complex networks; Audio-visual; multi-modal fusion; self attention; long short-term memory; continuous emotion recognition; Gated convolutional neural network; gated convolutional neural networks; Self attention; Stress test,,,emotion,No,Yes
scopus,A novel signal to image transformation and feature level fusion for multimodal emotion recognition,"Hatipoglu Yilmaz, B.; Kose, C.",2021,,66,,10.1515/bmt-2020-0229,"Emotion is one of the most complex and difficult expression to be predicted. Nowadays, many recognition systems that use classification methods have focused on different types of emotion recognition problems. In this paper, we aimed to propose a multimodal fusion method between electroencephalography (EEG) and electrooculography (EOG) signals for emotion recognition. Therefore, before the feature extraction stage, we applied different angle-Amplitude transformations to EEG-EOG signals. These transformations take arbitrary time domain signals and convert them two-dimensional images named as Angle-Amplitude Graph (AAG). Then, we extracted image-based features using a scale invariant feature transform method, fused these features originates basically from EEG-EOG and lastly classified with support vector machines. To verify the validity of these proposed methods, we performed experiments on the multimodal DEAP dataset which is a benchmark dataset widely used for emotion analysis with physiological signals. In the experiments, we applied the proposed emotion recognition procedures on the arousal-valence dimensions. We achieved (91.53%) accuracy for the arousal space and (90.31%) for the valence space after fusion. Experimental results showed that the combination of AAG image features belonging to EEG-EOG signals in the baseline angle amplitude transformation approaches enhanced the classification performance on the DEAP dataset. © 2021 Walter de Gruyter GmbH, Berlin/Boston 2021.",emotion; Speech recognition; emotion recognition; Physiological signals; human; Multimodal emotion recognition; Classification (of information); multimodal; controlled study; electroencephalography; Electroencephalography; Biomedical signal processing; EEG; electrooculography; Electrophysiology; human experiment; Support vector machines; Image enhancement; article; Time domain analysis; arousal; feature extraction; support vector machine; Classification methods; Scale invariant feature transforms; Classification performance; EOG; Amplitude transformations; feature level fusion; Image transformations; scale invariant feature transform; signal to image transformation; Two dimensional images; validity,,,emotion,No,Yes
scopus,Learning deep multimodal affective features for spontaneous speech emotion recognition,"Zhang, S.; Tao, X.; Chuang, Y.; Zhao, X.",2021,,127,,10.1016/j.specom.2020.12.009,"Recently, spontaneous speech emotion recognition has become an active and challenging research subject. This paper proposes a new method of spontaneous speech emotion recognition by using deep multimodal audio feature learning based on multiple deep convolutional neural networks (multi-CNNs). The proposed method initially generates three different audio inputs for multi-CNNs so as to learn deep multimodal segment-level features from the original 1D audio signal in three aspects: 1) a 1D CNN for 1D raw waveform modeling, 2) a 2D CNN for 2D time-frequency Mel-spectrogram modeling, and 3) a 3D CNN for temporal-spatial dynamic modeling. Then, an average-pooling is performed on the obtained segment-level classification results from 1D, 2D, and 3D CNN networks, to produce utterance-level classification results. Finally, a score-level fusion strategy is adopted as a multi-CNN fusion method to integrate different utterance-level classification results for final emotion classification. The learned deep multimodal audio features are shown to be complementary to each other so that they are combined in a multi-CNN fusion network to achieve significantly improved emotion classification performance. Experiments are conducted on two challenging spontaneous emotional speech datasets, i.e., the AFEW5.0 and BAUM-1 s databases, demonstrating the promising performance of our proposed method. © 2020",Speech emotion recognition; Deep neural networks; Speech recognition; Deep learning; Classification (of information); Convolutional neural networks; Emotion classification; Emotional speech; Classification results; Deep multimodal feature learning; Research subjects; Score-level fusion; Spatial dynamic modeling; Spontaneous speech; Temporal-spatial; Waveform modeling,,,emotion,No,Yes
scopus,Bimodal Emotion Recognition for the Patients with Depression,"Wang, X.; Zhao, S.; Wang, Y.",2021,,,,10.1109/ICSIP52628.2021.9688837,"With the rapid development of the society, over three hundred million people from worldwide suffer from depression, which has become one of the most serious health problems in the world. On the other hand, emotion recognition research works barely focuses on the depression patients although their emotion differs from that of the normal people obviously. Firstly, a speech/text emotion dataset was built up under the dialogue scenes with both the depressed patients and the normal people, and the speech/text fragments were classified into five common emotions: sadness, anger, happy, fear and neutral. Secondly, a bimodal emotion recognition algorithm is proposed, which uses a multimodal Transformer model as the feature fusion module. The experimental results show that it achieves an accuracy of 69.2% for the normal people and 59.4% for the depression patients.  © 2021 IEEE.",Depression; Multi-modal; Speech recognition; Emotion recognition; Feature fusion; Features fusions; Classifieds; Transformer modeling; Recognition algorithm; Bimodal emotion recognition; Multimodal transformer; Text fragments,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition with High-Level Speech and Text Features,"Makiuchi, M.R.; Uto, K.; Shinoda, K.",2021,,,,10.1109/ASRU51503.2021.9688036,"Automatic emotion recognition is one of the central concerns of the Human-Computer Interaction field as it can bridge the gap between humans and machines. Current works train deep learning models on low-level data representations to solve the emotion recognition task. Since emotion datasets often have a limited amount of data, these approaches may suffer from overfitting, and they may learn based on superficial cues. To address these issues, we propose a novel cross-representation speech model, inspired by disentangle-ment representation learning, to perform emotion recognition on wav2vec 2.0 speech features. We also train a CNN-based model to recognize emotions from text features extracted with Transformer-based models. We further combine the speech-based and text-based results with a score fusion approach. Our method is evaluated on the IEMOCAP dataset in a 4-class classification problem, and it surpasses current works on speech-only, text-only, and multimodal emotion recognition. © 2021 IEEE.",Speech recognition; deep learning; Emotion recognition; Deep learning; Character recognition; Multimodal emotion recognition; 'current; Human computer interaction; Classification (of information); Speech; Multi-modality; Speech features; Automatic emotion recognition; multimodality; Text feature; wav2vec 2.0; Wav2vec 2.0; disentanglement representation learning; Disentanglement representation learning,,,emotion,No,No
scopus,Multimodal Emotion Recognition of Hand-Object Interaction,"Niewiadomski, R.; Sciutti, A.",2021,,,,10.1145/3397481.3450636,"In this paper, we investigate whether information related to touches and rotations impressed to an object can be effectively used to classify the emotion of the agent manipulating it. We specifically focus on sequences of basic actions (e.g., grasping, rotating), which are constituents of daily interactions. We use the iCube, a 5 cm cube covered with tactile sensors and embedded with an accelometer, to collect a new dataset including 11 persons performing action sequences associated with 4 emotions: Anger, sadness, excitement and gratitude. Next, we propose 17 high-level hand-crafted features based on the tactile and kinematics data derived from the iCube. Twelve of these features vary significantly as a function of the emotional context in which the action sequence was performed. In particular, a larger surface of the object is engaged in physical contact for anger and excitement, than for sadness. Furthermore, the average duration of interactions labeled as sad, is longer than for the remaining 3 emotions. More rotations are performed for anger and excitement than for sadness and gratitude. The accuracy of a classification experiment in the case of four emotions reaches 0.75. This result shows that the emotion recognition during hand-object interactions is possible and it may foster development of new intelligent user interfaces.  © 2021 ACM.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Classification (of information); Emotion classification; User interfaces; Action sequences; Affective touch; Basic actions; Hand-Object Interaction; Intelligent User Interfaces; Object interactions; Palmprint recognition; Physical contacts; Tactile sensors,,,emotion,Yes,Yes
scopus,Multimodal speech emotion recognition based on aligned attention mechanism,"Liu, M.; Xue, N.; Huo, M.",2021,,,,10.1109/ICUS52573.2021.9641435,"Speech emotion recognition has always been an important and challenging task in the field of Human Computer Interaction(HCI). With the development of automatic speech recognition (ASR) technology, people can use ASR technology to generate text from speech, and then build a multimodal model to recognize emotions. However, the existing multimodal speech emotion recognition models mostly train the optimal single-modal models separately and merge them at the decision-making layer. These models don't consider the interaction between speech and text, so their performance is poor. In this paper, we propose an aligned attention mechanism to merge speech and text modalities. First, use silence removal technology to remove pauses and silences in the speech, so that the text and speech can be more accurately aligned later; second, use a bidirectional LSTM encoder to separately encode the speech and text sequences, and then each word coding and its acoustics realize that the corresponding speech frame coding is input into the attention network together to obtain the prosodic feature vector representing each word; finally, it is combined with the text feature and input into the sequence model for emotion recognition. We used the IEMOCAP dataset to train our model. The experimental results which show that the model has a weighted accuracy of 1.66% higher than the existing optimal model. © 2021 IEEE.",Long short-term memory; Attention; Speech emotion recognition; Attention mechanisms; Multi-modal; Speech recognition; deep learning; Behavioral research; Deep learning; Character recognition; Human computer interaction; Multimodal; Speech; Decision making; Recognition models; Multimodal models; Single-modal; Modal models; Automatic Speech Recognition Technology,,,emotion,No,Yes
scopus,Modeling Emotion in Complex Stories: The Stanford Emotional Narratives Dataset,"Ong, D.C.; Wu, Z.; Zhi-Xuan, T.; Reddan, M.; Kahhale, I.; Mattek, A.; Zaki, J.",2021,,12,,10.1109/TAFFC.2019.2955949,"Human emotions unfold over time, and more affective computing research has to prioritize capturing this crucial component of real-world affect. Modeling dynamic emotional stimuli requires solving the twin challenges of time-series modeling and of collecting high-quality time-series datasets. We begin by assessing the state-of-the-art in time-series emotion recognition, and we review contemporary time-series approaches in affective computing, including discriminative and generative models. We then introduce the first version of the Stanford Emotional Narratives Dataset (SENDv1): a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models. We demonstrate several baseline and state-of-the-art modeling approaches on the SEND, including a Long Short-Term Memory model and a multimodal Variational Recurrent Neural Network, which perform comparably to the human-benchmark. We end by discussing the implications for future research in time-series affective computing. © 2010-2012 IEEE.",Speech recognition; Emotion recognition; Emotional valences; Affective Computing; State of the art; Affective computing; Recurrent neural networks; Statistical tests; Complex networks; Time series; affect sensing and analysis; multi-modal recognition; Short term memory; emotional corpora; Implications for futures; Modeling emotions; Time series modeling,,,emotion,Yes,No
scopus,You made me feel this way: Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data,"Boateng, G.; Hilpert, P.; Bodenmann, G.; Neysari, M.; Kowatsch, T.",2021,,,,10.1145/3461615.3485424,"How romantic partners interact with each other during a conflict influences how they feel at the end of the interaction and is predictive of whether the partners stay together in the long term. Hence understanding the emotions of each partner is important. Yet current approaches that are used include self-reports which are burdensome and hence limit the frequency of this data collection. Automatic emotion prediction could address this challenge. Insights from psychology research indicate that partners' behaviors influence each other's emotions in conflict interaction and hence, the behavior of both partners could be considered to better predict each partner's emotion. However, it is yet to be investigated how doing so compares to only using each partner's own behavior in terms of emotion prediction performance. In this work, we used BERT to extract linguistic features (i.e., what partners said) and openSMILE to extract paralinguistic features (i.e., how they said it) from a data set of 368 German-speaking Swiss couples (N = 736 individuals) who were videotaped during an 8-minutes conflict interaction in the laboratory. Based on those features, we trained machine learning models to predict if partners feel positive or negative after the conflict interaction. Our results show that including the behavior of the other partner improves the prediction performance. Furthermore, for men, considering how their female partners spoke is most important and for women considering what their male partner said is most important in getting better prediction performance. This work is a step towards automatically recognizing each partners' emotion based on the behavior of both, which would enable a better understanding of couples in research, therapy, and the real world.  © 2021 ACM.",emotion recognition; Emotion recognition; Multi-modal fusion; 'current; multimodal fusion; Video recording; Linguistics; Forecasting; Emotion predictions; Speech data; Paralinguistic; conflict; Conflict; Conflict interaction; Couple; couples; linguistic; paralinguistic; Prediction performance,,,emotion,No,Yes
scopus,User Emotion Recognition Method Based on Facial Expression and Speech Signal Fusion,"Lu, F.; Zhang, L.; Tian, G.",2021,,,,10.1109/ICIEA51954.2021.9516216,"In human-computer interaction, it is an urgent problem to use facial expressions and speech information to identify the user's continuous emotions, and the key factors affecting the recognition accuracy are the data deficiencies during the fusion of speech and facial information, and the abnormal frames in the video. In order to solve these problems, a user emotion recognition system based on the fusion of facial expressions and speech multimodality is designed. In the part of facial expressions, Gabor transform continuous emotion recognition method based on data increments is proposed. In the part of speech information, Mel-scale Frequency Cepstral Coefficients (MFCC) is used to extract speech features, and user emotions are recognize through transfer learning. Finally, in the late fusion, multiple linear regression is used for multi-modality to verify the method in this paper. This paper uses the AVEC2013 dataset with Arousal-Valence label to conduct a valid experiment on the proposed method. The experimental results prove that the method improves the accuracy of user emotion recognition. © 2021 IEEE.",Speech recognition; emotion recognition; Emotion recognition; Face recognition; Facial Expressions; Multi-modal fusion; Learning systems; Human computer interaction; Speech; multimodal fusion; Multi-modality; transfer learning; Transfer learning; Recognition methods; Speech information; User emotions; arousal-valence; Arousal-valence; gabor transform; Gabor transform; Linear regression,,,emotion,Yes,Yes
scopus,Attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional LSTM,"Huddar, M.G.; Sannakki, S.S.; Rajpurohit, V.S.",2021,,80,,10.1007/s11042-020-10285-x,"Due to the availability of an enormous amount of multimodal content on the social web and its applications, automatic sentiment analysis, and emotion detection has become an important and widely researched topic. Improving the quality of multimodal fusion is an important issue in this field of research. In this paper, we present a novel attention-based multimodal contextual fusion strategy, which extract the contextual information among the utterances before fusion. Initially, we fuse two-two modalities at a time and finally, we fuse all three modalities. We use a bidirectional LSTM with an attention model for extracting important contextual information among the utterances. The proposed model was tested on IEMOCAP dataset for emotion classification and CMU-MOSI dataset for sentiment classification. By incorporating the contextual information among utterances in the same video, our proposed method outperforms the existing methods by over 3% in emotion classification and over 2% in sentiment classification. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Long short-term memory; Multi-modal fusion; Emotion detection; Sentiment analysis; Classification (of information); Multimodal fusion; Sentiment classification; Emotion classification; Contextual information; Fusion strategies; Attention model; ITS applications; Bidirectional LSTM,,,emotion,No,No
scopus,Context-Aware Based Visual-Audio Feature Fusion for Emotion Recognition,"Cheng, H.; Tie, Y.; Qi, L.; Jin, C.",2021,,2021-July,,10.1109/IJCNN52387.2021.9533473,"Video emotion recognition is a significant branch in the field of emotion computing. However, traditional recognition works mainly focus on human features, ignoring the contextual clues of video scenes and objects. In our work, we propose a context-aware framework for bi-modal video emotion recognition. Unlike existing methods that directly extract features of the entire video frame, we extract key frames and key regions of videos to obtain emotional cues contained in video scenes and objects. Specifically, for visual stream, the hierarchical Bidirectional Long-Short Term Memory (Bi-LSTM) is applied to summarize video scenes and find key frames that mostly contribute to video emotion; Meantime, we introduce the Region Proposal Network (RPN) to extract corresponding features of object regions in video frames and construct the emotional similarity graph. After using the Feedforward Neural Network (FNN) to assign different weight coefficients to different regions, the Graph Convolutional Network (GCN) is used to reason about the connections between key regions. Moreover, the context information of the frame-level Log-Mel spectrum fragments supplement the visual information. Finally, we fuse the visual and acoustics features by adaptive gated multimodal fusion module for video emotion classification. We conduct experiments on Video Emotion-8 and Ekman-6 datasets. The experimental results demonstrate that our model achieves better classification accuracy than several baseline models. © 2021 IEEE.",Long short-term memory; Speech recognition; Emotion recognition; Multimodal emotion recognition; multimodal emotion recognition; Convolutional neural networks; Convolution; Feedforward neural networks; Convolutional networks; Graph convolutional network; graph convolutional network; Context-Aware; relationship reasoning; Relationship reasoning; Scene summarizations; Video objects; Video scene; Video scene summarization; video scenes summarization,,,emotion,No,Yes
scopus,Robust Speech Emotion Recognition System Through Novel ER-CNN and Spectral Features,"Zeeshan, M.; Qayoom, H.; Hassan, F.",2021,,,,10.1109/ISAECT53699.2021.9668480,"The speech is most fundamental way of communication among the humans and an important method for human computer interaction (HCI) by employing the microphone. Measurable emotion recognition from the speech signal by employing microphone is an emerging and interesting area of research in HCI such as human reboot interaction, healthcare, virtual reality, emergency call, and behavior assessment. In this paper, we proposed a novel integration of spectral features comprises of mel-spectral frequency coefficients (MFCC), root mean square energy (RMSE), and zero crossing rate (ZCR) to represent complex audio signal. For the classification purpose, we designed a novel convolutional neural network called emotion recognition neural network (ER-CNN) to classify different emotions such as angry, disgust, fear, happy, neutral, and sad. The proposed method Speech emotion recognition (SER-CNN) obtained an equal error rate (EER) of 1.34%, an accuracy of 94.99%, precision of 94.96%, recall of 94.98%, and F1-score of 94.96%. We evaluated the performance of the proposed system SER-CNN on the standard dataset crowd-sourced emotional multimodal actors (CREMA-D). Experimental results of the proposed method and comparative analysis against the existing methods show that our method has superior performance and can reliably be used for the emotion detection.  © 2021 IEEE.",Speech emotion recognition; Performance; Deep neural networks; Speech recognition; Emotion recognition; Deep learning; Human computer interaction; Convolutional neural networks; Convolution; Convolutional neural network; Virtual reality; Speech communication; Neural networks; Neural-networks; MFCC; Frequency coefficient; Mel-spectral frequency coefficient; Microphones; Spectral feature; Spectral features; Spectral frequency,,,emotion,No,Yes
scopus,Hierarchical Attention Approach in Multimodal Emotion Recognition for Human Robot Interaction,"Abdullah, M.; Ahmad, M.; Han, D.",2021,,,,10.1109/ITC-CSCC52171.2021.9501446,"The ability to perceive human emotions is one of the key elements that may promise a natural, genuine and more reliable human robot interaction. Though emotional perception in human robot interaction has been challenged by many difficulties, the lack of contextual understanding is can be attributed as the biggest hurdle in this regard. Most of the literature refers to the datasets developed in controlled environment to validate the performance of their systems which happens to be really good. Still those systems are very far from achieving that kind of performance in real-life scenarios. In this paper a multimodal emotion recognition strategy is presented, that uses voice features in addition to the facial expressions to determine the emotional state of the user. A hierarchical attention layer is used to for feature fusion purpose. The final system is end-to-end trainable, multimodal approach makes it more resilient to the changes in environment. The achieved 76.3% accuracy for eNterface05 video dataset, which is higher than the any single modality approach in the comparison.  © 2021 IEEE.",Social robots; Emotion Recognition; Emotional state; Speech recognition; Behavioral research; Facial Expressions; Multimodal emotion recognition; Affective Computing; Multimodal; Feature fusion; Multi-modal approach; Video dataset; Man machine systems; Internet of Things; Computer circuits; HCI; Acoustic Emotion Recognition; Contextual understanding; Controlled environment; Facial Expressions Recognition; HRI; Human Computer Interaction; Human Robot Interaction,,,emotion,No,Yes
scopus,Multimodal sentiment and emotion recognition in hyperbolic space,"Araño, K.A.; Orsenigo, C.; Soto, M.; Vercellis, C.",2021,,184,,10.1016/j.eswa.2021.115507,"Prior approaches for multimodal sentiment and emotion recognition (SER) exploit input data representations and neural networks based on the classical Euclidean geometry. Recently, however, the hyperbolic metric proved to be a powerful tool for data mapping, being able to capture the hierarchical structure of the relations among elements in the data. In this paper we propose the use of hyperbolic learning for SER, and show that the inclusion in the neural network of hyperbolic structures mapping the input into the hyperbolic space can improve the quality of the predictions. The benefits brought by the hyperbolic features are evaluated by developing extensions of existing methods following two approaches. From one side, we modified state-of-the-art models by including hyperbolic output layers. From the other, we generated hybrid neural network architectures by combining hyperbolic and Euclidean layers according to different schemes. The proposed hyperbolic models were tested on several classification tasks applied to benchmark multimodal SER datasets. Experiments gave strong evidence that in both simple and complex networks the introduction of a hyperbolic structure results in an improvement of the model accuracy. Specifically, the combined use of hyperbolic and Euclidean layers showed superior performance in almost all the classification tasks. © 2021 Elsevier Ltd",Multi-modal; Speech recognition; Emotion recognition; Deep learning; Sentiment analysis; Classification (of information); Multilayer neural networks; Network architecture; Complex networks; Neural-networks; Mapping; Euclidean; Hyperbolic learning; Hyperbolic spaces; Hyperbolic structures; Mmultimodal machine learning,,,emotion,No,Yes
scopus,DETECTING HIGHLIGHTED VIDEO CLIPS THROUGH EMOTION-ENHANCED AUDIO-VISUAL CUES,"Hu, L.; He, W.; Zhang, L.; Xu, T.; Xiong, H.; Chen, E.",2021,,,,10.1109/ICME51207.2021.9428252,"Recent years have witnessed the growing research interests in video highlight detection. Existing studies mainly focus on detecting highlights in user-generated videos with simple topics based on visual content. However, relying solely on visual features limits the ability of conventional methods to capture highlights for videos with more complicated semantics, like movies. Therefore, we propose to mine the emotional information in video sounds to enhance highlight detection. Specifically, we design a novel emotion-enhanced framework with multi-stage fusion to detect highlights for complex videos. Along this line, we first extract multi-grained features from the audio waves. Then, the tailored-designed intra-modal fusion is applied on audio features to obtain emotional representation. Furthermore, the cross-modal fusion is developed to generate comprehensive representation of clip by merging audio emotional representations and visual features. This representation can be leveraged for predicting highlight probability. Finally, extensive experiments on real-world datasets demonstrate the effectiveness of our method. © 2021 IEEE",Semantics; Multi-modal; Multi-modal fusion; multimodal fusion; Audio-visual; Emotional representations; Video-clips; Visual feature; Video analysis; Highlights detection; Multimodal video analyse; multimodal video analysis; video highlight detection; Video highlight detection,,,emotion,No,No
scopus,A multimodal hierarchical approach to speech emotion recognition from audio and text[Formula presented],"Singh, P.; Srivastava, R.; Rana, K.P.S.; Kumar, V.",2021,,229,,10.1016/j.knosys.2021.107316,"Speech emotion recognition (SER) plays a crucial role in improving the quality of man–machine interfaces in various fields like distance learning, medical science, virtual assistants, and automated customer services. A deep learning-based hierarchical approach is proposed for both unimodal and multimodal SER systems in this work. Of these, the audio-based unimodal system proposes using a combination of 33 features, which include prosody, spectral, and voice quality-based audio features. Further, for the multimodal system, both the above-mentioned audio features and additional textual features are used. Embeddings from Language Models v2 (ELMo v2) is implemented to extract word and character embeddings which helped to capture the context-dependent aspects of emotion in text. The proposed models’ performances are evaluated on two audio-only unimodal datasets – SAVEE and RAVDESS, and one audio-text multimodal dataset – IEMOCAP. The proposed hierarchical models offered SER accuracies of 81.2%, 81.7%, and 74.5% on the RAVDESS, SAVEE, and IEMOCAP datasets, respectively. Further, these results are also benchmarked against recently reported techniques, and the reported performances are found to be superior. Therefore, based on the presented investigations, it is concluded that the application of a deep learning-based network in a hierarchical manner significantly improves SER over generic unimodal and multimodal systems. © 2021 Elsevier B.V.",Speech emotion recognition; Embeddings; Multi-modal; Speech recognition; Deep learning; Character recognition; Unimodal; Multimodal; Audio features; Hierarchical systems; Multimodal system; Hierarchical approach; Lexical features; Man-machine interface,,,emotion,No,Yes
scopus,Multimodal and Context-Aware Emotion Perception Model with Multiplicative Fusion,"Mittal, T.; Bera, A.; Manocha, D.",2021,,28,,10.1109/MMUL.2021.3068387,"We present a learning model for multimodal context-aware emotion recognition. Our approach combines multiple human co-occurring modalities (such as facial, audio, textual, and pose/gaits) and two interpretations of context. To gather and encode background semantic information for the first context interpretation from the input image/video, we use a self-attention-based CNN to encode. Similarly, for modeling the sociodynamic interactions among people (second context interpretation) in the input image/video, we use depth maps. We use multiplicative fusion to combine the modality and context channels, which learn to focus on the more informative input channels and suppress others for every incoming datapoint. We demonstrate the efficiency of our model on four benchmark emotion recognition datasets (IEMOCAP, CMU-MOSEI, EMOTIC, and GroupWalk). Our model outperforms on state of the art (SOTA) learning methods with an average 5\%-9\%5%-9% increase over all the datasets. We also perform ablation studies to motivate the importance of multimodality, context, and multiplicative fusion.  © 1994-2012 IEEE.",Semantics; Speech recognition; Emotion recognition; Learning systems; Affective Computing; Learning models; Encoding (symbols); Multimodal Fusion; Multimodal Learning; Video cameras; Learning methods; Dynamic interaction; Input channels; Multi modality; Perceived Emotion; Perception Modeling; Semantic information,,,emotion,No,No
scopus,Emotion Recognition from Multimodal Physiological Signals Using a Regularized Deep Fusion of Kernel Machine,"Zhang, X.; Liu, J.; Shen, J.; Li, S.; Hou, K.; Hu, B.; Gao, J.; Zhang, T.",2021,,51,,10.1109/TCYB.2020.2987575,"These days, physiological signals have been studied more broadly for emotion recognition to realize emotional intelligence in human-computer interaction. However, due to the complexity of emotions and individual differences in physiological responses, how to design reliable and effective models has become an important issue. In this article, we propose a regularized deep fusion framework for emotion recognition based on multimodal physiological signals. After extracting the effective features from different types of physiological signals, we construct ensemble dense embeddings of multimodal features using kernel matrices, and then utilize a deep network architecture to learn task-specific representations for each kind of physiological signal from these ensemble dense embeddings. Finally, a global fusion layer with a regularization term, which can efficiently explore the correlation and diversity among all of the representations in a synchronous optimization process, is designed to fuse generated representations. Experiments on two benchmark datasets show that this framework can improve the performance of subject-independent emotion recognition compared to single-modal classifiers or other fusion methods. Data visualization also demonstrates that the final fusion representation exhibits higher class-separability power for emotion recognition. © 2013 IEEE.",Humans; Emotions; emotion; Embeddings; Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Physiology; human; Human computer interaction; Classification (of information); learning; Learning; electroencephalography; Electroencephalography; Benchmarking; Biomedical signal processing; Physiological models; multimodal fusion; Network architecture; Multimodal features; Data visualization; Benchmark datasets; Emotional intelligence; Deep neural network; Physiological response; Individual Differences; Class separability; kernel machine; Regularization terms,,,emotion,No,Yes
scopus,Emotion Detection in Conversation Using Class Weights,"Dave, C.; Khare, M.",2021,,,,10.1109/ISCMI53840.2021.9654957,"Emotion detection in conversations is a crucial research problem for developing a human-like intelligence system. The task is to predict an emotion label for all the utterances in a conversation. We observed that datasets generally have an imbalance in classes. This can cause inefficient testing results as the model will be trained biased towards the majority class. This paper proposes that using class weights to model the architectures will improve the state-of-an-art benchmark results. The proposed approach provides a way to achieve good classification results even when the datasets are skewed. This helps the model to achieve a better prediction score; especially by considering the minority class samples in a balanced manner along with majority class samples. We perform the prediction on two publicly available datasets, DailyDialog and IEMOCAP, unimodal and multimodal, respectively. An improvement in the F1 scores was observed for both datasets. However, DailyDialog showed significant improvement than IEMOCAP because a higher imbalance was seen in the former than the latter. © 2021 IEEE.",Emotion; Unimodal; Emotion detection; Classification (of information); Forecasting; Neural-networks; Data imbalance; neural networks; emotions; emotion detection; Research problems; Classification results; Class weight; class weights; data imbalance; Human-like intelligence; Intelligence systems,,,emotion,No,Yes
scopus,VREED: Virtual Reality Emotion Recognition Dataset Using Eye Tracking and Physiological Measures,"Tabbaa, L.; Searle, R.; Mirzaee Bafti, S.; Hossain, M.M.; Intarasisrisawat, J.; Glancy, M.; Ang, C.S.",2021,,5,,10.1145/3495002,"The paper introduces a multimodal affective dataset named VREED (VR Eyes: Emotions Dataset) in which emotions were triggered using immersive 360 Video-Based Virtual Environments (360-VEs) delivered via Virtual Reality (VR) headset. Behavioural (eye tracking) and physiological signals (Electrocardiogram (ECG) and Galvanic Skin Response (GSR)) were captured, together with self-reported responses, from healthy participants (n=34) experiencing 360-VEs (n=12, 1-3 min each) selected through focus groups and a pilot trial. Statistical analysis confirmed the validity of the selected 360-VEs in eliciting the desired emotions. Preliminary machine learning analysis was carried out, demonstrating state-of-the-art performance reported in affective computing literature using non-immersive modalities. VREED is among the first multimodal VR datasets in emotion recognition using behavioural and physiological signals. VREED is made publicly available on Kaggle 1. We hope that this contribution encourages other researchers to utilise VREED further to understand emotional responses in VR and ultimately enhance VR experiences design in applications where emotional elicitation plays a key role, i.e. healthcare, gaming, education, etc.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Multi-modal; Speech recognition; Eye tracking; Emotion recognition; Physiological signals; Affective Computing; Electroencephalography; Biomedical signal processing; Electrophysiology; Physiological models; Electrocardiography; Dataset; Virtual reality; ECG; Galvanic skin response; GSR; Eye-tracking; Immersive; Physiological measures; Virtual Reality; Virtual-reality headsets,,,emotion,No,Yes
scopus,DUAL-WAVEFORM EMOTION RECOGNITION MODEL FOR CONVERSATIONS,"Zhang, J.; Liu, Z.; Liu, P.; Wu, B.",2021,,,,10.1109/ICME51207.2021.9428327,"Emotion recognition in conversations has broad application prospects for the current era of information explosion. Existing methods fail to utilize the detailed emotional information in speech. In this paper, we introduce a dual-waveform emotion recognition model for conversations. We 1) design a waveform-attention module capturing emotion features from source waveform and synthetic waveform; 2) use efficacy coefficient mechanism for fine-grained multi-modal information fusion; 3) present a new dialogue emotion detection module. Extensive experiments show that our method achieves the state-of-the-art performance on IEMOCAP and SEMAINE datasets. © 2021 IEEE Computer Society. All rights reserved.",Speech recognition; emotion recognition; Emotion recognition; Multi-modal fusion; 'current; Features extraction; multimodal fusion; Recognition models; feature extraction; Feature extraction; Application prospect; Broad application; Dialog emotion; dialogue emotion; Information explosion; Waveforms,,,emotion,No,Yes
scopus,Maximum weight multi-modal information fusion algorithm of electroencephalographs and face images for emotion recognition,"Wang, M.; Huang, Z.; Li, Y.; Dong, L.; Pan, H.",2021,,94,,10.1016/j.compeleceng.2021.107319,"In view of the low accuracy of the traditional emotion recognition methods based on facial expressions, an emotion recognition method based on maximum weight multi-modal information fusion of electroencephalographs (EEGs) and facial expression information is proposed in this paper. First, the induced emotional EEG data is converted into the corresponding EEG topographic map data and sent to the convolutional network for training and outputting decision information. Second, the illumination compensation method is utilized to filter the noise of the face image data. Then, the face image data is trained in the multi-scale feature extraction network, and the decision information is output. Finally, aiming at the decision-level information fusion, a weighted fusion method is proposed in this paper for emotion recognition. Experimental tests show that the recognition accuracy of the multi-scale feature extraction network on the CK+ data set and Fer2013 data reached 94.4% and 72%, respectively. Simultaneously, the multi-modal information fusion method achieves 92.6% accuracy in emotion recognition. © 2021",Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Multi-modal information; Electroencephalography; Information fusion; Extraction; Statistical tests; Feature extraction; EEG signals; Multimodal information fusion; Multimodal information; Recognition methods; Weighted fusion; Electroencephalograph signals; Face images; Illumination compensation; Maps; Multi-scale feature extraction,,,emotion,No,Yes
scopus,Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation,"Chen, F.; Sun, Z.; Ouyang, D.; Liu, X.; Shao, J.",2021,,,,10.1145/3474085.3475661,"Multi-sensory data has exhibited a clear advantage in expressing richer and more complex feelings, on the Emotion Recognition in Conversation (ERC) task. Yet, current methods for multimodal dynamics that aggregate modalities or employ additional modality-specific and modality-shared networks are still inadequate in balancing between the sufficiency of multimodal processing and the scalability to incremental multi-sensory data type additions. This incurs a bottleneck of performance improvement of ERC. To this end, we present MetaDrop, a differentiable and end-to-end approach for the ERC task that learns module-wise decisions across modalities and conversation flows simultaneously, which supports adaptive information sharing pattern and dynamic fusion paths. Our framework mitigates the problem of modelling complex multimodal relations while ensuring it enjoys good scalability to the number of modalities. Experiments on two popular multimodal ERC datasets show that MetaDrop achieves new state-of-the-art results. © 2021 ACM.",Multi-modal; Speech recognition; emotion recognition; Emotion recognition; 'current; Complex networks; Dynamics; Sensory data; Multimodal dynamics; Contextual dynamic; contextual dynamics; Datatypes; Multi-Sensory; multimodal dynamics; Multimodal processing; Scalability; Shared network,,,emotion,No,Yes
scopus,Hybrid System for Emotion Recognition Based on Facial Expressions and Body Gesture Recognition,"Atanassov, A.V.; Pilev, D.I.; Tomova, F.N.; Kuzmanova, V.D.",2021,,,,10.1109/ICAI52893.2021.9639829,"This paper considers a hybrid multimodal model for improvement of the human emotion recognition based on facial expression and body gesture recognition. The paper extends the author's investigations related to the usage of pre-trained models of deep learning neural networks (DNN) for facial emotion recognition (FER) with addition of the emotions extracted from body language. In order to extract emotions from upper body gestures a second model of DNN was developed and trained with specific datasets. The information regarding recognized emotions obtained by both models is more accurate and can be used in education, medicine, psychology, product advertisement, marketing, human-machine interfaces, etc. In our case, it aims to personalize the lecture material of the students during their online training, taking into account their emotional state.  © 2021 IEEE.",Deep neural networks; Speech recognition; deep learning; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Convolutional neural networks; Convolutional neural network; Interface states; Multimodal models; Body gesture; Gesture recognition; DNN; Gestures recognition; Education computing; convolutional neural networks; Learning neural networks; Deep learning neural network; facial and body emotion recognition; Facial and body emotion recognition; Hybrid systems,,,emotion,No,No
scopus,Multimodal Emotion Recognition with Factorized Bilinear Pooling and Adversarial Learning,"Miao, H.; Zhang, Y.; Wang, D.; Feng, S.",2021,,,,10.1145/3487075.3487164,"With the fast development of social networks, the massive growth of the number of multimodal data such as images and texts allows people have higher demands for information processing from an emotional perspective. Emotion recognition requires a higher ability for the computer to simulate high-level visual perception understanding. However, existing methods often focus on the single-modality investigation. In this work, we propose a multimodal model based on factorized bilinear pooling (FBP) and adversarial learning for emotion recognition. In our model, a multimodal feature fusion network is proposed to encode the inter-modality features under the guidance of the FBP to help the visual and textual feature representation learn from each other interactively. Beyond that, we propose an adversarial network by introducing two discriminative classification tasks, emotion recognition and multimodal fusion prediction. Our entire method can be implemented end-to-end by using a deep neural network framework. Experimental results indicate that our proposed model achieves competitive performance on the extended FI dataset. Progressive results prove the ability of our model for emotion recognition against other single- and multi-modality works respectively.. © 2021 Association for Computing Machinery. All rights reserved.",Deep neural networks; Speech recognition; Behavioral research; Emotion recognition; Multi-modal data; Multimodal emotion recognition; Multimodal feature fusions; Multimodal models; Adversarial learning; Model-based OPC; Factorized bilinear pooling; High demand; Visual perception,,,emotion,No,Yes
scopus,MUSER: MUltimodal Stress Detection using Emotion Recognition as an Auxiliary Task,"Yao, Y.; Papakostas, M.; Burzo, M.; Abouelenien, M.; Mihalcea, R.",2021,,,,,"The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER – a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results. © 2021 Association for Computational Linguistics.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Affective Computing; Human computer interaction; Learning algorithms; Multitask learning; Affective state; Stress detection; Stresses; Human stress; Artificial intelligent; Intelligent agents; Inter-dependences; Modeling architecture,,,emotion,No,No
scopus,COIN: Conversational Interactive Networks for Emotion Recognition in Conversation,"Zhang, H.; Chai, Y.",2021,,,,10.18653/v1/2021.maiworkshop-1.3,"Emotion recognition in conversation has received considerable attention recently because of its practical industrial applications. Existing methods tend to overlook the immediate mutual interaction between different speakers in the speaker-utterance level, or apply single speaker-agnostic RNN for utterances from different speakers. We propose COIN, a conversational interactive model to mitigate this problem by applying state mutual interaction within history contexts. In addition, we introduce a stacked global interaction module to capture the contextual and inter-dependency representation in a hierarchical manner. To improve the robustness and generalization during training, we generate adversarial examples by applying the minor perturbations on multimodal feature inputs, unveiling the benefits of adversarial examples for emotion detection. The proposed model empirically achieves the current state-of-the-art results on the IEMOCAP benchmark dataset. © 2021 Association for Computational Linguistics",Speech recognition; Emotion recognition; Emotion detection; Multimodal features; Interaction modules; Generalisation; Feature input; Global interaction; Inter-dependencies; Interactive models; Mutual interaction,,,emotion,No,No
scopus,Sentiment and Emotion Analysis for Effective Human-Machine Interaction During Covid-19 Pandemic,"Prasad, G.; Dikshit, A.; Lalitha, S.",2021,,,,10.1109/SPIN52536.2021.9566147,"With the onset of Covid-19, interactions between humans and machines have increased at a rapid rate. Helping the machine identify the emotion and sentiment of the user plays a key role in making these interactions feel more natural. To do so, existing models for Speech Emotion Recognition (SER) and Sentiment Analysis (SA) focus on the detection of either only emotion or sentiment on acted databases. Unlike these existing works, this work presents a simple model with a comparatively small speech feature vector, to detect both emotion and sentiment from the spontaneous database, Multimodal Emotion Lines Dataset (MELD). This contains voice samples similar to those in a real-time environment. Speech features such as Mel Frequency Cepstral Coefficients (MFCC), Entropy, Teager Energy Operator have been extracted from the voice samples and are classified using Logit Boost, Logistic and Multiclass classifier. The performance of the model is improved by using feature selection techniques such as Backward elimination and Gaussian distribution coefficients. The proposed model is simple, and the results are comparable to existing work on the MELD database. © 2021 IEEE",Speech emotion recognition; Multi-modal; Speech recognition; Sentiment analysis; Classification (of information); Emotion analysis; sentiment analysis; Speech features; Features selection; Feature extraction; Human machine interaction; feature selection; Spontaneous dataset; Database systems; MELD; speech emotion recognition; Multimodal emotion line dataset; Rapid rate; spontaneous dataset,,,emotion,No,Yes
scopus,Emotion Transformer Fusion: Complementary Representation Properties of EEG and Eye Movements on Recognizing Anger and Surprise,"Wang, Y.; Jiang, W.-B.; Li, R.; Lu, B.-L.",2021,,,,10.1109/BIBM52615.2021.9669556,"Emotion recognition plays an important role in di-agnosing and treating many mental disorders as well as affective computing. Among six basic emotions, anger and surprise are relatively hard to be elicited in lab settings, and the complementary representation properties of encephalography (EEG) and eye movement signals on recognizing anger and surprise emotions remain unknown. Although the transformer architecture has the ability of parallelism which avoids many sequential operations as recurrent and convolutional layers, the knowledge of its performance and effectiveness on multimodal emotion recognition from EEG and eye movement signals is limited. To tackle these issues, we elaborately design the experiment and stimuli materials to effectively elicit surprise, anger, and neutral emotions, and propose an Emotion Transformer Fusion (ETF) model based on pure attention mechanism. Results of extensive experiments with multiple models on our dataset indicate that the complementary information of EEG and eye movements significantly improves the performance of discriminating anger, surprise and neutral emotions. Meanwhile, our proposed architecture outperforms baseline models with higher parallelism, which proves the capability of Transformer based architecture on multimodal emotion recognition with EEG and eye movement signals.  © 2021 IEEE.",Attention; Transformer; Performance; Speech recognition; Emotion recognition; Multimodal emotion recognition; Affective Computing; multimodal emotion recognition; attention; Eye movements; transformer; Mental disorders; anger; Architecture; Anger; Property; surprise; Surprize,,,emotion,No,Yes
scopus,An Audio Processing Approach using Ensemble Learning for Speech-Emotion Recognition for Children with ASD,"Valles, D.; Matin, R.",2021,,,,10.1109/AIIoT52608.2021.9454174,"Children with Autism Spectrum Disorder (ASD) find it difficult to detect human emotions in social interactions. A speech emotion recognition system was developed in this work, which aims to help these children to better identify the emotions of their communication partner. The system was developed using machine learning and deep learning techniques. Through the use of ensemble learning, multiple machine learning algorithms were joined to provide a final prediction on the recorded input utterances. The ensemble of models includes a Support Vector Machine (SVM), a Multi-Layer Perceptron (MLP), and a Recurrent Neural Network (RNN). All three models were trained on the Ryerson Audio-Visual Database of Emotional Speech and Songs (RAVDESS), the Toronto Emotional Speech Set (TESS), and the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D). A fourth dataset was used, which was created by adding background noise to the clean speech files from the datasets previously mentioned. The paper describes the audio processing of the samples, the techniques used to include the background noise, and the feature extraction coefficients considered for the development and training of the models. This study presents the performance evaluation of the individual models to each of the datasets, inclusion of the background noises, and the combination of using all of the samples in all three datasets. The evaluation was made to select optimal hyperparameters configuration of the models to evaluate the performance of the ensemble learning approach through majority voting. The overall performance of the ensemble learning reached a peak accuracy of 66.5%, reaching a higher performance emotion classification accuracy than the MLP model which reached 65.7%. © 2021 IEEE.",Speech emotion recognition; Speech recognition; Learning systems; Speech; Support vector machines; SVM; Learning algorithms; Multilayer neural networks; Recurrent neural networks; Emotion classification; Audio acoustics; RNN; Audio signal processing; Speech communication; Internet of things; Speech analysis; emotions; Ensemble; Speech emotion recognition systems; Autism; Acoustic noise; Communication partners; Ensemble learning approach; Extraction coefficients; MLP; Multi layer perceptron; Recurrent neural network (RNN),,,emotion,No,Yes
scopus,Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition,"Praveen, R.G.; Granger, E.; Cardinal, P.",2021,,,,10.1109/FG52635.2021.9667055,"Multimodal analysis has recently drawn much interest in affective computing, since it can improve the overall accuracy of emotion recognition over isolated uni-modal approaches. The most effective techniques for multimodal emotion recognition efficiently leverage diverse and complimentary sources of information, such as facial, vocal, and physiological modalities, to provide comprehensive feature representations. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos, where complex spatiotemporal relationships may be captured. Most of the existing fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complimentary nature of audiovisual (A-V) modalities. We introduce a cross-attentional fusion approach to extract the salient features across A - V modalities, allowing for accurate prediction of continuous values of valence and arousal. Our new cross-attentional A - V fusion model efficiently leverages the inter-modal relationships. In particular, it computes cross-attention weights to focus on the more contributive features across individual modalities, and thereby combine contributive feature representations, which are then fed to fully connected layers for the prediction of valence and arousal. The effectiveness of the proposed approach is validated experimentally on videos from the RECOLA and Fatigue (private) data-sets. Results indicate that our cross-attentional A - V fusion model is a cost-effective approach that outperforms state-of-the-art fusion approaches. Code is available: https://github.com/praveena2j/Cross-Attentional-AV-Fusion. © 2021 IEEE.",Speech recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; Affective Computing; Fusion model; Recurrent neural networks; Multimodal analysis; Audio-visual fusion; Cost effectiveness; Feature representation; Overall accuracies; Sources of informations; Spatio-temporal relationships,,,emotion,No,Yes
scopus,Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality,"Yan, X.; Zhao, L.-M.; Lu, B.-L.",2021,,,,10.1145/3474085.3475701,"Multimodal emotion recognition has long been a popular topic in affective computing since it significantly enhances the performance compared with that of a single modality. Among all, the combination of electroencephalography (EEG) and eye movement signals is one of the most attractive practices due to their complementarity and objectivity. However, the high cost and inconvenience of EEG signal acquisition severely hamper the popularization of multimodal emotion recognition in practical scenarios, while eye movement signals are much easier to acquire. To increase the feasibility and the generalization ability of emotion decoding without compromising the performance, we propose a generative adversarial network-based framework. In our model, a single modality of eye movements is used as input and it is capable of mapping the information onto multimodal features. Experimental results on SEED series datasets with different emotion categories demonstrate that our model with multimodal features generated by the single eye movement modality maintains competitive accuracies compared to those with multimodality input and drastically outperforms those single-modal emotion classifiers. This illustrates that the model has the potential to reduce the dependence on multimodalities without sacrificing performance which makes emotion recognition more applicable and practicable. © 2021 ACM.",Performance; Speech recognition; Multimodal emotion recognition; Affective Computing; Classification (of information); multimodal emotion recognition; Electroencephalography; Electrophysiology; Eye movements; Multi-modality; Multimodal features; Generative adversarial networks; eeg; Eeg; eye movements; generative adversarial networks; Generalization ability; High costs; Network-based framework; Signal acquisitions,,,emotion,No,Yes
scopus,Emotion Detection for Conversations Based on Reinforcement Learning Framework,"Huang, X.; Ren, M.; Han, Q.; Shi, X.; Nie, J.; Nie, W.; Liu, A.-A.",2021,,28,,10.1109/MMUL.2021.3065678,"In this article, we propose a novel reinforcement learning network that keeps track of the gradual emotional changes from every utterance throughout the conversation and uses this information for each utterance's emotion detection. Concretely, we first establish an agent and, then, utilize sliding windows to extract the accumulated emotional information before the current utterance. We define the concatenation of accumulated emotional information and the contextual information as the state of the reinforcement learning framework. The action of the established agent is formulated as the emotional label of the current utterance. On this basis, we formulate the progressive emotional interaction process throughout the conversation as a sequential decision problem and solve it with the reinforcement learning framework. Detailed evaluations on the published multimodal MELD dataset demonstrate the effectiveness of our approach.  © 1994-2012 IEEE.",Multi-modal; Emotion detection; Reinforcement learning; Emotional information; Contextual information; Sliding Window; Emotional interactions; Emotional change; Multimedia systems; Sequential decisions; Software engineering,,,emotion,Yes,No
scopus,Inferring Emotion from Large-scale Internet Voice Data: A Semi-supervised Curriculum Augmentation based Deep Learning Approach,"Zhou, S.; Jia, J.; Wu, Z.; Yang, Z.; Wang, Y.; Chen, W.; Meng, F.; Huang, S.; Shen, J.; Wang, X.",2021,,7,,,"Effective emotion inference from user queries helps to give a more personified response for Voice Dialogue Applications(VDAs). The tremendous amounts of VDA users bring in diverse emotion expressions. How to achieve a high emotion inferring performance from large-scale Internet Voice Data in VDAs? Traditionally, researches on speech emotion recognition are based on acted voice datasets, which have limited speakers but strong and clear emotion expressions. Inspired by this, in this paper, we propose a novel approach to leverage acted voice data with strong emotion expressions to enhance large-scale unlabeled internet voice data with diverse emotion expressions for emotion inferring. Specifically, we propose a novel semi-supervised multi-modal curriculum augmentation deep learning framework. First, to learn more general emotion cues, we adopt a curriculum learning based epoch-wise training strategy, which trains our model guided by strong and balanced emotion samples from acted voice data and sub-sequently leverages weak and unbalanced emotion samples from internet voice data.Second, to employ more diverse emotion expressions, we design a Multi-path Mixmatch Multimodal Deep Neural Network(MMMD), which effectively learns feature representations for multiple modalities and trains labeled and unlabeled data in hybrid semi-supervised methods for superior generalisation and robustness. Experiments on an internet voice dataset with 500,000 utterances show our method outperforms (+10.09% in terms of F1) several alternative baselines, while an acted corpus with 2,397 utterances contributes 4.35%. To further compare our method with state-of-the-art techniques in traditionally acted voice datasets, we also conduct experiments on public dataset IEMOCAP. The results reveal the effectiveness of the proposed approach. *Corresponding author: J. Jia (jjia@mail.tsinghua.edu.cn) Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Performance; Deep neural networks; Multi-modal; Speech recognition; Learn+; Supervised learning; Emotion expression; Curricula; Semi-supervised; Learning approach; Large scale Internet; User query; Voice data; Voice dialogue,,,emotion,Yes,Yes
scopus,Multimodal Emotion Recognition Using Different Fusion Techniques,"Subramanian, G.; Cholendiran, N.; Prathyusha, K.; Balasubramanain, N.; Aravinth, J.",2021,,,,10.1109/ICBSII51839.2021.9445146,"Human beings have the ability to understand and visualize various emotions on a daily basis. This could be done by noticing various features such as facial muscle movements, speech, hand gestures, etc. The automated emotion recognition is an important issue and has also been a lively research topic for the modern time. At the moment, several research workers have taken part in inheriting two or more unimodals for better understanding. This paper shows an approach for emotion recognition that uses three modalities: facial images, audio signals, and electroencephalogram (EEG) signals from FER and Ck+, RAVDESS and SEED-IV datasets respectively. Finally, various fusion techniques were approached and each of these fusion methods gave different results. The maximum accuracy of 71.24% was obtained with help of an autoencoder approach when combined with SVM classifier.  © 2021 IEEE.",Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Multimodal; Electroencephalography; Biomedical signal processing; Feature fusion; Support vector machines; Electroencephalogram (EEG); Fusion methods; Autoencoder; Fusion techniques; Convolutional neural network (CNN); Electroencephalogram signals; Research topics; Maximum accuracies; SVM classifiers,,,emotion,No,Yes
scopus,"Me, myself, and ire: Effects of automatic transcription quality on emotion, sarcasm, and personality detection","Culnan, J.; Park, S.; Krishnaswamy, M.; Sharp, R.",2021,,,,,"In deployment, systems that use speech as input must make use of automated transcriptions. Yet, typically when these systems are evaluated, gold transcriptions are assumed. We explicitly examine the impact of transcription errors on the downstream performance of a multi-modal system on three related tasks from three datasets: emotion, sarcasm, and personality detection. We include three separate transcription tools and show that while all automated transcriptions propagate errors that substantially impact downstream performance, the open-source tools fair worse than the paid tool, though not always straightforwardly, and word error rates do not correlate well with downstream performance. We further find that the inclusion of audio features partially mitigates transcription errors, but that a naive usage of a multi-task setup does not. We make available all code and data splits needed to reproduce all of our experiments. © 2021 Association for Computational Linguistics.",Performance; Multi tasks; Computational linguistics; Down-stream; Audio features; Multimodal system; Word error rate; Automatic transcription; Errors; Open source tools; Personality detections,,,emotion,No,No
scopus,Speech Emotion Recognition Based on Secondary Feature Reconstruction,"Yuan, Z.; Li, S.; Zhang, W.; Du, R.; Sun, X.; Wang, H.",2021,,,,10.1109/ICCIA52886.2021.00036,"In the field of speech emotion recognition, most methods usually extract the audio spectrum first and then use image classification models to identify emotion categories. However, the spectrum contains timing information, so it is unreasonable to use the image classification model in natural scenes. Based on this, a Res-Trans model is proposed, which performs secondary time-series feature reconstruction on the extracted audio features. Compared with traditional methods, the Res-Trans method achieves a new state-of-the-art performance on multimodal emotion recognition dataset. At the same time, we propose a Horizontal Mixup data enhancement method suitable for audio spectrum enhancement, and experiments have verified the effectiveness of proposed method. Finally, we add an extra voiceprint recognition task to regularize the feature extraction network. After the integration of models, our model won first place in the 2020 iFLYTEK Multimodal Emotion Analysis and Recognition Challenge. The proposed Res-Trans model will be published soon 1.  © 2021 IEEE.",Speech emotion recognition; Speech recognition; deep learning; Deep learning; Classification (of information); Feature reconstruction; Image classification; speech emotion recognition; Classification models; Images classification; Audio spectrum; Horizontal mixup; Horizontal Mixup; Res-trans; Res-Trans; Timing information,,,emotion,No,Yes
scopus,Relating whole-brain functional connectivity to self-reported negative emotion in a large sample of young adults using group regularized canonical correlation analysis,"Tozzi, L.; Tuzhilina, E.; Glasser, M.F.; Hastie, T.J.; Williams, L.M.",2021,,237,,10.1016/j.neuroimage.2021.118137,"The goal of our study was to use functional connectivity to map brain function to self-reports of negative emotion. In a large dataset of healthy individuals derived from the Human Connectome Project (N = 652), first we quantified functional connectivity during a negative face-matching task to isolate patterns induced by emotional stimuli. Then, we did the same in a complementary task-free resting state condition. To identify the relationship between functional connectivity in these two conditions and self-reports of negative emotion, we introduce group regularized canonical correlation analysis (GRCCA), a novel algorithm extending canonical correlations analysis to model the shared common properties of functional connectivity within established brain networks. To minimize overfitting, we optimized the regularization parameters of GRCCA using cross-validation and tested the significance of our results in a held-out portion of the data set using permutations. GRCCA consistently outperformed plain regularized canonical correlation analysis. The only canonical correlation that generalized to the held-out test set was based on resting state data (r = 0.175, permutation test p = 0.021). This canonical correlation loaded primarily on Anger-aggression. It showed high loadings in the cingulate, orbitofrontal, superior parietal, auditory and visual cortices, as well as in the insula. Subcortically, we observed high loadings in the globus pallidus. Regarding brain networks, it loaded primarily on the primary visual, orbito-affective and ventral multimodal networks. Here, we present the first neuroimaging application of GRCCA, a novel algorithm for regularized canonical correlation analyses that takes into account grouping of the variables during the regularization scheme. Using GRCCA, we demonstrate that functional connections involving the visual, orbito-affective and multimodal networks are promising targets for investigating functional correlates of subjective anger and aggression. Crucially, our approach and findings also highlight the need of cross-validation, regularization and testing on held out data for correlational neuroimaging studies to avoid inflated effects. © 2021",Adult; Female; Humans; Young Adult; Male; Emotion; adult; female; human; male; physiology; controlled study; procedures; correlation analysis; human experiment; Brain; algorithm; nerve cell network; neuroimaging; article; anger; fear; facial recognition; cross validation; self report; young adult; brain; functional connectivity; nuclear magnetic resonance imaging; diagnostic imaging; Anger; Facial Recognition; Functional connectivity; Self Report; perception; Magnetic Resonance Imaging; major clinical study; connectome; Connectome; human tissue; Canonical correlations; Face-matching; Fear; globus pallidus; insula; Negative valence; Nerve Net; Resting state; Social Perception; validation process; visual cortex,,,emotion,No,No
scopus,MMTrans-MT: A Framework for Multimodal Emotion Recognition Using Multitask Learning,"Shen, J.; Zheng, J.; Wang, X.",2021,,,,10.1109/ICACI52617.2021.9435906,"With the development of deep learning, emotion recognition tasks are more inclined to use multimodal data and adequate supervised information to improve accuracy. In this work, MMTrans-MT (Multimodal Transformer-Multitask), the framework for multimodal emotion recognition using multitask learning is proposed. It has three modules: modalities representation module, multimodal fusion module, and multitask output module. Three modalities, i.e, words, audio and video, are comprehensively utilized to carry out emotion recognition by a simple but efficient fusion model based on Transformer. As for multitask learning, the two tasks are defined as categorical emotion classification and dimensional emotion regression. Considering a potential mapping relationship between two kinds of emotion model, multitask learning is adopted to make the two tasks promote each other and improve recognition accuracy. We conduct experiments on CMU-MOSEI and IEMOCAP datasets. Comprehensive experiments show that the accuracy of recognition using multimodal information is higher than that using unimodal information. Adopting multitask learning promotes the performance of emotion recognition.  © 2021 IEEE.",Transformer; Speech recognition; Emotion recognition; Multi-modal information; Deep learning; Multi-modal data; Multi-modal fusion; Multimodal emotion recognition; Learning systems; Multimodal fusion; Multitask learning; Multi-task learning; Emotion classification; Recognition accuracy; Emotion model; Intelligent computing; Potential mapping,,,emotion,No,Yes
scopus,"The MuSe 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress","Stappen, L.; Baird, A.; Christ, L.; Schumann, L.; Sertolli, B.; Meßner, E.-M.; Cambria, E.; Zhao, G.; Schuller, B.W.",2021,,,,10.1145/3475957.3484450,"Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the tasks of sentiment and emotion, as well as physiological-emotion and emotion-based stress recognition through more comprehensively integrating the audio-visual, language, and biological signal modalities. The purpose of MuSe 2021 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), the sentiment analysis community (symbol-based), and the health informatics community. We present four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus on continuous emotion (valence and arousal) prediction; MuSe-Sent, in which participants recognise five classes each for valence and arousal; and MuSe-Physio, in which the novel aspect of 'physiological-emotion' is to be predicted. For this year's challenge, we utilise the MuSe-CaR dataset focusing on user-generated reviews and introduce the Ulm-TSST dataset, which displays people in stressful depositions. This paper also provides detail on the state-of-the-art feature sets extracted from these datasets for utilisation by our baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each sub-challenge, a competitive baseline for participants is set; namely, on test, we report a Concordance Correlation Coefficient (CCC) of .4616 CCC for MuSe-Wilder; .5088 CCC for MuSe-Stress, and .4908 CCC for MuSe-Physio. For MuSe-Sent an F1 score of 32.82% is obtained. © 2021 ACM.",Multi-modal; Physiology; Multi-modal fusion; multimodal sentiment analysis; Affective Computing; Sentiment analysis; multimodal fusion; Multimodal sentiment analyse; Recurrent neural networks; Electrodermal activity; Challenge; Stress detection; affective computing; Correlation coefficient; benchmark; Benchmark; challenge; electrodermal activity; stress detection,,,emotion,No,Yes
scopus,MindLink-Eumpy: An Open-Source Python Toolbox for Multimodal Emotion Recognition,"Li, R.; Liang, Y.; Liu, X.; Wang, B.; Huang, W.; Cai, Z.; Ye, Y.; Qiu, L.; Pan, J.",2021,,15,,10.3389/fnhum.2021.621493,"Emotion recognition plays an important role in intelligent human–computer interaction, but the related research still faces the problems of low accuracy and subject dependence. In this paper, an open-source software toolbox called MindLink-Eumpy is developed to recognize emotions by integrating electroencephalogram (EEG) and facial expression information. MindLink-Eumpy first applies a series of tools to automatically obtain physiological data from subjects and then analyzes the obtained facial expression data and EEG data, respectively, and finally fuses the two different signals at a decision level. In the detection of facial expressions, the algorithm used by MindLink-Eumpy is a multitask convolutional neural network (CNN) based on transfer learning technique. In the detection of EEG, MindLink-Eumpy provides two algorithms, including a subject-dependent model based on support vector machine (SVM) and a subject-independent model based on long short-term memory network (LSTM). In the decision-level fusion, weight enumerator and AdaBoost technique are applied to combine the predictions of SVM and CNN. We conducted two offline experiments on the Database for Emotion Analysis Using Physiological Signals (DEAP) dataset and the Multimodal Database for Affect Recognition and Implicit Tagging (MAHNOB-HCI) dataset, respectively, and conducted an online experiment on 15 healthy subjects. The results show that multimodal methods outperform single-modal methods in both offline and online experiments. In the subject-dependent condition, the multimodal method achieved an accuracy of 71.00% in the valence dimension and an accuracy of 72.14% in the arousal dimension. In the subject-independent condition, the LSTM-based method achieved an accuracy of 78.56% in the valence dimension and an accuracy of 77.22% in the arousal dimension. The feasibility and efficiency of MindLink-Eumpy for emotion recognition is thus demonstrated. © Copyright © 2021 Li, Liang, Liu, Wang, Huang, Cai, Ye, Qiu and Pan.",convolutional neural network; emotion; facial expression; adult; female; human; male; multimodal emotion recognition; controlled study; electroencephalogram; human experiment; article; prediction; arousal; long short term memory network; support vector machine; transfer of learning; clinical article; long short-term memory network (LSTM); feasibility study; multitask convolutional neural network (CNN); software; subject-independent method; support vector machine (SVM),,,emotion,No,Yes
scopus,BECMER: A Fusion Model Using BERT and CNN for Music Emotion Recognition,"Sung, B.-H.; Wei, S.-C.",2021,,,,10.1109/IRI51335.2021.00068,"Music emotion analysis has been an ever-growing field of research in music in-formation retrieval. To solve the cold start problem of content-based recommendation systems, a method for automatic music labeling is needed. Due to recent advances, neural networks can be used to extract audio features for a wide variety of tasks. When humans listen to a song, it is the music or the lyrics that touch the heart the most. Therefore, this study will try to predict the type of music emotion based on the audio signal and the lyrics information. For model building, convolutional neural networks (CNNs) will be used on the audio signals and natural language processing (NLP) models on the lyrics. A new dataset ABP is compiled from three datasets of Western pop music where each song contains valence and arousal values judged by humans. The type of music emotion will be categorized based on the four quadrants formed by the valence and arousal axes. It is con-firmed in the experiment that use of audio and lyrics information to classify the emotions of songs has a better classification performance than use of the au-dio-only learning methods in previous studies. Compared with a related work, this study has improved the accuracy of the audio model and the lyrics model by 816%.  © 2021 IEEE.",Multi-modal; CNN; Emotion recognition; Learning systems; Classification (of information); Emotion analysis; Music; Music emotion classifications; Convolutional neural networks; Convolutional neural network; Fusion model; Audio acoustics; Recommender systems; NLP; Natural language processing systems; Music emotions; Audio signal; Cold start problems; Multimodal music emotion classification; Multimodal Music Emotion Classification,,,emotion,Yes,Yes
scopus,End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss,"Huynh, V.T.; Yang, H.-J.; Lee, G.-S.; Kim, S.-H.",2021,,28,,10.1109/MMUL.2021.3080305,"This work presents an approach for emotion recognition in video through the interaction of visual, audio, and language information in an end-to-end learning manner with three key points: 1) lightweight feature extractor, 2) attention strategy, and 3) adaptive loss. We proposed a lightweight deep architecture with approximately 1 MB, which for the most crucial part, accounts for feature extraction, in the emotion recognition systems. The relationship in regard to the time dimension of features is explored with temporal convolutional network instead of RNNs-based architecture to leverage the parallelism and avoid the challenge of vanishing gradient. The attention strategy is employed to adjust the knowledge of temporal networks based on the time dimension and learning of each modality's contribution to the final results. The interaction between the modalities is also investigated when training with adaptive objective function, which adjusts the network's gradient. The experimental results obtained on a large-scale dataset for emotion recognition on Koreans demonstrate the superiority of our method when employing attention mechanism and adaptive loss during training.  © 1994-2012 IEEE.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Affective Computing; Visual languages; Convolutional neural networks; Network architecture; Large dataset; Feature extraction; Sentiment Analysis; Convolutional networks; Multimodal Learning; Deep architectures; End-to-End Learning; Language informations; Large-scale dataset; Objective functions,,,emotion,No,No
scopus,Affect-Predictive Models: Predicting Emotional Responses Directly to Stimuli,"Zaitsev, I.",2021,,,,10.1145/3498851.3498962,"Historically, in the field of Affective Computing the research focus was on recognizing emotions expressed by humans. In our work, we show that it is possible to predict emotional reaction as central tendency directly to a stimulus, prior to its actual exposure to any human. This is achieved by training new Affect-Predictive machine learning models, which leverage the large amount of weak emotional signals of aggregated and fully anonymized reactions of online users to a vast variety of textual and visual stimuli. Based on our Affect-Predictive computer vision model we (a) set a new benchmark to evaluate its predictive power on an open-access affective image set, (b) generate affective saliency maps and (c) discuss a few instances of peculiar visual patterns learned by the model. We theorize that Affect-Predictive models can be used to learn implicit patterns allowing AI agents to see the world and react in a more human-like way: imagine an autonomous vehicle that slows down automatically when detecting something highly surprising or negative. Using our Affect-Predictive natural language model we demonstrate that it is possible to predict general emotional response to a piece of text from the reader perspective and how it can be used on practice to improve social listening. We conclude with a discussion on the broader implication of the Affect-Predictive models: human emotional reactions can be treated as natural encoders of multimodal stimuli capturing just enough semantics to allow instantaneous decision-making; the ability to automatically predict such reactions directly to stimuli opens up a lot of new opportunities in the field of Affective Computing.  © 2021 ACM.",Semantics; Machine learning; Behavioral research; Affective Computing; machine learning; Decision making; Forecasting; Machine learning models; Natural language processing systems; Predictive models; Emotional response; affective computing; Research focus; Recognizing emotions; Emotional reactions; Affect-predictive model; affect-predictive models; Central tendencies; emotional response; stimuli-reactions datasets; Stimulus-reaction dataset,,,emotion,No,No
scopus,A multimodal emotion recognition method based on facial expressions and electroencephalography,"Tan, Y.; Sun, Z.; Duan, F.; Solé-Casals, J.; Caiafa, C.F.",2021,,70,,10.1016/j.bspc.2021.103029,"Human-robot interaction (HRI) systems play a critical role in society. However, most HRI systems nowadays still face the challenge of disharmony, resulting in an inefficient communication between the human and the robot. In this paper, a multimodal emotion recognition method is proposed to establish an HRI system with a low sense of disharmony. This method is based on facial expressions and electroencephalography (EEG). The image classification method of facial expressions and the suitable feature extraction method of EEG were investigated based on the public datasets. And then these methods were applied to both images and EEG data acquired by ourselves. In addition, the Monte Carlo method was used to merge the results and solve the problem of having a small dataset. The multimodal emotion recognition method was combined with the HRI system, where it achieved a recognition rate of 83.33%. Furthermore, in order to evaluate the HRI system from the user's point of view, a perceptual assessment method was proposed to evaluate our system, in which the system was scored by the participants based on their experience, achieving an average score of 7 (the scores were ranged from 0 to 10). Experimental results demonstrate the effectiveness and feasibility of the multimodal emotion recognition method, which can be useful to reduce the sense of disharmony of HRI systems. © 2021 Elsevier Ltd",convolutional neural network; Facial expressions; emotion; facial expression; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; adult; female; human; male; Multimodal emotion recognition; Human robot interaction; Classification (of information); Article; recognition; electroencephalography; Electroencephalography; Electrophysiology; human experiment; normal human; feature extraction; young adult; Man machine systems; Humans-robot interactions; Recognition methods; Classification methods; Feature extraction methods; Monte Carlo method; Monte Carlo methods; Interaction systems; perception; Images classification; feasibility study; Human-robot interaction system; measurement accuracy,,,emotion,No,No
scopus,MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos,"Shen, G.; Wang, X.; Duan, X.; Li, H.; Zhu, W.",2020,,,,10.1145/3394171.3413909,"Humans can perceive subtle emotions from various cues and contexts, even without hearing or seeing others. However, existing video datasets mainly focus on recognizing the emotions of the speakers from complete modalities. In this work, we present the task of multimodal emotion reasoning in videos. Beyond directly recognizing emotions from multimodal signals of target persons, this task requires a machine capable of reasoning about human emotions from the contexts and surrounding world. To facilitate the study towards this task, we introduce a new dataset, MEmoR, that provides fine-grained emotion annotations for both speakers and non-speakers. The videos in MEmoR are collected from TV shows closely in real-life scenarios. In these videos, while speakers may be non-visually described, non-speakers always deliver no audio-textual signals and are often visually inconspicuous. This modality-missing characteristic makes MEmoR a more practical yet challenging testbed for multimodal emotion reasoning. In support of various reasoning behaviors, the proposed MEmoR dataset provides both short-term contexts and external knowledge. We further propose an attention-based reasoning approach to model the intra-personal emotion contexts, inter-personal emotion propagation, and the personalities of different individuals. Experimental results demonstrate that our proposed approach outperforms related baselines significantly. We isolate and analyze the validity of different reasoning modules across various emotions of speakers and non-speakers. Finally, we draw forth several future research directions for multimodal emotion reasoning with MEmoR, aiming to empower high Emotional Quotient (EQ) in modern artificial intelligence systems. The code and dataset released on https://github.com/sunlightsgy/MEmoR.  © 2020 ACM.",emotion recognition; Behavioral research; Artificial intelligence; multimodal; Human emotion; dataset; Audition; Recognizing emotions; reasoning; Artificial intelligence systems; Based reasonings; External knowledge; Future research directions; Subtle emotions; Turing machines; Video datasets,,,emotion,No,No
scopus,Temporal aggregation of audio-visual modalities for emotion recognition,"Birhala, A.; Ristea, C.N.; Radoi, A.; Dutu, L.C.",2020,,,,10.1109/TSP49548.2020.9163474,"Emotion recognition has a pivotal role in affective computing and in human-computer interaction. The current technological developments lead to increased possibilities of collecting data about the emotional state of a person. In general, human perception regarding the emotion transmitted by a subject is based on vocal and visual information collected in the first seconds of interaction with the subject. As a consequence, the integration of verbal (i.e., speech) and non-verbal (i.e., image) information seems to be the preferred choice in most of the current approaches towards emotion recognition. In this paper, we propose a multimodal fusion technique for emotion recognition based on combining audio-visual modalities from a temporal window with different temporal offsets for each modality. We show that our proposed method outperforms other methods from the literature and human accuracy rating. The experiments are conducted over the open-access multimodal dataset CREMA-D. © 2020 IEEE.",convolutional neural network; Speech recognition; emotion recognition; Behavioral research; Emotion recognition; Multi-modal fusion; Affective Computing; Human computer interaction; data fusion; Multi-modal dataset; Signal processing; Visual information; multimodal information; spectrogram; asynchronous data; Human perception; Technological development; Temporal aggregation,,,emotion,No,Yes
scopus,Attention-based multi-modal sentiment analysis and emotion detection in conversation using rnn,"Huddar, M.G.; Sannakki, S.S.; Rajpurohit, V.S.",2021,,6,,10.9781/ijimai.2020.07.004,"The availability of an enormous quantity of multimodal data and its widespread applications, automatic sentiment analysis and emotion classification in the conversation has become an interesting research topic among the research community. The interlocutor state, context state between the neighboring utterances and multimodal fusion play an important role in multimodal sentiment analysis and emotion detection in conversation. In this article, the recurrent neural network (RNN) based method is developed to capture the interlocutor state and contextual state between the utterances. The pair-wise attention mechanism is used to understand the relationship between the modalities and their importance before fusion. First, two-two combinations of modalities are fused at a time and finally, all the modalities are fused to form the trimodal representation feature vector. The experiments are conducted on three standard datasets such as IEMOCAP, CMU-MOSEI, and CMU-MOSI. The proposed model is evaluated using two metrics such as accuracy and F1-Score and the results demonstrate that the proposed model performs better than the standard baselines. © 2021, Universidad Internacional de la Rioja. All rights reserved.",Sentiment Analysis; Multimodal Fusion; Emotion Detection; Attention Model; Contextual Information; Interlocutor State,,,emotion,No,Yes
scopus,Deep learning-based late fusion of multimodal information for emotion classification of music video,"Pandeya, Y.R.; Lee, J.",2021,,80,,10.1007/s11042-020-08836-3,"Affective computing is an emerging area of research that aims to enable intelligent systems to recognize, feel, infer and interpret human emotions. The widely spread online and off-line music videos are one of the rich sources of human emotion analysis because it integrates the composer’s internal feeling through song lyrics, musical instruments performance and visual expression. In general, the metadata which music video customers to choose a product includes high-level semantics like emotion so that automatic emotion analysis might be necessary. In this research area, however, the lack of a labeled dataset is a major problem. Therefore, we first construct a balanced music video emotion dataset including diversity of territory, language, culture and musical instruments. We test this dataset over four unimodal and four multimodal convolutional neural networks (CNN) of music and video. First, we separately fine-tuned each pre-trained unimodal CNN and test the performance on unseen data. In addition, we train a 1-dimensional CNN-based music emotion classifier with raw waveform input. The comparative analysis of each unimodal classifier over various optimizers is made to find the best model that can be integrate into a multimodal structure. The best unimodal modality is integrated with corresponding music and video network features for multimodal classifier. The multimodal structure integrates whole music video features and makes final classification with the SoftMax classifier by a late feature fusion strategy. All possible multimodal structures are also combined into one predictive model to get the overall prediction. All the proposed multimodal structure uses cross-validation to overcome the data scarcity problem (overfitting) at the decision level. The evaluation results using various metrics show a boost in the performance of the multimodal architectures compared to each unimodal emotion classifier. The predictive model by integration of all multimodal structure achieves 88.56% in accuracy, 0.88 in f1-score, and 0.987 in area under the curve (AUC) score. The result suggests human high-level emotions are automatically well classified in the proposed CNN-based multimodal networks, even though a small amount of labeled data samples is available for training. © 2020, The Author(s).",Semantics; CNN; Modal analysis; Multi-modal information; Deep learning; Classification (of information); Metadata; Convolutional neural networks; Emotion classification; Statistical tests; Late fusion; Intelligent systems; Multimodal architectures; Multimodal classifiers; Area under the curves; Comparative analysis; High level semantics; Multimodal approach; Multimodal structure; Music video dataset; Musical instruments; Predictive analytics,,,emotion,Yes,Yes
scopus,Multimodal Emotion Recognition Using Deep Generalized Canonical Correlation Analysis with an Attention Mechanism,"Lan, Y.-T.; Liu, W.; Lu, B.-L.",2020,,,,10.1109/IJCNN48605.2020.9207625,"Since multimodal learning is able to take advantage of the complementarity of multimodal signals, the performance of multimodal emotion recognition usually surpasses that based on a single modality. In this paper, we introduce deep generalized canonical correlation analysis with an attention mechanism (DGCCA-AM) to multimodal emotion recognition. This model extends the conventional canonical correlation analysis (CCA) from two modalities to arbitrarily numerous modalities and implements multimodal adaptive fusion with an attention mechanism. By adjusting the weights matrices to maximize the generalized correlation of different modalities, DGCCA-AM extracts emotion-related information from multiple modalities and discards noises. The attention mechanism allows a neural network to learn adaptive fusion weights for different modalities and produces a more effective multimodal fusion and superior emotion recognition performance. We evaluate DGCCA-AM on a public multimodal dataset, SEED-V. Our experimental results demonstrate that DGCCA-AM achieves a state-of-the-art mean accuracy of 82.11% and standard deviation of 2.76% for five emotion classifications with three modalities. © 2020 IEEE.",attention mechanism; Attention mechanisms; Speech recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; multimodal emotion recognition; Emotion classification; Multi-modal learning; Neural networks; Correlation methods; Multimodal deep learning; Canonical correlation analysis; Amplitude modulation; deep generalized canonical correlation analysis; Generalized canonical correlation analysis; Generalized correlation,,,emotion,No,Yes
scopus,"K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations","Park, C.Y.; Cha, N.; Kang, S.; Kim, A.; Khandoker, A.H.; Hadjileontiadis, L.; Oh, A.; Jeong, Y.; Lee, U.",2020,,7,,10.1038/s41597-020-00630-y,"Recognizing emotions during social interactions has many potential applications with the popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions arising in the wild as they were collected in constrained environments. Therefore, studying emotions in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. Distinct from previous datasets, it includes emotion annotations from all three available perspectives: self, debate partner, and external observers. Raters annotated emotional displays at intervals of every 5 seconds while viewing the debate footage, in terms of arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the first publicly available emotion dataset accommodating the multiperspective assessment of emotions during social interactions. © 2020, The Author(s).",Humans; Emotions; emotion; human; speech; Speech; arousal; Arousal; social behavior; Social Behavior,,,emotion,Yes,No
scopus,An Improved Multimodal Dimension Emotion Recognition Based on Different Fusion Methods,"Su, H.; Liu, B.; Tao, J.; Dong, Y.; Huang, J.; Lian, Z.; Song, L.",2020,,2020-December,,10.1109/ICSP48669.2020.9321008,"Continuous emotion recognition is a challenging task and a key part of human-computer interaction, especially multimodal emotion recognition can effectively improve the accuracy and robustness of recognition. But there are limited emotion data sets, and it is difficult to extract emotion features. We present a multi-level segmented decision level fusion emotion recognition model to improve the performance of emotion recognition. In this paper, we predict multi-modal dimensional emotional state on AVEC2017 dataset. Our model uses Bidirectional Long Short-Term Memory (BLSTM) as multi-level segmented emotional feature learning model, and uses the SVR model as fusion model of the decision layer. The BLSTM can model different forms of emotional information in time, and can also consider the impact of previous and later emotional features on current results. The SVR model can compensate for the redundant information of emotion recognition. At the same time, we also consider annotation delay and temporal pooling in our multi-modal dimensional emotion recognition model. Our multi-modal emotion recognition model achieves significant recognition improvements and provide the robustness. Finally, we compare the baseline methods which used the same dataset, and find that the CCC performance of our method is the best on arousal, which is 0.685. Our research shows that the proposed multi-layer segmentation decision level fusion emotion recognition model is conducive to improving performance.  © 2020 IEEE.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; multimodal fusion; Emotional information; Signal processing; Feature learning; Decision level fusion; dimensional emotion recognition; annotation delay; Baseline methods; BLSTM; Improving performance; multi-modal features; temporal pooling; Temporal pooling,,,emotion,Yes,Yes
scopus,Multimodal Emotion Recognition Fusion Analysis Adapting BERT with Heterogeneous Feature Unification,"Lee, S.; Han, D.K.; Ko, H.",2021,,9,,10.1109/ACCESS.2021.3092735,"Human communication includes rich emotional content, thus the development of multimodal emotion recognition plays an important role in communication between humans and computers. Because of the complex emotional characteristics of a speaker, emotional recognition remains a challenge, particularly in capturing emotional cues across a variety of modalities, such as speech, facial expressions, and language. Audio and visual cues are particularly vital for a human observer in understanding emotions. However, most previous work on emotion recognition has been based solely on linguistic information, which can overlook various forms of nonverbal information. In this paper, we present a new multimodal emotion recognition approach that improves the BERT model for emotion recognition by combining it with heterogeneous features based on language, audio, and visual modalities. Specifically, we improve the BERT model due to the heterogeneous features of the audio and visual modalities. We introduce the Self-Multi-Attention Fusion module, Multi-Attention fusion module, and Video Fusion module, which are attention based multimodal fusion mechanisms using the recently proposed transformer architecture. We explore the optimal ways to combine fine-grained representations of audio and visual features into a common embedding while combining a pre-trained BERT model with modalities for fine-tuning. In our experiment, we evaluate the commonly used CMU-MOSI, CMU-MOSEI, and IEMOCAP datasets for multimodal sentiment analysis. Ablation analysis indicates that the audio and visual components make a significant contribution to the recognition results, suggesting that these modalities contain highly complementary information for sentiment analysis based on video input. Our method shows that we achieve state-of-the-art performance on the CMU-MOSI, CMU-MOSEI, and IEMOCAP dataset.  © 2013 IEEE.",Speech recognition; BERT; Modal analysis; Multimodal emotion recognition; Sentiment analysis; Visual languages; transformer; Human communications; Emotional recognition; Linguistic information; attention based multimodal; Audio and visual cues; heterogeneous features; Heterogeneous features; Non-verbal information; State-of-the-art performance,,,emotion,No,Yes
scopus,Multimodal Classification of Emotions in Latin Music,"Catharin, L.G.; Ribeiro, R.P.; Silla, C.N.; Costa, Y.M.G.; Feltrim, V.D.",2020,,,,10.1109/ISM.2020.00038,"In this study we classified the songs of the Latin Music Mood Database (LMMD) according to their emotion using two approaches: single-step classification, which consists of classifying the songs by emotion, valence, arousal and quadrant; and multistep classification, which consists of using the predictions of the best valence and arousal classifiers to classify quadrants and the best valence, arousal and quadrant predictions as features to classify emotions. Our hypothesis is that breaking the emotion classification in smaller problems would reduce complexity and improve results. Our best single-step emotion and valence classifiers used multimodal sets of features extracted from lyrics and audio. Our best arousal classifier used features extracted from lyrics and SMOTE to mitigate the dataset imbalance. The proposed multistep emotion classifier, which uses the predictions of a multistep quadrant classifier, improved the single-step classifier performance, reaching 0.605 of mean f-measure. These results show that using valence, arousal, and consequently, quadrant information can improve the prediction of specific emotions.  © 2020 IEEE.",Multi-modal; Classification (of information); Emotion classification; Forecasting; multimodal emotion classification; Classification of emotions; Classifier performance; LMMD; MER; Multi-step; Multi-step classification; Sets of features; Single-step,,,emotion,No,Yes
scopus,Attentively-Coupled Long Short-Term Memory for Audio-Visual Emotion Recognition,"Hsu, J.-H.; Wu, C.-H.",2020,,,,,"There have been more and more studies on emotion recognition through multiple modalities. In the existing audiovisual emotion recognition methods, few studies focused on modeling emotional fluctuations in the signals. Besides, how to fuse multimodal signals, such as audio-visual signals, is still a challenging issue. In this paper, segments of audio-visual signals are extracted and considered as the recognition unit to characterize the emotional fluctuation. An Attentively-Coupled long-short term memory (ACLSTM) is proposed to combine the audio-based and visual-based LSTMs to improve the emotion recognition performance. In the Attentively-Coupled LSTM, the Coupled LSTM is used as the fusion model, and the neural tensor network (NTN) is employed for attention estimation to obtain the segment-based emotion consistency between audio and visual segments. Compared with previous approaches, the experimental results showed that the proposed method achieved the best results of 70.1% in multi-modal emotion recognition on the dataset BAUM-I.  © 2020 APSIPA.",Long short-term memory; Speech recognition; Emotion recognition; Brain; Fusion model; Multiple modalities; Audiovisual emotion recognition; Attention estimations; Audio-based; Recognition units; Segment-based,,,emotion,No,Yes
scopus,Emotion recognition by fusing time synchronous and time asynchronous representations,"Wu, W.; Zhang, C.; Woodland, P.C.",2021,,2021-June,,10.1109/ICASSP39728.2021.9414880,"In this paper, a novel two-branch neural network model structure is proposed for multimodal emotion recognition, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). To capture correlations between each word and its acoustic realisation, the TSB combines speech and text modalities at each input window frame and then uses pooling across time to form a single embedding vector. The TAB, by contrast, provides cross-utterance information by integrating sentence text embeddings from a number of context utterances into another embedding vector. The final emotion classification uses both the TSB and the TAB embeddings. Experimental results on the IEMOCAP dataset demonstrate that the two-branch structure achieves state-of-the-art results in 4-way classification with all common test setups. When using automatic speech recognition (ASR) output instead of manually transcribed reference text, it is shown that the cross-utterance information considerably improves robustness against ASR errors. Furthermore, by incorporating an extra class for all the other emotions, the final 5-way classification system with ASR hypotheses can be viewed as a prototype for more realistic emotion recognition systems. © 2021 IEEE",Embeddings; Speech recognition; Emotion recognition; Character recognition; Multimodal emotion recognition; State of the art; Classification (of information); Emotion classification; Statistical tests; Signal processing; Neural network model; Automatic speech recognition; Branch structure; Classification system,,,emotion,No,No
scopus,Improving Depression Level Estimation by Concurrently Learning Emotion Intensity,"Qureshi, S.A.; Dias, G.; Hasanuzzaman, M.; Saha, S.",2020,,15,,10.1109/MCI.2020.2998234,"Depression is considered a serious medical condition and a large number of people around the world are suffering from it. Within this context, a lot of studies have been proposed to estimate the degree of depression based on different features and modalities, specific to depression. Supported by medical studies that show how depression is a disorder of impaired emotion regulation, we propose a different approach, which relies on the rationale that the estimation of depression level can benefit from the concurrent learning of emotion intensity. To test this hypothesis, we design different attention-based multi-task architectures that concurrently regress/classify both depression level and emotion intensity using text data. Experiments based on two benchmark datasets, namely, the Distress Analysis Interview Corpus-a Wizard of Oz (DAIC-WOZ), and the CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) show that substantial performance improvements can be achieved when compared to emotion-unaware single-task and multitask approaches. © 2005-2012 IEEE.",Computer science; Multi-modal; Artificial intelligence; Benchmarking; Benchmark datasets; Emotion regulations; Emotion intensity; Medical conditions; Medical studies; Number of peoples; Wizard of Oz,,,emotion,No,Yes
scopus,Semi-supervised Multi-modal Emotion Recognition with Cross-Modal Distribution Matching,"Liang, J.; Li, R.; Jin, Q.",2020,,,,10.1145/3394171.3413579,"Automatic emotion recognition is an active research topic with wide range of applications. Due to the high manual annotation cost and inevitable label ambiguity, the development of emotion recognition dataset is limited in both scale and quality. Therefore, one of the key challenges is how to build effective models with limited data resource. Previous works have explored different approaches to tackle this challenge including data enhancement, transfer learning, and semi-supervised learning etc. However, the weakness of these existing approaches includes such as training instability, large performance loss during transfer, or marginal improvement. In this work, we propose a novel semi-supervised multi-modal emotion recognition model based on cross-modality distribution matching, which leverages abundant unlabeled data to enhance the model training under the assumption that the inner emotional status is consistent at the utterance level across modalities. We conduct extensive experiments to evaluate the proposed model on two benchmark datasets, IEMOCAP and MELD. The experiment results prove that the proposed semi-supervised learning model can effectively utilize unlabeled data and combine multi-modalities to boost the emotion recognition performance, which outperforms other state-of-the-art approaches under the same condition. The proposed model also achieves competitive capacity compared with existing approaches which take advantage of additional auxiliary information such as speaker and interaction context.  © 2020 ACM.",Semi-supervised learning; Speech recognition; Emotion recognition; multimodal emotion recognition; Transfer learning; Benchmark datasets; Automatic emotion recognition; semi-supervised learning; Auxiliary information; cross-modality distribution matching; Distribution matching; Interaction context; Manual annotation; State-of-the-art approach,,,emotion,Yes,Yes
scopus,A Cross-subject and Cross-modal Model for Multimodal Emotion Recognition,"Zhang, J.-M.; Yan, X.; Li, Z.-Y.; Zhao, L.-M.; Liu, Y.-Z.; Li, H.-L.; Lu, B.-L.",2021,,1517 CCIS,,10.1007/978-3-030-92310-5_24,"The combination of eye movements and electroencephalography (EEG) signals, representing the external subconscious behaviors and internal physiological responses, respectively, has been proved to be a dependable approach with high interpretability. However, EEG is unfeasible to be put into practical applications due to the inconvenience of data acquisition and inter-subject variability. To take advantage of EEG without being restricted by its limitations, we propose a cross-subject and cross-modal (CSCM) model with a specially designed structure called gradient reversal layer to bridge the modality differences and eliminate the subject variation, so that the CSCM model only requires eye movements and avoids using EEG in real applications. We verify our proposed model on two classic public emotion recognition datasets, SEED and SEED-IV. The competitive performance not only illustrates the efficacy of CSCM model but also sheds light on possible solutions to dealing with cross-subject variations and cross-modal differences simultaneously which help make effective emotion recognition practicable. © 2021, Springer Nature Switzerland AG.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Electroencephalography; Cross-modal; EEG; Electrophysiology; Physiological models; Eye movements; Interpretability; Data acquisition; Cross modality; Transfer learning; Physiological response; Modal models; Real applications; Bridges; Cross subject,,,emotion,No,Yes
scopus,Automatic Speech Recognition and Natural Language Understanding for Emotion Detection in Multi-party Conversations,"Popovic, I.; Culibrk, D.; Mirkovic, M.; Vukmirovic, S.",2020,,,,10.1145/3423325.3423737,"Conversational emotion and sentiment analysis approaches rely on Natural Language Understanding (NLU) and audio processing components to achieve the goal of detecting emotions and sentiment based on what is being said. While there has been marked progress in pushing the state-of-the-art of theses methods on benchmark multimodal data sets, such as the Multimodal EmotionLines Dataset (MELD), the advances still seem to lag behind what has been achieved in the domain of mainstream Automatic Speech Recognition (ASR) and NLU applications and we were unable to identify any widely used products, services or production-ready systems that would enable the user to reliably detect emotions from audio recordings of multi-party conversations. Published, state-of-the-art scientific studies of multi-view emotion recognition seem to take it for granted that a human-generated or edited transcript is available as input to the NLU modules, providing no information of what happens in a realistic application scenario, where audio only is available and the NLU processing has to rely on text generated by ASR. Motivated by this insight, we present a study designed to evaluate the possibility of applying widely-used state-of-the-art commercial ASR products as the initial audio processing component in an emotion-from-speech detection system. We propose an approach which relies on commercially available products and services, such as Google Speech-to-Text, Mozilla DeepSpeech and the NVIDIA NeMo toolkit to process the audio and applies state-of-the-art NLU approaches for emotion recognition, in order to quickly create a robust, production-ready emotion-from-speech detection system applicable to multi-party conversations.  © 2020 ACM.",Speech recognition; emotion recognition; Emotion recognition; Character recognition; Sentiment analysis; Speech; Benchmarking; Audio systems; Audio signal processing; Automatic speech recognition; affective computing; neural networks; automatic speech recognition; Analysis approach; Audio recordings; Multi-party conversations; natural language understanding; Natural language understanding; Products and services; Realistic applications; Scientific studies,,,emotion,No,No
scopus,Emotion Mining: from Unimodal to Multimodal Approaches,"Zucco, C.; Calabrese, B.; Cannataro, M.",2021,,12339 LNCS,,10.1007/978-3-030-82427-3_11,"In the last decade, Sentiment Analysis and Affective Computing have found applications in different domains. In particular, the interest of extracting emotions in healthcare is demonstrated by the various applications which encompass patient monitoring and adverse events prediction. Thanks to the availability of large datasets, most of which are extracted from social media platforms, several techniques for extracting emotion and opinion from different modalities have been proposed, using both unimodal and multimodal approaches. After introducing the basic concepts related to emotion theories, mainly borrowed from social sciences, the present work reviews three basic modalities used in emotion recognition, i.e. textual, audio and video, presenting for each of these i) some basic methodologies, ii) some among the widely used datasets for the training of supervised algorithms and iii) briefly discussing some deep Learning architectures. Furthermore, the paper outlines the challenges and existing resources to perform a multimodal emotion recognition which may improve performances by combining at least two unimodal approaches. architecture to perform multimodal emotion recognition. © 2021, The Author(s).",Speech recognition; Emotion recognition; Deep learning; Multimodal emotion recognition; Data mining; Affective Computing; Sentiment analysis; Affective computing; Brain; Computation theory; Multi-modal approach; Patient monitoring; Large dataset; Social media platforms; Learning architectures; Improve performance; Hardware accelerators; Neuroscience; Supervised algorithm; Text mining,,,emotion,No,No
scopus,Emotion Recognition for Human-Robot Interaction: Recent Advances and Future Perspectives,"Spezialetti, M.; Placidi, G.; Rossi, S.",2020,,7,,10.3389/frobt.2020.532279,"A fascinating challenge in the field of human–robot interaction is the possibility to endow robots with emotional intelligence in order to make the interaction more intuitive, genuine, and natural. To achieve this, a critical point is the capability of the robot to infer and interpret human emotions. Emotion recognition has been widely explored in the broader fields of human–machine interaction and affective computing. Here, we report recent advances in emotion recognition, with particular regard to the human–robot interaction context. Our aim is to review the state of the art of currently adopted emotional models, interaction modalities, and classification strategies and offer our point of view on future developments and critical issues. We focus on facial expressions, body poses and kinematics, voice, brain activity, and peripheral physiological responses, also providing a list of available datasets containing data from these modalities. © Copyright © 2020 Spezialetti, Placidi and Rossi.",multimodal data; machine learning; human-robot interaction; affective computing; emotion recognition (ER),,,emotion,No,No
scopus,Interactive Multimodal Attention Network for Emotion Recognition in Conversation,"Ren, M.; Huang, X.; Shi, X.; Nie, W.",2021,,28,,10.1109/LSP.2021.3078698,"In this letter, we propose a novel Interactive Multimodal Attention Network (IMAN) for emotion recognition in conversations. IMAN introduces a cross-modal attention fusion module to capture cross-modal interactions of multimodal information, and employs a conversational modeling module to explore the context information and speaker dependency of the whole conversation. Concretely, the cross-modal attention fusion module captures the cross-modal interactions and complementary information among the pre-extracted unimodal features from textual, visual, acoustic modalities based on the cross-modal attention block. Afterward, the updated features from each modality are fused to concentrate more on the informative modality and achieve a refined feature for each constituent utterance. The conversational modeling module defines three different gated recurrent units (GRUs) with respect to the context information, the speaker dependency, and the emotional state of utterances. In this way, we exploit the speaker dependency and contextual information to obtain the emotional state of utterances for emotion classification. Empirical evaluations on the multimodal benchmark IEMOCAP dataset demonstrate that our IMAN achieves competitive performance compared to the state-of-the-art approaches. © 1994-2012 IEEE.",Speech recognition; Multi-modal information; Cross-modal interaction; Classification (of information); Benchmarking; multimodal fusion; Recurrent neural networks; Emotion classification; Emotion recognition in conversations; recurrent neural network; Contextual information; Multimodal benchmark; State-of-the-art approach; Competitive performance; Empirical evaluations,,,emotion,No,Yes
scopus,Cross-Subject emotion recognition from EEG using Convolutional Neural Networks,"Zhong, X.; Yin, Z.; Zhang, J.",2020,,2020-July,,10.23919/CCC50068.2020.9189559,"Using electroencephalogram (EEG) signals for emotion detection has aroused widespread research concern. However, across subjects emotional recognition has become an insurmountable gap which researchers cannot step across for a long time due to the poor generalizability of features across subjects. In response to this difficulty, in this study, the moving average(MA) technology is introduced to smooth out short-term fluctuations and highlight longer-term trends or cycles. Based on the MA technology, an effective method for cross-subject emotion recognition was then developed, which designed a method of salient region extraction based on attention mechanism, with the purpose of enhancing the capability of representations generated by a network by modelling the interdependecices between the channels of its informative features. The effectiveness of our method was validated on a dataset for emotion analysis using physiological signals (DEAP) and the MAHNOB-HCI multimodal tagging database. Compared with recent similar works, the method developed in this study for emotion recognition across all subjects was found to be effective, and its accuracy was 66.23% for valence and 68.50% for arousal (DEAP) and 70.25% for valence and 73.27% for arousal (MAHNOB) on the Gamma frequency band. And benefiting from the strong representational learning capacity in the two-dimensional space, our method is efficient in emotion recognition especially on Beta and Gamma waves. © 2020 Technical Committee on Control Theory, Chinese Association of Automation.",Machine learning; Attention mechanisms; Speech recognition; Emotion recognition; Physiological signals; Deep learning; Emotion detection; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Classification; Emotional recognition; Cross-subject; Electroencephalogram signals; Learning capacity; Two dimensional spaces,,,emotion,No,Yes
scopus,Multimodal emotion recognition with hierarchical memory networks,"Lai, H.; Wu, K.; Li, L.",2021,,25,,10.3233/IDA-205183,"Emotion recognition in conversations is crucial as there is an urgent need to improve the overall experience of human-computer interactions. A promising improvement in this field is to develop a model that can effectively extract adequate contexts of a test utterance. We introduce a novel model, termed hierarchical memory networks (HMN), to address the issues of recognizing utterance level emotions. HMN divides the contexts into different aspects and employs different step lengths to represent the weights of these aspects. To model the self dependencies, HMN takes independent local memory networks to model these aspects. Further, to capture the interpersonal dependencies, HMN employs global memory networks to integrate the local outputs into global storages. Such storages can generate contextual summaries and help to find the emotional dependent utterance that is most relevant to the test utterance. With an attention-based multi-hops scheme, these storages are then merged with the test utterance using an addition operation in the iterations. Experiments on the IEMOCAP dataset show our model outperforms the compared methods with accuracy improvement.  © 2021-IOS Press. All rights reserved.",Speech recognition; emotion recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; multimodal; Memory network; memory network; Accuracy Improvement; Dyadic conversations; Global-storage; GRUs; Hierarchical memory; Local memory; Local output,,,emotion,No,Yes
scopus,Multimodal End-to-End Sparse Model for Emotion Recognition,"Dai, W.; Cahyawijaya, S.; Liu, Z.; Fung, P.",2021,,,,,"Existing works on multimodal affective computing tasks, such as emotion recognition, generally adopt a two-phase pipeline, first extracting feature representations for each single modality with hand-crafted algorithms and then performing end-to-end learning with the extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extraction algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-the-art models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain performance with around half the computation in the feature extraction part. © 2021 Association for Computational Linguistics.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; 'current; Affective Computing; Features extraction; Cross-modal; Extraction; Feature extraction; Pipelines; Computational linguistics; End to end; End-to-end models; Sparse models; Two phase,,,emotion,No,Yes
scopus,DEAR-MULSEMEDIA: Dataset for emotion analysis and recognition in response to multiple sensorial media,"Raheel, A.; Majid, M.; Anwar, S.M.",2021,,65,,10.1016/j.inffus.2020.08.007,"Traditionally, emotion recognition is performed in response to stimuli that engage either one (vision: image or hearing: audio) or two (vision and hearing: video) human senses. An immersive environment can be generated by engaging more than two human senses while interacting with multimedia content and is known as MULtiple SEnsorial media (mulsemedia). This study aims to create a new dataset of multimodal physiological signals to recognize emotions in response to such content. To this end, four multimedia clips are selected and synchronized with fan, heater, olfaction dispenser, and haptic vest to augment cold air, hot air, olfaction, and haptic effects respectively. Furthermore, physiological responses including electroencephalography (EEG), galvanic skin response (GSR), and photoplethysmography (PPG) are observed to analyze human emotional responses while experiencing mulsemedia content. A t-test applied using arousal and valence scores show that engaging more than two human senses evokes significantly different emotions. Statistical tests on EEG, GSR, and PPG responses also show a significant difference between multimedia and mulsemedia content. Classification accuracy of 85.18% and 76.54% is achieved for valence and arousal, respectively, using K-nearest neighbor classifier and feature-level fusion strategy. © 2020 Elsevier B.V.",Emotion recognition; Physiological signals; Electroencephalography; Electrophysiology; Physiological models; Photoplethysmography (PPG); Classification accuracy; Galvanic skin response; Classification; Signal processing; Physiological response; Audition; Nearest neighbor search; Feature level fusion; Immersive environment; K-nearest neighbor classifier; Modality Level Fusion; Multiple sensorial media,,,emotion,No,Yes
scopus,The 2nd Korean Emotion Recognition Challenge: Methods and Results,"Kim, S.; Huynh, V.T.; Thi, D.T.; Oh, A.; Lee, G.-S.; Yang, H.-J.; Kim, S.-H.",2021,,1405,,10.1007/978-3-030-81638-4_14,"The 2nd Korean Emotion Recognition Challenge (KERC2020) is a global challenge to promote the emotion recognition technologies by using audio-visual data analysis, especially for the emotion of Korean people. KERC2020 comprise of 1236 videos with each length from two to four seconds based on Korean movies are dramas. Around 68 participating teams compete to achieve state-of-the-art in recognizing stress, arousal, valence from Korean video in the wild. This paper provides a summary of dataset, methods and results in the challenge. © 2021, Springer Nature Switzerland AG.",Speech recognition; Emotion recognition; State of the art; Multimodal; Affective computing; Computer vision; Audio-visual data; Arousal; Valence; Global challenges; Korean emotion recognition; Participating teams; Stress,,,emotion,No,No
scopus,Learning fine-grained cross modality excitement for speech emotion recognition,"Li, H.; Ding, W.; Wu, Z.; Liu, Z.",2021,,1,,10.21437/Interspeech.2021-158,"Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from reallife speeches. We design a temporal alignment mean-max pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitement module to conduct sample-specific adjustment on cross modality embeddings and adaptively recalibrate the corresponding values by its aligned latent features from the other modality. Our proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. Further more, we conduct detailed ablation studies to show that our temporal alignment mean-max pooling mechanism and cross modality excitement significantly contribute to the promising results. In order to encourage the research reproducibility, we make the code publicly available at https://github.com/tal-ai/FG_CME.git. Copyright © 2021 ISCA.",Speech emotion recognition; Multi-modal; Speech recognition; Emotion recognition; Deep learning; Human computer interaction; Speech; Human-computer interaction; Cross modality; Multi-modal learning; Multimodal learning; Speech communication; Emotion expression; Fine grained; Learning approach; Max-pooling; Temporal alignment,,,emotion,No,Yes
scopus,Matching pursuit algorithm for enhancing EEG signal quality and increasing the accuracy and efficiency of emotion recognition,"Momennezhad, A.",2020,,65,,10.1515/bmt-2019-0327,"In this paper, we suggest an efficient, accurate and user-friendly brain-computer interface (BCI) system for recognizing and distinguishing different emotion states. For this, we used a multimodal dataset entitled ""MAHOB-HCI""which can be freely reached through an email request. This research is based on electroencephalogram (EEG) signals carrying emotions and excludes other physiological features, as it finds EEG signals more reliable to extract deep and true emotions compared to other physiological features. EEG signals comprise low information and signal-to-noise ratios (SNRs); so it is a huge challenge for proposing a robust and dependable emotion recognition algorithm. For this, we utilized a new method, based on the matching pursuit (MP) algorithm, to resolve this imperfection. We applied the MP algorithm for increasing the quality and SNRs of the original signals. In order to have a signal of high quality, we created a new dictionary including 5-scale Gabor atoms with 5000 atoms. For feature extraction, we used a 9-scale wavelet algorithm. A 32-electrode configuration was used for signal collection, but we used just eight electrodes out of that; therefore, our method is highly user-friendly and convenient for users. In order to evaluate the results, we compared our algorithm with other similar works. In average accuracy, the suggested algorithm is superior to the same algorithm without applying MP by 2.8% and in terms of f-score by 0.03. In comparison with corresponding works, the accuracy and f-score of the proposed algorithm are better by 10.15% and 0.1, respectively. So as it is seen, our method has improved past works in terms of accuracy, f-score and user-friendliness despite using just eight electrodes.  © 2020 Walter de Gruyter GmbH, Berlin/Boston 2020.","Humans; Emotions; emotion; Speech recognition; emotion recognition; Emotion recognition; Physiology; human; physiology; videorecording; Article; electroencephalography; Electroencephalography; procedures; Biomedical signal processing; EEG; electroencephalogram; human experiment; algorithm; Algorithms; signal processing; Signal Processing, Computer-Assisted; anger; disgust; fear; Multi-modal dataset; arousal; feature extraction; Physiological features; image processing; Interface states; Electrodes; anxiety; signal noise ratio; classification; Brain computer interface; Electroencephalogram signals; signal detection; data base; happiness; sadness; Signal to noise ratio; priority journal; measurement accuracy; action potential amplitude; ANOVA; BINARY-SVM; brain computer interface; Brain-Computer Interfaces; coherence; matching pursuit algorithm; Matching pursuit algorithms; Signal collection; User friendliness; Wavelet algorithms; wavelet coefficients",,,emotion,No,Yes
scopus,An overlapping sliding window and combined features based emotion recognition system for EEG signals,"Garg, S.; Patro, R.K.; Behera, S.; Tigga, N.P.; Pandey, R.",2021,,,,10.1108/ACI-05-2021-0130,"Purpose: The purpose of this study is to propose an alternative efficient 3D emotion recognition model for variable-length electroencephalogram (EEG) data. Design/methodology/approach: Classical AMIGOS data set which comprises of multimodal records of varying lengths on mood, personality and other physiological aspects on emotional response is used for empirical assessment of the proposed overlapping sliding window (OSW) modelling framework. Two features are extracted using Fourier and Wavelet transforms: normalised band power (NBP) and normalised wavelet energy (NWE), respectively. The arousal, valence and dominance (AVD) emotions are predicted using one-dimension (1D) and two-dimensional (2D) convolution neural network (CNN) for both single and combined features. Findings: The two-dimensional convolution neural network (2D CNN) outcomes on EEG signals of AMIGOS data set are observed to yield the highest accuracy, that is 96.63%, 95.87% and 96.30% for AVD, respectively, which is evidenced to be at least 6% higher as compared to the other available competitive approaches. Originality/value: The present work is focussed on the less explored, complex AMIGOS (2018) data set which is imbalanced and of variable length. EEG emotion recognition-based work is widely available on simpler data sets. The following are the challenges of the AMIGOS data set addressed in the present work: handling of tensor form data; proposing an efficient method for generating sufficient equal-length samples corresponding to imbalanced and variable-length data.; selecting a suitable machine learning/deep learning model; improving the accuracy of the applied model. © 2021, Shruti Garg, Rahul Kumar Patro, Soumyajit Behera, Neha Prerna Tigga and Ranjita Pandey.",Electroencephalography (EEG); 1D and 2D convolution neural network (CNN); Emotion recognition (ER),,,emotion,No,Yes
scopus,Towards Learning a Joint Representation from Transformer in Multimodal Emotion Recognition,"Deng, J.J.; Leung, C.H.C.",2021,,12960 LNAI,,10.1007/978-3-030-86993-9_17,"Emotion recognition has been extensively studied in a single modality in the last decade. However, humans express their emotions usually through multiple modalities like voice, facial expressions, or text. This paper proposes a new method to learn a joint emotion representation for multimodal emotion recognition. Emotion-based feature for speech audio is learned by an unsupervised triplet-loss objective, and a text-to-text transformer network is used to extract text embedding for latent emotional meaning. Transfer learning provides a powerful and reusable technique to help fine-tune emotion recognition models trained on mega audio and text datasets respectively. The extracted emotional information from speech audio and text embedding are processed by dedicated transformer networks. The alternating co-attention mechanism is used to construct a deep transformer network. Multimodal fusion is implemented by a deep co-attention transformer network. Experimental results show the proposed method for learning a joint emotion representation achieves good performance in multimodal emotion recognition. © 2021, Springer Nature Switzerland AG.",Embeddings; Speech recognition; Emotion recognition; Facial Expressions; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Learn+; Multimodal fusion; Multiple modalities; Transformer network; Speech audio; Emotion representation,,,emotion,No,Yes
scopus,An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis,"Li, X.; Zhang, X.; Yang, H.; Duan, W.; Dai, W.; Yin, L.",2020,,,,10.1109/FG47880.2020.00050,"Emotion is an experience associated with a particular pattern of physiological activity along with different physiological, behavioral and cognitive changes. One behavioral change is facial expression, which has been studied extensively over the past few decades. Facial behavior varies with a person's emotion according to differences in terms of culture, personality, age, context, and environment. In recent years, physiological activities have been used to study emotional responses. A typical signal is the electroencephalogram (EEG), which measures brain activity. Most of existing EEG-based emotion analysis has overlooked the role of facial expression changes. There exits little research on the relationship between facial behavior and brain signals due to the lack of dataset measuring both EEG and facial action signals simultaneously. To address this problem, we propose to develop a new database by collecting facial expressions, action units, and EEGs simultaneously. We recorded the EEGs and face videos of both posed facial actions and spontaneous expressions from 29 participants with different ages, genders, ethnic backgrounds. Differing from existing approaches, we designed a protocol to capture the EEG signals by evoking participants' individual action units explicitly. We also investigated the relation between the EEG signals and facial action units. As a baseline, the database has been evaluated through the experiments on both posed and spontaneous emotion recognition with images alone, EEG alone, and EEG fused with images, respectively. The database will be released to the research community to advance the state of the art for automatic emotion recognition. © 2020 IEEE.",Speech recognition; Emotion; Emotion recognition; Facial Expressions; Multimodal; Electroencephalography; Biomedical signal processing; EEG; Physiological models; Brain; Facial expression recognition; Database systems; Emotional response; Automatic emotion recognition; Gesture recognition; Pain; Research communities; Facial action; Psychophysiology; Action unit; Action unit recognition; Affection; Behavioral changes; Database; Electro-encephalogram (EEG); Fusion feature; Physiological activity,,,emotion,No,No
scopus,An Efficient Temporal Feature Aggregation of Audio-Video Signals for Human Emotion Recognition,"Singh, L.; Singh, S.; Aggarwal, N.; Singh, R.; Singla, G.",2021,,2021-October,,10.1109/ISPCC53510.2021.9609528,"Due to the significance of human behavioral intelligence in computing devices, this work focused on the facial expressions and speech of humans for their emotion recognition in multimodal (audio-video) signals. The audio-video signals consist of frames to represent the temporal activities of facial expressions and speech of humans. It become challenging to determine the efficient method to construct a spatial and temporal feature vector from the frame-wise spatial feature descriptor to describe the facial expressions and speech temporal information in audio-video signals. In this paper, an efficient temporal feature aggregation method is presented for human emotion recognition in audio-video signals. The Local Binary Pattern (LBP) feature of facial expressions and Mel Frequency Cepstral Coefficients (MFCCs) and its of speech are computed from each frame. The experiment analysis is performed to decide the efficient method for temporal feature aggregation, i.e., sum normalization or statistical functions, to construct a spatial and temporal feature vector. The multiclass Support Vector Machine (SVM) classification model is trained and tested to evaluate the performance of temporal feature aggregation method with LBP features and MFCCs and its features. The Bayesian optimization (BO) method determines the optimal hyper-parameters of the multiclass SVM classifier for emotion detection. The experiment analysis of proposed work is performed on publicly accessible and challenging Crowd-sourced Emotional Multimodal Actors-Dataset (CREMA-D) and compared with existing work.  © 2021 IEEE.",Speech recognition; Behavioral research; Facial Expressions; Audio videos; Multimodal emotion recognition; Human emotion recognition; Support vector machines; Multimodal Emotion Recognition; Temporal features; Audio-video signal; Video signal; Spatial features; Audio-Video Signals; Feature aggregation; Temporal feature aggregation; Temporal Feature Aggregation,,,emotion,No,Yes
scopus,AN EFFICIENT APPROACH FOR AUDIO-VISUAL EMOTION RECOGNITION WITH MISSING LABELS AND MISSING MODALITIES,"Ma, F.; Huang, S.-L.; Zhang, L.",2021,,,,10.1109/ICME51207.2021.9428219,"Audio-visual emotion recognition is important for human-machine interaction systems by combining the information of audio and visual modalities. Although great progress has been made by previous works using multimodal learning compared with unimodal learning, they still cannot effectively deal with two key challenges. Firstly, it is difficult or expensive to acquire labeled emotional data, which results in a large amount of data with missing labels. Secondly, emotional data often has missing modalities. To address these problems, we propose a unified deep learning framework to efficiently handle missing labels and missing modalities for audio-visual emotion recognition through correlation analysis. Specifically, we consider four types of emotional data during the training stage: complete, label missing, visual missing, and audio missing. We propose a correlation loss based on Hirschfeld-Gebelein-Rényi (HGR) maximal correlation to effectively capture the common information in different types of training data for emotion prediction. Experiments on the eNTERFACE'05 and RAVDESS datasets show that our deep learning approach has high effectiveness for audio-visual emotion recognition. © 2021 IEEE",Speech recognition; Emotion recognition; Deep learning; Audio systems; Missing modality; Visual modalities; Multi-modal learning; Audio-visual; missing modalities; Audio-visual emotion recognition; HGR maximal correlation; Hirschfeld-gebelein-renyi maximal correlation; Human machine interaction system; Maximal correlation; Missing label; missing labels,,,emotion,Yes,No
scopus,Multimodal Emotion Recognition Using Transfer Learning on Audio and Text Data,"Deng, J.J.; Leung, C.H.C.; Li, Y.",2021,,12951 LNCS,,10.1007/978-3-030-86970-0_39,"Emotion recognition has been extensively studied in a single modality in the last decade. However, humans express their emotions usually through multiple modalities like voice, facial expressions, or text. In this paper, we propose a new method to find a unified emotion representation for multimodal emotion recognition through speech audio, and text. Emotion-based feature representation from speech audio is learned by an unsupervised triplet-loss objective, and a text-to-text transformer network is constructed to extract latent emotional meaning. As deep neural network models trained by huge datasets exhaust a lot of unaffordable resources, transfer learning provides a powerful and reusable technique to help fine-tune emotion recognition models trained on mega audio and text datasets respectively. Automatic multimodal fusion of emotion-based features from speech audio and text is conducted by a new transformer. Both the accuracy and robustness of proposed method are evaluated, and we show that our method for multimodal fusion using transfer learning in emotion recognition achieves good results. © 2021, Springer Nature Switzerland AG.",Deep neural networks; Speech recognition; Emotion recognition; Facial Expressions; Audio data; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Multimodal fusion; Text data; Multiple modalities; Transfer learning; Transformer network; Speech audio,,,emotion,No,Yes
scopus,"Summary of MuSe 2020: Multimodal Sentiment Analysis, Emotion-target Engagement and Trustworthiness Detection in Real-life Media","Stappen, L.; Schuller, B.; Lefter, I.; Cambria, E.; Kompatsiaris, I.",2020,,,,10.1145/3394171.3421901,"The first Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 was a Challenge-based Workshop held in conjunction with ACM Multimedia'20. It addresses three distinct 'in-the-wild' Sub-challenges: sentiment/ emotion recognition (MuSe-Wild), emotion-target engagement (MuSe-Target) and trustworthiness detection (MuSe-Trust). A large multimedia dataset MuSe-CaR was used, which was specifically designed with the intention of improving machine understanding approaches of how sentiment (e.g. emotion) is linked to a topic in emotional, user-generated reviews. In this summary, we describe the motivation, first of its kind 'in-the-wild' database, challenge conditions, participation, as well as giving an overview of utilised state-of-the-art techniques. © 2020 Owner/Author.",Multi-modal; Emotion recognition; multimodal sentiment analysis; Sentiment analysis; multimodal fusion; Large dataset; affective computing; User-generated; Machine understanding; State-of-the-art techniques; Target engagements; user-generated data,,,emotion,No,No
scopus,Towards the explainability of multimodal speech emotion recognition,"Kumar, P.; Kaushik, V.; Raman, B.",2021,,4,,10.21437/Interspeech.2021-1718,"In this paper, a multimodal speech emotion recognition system has been developed, and a novel technique to explain its predictions has been proposed. The audio and textual features are extracted separately using attention-based Gated Recurrent Unit (GRU) and pre-trained Bidirectional Encoder Representations from Transformers (BERT), respectively. Then they are concatenated and used to predict the final emotion class. The weighted and unweighted emotion recognition accuracy of 71.7% and 75.0% has been achieved on Emotional Dyadic Motion Capture (IEMOCAP) dataset containing speech utterances and corresponding text transcripts. The training and predictions of network layers have been analyzed qualitatively through emotion embedding plots and quantitatively by analyzing the intersection matrices for various emotion classes’ embeddings. Copyright © 2021 ISCA.",Speech emotion recognition; Embeddings; Multi-modal; Speech recognition; Character recognition; Multimodal emotion recognition; Speech; Forecasting; Speech communication; Network layers; Audio features; Novel techniques; Speech emotion recognition systems; Deep network explainability; Embedding plot; Intersection matrix; Intersection matrixes,,,emotion,No,Yes
scopus,Quantum-inspired Neural Network for Conversational Emotion Recognition,"Li, Q.; Gkoumas, D.; Sordoni, A.; Nie, J.-Y.; Melucci, M.",2021,,15,,,"We provide a novel perspective on conversational emotion recognition by drawing an analogy between the task and a complete span of quantum measurement.We characterize different steps of quantum measurement in the process of recognizing speakers' emotions in conversation, and stitch them up with a quantum-like neural network. The quantum-like layers are implemented by complex-valued operations to ensure an authentic adoption of quantum concepts, which naturally enables conversational context modeling and multimodal fusion. We borrow an existing algorithm to learn the complexvalued network weights, so that the quantum-like procedure is conducted in a data-driven manner. Our model is comparable to state-of-the-art approaches on two benchmarking datasets, and provide a quantum view to understand conversational emotion recognition.  Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Speech recognition; Emotion recognition; Multi-modal fusion; Artificial intelligence; Learn+; Context models; Quantum theory; Model fusion; Complex-valued; Data driven; Network weights; Quantum inspired neural networks; Quantum measurement,,,emotion,No,No
scopus,Improving Human Emotion Recognition from Emotive Videos Using Geometric Data Augmentation,"Shoumy, N.J.; Ang, L.-M.; Rahaman, D.M.M.; Zia, T.; Seng, K.P.; Khatun, S.",2021,,12799 LNAI,,10.1007/978-3-030-79463-7_13,"Emotional recognition from videos or images requires large amount of data to obtain high performance and classification accuracy. However, large datasets are not always easily available. A good solution to this problem is to augment the data and extrapolate it to create a bigger dataset for training the classifier. In this paper, we evaluate the impact of different geometric data augmentation (GDA) techniques on emotion recognition accuracy using facial image data. The GDA techniques that were implemented were horizontal reflection, cropping, rotation separately and combined. In addition to this, our system was further evaluated with four different classifiers (Convolutional Neural Network (CNN), Linear Discriminant Analysis (LDA), K-Nearest Neighbor (kNN) and Decision Tree (DT)) to determine which of the four classifiers achieves the best results. In the proposed system, we used augmented data from a dataset (SAVEE) to perform training, and testing was carried out by the original data. A combination of GDA techniques using the CNN classifier was found to give the best performance of approximately 97.8%. Our system with GDA augmentation was shown to outperform previous approaches where only the original dataset was used for classifier training. © 2021, Springer Nature Switzerland AG.",Speech recognition; Emotion recognition; Convolutional neural networks; Human emotion recognition; Large dataset; Statistical tests; Classification accuracy; Intelligent systems; Multimodal data; Decision trees; Emotional recognition; Nearest neighbor search; Data augmentation; Data classification; Neural network; Discriminant analysis; Classifier training; Geometric data; K nearest neighbor (KNN); Linear discriminant analysis,,,emotion,No,Yes
scopus,Hierarchical Attention-Based Multimodal Fusion Network for Video Emotion Recognition,"Liu, X.; Li, S.; Wang, M.",2021,,2021,,10.1155/2021/5585041,"The context, such as scenes and objects, plays an important role in video emotion recognition. The emotion recognition accuracy can be further improved when the context information is incorporated. Although previous research has considered the context information, the emotional clues contained in different images may be different, which is often ignored. To address the problem of emotion difference between different modes and different images, this paper proposes a hierarchical attention-based multimodal fusion network for video emotion recognition, which consists of a multimodal feature extraction module and a multimodal feature fusion module. The multimodal feature extraction module has three subnetworks used to extract features of facial, scene, and global images. Each subnetwork consists of two branches, where the first branch extracts the features of different modes, and the other branch generates the emotion score for each image. Features and emotion scores of all images in a modal are aggregated to generate the emotion feature of the modal. The other module takes multimodal features as input and generates the emotion score for each modal. Finally, features and emotion scores of multiple modes are aggregated, and the final emotion representation of the video will be produced. Experimental results show that our proposed method is effective on the emotion recognition dataset. © 2021 Xiaodong Liu et al.",Image processing; emotion; Speech recognition; Emotion recognition; human; videorecording; Fusion modules; Multi-modal fusion; face; attention; human experiment; article; Image fusion; Extraction; feature extraction; Feature extraction; Multimodal feature fusions; Emotion feature; Context information; Recognition accuracy; Multimodal feature extractions; Subnetworks; Emotion scores,,,emotion,No,Yes
scopus,Fusionsense: Emotion classification using feature fusion of multimodal data and deep learning in a brain-inspired spiking neural network,"Tan, C.; Ceballos, G.; Kasabov, N.; Subramaniyam, N.P.",2020,,20,,10.3390/s20185328,"Using multimodal signals to solve the problem of emotion recognition is one of the emerging trends in affective computing. Several studies have utilized state of the art deep learning methods and combined physiological signals, such as the electrocardiogram (EEG), electroencephalogram (ECG), skin temperature, along with facial expressions, voice, posture to name a few, in order to classify emotions. Spiking neural networks (SNNs) represent the third generation of neural networks and employ biologically plausible models of neurons. SNNs have been shown to handle Spatio-temporal data, which is essentially the nature of the data encountered in emotion recognition problem, in an efficient manner. In this work, for the first time, we propose the application of SNNs in order to solve the emotion recognition problem with the multimodal dataset. Specifically, we use the NeuCube framework, which employs an evolving SNN architecture to classify emotional valence and evaluate the performance of our approach on the MAHNOB-HCI dataset. The multimodal data used in our work consists of facial expressions along with physiological signals such as ECG, skin temperature, skin conductance, respiration signal, mouth length, and pupil size. We perform classification under the Leave-One-Subject-Out (LOSO) cross-validation mode. Our results show that the proposed approach achieves an accuracy of 73.15% for classifying binary valence when applying feature-level fusion, which is comparable to other deep learning methods. We achieve this accuracy even without using EEG, which other deep learning methods have relied on to achieve this level of accuracy. In conclusion, we have demonstrated that the SNN can be successfully used for solving the emotion recognition problem with multimodal data and also provide directions for future research utilizing SNN for Affective computing. In addition to the good accuracy, the SNN recognition system is requires incrementally trainable on new data in an adaptive way. It only one pass training, which makes it suitable for practical and on-line applications. These features are not manifested in other methods for this problem. c© 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Humans; Emotions; emotion; Speech recognition; Emotion recognition; Physiological signals; Physiology; Deep learning; Deep Learning; human; Neural Networks, Computer; Learning systems; Classification (of information); electroencephalography; Electroencephalography; Biomedical signal processing; Brain; Electrocardiography; Recognition systems; Emotion classification; Facial emotion recognition; Neural networks; Multimodal data; brain; diagnostic imaging; Feature level fusion; Evolving Spiking Neural Networks (eSNNs); NeuCube; On-line applications; Spatio-temporal data; Spiking neural networks",,,emotion,No,Yes
scopus,Multimodal Cross- And Self-Attention Network for Speech Emotion Recognition,"Sun, L.; Liu, B.; Tao, J.; Lian, Z.",2021,,2021-June,,10.1109/ICASSP39728.2021.9414654,"Speech Emotion Recognition (SER) requires a thorough understanding of both the linguistic content of an utterance (i.e., textual information) and how the speaker utters it (i.e., acoustic information). The one vital challenge in SER is how to effectively fuse these two kinds of information. In this paper, we propose a novel Multimodal Cross- and Self-Attention Network (MCSAN) to tackle this problem. The core of MCSAN is to employ the parallel cross- and selfattention modules to explicitly model both inter- and intra-modal interactions of audio and text. Specifically, the cross-attention module utilizes the cross-attention mechanism to guide one modality to attend to the other modality and update the features accordingly. Similarly, the self-attention module employs the self-attention mechanism to propagate information within each modality. We evaluate MCSAN on two benchmark datasets, IEMOCAP and MELD. Experimental results demonstrate that our proposed model achieves stateof- the-art performance on both datasets.  ©2021 IEEE.",Speech emotion recognition; Attention mechanisms; Multi-modal; Speech recognition; Modal interactions; Cross-attention; Multimodal fusion; Self-attention; Arts computing; Benchmark datasets; Signal processing; Acoustic information; Textual information; State-of-the-art performance,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition in Polish (Student Consortium),"Rupauliha, K.; Goyal, A.; Saini, A.; Shukla, A.; Swaminathan, S.",2020,,,,10.1109/BigMM50055.2020.00054,"Multimodal emotion recognition is a challenging task because emotions can be expressed through various forms and modalities. It can be applied in various fields, for example, human-computer interaction, crime, healthcare, multimedia retrieval, etc. In recent times, neural networks have achieved overwhelming success in determining emotional states. Motivated by these advancements, we present a multimodal emotion recognition system that uses the following modalities: facial expression, speech, and body language. This paper specifies techniques used for the Emotion Recognition on Polish problem based Polish Emotion dataset. To recognise the current state of emotion in the various given videos, preprocessing of data and extraction of robust features is employed. For the given task, we have made use of facial landmark detection, and Mel-frequency cepstral coefficients (MFCCs) for speech audio. The presented data was in the format of variable length videos, subsequently, which led to underperformance by traditional algorithms for classification. Thus, they could not be used. Therefore, we implemented several long short-term memory (LSTM) networks. Each particular modality was trained for its specific LSTM model, in order to return the emotion having the highest probability at a given instance. Finally, each individual model is fused together using a weight average approach, where in context to all the modalities, the emotion having highest probability is the desirable emotion. © 2020 IEEE.",Long short-term memory; Speech recognition; Emotion recognition; Facial Expressions; Multimodal emotion recognition; Human computer interaction; Multimedia Retrieval; Mel-frequency cepstral coefficients; MFCC; Big data; artificial neural networks; Medical computing; facial landmark detection; Facial landmark detection; Individual modeling; kinect body capture; long short-term memory networks; Pre-processing of data; recurrent neural networks,,,emotion,No,No
scopus,Multi-Modal Emotion Recognition from Speech and Facial Expression Based on Deep Learning,"Cai, L.; Dong, J.; Wei, M.",2020,,,,10.1109/CAC51589.2020.9327178,"The rapid development of emotion recognition contributes to the realization of highly harmonious human-computer interaction experience. Taking into account the complementarity of the emotional information of speech and facial expressions, and breaking through the single modal emotion recognition limitation of single emotional features, this paper proposed a method that combines speech and facial expression features. We CNN and LSTM to learn speech emotion features. Simultaneously, multiple small-scale kernel convolution block was designed to extract facial expression features. Finally, we used DNNs to fuse speech and facial expression features. The multimodal emotion recognition model was tested on the IEMOCAP dataset. Compared with the single modal of speech and facial expression, the overall recognition accuracy of our proposed model has been increased by 10.05% and 11.27%, respectively. © 2020 IEEE.",Long short-term memory; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Deep learning; Multimodal emotion recognition; Human computer interaction; multimodal emotion recognition; Speech; human-computer interaction; Emotional information; Speech emotions; speech emotion recognition; Recognition accuracy; Emotion recognition from speech; Kernel convolution,,,emotion,No,Yes
scopus,Deep learning method for selecting effective models and feature groups in emotion recognition using an asian multimodal database,"Maeng, J.-H.; Kang, D.-H.; Kim, D.-H.",2020,,9,,10.3390/electronics9121988,"Emotional awareness is vital for advanced interactions between humans and computer systems. This paper introduces a new multimodal dataset called MERTI-Apps based on Asian physiological signals and proposes a genetic algorithm (GA)—long short-term memory (LSTM) deep learning model to derive the active feature groups for emotion recognition. This study developed an annotation labeling program for observers to tag the emotions of subjects by their arousal and valence during dataset creation. In the learning phase, a GA was used to select effective LSTM model parameters and determine the active feature group from 37 features and 25 brain lateralization features extracted from the electroencephalogram (EEG) time, frequency, and time–frequency domains. The proposed model achieved a root-mean-square error (RMSE) of 0.0156 in terms of the valence regression performance in the MAHNOB-HCI dataset, and RMSE performances of 0.0579 and 0.0287 in terms of valence and arousal regression performance, and 65.7% and 88.3% in terms of valence and arousal accuracy in the in-house MERTI-Apps dataset, which uses Asian-population-specific 12-channel EEG data and adds an additional brain lateralization (BL) feature. The results revealed 91.3% and 94.8% accuracy in the valence and arousal domain in the DEAP dataset owing to the effective model selection of a GA. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Machine learning; Emotion analysis; Dataset; Signal analysis; Feature evaluation and selection,,,emotion,Yes,Yes
scopus,Contactless Human Emotion Analysis Across Different Modalities,"Nahid, N.; Rahman, A.; Ahad, M.A.R.",2021,,200,,,"Emotion recognition and analysis is an essential part of affective computing which plays a vital role nowadays in healthcare, security systems, education, etc. Numerous scientific researches have been conducted developing various types of strategies, utilizing methods in different areas to identify human emotions automatically. Different types of emotions are distinguished through the combination of data from facial expressions, speech, and gestures. Also, physiological signals, e.g., EEG (Electroencephalogram), EMG (Electromyogram), EOG (Electrooculogram), blood volume pulse, etc. provide information on emotions. The main idea of this paper is to identify various emotion recognition techniques and denote relevant benchmark data sets and specify algorithms with state-of-the-art results. We have also given a review of multimodal emotion analysis, which deals with various fusion techniques of the available emotion recognition modalities. The results of the existing literature show that emotion recognition works best and gives satisfactory accuracy if it uses multiple modalities in context. At last, a survey of the rest of the problems, challenges, and corresponding openings in this field is given. © 2021, Springer Nature Switzerland AG.",,,,emotion,No,Yes
scopus,Recognition of Human Emotion using Radial Basis Function Neural Networks with Inverse Fisher Transformed Physiological Signals,"Abayomi, A.; Olugbara, O.O.; Heukelman, D.",2021,,13,,10.30880/ijie.2021.13.06.001,"Emotion is a complex state of human mind influenced by body physiological changes and interdependent external events thus making an automatic recognition of emotional state a challenging task. A number of recognition methods have been applied in recent years to recognize human emotion. The motivation for this study is therefore to discover a combination of emotion features and recognition method that will produce the best result in building an efficient emotion recognizer in an affective system. We introduced a shifted tanh normalization scheme to realize the inverse Fisher transformation applied to the DEAP physiological dataset and consequently performed series of experiments using the Radial Basis Function Artificial Neural Networks (RBFANN). In our experiments, we have compared the performances of digital image based feature extraction techniques such as the Histogram of Oriented Gradient (HOG), Local Binary Pattern (LBP) and the Histogram of Images (HIM). These feature extraction techniques were utilized to extract discriminatory features from the multimodal DEAP dataset of physiological signals. Experimental results obtained indicate that the best recognition accuracy was achieved with the EEG modality data using the HIM features extraction technique and classification done along the dominance emotion dimension. The result is very remarkable when compared with existing results in the literature including deep learning studies that have utilized the DEAP corpus and also applicable to diverse fields of engineering studies. © Universiti Tun Hussein Onn Malaysia Publisher’s Office",feature extraction; physiological signal; Neural network; fisher transform; human emotion,,,emotion,No,Yes
scopus,ICDAR 2021 Competition on Multimodal Emotion Recognition on Comics Scenes,"Nguyen, N.-V.; Vu, X.-S.; Rigaud, C.; Jiang, L.; Burie, J.-C.",2021,,12824 LNCS,,10.1007/978-3-030-86337-1_51,"The paper describes the “Multimodal Emotion Recognition on Comics scenes” competition presented at the ICDAR conference 2021. This competition aims to tackle the problem of emotion recognition of comic scenes (panels). Emotions are assigned manually by multiple annotators for each comic scene of a subset of a public large-scale dataset of golden age American comics. As a multi-modal analysis task, the competition proposes to extract the emotions of comic characters in comic scenes based on visual information, text in speech balloons or captions and the onomatopoeia. Participants were competing on CodaLab.org from December 16 th 2020 to March 31 th 2021. The challenge has attracted 145 registrants, 21 teams have joined the public test phase, and 7 teams have competed in the private test phase. In this paper we present the motivation, dataset preparation, task definition of the competition, the analysis of participant’s performance and submitted methods. We believe that the competition have drawn attention from the document analysis community in both fields of computer vision and natural language processing on the task of emotion recognition in documents. © 2021, Springer Nature Switzerland AG.",Performance; Speech recognition; Emotion recognition; Modal analysis; Multi-modal fusion; Multimodal emotion recognition; Classification (of information); Multimodal fusion; Large dataset; Natural language processing systems; Visual information; Large-scale datasets; Documents analysis; Multi-label classification; Scene-based; Test phasis,,,emotion,No,Yes
scopus,MDA: Multimodal Data Augmentation Framework for Boosting Performance on Sentiment/Emotion Classification Tasks,"Xu, N.; Mao, W.; Wei, P.; Zeng, D.",2021,,36,,10.1109/MIS.2020.3026715,"Multimodal data analysis has drawn increasing attention with the explosive growth of multimedia data. Although traditional unimodal data analysis tasks have accumulated abundant labeled datasets, there are few labeled multimodal datasets due to the difficulty and complexity of multimodal data annotation, nor is it easy to directly transfer unimodal knowledge to multimodal data. Unfortunately, there is little related data augmentation work in multimodal domain, especially for image-text data. In this article, to address the scarcity problem of labeled multimodal data, we propose a Multimodal Data Augmentation framework for boosting the performance on multimodal image-text classification task. Our framework learns a cross-modality matching network to select image-text pairs from existing unimodal datasets as the multimodal synthetic dataset, and uses this dataset to enhance the performance of classifiers. We take the multimodal sentiment analysis and multimodal emotion analysis as the experimental tasks and the experimental results show the effectiveness of our framework for boosting the performance on multimodal classification task.  © 2001-2011 IEEE.",Modal analysis; Sentiment analysis; Classification (of information); Image enhancement; Data handling; Image classification; Classification tasks; Data augmentation; multimodal classification; Multimodal data analysis; Multimodal domains; Multimodal datasets; cross-modality matching; Explosive growth; Matching networks; Performance of classifier; synthetic dataset,,,emotion,Yes,Yes
scopus,MED: multimodal emotion dataset in the wild,"Chen, J.; Wang, K.; Zhao, C.; Yin, C.; Huang, Z.",2020,,25,,10.11834/jig.200215,"Objective: Emotion recognition or affective computing is crucial in various human-computer interactions, including interaction with artificial intelligence (AI) assistants, such as home robots, Google assistant, Alexa, and even self-driving cars. AI assistants or other forms of technology can also be used to identify a person's emotional or cognitive state to help people live a happy, healthy, and productive life and even help with mental health treatment. Adding emotion recognition to human-machine systems can help the computer recognize emotion and intention of users when speaking and give an appropriate response. To date, computers inaccurately capture and interpret user emotions and intentions mainly because of the different datasets used when developing an intelligent system and lack of data collection in an actual application environment that reduces system robustness. The simple dataset collected in the laboratory environment, which uses an unreasonable induction method of emotion generation, is typically characterized by a solid background and uniform and strong illumination. The resulting emotion category is very exaggerated but untrue. User age, gender, and ethnicity as well as complexity of the application environment and diversity of collection angles in the actual application process are problems that need solutions when developing a system. Therefore, application of systems developed in the laboratory environment is difficult in the real world. Method: Creating a dataset from the real environment can solve the problem of inconsistency between datasets used in software development and the real-world application. Wild datasets, especially multimodal sentiment datasets containing dynamic information, are limited. Therefore, The paper collected and annotated a new multimodal emotion dataset (MED) in the real environment. First, five collectors watched videos from various data sources with different content, such as TV series, movies, talk shows, and live broadcasts, and extracted over 2 500 video clips containing emotional states. Second, the video frame of each video is obtained and saved in a folder to determine the video sequence. The pedestrian detection model is used to obtain valid video frames because only some video frames contain valid person or face information. Clips without people are considered invalid video frames and undetected. The resulting video frame containing only personal information can be used to investigate postural emotional information, such as limbs. Posture emotion can be used to assess the emotional state of a person when the face is blocked or the character has a large motion range. Facial expressions account for a large proportion of emotional judgment. Third, two methods are used to face detection. Finally, Annotators manually annotated video sequences of detected people and faces although the staff collected videos according to the emotional state in the manual cutting process. Given that humans will have deviations in emotional judgment and each person has a different sensitivity to emotion, the paper used crowdsourcing method to make annotations. Crowdsourcing methods are used in the collection of many datasets, such as ImageNet and RAF. Fifteen taggers with professional emotional information training independently tagged all the video clips. A total of 1 839 video clips were obtained on the basis of seven types of emotions after annotation. Result: Different divisions of the dataset are presented in the study. The dataset is classified into training and verification sets by 0.65 : 0.35 according to acted facial expression in the wild(AFEW) division. The amount of data for each type of emotion in the AFEW and MED datasets are then compared and presented in the form of a graph. MED has more quantities for each type of emotion than AFEW. The paper evaluates the dataset using a large number of deep and machine learning algorithms and provides baselines for each modality. First, classic machine learning methods, such as local binary patterns(LBP), histogram of oriented gradient(HOG), and Gabor wavelet are applied to obtain the baseline of the CK+ dataset. The same method is applied to the MED dataset, and accuracy decreases by more than 50%. Data collected in the real environment is complicated. The algorithm developed using the dataset in the laboratory environment is unsuitable for the real environment. Hence, creating the dataset in the real environment is necessary. The comparison of AFEW and MED datasets verifies that data of MED are reasonable and effective. The baseline of facial expression recognition and the two other modalities also are provided. The results indicate other modalities can be used as an auxiliary method for comprehensively assessing emotions, especially when the face is blocked or the face information is unavailable. Finally, the accuracy of emotion recognition improves by 4.03% through multimodal fusion. Conclusion: MED is a multimodal real-world dataset that expands the existing multimodal dataset. Researchers can develop a deep learning algorithm by combining MED with other datasets to form a large multimodal database that contains multiple languages and ethnicities, promote cross-cultural emotion recognition and perception analysis of different emotion evaluations, and improve the performance of automatic emotion computing systems in real applications. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.",Facial expression; Multimodal; Dataset; Speech emotion; Body posture; In the wild,,,emotion,Yes,Yes
scopus,MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation,"Hu, J.; Liu, Y.; Zhao, J.; Jin, Q.",2021,,,,,"Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users' emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting. © 2021 Association for Computational Linguistics",Multi-modal; Speech recognition; Emotion recognition; Multi-modal information; Multi-modal fusion; Speech processing; Convolution; Convolutional networks; Contextual information; Benchmark datasets; Model-based OPC; User emotions; Affective dialogue systems,,,emotion,No,No
scopus,CTNet: Conversational Transformer Network for Emotion Recognition,"Lian, Z.; Liu, B.; Tao, J.",2021,,29,,10.1109/TASLP.2021.3049898,"Emotion recognition in conversation is a crucial topic for its widespread applications in the field of human-computer interactions. Unlike vanilla emotion recognition of individual utterances, conversational emotion recognition requires modeling both context-sensitive and speaker-sensitive dependencies. Despite the promising results of recent works, they generally do not leverage advanced fusion techniques to generate the multimodal representations of an utterance. In this way, they have limitations in modeling the intra-modal and cross-modal interactions. In order to address these problems, we propose a multimodal learning framework for conversational emotion recognition, called conversational transformer network (CTNet). Specifically, we propose to use the transformer-based structure to model intra-modal and cross-modal interactions among multimodal features. Meanwhile, we utilize word-level lexical features and segment-level acoustic features as the inputs, thus enabling us to capture temporal information in the utterance. Additionally, to model context-sensitive and speaker-sensitive dependencies, we propose to use the multi-head attention based bi-directional GRU component and speaker embeddings. Experimental results on the IEMOCAP and MELD datasets demonstrate the effectiveness of the proposed method. Our method shows an absolute 2.1 ∼6.2% performance improvement on weighted average F1 over state-of-the-art strategies.  © 2014 IEEE.",Speech recognition; Emotion recognition; Cross-modal interaction; Human computer interaction; multimodal fusion; Multimodal features; Multi-modal learning; conversational emotion recognition; speaker-sensitive modeling; Acoustic features; Weighted averages; Temporal information; Context sensitive; Context-sensitive modeling; conversational transformer network (CTNet),,,emotion,No,Yes
scopus,"Multimodal(Audio, Facial and Gesture) based Emotion Recognition challenge","Wei, G.; Jian, L.; Mo, S.",2020,,,,10.1109/FG47880.2020.00142,"The emotion recognition in the wild has been a hot research topic in the field of a affective computing. Though some progresses have been achieved, the emotion recognition in the wild is still an unsolved problem due to the challenge of head movement, face deformation, illumination variation etc. To deal with these unconstrained challenges, we expand the focus to several expression forms to facilitate research on emotion recognition in the wild. The state-of-the-art CNN based object recognition models are employed to facilitate the facial expression recognition performance such as FAN[1],TSM[2]. The best experimental result shows that the overall accuracy of our algorithm on the validation dataset of the challenge is 76.43%. © 2020 IEEE.",Speech recognition; Emotion recognition; Face recognition; Affective Computing; Facial expression recognition; Object recognition; Gesture recognition; Hot research topics; Overall accuracies; Face deformations; Illumination variation; Unsolved problems,,,emotion,No,Yes
scopus,Speaker-Invariant Adversarial Domain Adaptation for Emotion Recognition,"Yin, Y.; Huang, B.; Wu, Y.; Soleymani, M.",2020,,,,10.1145/3382507.3418813,"Automatic emotion recognition methods are sensitive to the variations across different datasets and their performance drops when evaluated across corpora. We can apply domain adaptation techniques e.g., Domain-Adversarial Neural Network (DANN) to mitigate this problem. Though the DANN can detect and remove the bias between corpora, the bias between speakers still remains which results in reduced performance. In this paper, we propose Speaker-Invariant Domain-Adversarial Neural Network (SIDANN) to reduce both the domain bias and the speaker bias. Specifically, based on the DANN, we add a speaker discriminator to unlearn information representing speakers' individual characteristics with a gradient reversal layer (GRL). Our experiments with multimodal data (speech, vision, and text) and the cross-domain evaluation indicate that the proposed SIDANN outperforms (+5.6% and +2.8% on average for detecting arousal and valence) the DANN model, suggesting that the SIDANN has a better domain adaptation ability than the DANN. Besides, the modality contribution analysis shows that the acoustic features are the most informative for arousal detection while the lexical features perform the best for valence detection.  © 2020 ACM.",Speech recognition; emotion recognition; Emotion recognition; Multi-modal data; domain adaptation; Domain adaptation; Neural networks; multimodal learning; Automatic emotion recognition; neural networks; Interactive computer systems; Acoustic features; Contribution analysis; Cross-domain evaluations; Individual characteristics,,,emotion,No,Yes
scopus,Multimodal Attention-Mechanism for Temporal Emotion Recognition,"Ghaleb, E.; Niehues, J.; Asteriadis, S.",2020,,2020-October,,10.1109/ICIP40778.2020.9191019,"Exploiting the multimodal and temporal interaction between audio-visual channels is essential for automatic audio-video emotion recognition (AVER). Modalities' strength in emotions and time-window of a video-clip could be further utilized through a weighting scheme such as attention mechanism to capture their complementary information. The attention mechanism is a powerful approach for sequence modeling, which can be employed to fuse audio-video cues overtime. We propose a novel framework which consists of biaudio-visual time-windows that span short video-clips labeled with discrete emotions. Attention is used to weigh these time windows for multimodal learning and fusion. Experimental results on two datasets show that the proposed methodology can achieve an enhanced multimodal emotion recognition. © 2020 IEEE.",Image processing; Attention mechanisms; Multi-modal; Speech recognition; Emotion recognition; Audio videos; Multimodal emotion recognition; attention; Multi-modal learning; multimodal learning; Video cameras; Time windows; audiovisual emotion recognition; Sequence modeling,,,emotion,No,No
scopus,Multimodal Emotion Recognition Using a Hierarchical Fusion Convolutional Neural Network,"Zhang, Y.; Cheng, C.; Zhang, Y.",2021,,9,,10.1109/ACCESS.2021.3049516,"In recent years, deep learning has been increasingly used in the field of multimodal emotion recognition in conjunction with electroencephalogram. Considering the complexity of recording electroencephalogram signals, some researchers have applied deep learning to find new features for emotion recognition. In previous studies, convolutional neural network model was used to automatically extract features and complete emotion recognition, and certain results were obtained. However, the extraction of hierarchical features with convolutional neural network for multimodal emotion recognition remains unexplored. Therefore, this paper proposes a hierarchical fusion convolutional neural network model to mine the potential information in the data by constructing different network hierarchical structures, extracting multiscale features, and using feature-level fusion to fuse the global features formed by combining weights with manually extracted statistical features to form the final feature vector. This paper conducts binary classification experiments on the valence and arousal dimensions of the DEAP and MAHNOB-HCI data sets to evaluate the performance of the proposed model. The results show that the model proposed in this paper can achieve accuracies of 84.71% and 89.00% on the two corresponding data sets, indicating that the model proposed in this paper is superior to other deep learning emotion classification models in feature extraction and fusion.  © 2013 IEEE.",Speech recognition; Deep learning; Multimodal emotion recognition; Data mining; Classification (of information); multimodal emotion recognition; Electroencephalography; Convolutional neural networks; electroencephalogram; Convolution; Emotion classification; Extraction; Electroencephalogram signals; Hierarchical fusions; Multi-scale features; Binary classification; hierarchical convolutional neural network; Hierarchical features; multiscale features; Network hierarchical structures,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition with Temporal and Semantic Consistency,"Chen, B.; Cao, Q.; Hou, M.; Zhang, Z.; Lu, G.; Zhang, D.",2021,,29,,10.1109/TASLP.2021.3129331,"Automated multimodal emotion recognition has become an emerging but challenging research topic in the fields of affective learning and sentiment analysis. The existing works mainly focus on developing multimodal fusion strategies to incorporate different emotion-related features. However, they fail to explore the inherent contextual consistency to reconcile the emotional information across modalities. In this paper, we propose a novel Time and Semantic Interaction Network (TSIN), which concurrently incorporates the advantages of temporal and semantic consistency into the multimodal emotion recognition task. Specifically, a well-designed Speech and Text Embedding (STE) module is devoted to formulating the initial embedding spaces by respectively building the modality-specific representations of speech and text. Instead of separately learning or directly fusing the acoustic and textual features, we propose a well-defined Time and Semantic Interaction (TSI) module to conduct the emotional parsing and sentiment refining by performing the fine-grained temporal alignment and cross-modal semantic interaction. Benefitting from temporal and semantic consistency constraints, both speech-text embeddings can be interactively optimized and fine-tuned in the learning process. In this way, the learnt acoustics and textual features can jointly and efficiently predict the final emotional state. Extensive experiments on the IEMOCAP dataset demonstrate the superiorities of our TSIN framework in comparison with state-of-the-art baselines. © 2014 IEEE.",Semantics; Speech recognition; Emotion recognition; Modal analysis; Data mining; Sentiment analysis; Task analysis; Speech processing; Convolutional networks; Graph convolutional network; Multi-labels; Co-occurrence; Image recognition; Label co-occurrence; Label images; Multi-label image recognition; Semantic similarity; Semantic-interactive,,,emotion,No,Yes
scopus,Image-text multimodal emotion classification via multi-view attentional network,"Yang, X.; Feng, S.; Wang, D.; Zhang, Y.",2021,,23,,10.1109/TMM.2020.3035277,"Compared with single-modal content, multimodal data can express users' feelings and sentiments more vividly and interestingly. Therefore, multimodal sentiment analysis has become a popular research topic. However, most existing methods either learn modal sentiment feature independently, without considering their correlations, or they simply integrate multimodal features. In addition, most publicly available multimodal datasets are labeled by sentiment polarities, while the emotions expressed by users are specific. Based on this observation, in this paper, we build a large-scale image-text emotion dataset (i.e., labeled by different emotions), called TumEmo, with more than 190,000 instances from Tumblr.1 We further propose a novel multimodal emotion analysis model based on the Multi-view Attentional Network (MVAN), which utilizes a memory network that is continually updated to obtain the deep semantic features of image-text. The model includes three stages: Feature mapping, interactive learning, and feature fusion. In the feature mapping stage, we leverage image features from an object viewpoint and a scene viewpoint to capture effective information for multimodal emotion analysis. Then, an interactive learning mechanism is adopted that uses the memory network; this mechanism extracts single-modal emotion features and interactively models the cross-view dependencies between the image and text. In the feature fusion stage, multiple features are deeply fused using a multilayer perceptron and a stacking-pooling module. The experimental results on the MVSA-Single, MVSA-Multiple, and TumEmo datasets show that the proposed MVAN outperforms strong baseline models by large margins.  © 1999-2012 IEEE.",Semantics; Attention mechanisms; Multi-modal; Modal analysis; Learning systems; Social media; Social networking (online); Sentiment analysis; Emotion analysis; Multimodal emotion analyse; Multimodal emotion analysis; Image analysis; Multi-views; Memory network; Image classification; Mapping; Single-modal; Image texts; Multi-view attention mechanism,,,emotion,Yes,No
scopus,Emotion Recognition Using Multimodalities,"Kharat, A.; Patel, A.; Bhatt, D.; Parikh, N.; Rathore, H.",2021,,1375 AIST,,10.1007/978-3-030-73050-5_31,"Emotion is an intrinsic part of human nature, and it plays a significant role in how we (as humans) think and behave, which drives us to make decisions and take actions. Emotion can be recognized by processing different types of data. Since the identification of human behavior with just a single form of expression is typically hard and thus it make emotion recognition is a challenging task. Recent approaches have concentrated on a single modality of conversation for emotion recognition. However, purely relying on the single modality (a form of expression) of data may not capture emotion in-depth when multi-party and multimodality are involved in the conversion. To fill this gap, we have used Multimodal EmotionLines Dataset (MELD), a broader and enhanced version of the EmotionLine dataset. MELD dataset is built from Friends, an American television sitcom aired by NBC. In Emotion recognition, we propose to detect and recognize different emotions through text, video, and audio modalities. We extended the emotion analysis based on video modality from MELD dataset with reduced number of extracted pixel features. Our experiment results show that with video modality, we can still maintain an F-score of 59 with comparatively less features and reduced testing time by a factor of 88.89%. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Image processing; Machine learning; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Deep learning; Character recognition; Emotion analysis; Multi-modality; Digital storage; Intelligent systems; Human behaviors; Multimodalities; Sodium compounds; Biomimetics; Human nature; Testing time; Video modality,,,emotion,No,No
scopus,Recognition of anger and neutral emotions in speech with different languages,"Horkous, H.; Guerti, M.",2021,,10,,10.12785/IJCDS/100154,"Speech emotion recognition is a very interesting area. It has board applications in man-machine interaction. In this work, the influence of speech features on the recognition of anger and neutral emotions in different languages is studied. And on the other hand, the influence of anger and neutral emotions for classifying male and female gender in different languages is also studied. Four databases in different languages are used to achieve our purpose. These databases are Algerian Dialect Emotional Database (ADED), Berlin Database of Emotional Speech (EMO-DB), Sharif Emotional Speech Database (ShEMO) and Crowd-sourced Emotional Multimodal Actors Dataset (CREAMA-D). The databases are exploited for extracting the features that used in the recognition and classification systems. The features extracted are the pitch, intensity, formants and MFCCs (Mel Frequency Cepstral Coefficients) parameters. The results obtained show us that the use a combination of features improve the performance of recognition in all the databases. It was showed also in the results that the classification of gender classes is influenced by the type of emotion and the language of databases. © 2021 University of Bahrain. All rights reserved.",Emotion; MFCCs; ADED; CREAMA-D; EMO-DB; Formants; Iintensity; Pitch; ShEMO,,,emotion,No,Yes
scopus,Missing modality imagination network for emotion recognition with uncertain missing modalities,"Zhao, J.; Li, R.; Jin, Q.",2021,,,,,"Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions. Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at https://github.com/AIM3RUC/MMIN. © 2021 Association for Computational Linguistics",Performance; Multi-modal; Speech recognition; Emotion recognition; Multi-modal fusion; Learn+; Benchmarking; Computational linguistics; Benchmark datasets; Real-world; Condition; Testing conditions; Unified Modeling,,,emotion,No,Yes
scopus,A comparative analysis of machine learning methods for emotion recognition using EEG and peripheral physiological signals,"Doma, V.; Pirouz, M.",2020,,7,,10.1186/s40537-020-00289-7,"Emotion recognition using brain signals has the potential to change the way we identify and treat some health conditions. Difficulties and limitations may arise in general emotion recognition software due to the restricted number of facial expression triggers, dissembling of emotions, or among people with alexithymia. Such triggers are identified by studying the continuous brainwaves generated by human brain. Electroencephalogram (EEG) signals from the brain give us a more diverse insight on emotional states that one may not be able to express. Brainwave EEG signals can reflect the changes in electrical potential resulting from communications networks between neurons. This research involves analyzing the epoch data from EEG sensor channels and performing comparative analysis of multiple machine learning techniques [namely Support Vector Machine (SVM), K-nearest neighbor, Linear Discriminant Analysis, Logistic Regression and Decision Trees each of these models] were tested with and without principal component analysis (PCA) for dimensionality reduction. Grid search was also utilized for hyper-parameter tuning for each of the tested machine learning models over Spark cluster for lowered execution time. The DEAP Dataset was used in this study, which is a multimodal dataset for the analysis of human affective states. The predictions were based on the labels given by the participants for each of the 40 1-min long excerpts of music. music. Participants rated each video in terms of the level of arousal, valence, like/dislike, dominance and familiarity. The binary class classifiers were trained on the time segmented, 15 s intervals of epoch data, individually for each of the 4 classes. PCA with SVM performed the best and produced an F1-score of 84.73% with 98.01% recall in the 30th to 45th interval of segmentation. For each of the time segments and “a binary training class” a different classification model converges to a better accuracy and recall than others. The results prove that different classification models must be used to identify different emotional states. © 2020, The Author(s).",Machine learning; Emotion Recognition; Emotional state; Speech recognition; Emotion recognition; Learning systems; Electroencephalography; Biomedical signal processing; Support vector machines; Support vectors machine; Machine-learning; Decision trees; Classification; Principal component analysis; Principal-component analysis; Brain wave; Nearest neighbor search; Electroencephalogram signals; Multi channel; Discriminant analysis; Comparative analyzes; Multi-channel EEG; Multi-channel electroencephalogram,,,emotion,Yes,Yes
scopus,Ontological Model for Contextual Data Defining Time Series for Emotion Recognition and Analysis,"Zawadzka, T.; Waloszek, W.; Karpus, A.; Zapalowska, S.; Wrobel, M.R.",2021,,9,,10.1109/ACCESS.2021.3132728,"One of the major challenges facing the field of Affective Computing is the reusability of datasets. Existing affective-related datasets are not consistent with each other, they store a variety of information in different forms, different formats, and the terms used to describe them are not unified. This paper proposes a Recording Ontology for Affective-related Datasets (ROAD) as a solution to this problem, by formally describing the datasets and unifying the terms used. The developed ontology allows information about the origin and meaning of the data to be modeled, i.e., time series, representing both emotional states and features derived from biosignals. Furthermore, the ROAD ontology is extensible and not application-oriented, thus it can be used to store data from a wide range of Affective Computing experiments. The ontology was validated by modeling data obtained from one experiment on AMIGOS dataset (A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS). The approach proposed in the paper can be used both by researchers who create new datasets or want to reuse existing ones, and for those who want to process data from experiments in a more automated way. © 2013 IEEE.",emotion; Emotion; Emotion recognition; Affective Computing; Emotion analysis; Affective computing; Dataset; Time series; Times series; dataset; Time series analysis; Ontology; Ontology's; conceptualization; Conceptualization; Ontological modeling; ontology; ontology development; Ontology development; Reusability; Roads and streets; time series,,,emotion,No,No
scopus,Analysis of DEAP Dataset for Emotion Recognition,"Kulkarni, S.; Patil, P.R.",2021,,1312 AISC,,10.1007/978-981-33-6176-8_8,"In affective computing, emotion classification has a significant role and renders many applications in the areas like neuroscience, entertainment, neuro-marketing, and education. These applications comprise classification of neurological disorder, false detection, recognition of stress and pain level, and finding level of attention. The traditional methods used for emotion recognition are facial expressions or voice tone. However, the outcomes of facial signs and verbal language can indicate the unfair and unclear results. Hence, investigators have started the usage of EEG (Encephalogram) method for analyzing the brain signals to recognize the various emotions. EEG-based emotion recognition has an ability to modify the way that we detect some health disorders. Brain signals reveal variations in electrical potential as a result of communication among thousands of neurons. This research article includes analysis of human affective state using DEAP- “Dataset for Emotion Analysis using Physiological Signals”. It is a multimodal dataset, where 40 channels are used, 32 subjects participated, and 40 one-minute video pieces of music was shown to them. The participants evaluated each music video with respect to Valence, Arousal, Dominance, and Likings. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Emotions; Speech recognition; Emotion recognition; Facial Expressions; Physiological signals; Affective Computing; Biomedical signal processing; EEG; Emotion classification; Multi-modal dataset; Classification; Arousal; Valence; Dominance; Electrical potential; Liking; Neurological disorders; Neurology,,,emotion,No,Yes
scopus,A Comparative Study of Arousal and Valence Dimensional Variations for Emotion Recognition Using Peripheral Physiological Signals Acquired from Wearable Sensors,"Alskafi, F.A.; Khandoker, A.H.; Jelinek, H.F.",2021,,,,10.1109/EMBC46164.2021.9630759,"Wearable sensors have made an impact on healthcare and medicine by enabling out-of-clinic health monitoring and prediction of pathological events. Further advancements made in the analysis of multimodal signals have been in emotion recognition which utilizes peripheral physiological signals captured by sensors in wearable devices. There is no universally accepted emotion model, though multidimensional methods are often used, the most popular of which is the two-dimensional Russell's model based on arousal and valence. Arousal and valence values are discrete, usually being either binary with low and high labels along each dimension creating four quadrants or 3-valued with low, neutral, and high labels. In day-to-day life, the neutral emotion class is the most dominant leaving emotion datasets with the inherent problem of class imbalance. In this study, we show how the choice of values in the two-dimensional model affects the emotion recognition using multiple machine learning algorithms. Binary classification resulted in an accuracy of 87.2% for arousal and up to 89.5% for valence. Maximal 3-class classification accuracy was 80.9% for arousal and 81.1% for valence. For the joined classification of arousal and valence, the four-quadrant model reached 87.8%, while the nine-class model had an accuracy of 75.8%. This study can be used as a basis for further research into feature extraction for better overall classification performance. © 2021 IEEE.",Machine Learning; Emotions; Machine learning; emotion; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; machine learning; Biomedical signal processing; algorithm; Algorithms; Learning algorithms; Emotion models; arousal; Wearable sensors; Health monitoring; Wearable Electronic Devices; Arousal; Wearable devices; Two-dimensional; Signal analysis; Comparatives studies; Dimensional variations; electronic device; Four-quadrant,,,emotion,No,Yes
scopus,Feature fusion for multimodal emotion recognition based on deep canonical correlation analysis,"Zhang, K.; Li, Y.; Wang, J.; Wang, Z.; Li, X.",2021,,28,,10.1109/LSP.2021.3112314,"Fusion of multimodal features is a momentous problem for video emotion recognition. As the development of deep learning, directly fusing feature matrixes of each mode through neural networks at feature level becomes mainstream method. However, unlike unimodal issues, for multimodal analysis, finding the correlations between different modal is as important as discovering effective unimodal features. To make up the deficiency in unearthing the intrinsic relationships between multimodal, a novel modularized multimodal emotion recognition model based on deep canonical correlation analysis (MERDCCA) is proposed in this letter. In MERDCCA, four utterances are gathered as a new group and each utterance contains text, audio and visual information as multimodal input. Gated recurrent unit layers are used to extract the unimodal features. Deep canonical correlation analysis based on encoder-decoder network is designed to extract cross-modal correlations by maximizing the relevance between multimodal. The experiments on two public datasets show that MERDCCA achieves the better results.  © 1994-2012 IEEE.",Multi-modal; Speech recognition; Modal analysis; Deep learning; Multimodal emotion recognition; Unimodal; multimodal emotion recognition; Features fusions; Recognition models; Multimodal features; Correlation methods; Model-based OPC; Canonical correlations analysis; Gated recurrent unit; Deep canonical correlation analyse; Deep canonical correlation analysis; gated recurrent unit,,,emotion,No,No
scopus,Automatic emotion recognition for cultural heritage,"Baecchi, C.; Ferracani, A.; Bimbo, A.D.",2020,,949,,10.1088/1757-899X/949/1/012043,"In this work we present an automatic emotion recognition system for the re-use of multimedia content and storytelling for cultural heritage. A huge amount of heterogeneous multimedia data on cultural heritage is available in online and offline databases that can be used and adapted to produce new content. In the real world, human video editors may want to select the video sequences composing the final video with the intention to induce an emotional reaction in the viewer (e.g. happiness, excitement, sadness). Usually they try to achieve this result following their personal judgement. However, this task of video selection could benefit a lot from the exploitation of an automatic sentiment classification system. Our system can help the editor in choosing the video sequences that best fit the desired emotion to be induced. First-of-all the system splits the video in scenes. Then it classifies them using a multimodal classifier which combines temporal features extracted form LSTM, sentiment-related features obtained through a DNN, audio features and motion-related features. The system learns which features are more important and exploits them to classify the scenes in terms of valence and arousal which are well known to correlate with induced emotions. Finally it provides an online video composer which allows the editor to search, filter and compose the scenes in a new video using sentiment information. To train the classifier we also collected and annotated a small dataset of both users recorded videos and professional ones downloaded from the web. © 2020 Institute of Physics Publishing. All rights reserved.",,,,emotion,Yes,Yes
scopus,Multimodal physiological-based emotion recognition,"Sharma, A.; Canavan, S.",2021,,12662 LNCS,,10.1007/978-3-030-68790-8_9,"In this paper, we propose a multimodal approach to emotion recognition using physiological signals by showing how these signals can be combined and used to accurately identify a wide range of emotions such as happiness, sadness, and pain. The proposed approach combines multiple signal types such as blood pressure, respiration, and pulse rate into one feature vector representation of emotion. Using this feature vector, we train a deep convolutional neural network to recognize emotions from 2 state-of-the-art datasets, namely DEAP and BP4D+. On BP4D+, we achieve an average, subject independent, emotion recognition accuracy of 94% for 10 emotions. We also detail subject-specific experiments, as well as gender specific models of emotion. On DEAP, we achieve 86.09%, 90.61%, 90.48%, and 90.95% for valence, arousal, liking, and dominance respectively, for single-trial classification. © Springer Nature Switzerland AG 2021.",Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Deep learning; State of the art; Convolutional neural networks; Biomedical signal processing; Multi-modal approach; Physiological data; Blood pressure; Feature vectors; Single-trial classifications; Subject-specific,,,emotion,No,Yes
scopus,Emotion assessment using feature fusion and decision fusion classification based on physiological data: Are we there yet?,"Bota, P.; Wang, C.; Fred, A.; Silva, H.",2020,,20,,10.3390/s20174723,"Emotion recognition based on physiological data classification has been a topic of increasingly growing interest for more than a decade. However, there is a lack of systematic analysis in literature regarding the selection of classifiers to use, sensor modalities, features and range of expected accuracy, just to name a few limitations. In this work, we evaluate emotion in terms of low/high arousal and valence classification through Supervised Learning (SL), Decision Fusion (DF) and Feature Fusion (FF) techniques using multimodal physiological data, namely, Electrocardiography (ECG), Electrodermal Activity (EDA), Respiration (RESP), or Blood Volume Pulse (BVP). The main contribution of our work is a systematic study across five public datasets commonly used in the Emotion Recognition (ER) state-of-the-art, namely: (1) Classification performance analysis of ER benchmarking datasets in the arousal/valence space; (2) Summarising the ranges of the classification accuracy reported across the existing literature; (3) Characterising the results for diverse classifiers, sensor modalities and feature set combinations for ER using accuracy and F1-score; (4) Exploration of an extended feature set for each modality; (5) Systematic analysis of multimodal classification in DF and FF approaches. The experimental results showed that FF is the most competitive technique in terms of classification accuracy and computational complexity. We obtain superior or comparable results to those reported in the state-of-the-art for the selected datasets. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Female; Humans; Male; Emotions; Machine learning; emotion; Speech recognition; Emotion recognition; Physiological signals; Physiology; female; human; male; Classification (of information); Benchmarking; Electrocardiography; electrocardiography; arousal; heart rate; Classification accuracy; breathing; Electrodermal activity; Signal processing; Arousal; Physiological data; Respiration; Blood volume pulse; Classification performance; Heart Rate; Extended features; supervised machine learning; Supervised Machine Learning; Systematic analysis,,,emotion,No,Yes
scopus,Deep emotion recognition through upper body movements and facial expression,"Aqdus Ilyas, C.M.; Nunes, R.; Nasrollahi, K.; Rehm, M.; Moeslund, T.B.",2021,,5,,,"Despite recent significant advancements in the field of human emotion recognition, applying upper body movements along with facial expressions present severe challenges in the field of human-robot interaction. This article presents a model that learns emotions through upper body movements and corresponds with facial expressions. Once this correspondence is mapped, tasks such as emotion and gesture recognition can easily be identified using facial features and movement vectors. Our method uses a deep convolution neural network trained on benchmark datasets exhibiting various emotions and corresponding body movements. Features obtained through facial movements and body motion are fused to get emotion recognition performance. We have implemented various fusion methodologies to integrate multimodal features for non-verbal emotion identification. Our system achieves 76.8% accuracy of emotion recognition through upper body movements only, surpassing 73.1% on the FABO dataset. In addition, employing multimodal compact bilinear pooling with temporal information surpassed the state-of-the-art method with an accuracy of 94.41% on the FABO dataset. This system can lead to better human-machine interaction by enabling robots to recognize emotions and body actions and react according to their emotions, thus enriching the user experience. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",Social robots; Facial expressions; Deep neural networks; Speech recognition; Emotion recognition; Face recognition; Deep learning; Convolutional neural networks; Human emotion recognition; Multimodal features; Man machine systems; Emotion identifications; Human machine interaction; Computer vision; State-of-the-art methods; Convolution neural network; Temporal information; Body movements; Computer graphics; User experience,,,emotion,No,Yes
scopus,"Multi-modal Emotion Recognition Based on Deep Learning in Speech, Video and Text","Zhang, X.; Wang, M.-J.; Guo, X.-D.",2020,,,,10.1109/ICSIP49896.2020.9339464,"Emotions are a concrete manifestation of human communication, and the research on emotion recognition has gradually increased. Recently, researchers have attached great importance to multi-modal emotion recognition, and in the field of speech, video, text and physiological signal emotion recognition, a lot of research work has been carried out. Multimodal emotion recognition complements each other by fusing information between different modalities, thereby improving the final recognition rate. This paper preprocesses the three modes of speech, video and text of the IEMOCAP dataset, uses deep learning neural networks to extract emotional features, and performs information fusion at the feature layer. There are five types of emotions: angry, excited, sad, neutral and happy. From the results, the accuracy of the three-mode emotion recognition model of the training set is 0.9541, and that of the verification set is 0.68383. Compared to speech emotion recognition improved by 0.11751.  © 2020 IEEE.",Image processing; Speech emotion recognition; Deep neural networks; Multi-modal; Speech recognition; deep learning; Emotion recognition; Physiological signals; Deep learning; Character recognition; Multimodal emotion recognition; Multilayer neural networks; Human communications; Training sets; Learning neural networks; feature level fusion,,,emotion,No,Yes
scopus,Audiovisual Classification of Group Emotion Valence Using Activity Recognition Networks,"Pinto, J.R.; Goncalves, T.; Pinto, C.; Sanhudo, L.; Fonseca, J.; Goncalves, F.; Carvalho, P.; Cardoso, J.S.",2020,,,,10.1109/IPAS50080.2020.9334943,"Despite recent efforts, accuracy in group emotion recognition is still generally low. One of the reasons for these underwhelming performance levels is the scarcity of available labeled data which, like the literature approaches, is mainly focused on still images. In this work, we address this problem by adapting an inflated ResNet-50 pretrained for a similar task, activity recognition, where large labeled video datasets are available. Audio information is processed using a Bidirectional Long Short-Term Memory (Bi-LSTM) network receiving extracted features. A multimodal approach fuses audio and video information at the score level using a support vector machine classifier. Evaluation with data from the EmotiW 2020 AV Group-Level Emotion sub-challenge shows a final test accuracy of 65.74% for the multimodal approach, approximately 18% higher than the official baseline. The results show that using activity recognition pretraining offers performance advantages for group-emotion recognition and that audio is essential to improve the accuracy and robustness of video-based recognition. © 2020 IEEE.",Image processing; Long short-term memory; Speech recognition; deep learning; Classification (of information); recognition; Multi-modal approach; Support vector machines; Large dataset; video; Audio and video; Group emotions; audio; valence; Audio information; Video datasets; activity; Activity recognition; group emotion; Performance level; Support vector machine classifiers,,,emotion,Yes,Yes
scopus,Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition,"Wang, Y.; Shen, G.; Xu, Y.; Li, J.; Zhao, Z.",2021,,1,,10.21437/Interspeech.2021-2004,"Various studies have confirmed the necessity and benefits of leveraging multimodal features for SER, and the latest research results show that the temporal information captured by the transformer is very useful for improving multimodal speech emotion recognition. However, the dependency between different modalities and high-level temporal-feature learning using a deeper transformer is yet to be investigated. Thus, we propose a multimodal transformer with sharing weights for speech emotion recognition. The proposed network shares the weights across the modalities in each transformer layer to learn the correlation among multiple modalities. In addition, since the emotion contained in a speech generally include audio and text features, both of which have not only internal dependence but also mutual dependence, we design a deep multimodal attention mechanism to capture these two kinds of emotional dependence. We evaluated our model on the publicly available IEMOCAP dataset. The experimental results demonstrate that the proposed model yielded a promising result. Copyright ©2021 ISCA.",Speech emotion recognition; Transformer; Multi-modal; Speech recognition; Speech; Multimodal features; Speech communication; Multimodal attention; Temporal features; Temporal information; Mutual correlations; Research results; Sharing weight; Sharing weights,,,emotion,No,Yes
scopus,An End-To-End Emotion Recognition Framework Based on Temporal Aggregation of Multimodal Information,"Radoi, A.; Birhala, A.; Ristea, N.-C.; Dutu, L.-C.",2021,,9,,10.1109/ACCESS.2021.3116530,"Humans express and perceive emotions in a multimodal manner. The multimodal information is intrinsically fused by the human sensory system in a complex manner. Emulating a temporal desynchronisation between modalities, in this paper, we design an end-to-end neural network architecture, called TA-AVN, that aggregates temporal audio and video information in an asynchronous setting in order to determine the emotional state of a subject. The feature descriptors for audio and video representations are extracted using simple Convolutional Neural Networks (CNNs), leading to real-time processing. Undoubtedly, collecting annotated training data remains an important challenge when training emotion recognition systems, both in terms of effort and expertise required. The proposed approach solves this problem by providing a natural augmentation technique that allows achieving a high accuracy rate even when the amount of annotated training data is limited. The framework is tested on three challenging multimodal reference datasets for the emotion recognition task, namely the benchmark datasets CREMA-D and RAVDESS, and one dataset from the FG2020's challenge related to emotion recognition. The results prove the effectiveness of our approach and our end-to-end framework achieves state-of-the-art results on the CREMA-D and RAVDESS datasets. © 2013 IEEE.",convolutional neural network; Multi-modal; Speech recognition; Behavioral research; Emotion recognition; Multi-modal information; Multi-modal data; multimodal data; Convolution; Convolutional neural network; Data handling; Network architecture; Audio and video; Neural networks; End to end; Audio-visual information; Augmentation techniques; audiovisual information; augmentation techniques; real-time processing; Realtime processing,,,emotion,Yes,Yes
scopus,Two Stage Emotion Recognition using Frame-level and Video-level Features,"Viegas, C.",2020,,,,10.1109/FG47880.2020.00143,"This paper compares a seven class classifier with a two stage classification for categorical emotion recognition. We use the Multimodal Emotion Recognition (MER) Dataset of the FG2020 competition which apart from video contains skeleton data collected with a Microsoft Kinect. We compare the performance of different unimodal features as well as various combinations of multimodal features. We also compare frame-level features with video-level features. We achieved 50parcent accuracy using multimodal video-level features and two stage classification on one hand, 49 % accuracy is achieved with the seven class classifier on the other hand. © 2020 IEEE.",Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Multimodal emotion recognition; Unimodal; Multimodal features; Gesture recognition; Microsoft kinect,,,emotion,No,Yes
scopus,Design of a hierarchy modular neural network and its application in multimodal emotion recognition,"Li, W.; Chu, M.; Qiao, J.",2019,,23,,10.1007/s00500-018-03735-0,"Achievement of the fusion for different modalities is a critical issue for multimodal emotion recognition. Feature-level fusion methods cannot deal with missing or corrupted data, while decision-level fusion methods may lose the correlation information between different modalities. To solve the above problems, a hierarchy modular neural network (HMNN) is proposed and is applied for multimodal emotion recognition. First, an HMNN is constructed to mimic the hierarchy modular architecture as demonstrated in the human brain. Each module contains several submodules dealing with features from different modalities. Connections are built between submodules within the same module and between corresponding submodules from different modules. Then, a learning algorithm based on Hebbian learning is used to train the connection weights in HMNN, which simulates the learning mechanism of the human brain. HMNN recognizes the label based on the activity level of each module and adopts the winner-take-all strategy. Finally, the proposed HMNN is applied on a public dataset for multimodal emotion recognition. Experimental results show that the proposed HMNN improves the recognition results, when compared with other decision-fusion methods, including support vector machine, as well as neural networks such as back-propagation and radial basis function neural networks. Furthermore, the inter-submodule connections in one module realizes information integration from different modalities and improves the performance of HMNN. Besides, the experiments suggest the effectiveness of HMNN on dealing with missing/corrupted data. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Speech recognition; Multimodal emotion recognition; Brain; Learning algorithms; Backpropagation; Image segmentation; Submodules; Decision fusion methods; Hebbian learning; Hebbian learning rule; Hierarchy modular neural network (HMNN); Information integration; Inter-submodule connections; Modular architectures; Modular neural networks; Radial basis function networks; Radial basis function neural networks,,,emotion,Yes,Yes
scopus,Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition,"Li, C.; Bao, Z.; Li, L.; Zhao, Z.",2020,,57,,10.1016/j.ipm.2019.102185,"Emotional recognition contributes to automatically perceive the user's emotional response to multimedia content through implicit annotation, which further benefits establishing effective user-centric services. Physiological-based ways have increasingly attract researcher's attention because of their objectiveness on emotion representation. Conventional approaches to solve emotion recognition have mostly focused on the extraction of different kinds of hand-crafted features. However, hand-crafted feature always requires domain knowledge for the specific task, and designing the proper features may be more time consuming. Therefore, exploring the most effective physiological-based temporal feature representation for emotion recognition becomes the core problem of most works. In this paper, we proposed a multimodal attention-based BLSTM network framework for efficient emotion recognition. Firstly, raw physiological signals from each channel are transformed to spectrogram image for capturing their time and frequency information. Secondly, Attention-based Bidirectional Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) are utilized to automatically learn the best temporal features. The learned deep features are then fed into a deep neural network (DNN) to predict the probability of emotional output for each channel. Finally, decision level fusion strategy is utilized to predict the final emotion. The experimental results on AMIGOS dataset show that our method outperforms other state of art methods. © 2019 Elsevier Ltd",Long short-term memory; Deep neural networks; Speech recognition; Behavioral research; Emotion recognition; Physiological signals; Physiology; Deep learning; Multi-modal fusion; Arts computing; EEG signals; Multimedia contents; Multimedia content; Multimedia services,,,emotion,Yes,No
scopus,Mutual correlation attentive factors in dyadic fusion networks for speech emotion recognition,"Gu, Y.; Li, W.; Lyu, X.; Chen, S.; Ivan, M.; Sun, W.; Li, X.",2019,,,,10.1145/3343031.3351039,"Emotion recognition in dyadic communication is challenging because: 1. Extracting informative modality-specific representations requires disparate feature extractor designs due to the heterogenous input data formats. 2. How to effectively and efficiently fuse unimodal features and learn associations between dyadic utterances are critical to the model generalization in actual scenario. 3. Disagreeing annotations prevent previous approaches from precisely predicting emotions in context. To address the above issues, we propose an efficient dyadic fusion network that only relies on an attention mechanism to select representative vectors, fuse modality-specific features, and learn the sequence information. Our approach has three distinct characteristics: 1. Instead of using a recurrent neural network to extract temporal associations as in most previous research, we introduce multiple sub-view attention layers to compute the relevant dependencies among sequential utterances; this significantly improves model efficiency. 2. To improve fusion performance, we design a learnable mutual correlation factor inside each attention layer to compute associations across different modalities. 3. To overcome the label disagreement issue, we embed the labels from all annotators into a k-dimensional vector and transform the categorical problem into a regression problem; this method provides more accurate annotation information and fully uses the entire dataset. We evaluate the proposed model on two published multimodal emotion recognition datasets: IEMOCAP and MELD. Our model significantly outperforms previous state-of-the-art research by 3.8%-7.5% accuracy, using a more efficient model. © 2019 Association for Computing Machinery.",Speech emotion recognition; Attention mechanisms; Speech recognition; Multi-modal fusion; Multimodal emotion recognition; Multilayer neural networks; Recurrent neural networks; Attention Mechanism; Speech communication; Multimodal Fusion Network; Speech Emotion Recognition; Mutual correlations; Dyadic Communication; Mutual Correlation Attentive Factor; Mutual correlation factor; Sequence informations; Temporal association,,,emotion,Yes,Yes
scopus,Combining facial expressions and electroencephalography to enhance emotion recognition,"Huang, Y.; Yang, J.; Liu, S.; Pan, J.",2019,,11,,10.3390/fi11050105,"Emotion recognition plays an essential role in human-computer interaction. Previous studies have investigated the use of facial expression and electroencephalogram (EEG) signals from single modal for emotion recognition separately, but few have paid attention to a fusion between them. In this paper, we adopted a multimodal emotion recognition framework by combining facial expression and EEG, based on a valence-arousal emotional model. For facial expression detection, we followed a transfer learning approach for multi-task convolutional neural network (CNN) architectures to detect the state of valence and arousal. For EEG detection, two learning targets (valence and arousal) were detected by different support vector machine (SVM) classifiers, separately. Finally, two decision-level fusion methods based on the enumerate weight rule or an adaptive boosting technique were used to combine facial expression and EEG. In the experiment, the subjects were instructed to watch clips designed to elicit an emotional response and then reported their emotional state. We used two emotion datasets-a Database for Emotion Analysis using Physiological Signals (DEAP) and MAHNOB-human computer interface (MAHNOB-HCI)-to evaluate our method. In addition, we also performed an online experiment to make our method more robust. We experimentally demonstrated that our method produces state-of-the-art results in terms of binary valence/arousal classification, based on DEAP and MAHNOB-HCI data sets. Besides this, for the online experiment, we achieved 69.75% accuracy for the valence space and 70.00% accuracy for the arousal space after fusion, each of which has surpassed the highest performing single modality (69.28% for the valence space and 64.00% for the arousal space). The results suggest that the combination of facial expressions and EEG information for emotion recognition compensates for their defects as single information sources. The novelty of this work is as follows. To begin with, we combined facial expression and EEG to improve the performance of emotion recognition. Furthermore, we used transfer learning techniques to tackle the problem of lacking data and achieve higher accuracy for facial expression. Finally, in addition to implementing the widely used fusion method based on enumerating different weights between two models, we also explored a novel fusion method, applying boosting technique. © 2019 by the authors.",Facial expressions; Speech recognition; Emotion recognition; Face recognition; Facial Expressions; Multimodal emotion recognition; Human computer interaction; Classification (of information); Electroencephalography; Biomedical signal processing; EEG; Electrophysiology; Convolutional neural network; Support vector machines; Transfer learning; Neural networks; Electroencephalogram signals; Decision level fusion; Decision-level fusion; Facial expression detections,,,emotion,No,Yes
scopus,Multimodal and multi-view models for emotion recognition,"Aguilar, G.; Rozgic, V.; Wang, W.; Wang, C.",2020,,,,,"Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs. We first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide. Then, we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function. Our multimodal model outperforms the previous state of the art on the USC-IEMOCAP dataset reported on lexical and acoustic information. Additionally, our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features. © 2019 Association for Computational Linguistics",Semantics; Attention mechanisms; Speech recognition; Emotion recognition; Object recognition; Computational linguistics; Acoustic information; Lexical information; Multimodal modeling; Semantic information; Multi-view learning; Multi-view models,,,emotion,No,No
scopus,Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition,"Liu, P.; Li, K.; Meng, H.",2020,,2020-October,,10.21437/Interspeech.2020-2067,"Emotion recognition is a challenging and actively-studied research area that plays a critical role in emotion-aware human-computer interaction systems. In a multimodal setting, temporal alignment between different modalities has not been well investigated yet. This paper presents a new model named as Gated Bidirectional Alignment Network (GBAN), which consists of an attention-based bidirectional alignment network over LSTM hidden states to explicitly capture the alignment relationship between speech and text, and a novel group gated fusion (GGF) layer to integrate the representations of different modalities. We empirically show that the attention-aligned representations outperform the last-hidden-states of LSTM significantly, and the proposed GBAN model outperforms existing state-ofthe-art multimodal approaches on the IEMOCAP dataset. Copyright © 2020 ISCA",Long short-term memory; Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Network models; Multimodal emotion recognition; Human computer interaction; Information fusion; Speech communication; Alignment; Attention model; Research areas; Fusion layers; Human-computer interaction system; Temporal alignment; Attention models; Hidden state,,,emotion,No,No
scopus,Multimodal emotion recognition using cross-modal attention and 1D convolutional neural networks,"Krishna, D.N.; Patil, A.",2020,,2020-October,,10.21437/Interspeech.2020-1190,"In this work, we propose a new approach for multimodal emotion recognition using cross-modal attention and raw waveform based convolutional neural networks. Our approach uses audio and text information to predict the emotion label. We use an audio encoder to process the raw audio waveform to extract high-level features from the audio, and we use text encoder to extract high-level semantic information from text. We use cross-modal attention where the features from audio encoder attend to the features from text encoder and vice versa. This helps in developing interaction between speech and text sequences to extract most relevant features for emotion recognition. Our experiments show that the proposed approach obtains the state of the art results on IEMOCAP dataset [1]. We obtain 1.9% absolute improvement in accuracy compared to the previous state of the art method [2]. Our proposed approach uses 1D convolutional neural network to process the raw waveform instead of spectrogram features. Our experiments also shows that processing raw waveform gives a 0.54% improvement over spectrogram based modal. Copyright © 2020 ISCA",Semantics; Emotion Recognition; Speech recognition; Character recognition; Multimodal emotion recognition; Signal encoding; Convolutional neural networks; Cross-modal; Convolution; Convolutional neural network; Cross-modal attention; Speech communication; 1d-CNN; Spectrograms; Spectrographs; New approaches; Audio information; Waveforms; 1D-CNNs; Audio encoders,,,emotion,Yes,Yes
scopus,Multi-level feature optimization and multimodal contextual fusion for sentiment analysis and emotion classification,"Huddar, M.G.; Sannakki, S.S.; Rajpurohit, V.S.",2020,,36,,10.1111/coin.12274,"The availability of the humongous amount of multimodal content on the internet, the multimodal sentiment classification, and emotion detection has become the most researched topic. The feature selection, context extraction, and multi-modal fusion are the most important challenges in multimodal sentiment classification and affective computing. To address these challenges this paper presents multilevel feature optimization and multimodal contextual fusion technique. The evolutionary computing based feature selection models extract a subset of features from multiple modalities. The contextual information between the neighboring utterances is extracted using bidirectional long-short-term-memory at multiple levels. Initially, bimodal fusion is performed by fusing a combination of two unimodal modalities at a time and finally, trimodal fusion is performed by fusing all three modalities. The result of the proposed method is demonstrated using two publically available datasets such as CMU-MOSI for sentiment classification and IEMOCAP for affective computing. Incorporating a subset of features and contextual information, the proposed model obtains better classification accuracy than the two standard baselines by over 3% and 6% in sentiment and emotion classification, respectively. © 2020 Wiley Periodicals, Inc.",Long short-term memory; Multi-modal fusion; Sentiment analysis; Classification (of information); multimodal fusion; Sentiment classification; Emotion classification; Feature extraction; Classification accuracy; Contextual information; feature selection; bidirectional LSTM; contextual information; evolutionary computing; Evolutionary computing; Feature optimizations,,,emotion,No,Yes
scopus,Semi-supervised multimodal emotion recognition with improved wasserstein GANs,"Liang, J.; Chen, S.; Jin, Q.",2019,,,,10.1109/APSIPAASC47483.2019.9023144,"Automatic emotion recognition has faced the challenge of lacking large-scale human labeled dataset for model learning due to the expensive data annotation cost and inevitable label ambiguity. To tackle such challenge, previous works have explored to transfer emotion label from one modality to the other modality assuming that the supervised annotation does exist in one modality or explored semi-supervised learning strategies to take advantage of large amount of unlabeled data with the focus on a single modality. In this work, we address the multimodal emotion recognition problem with the acoustic and visual modalities and propose a multi-modal network structure of the semi-supervised learning approach with an improved generative adversarial network CT-GAN. Extensive experiments conducted on a multi-modal emotion recognition corpus demonstrate the effectiveness of the proposed approach and prove that utilizing unlabeled data via GANs and combining multi-modalities both benefit the classification performance. We also carry out some detailed analysis experiments such as influence of unlabeled data quantity on recognition performance and impact of different normalization strategies for semi-supervised learning etc. © 2019 IEEE.",Semi-supervised learning; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Large dataset; Visual modalities; Transfer learning; Automatic emotion recognition; Multimodal network; Classification performance; Labeled data; Adversarial networks; Normalization strategies,,,emotion,Yes,Yes
scopus,Generating Emotional Social Chatbot Responses with a Consistent Speaking Style,"Zhang, J.; Yang, Y.; Chen, C.; He, L.; Yu, Z.",2020,,12431 LNAI,,10.1007/978-3-030-60457-8_5,"Emotional conversation plays a vital role in creating more human-like conversations. Although previous works on emotional conversation generation have achieved promising results, the issue of the speaking style inconsistency still exists. In this paper, we propose a Style-Aware Emotional Dialogue System (SEDS) to enhance speaking style consistency through detecting user’s emotions and modeling speaking styles in emotional response generation. Specifically, SEDS uses an emotion encoder to perceive the user’s emotion from multimodal inputs, and tracks speaking styles through jointly optimizing a generator that is augmented with a personalized lexicon to capture explicit word-level speaking style features. Additionally, we propose an auxiliary task, a speaking style classification task, to guide SEDS to learn the implicit form of speaking style during the training process. We construct a multimodal dialogue dataset and make the alignment and annotation to verify the effectiveness of the model. Experimental results show that our SEDS achieves a significant improvement over other strong baseline models in terms of perplexity, emotion accuracy and style consistency. © 2020, Springer Nature Switzerland AG.",Behavioral research; Multimodal; Speech processing; Dialogue systems; Natural language processing systems; Classification tasks; Multimodal dialogue; Emotional response; Training process; Multimodal inputs; Emotional conversation; Speaking style; Speaking styles; Style consistency,,,emotion,Yes,Yes
scopus,Multimodal speech emotion recognition using cross attention with aligned audio and text,"Lee, Y.; Yoon, S.; Jung, K.",2020,,2020-October,,10.21437/Interspeech.2020-2312,"In this paper, we propose a novel speech emotion recognition model called Cross Attention Network (CAN) that uses aligned audio and text signals as inputs. It is inspired by the fact that humans recognize speech as a combination of simultaneously produced acoustic and textual signals. First, our method segments the audio and the underlying text signals into equal number of steps in an aligned way so that the same time steps of the sequential signals cover the same time span in the signals. Together with this technique, we apply the cross attention to aggregate the sequential information from the aligned signals. In the cross attention, each modality is aggregated independently by applying the global attention mechanism onto each modality. Then, the attention weights of each modality are applied directly to the other modality in a crossed way, so that the CAN gathers the audio and text information from the same time steps based on each modality. In the experiments conducted on the standard IEMOCAP dataset, our model outperforms the state-of-the-art systems by 2.66% and 3.18% relatively in terms of the weighted and unweighted accuracy. © 2020 ISCA",Speech emotion recognition; Attention mechanisms; Emotion Recognition; Multi-modal; Speech recognition; Deep learning; Character recognition; Attention mechanism; Recognition models; Multi-modal learning; Multimodal learning; Speech communication; Time step; Audio information; Sequential information; Time span,,,emotion,No,Yes
scopus,Dynamic facial features in positive-emotional speech for identification of depressive tendencies,"Liu, J.-Q.; Huang, Y.; Huang, X.-Y.; Xia, X.-T.; Niu, X.-X.; Lin, L.; Chen, Y.-W.",2020,,192,,10.1007/978-981-15-5852-8_12,"Depressive symptoms in young people may persist into adulthood and develop into depression. Early screening of depressive tendencies in university students helps to reduce the number and intensity of their depressive episodes. Automatic prediction of depression from visual clues has gained increasing interest in recent years. This study proposes a deep learning model for depressive tendencies prediction, which extracts and fuses the dynamic facial features when participants were uttering positive text materials. The effectiveness of the proposed method is validated on our original multimodal behavioral dataset of Chinese university students. The results demonstrated that dynamic facial expression features can potentially reveal depressive tendencies. The average detection accuracy of individually uttered sentence is 67.32%, increasing to 71.4% when multiple sentences were fused. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2020.",Health care; Deep learning; Learning models; Diagnosis; Dynamic facial expression; Emotional speech; Fusion; Detection accuracy; Long Short-Term Memory (LSTM); Automatic prediction; Chinese universities; Convolutional Neural Network (CNN); Depressive symptom; Depressive tendencies; Dynamic facial features; Emotion voice stimuli; University students,,,emotion,No,Yes
scopus,Biosignal-Based Multimodal Emotion Recognition in a Valence-Arousal Affective Framework Applied to Immersive Video Visualization,"Pinto, J.; Fred, A.; Da Silva, H.P.",2019,,,,10.1109/EMBC.2019.8857852,"Many emotion recognition schemes have been proposed in the state-of-the-art. They generally differ in terms of the emotion elicitation methods, target emotional states to recognize, data sources or modalities, and classification techniques. In this work several biosignals are explored for emotion assessment during immersive video visualization, collecting multimodal data from Electrocardiography (ECG), Electrodermal Activity (EDA), Blood Volume Pulse (BVP) and Respiration sensors. Participants reported their emotional state of the day (baseline), and provided self-assessment of the emotion experienced in each video through the Self-Assessment Manikin (SAM), in the valence-arousal space. Multiple physiological and statistical features extracted from the biosignals were used as inputs to an emotion recognition workflow, targeting user-independent classification with two classes per dimension. Support Vector Machines (SVM) were used, as it is considered one of the most promising classifiers in the field. The proposed approach lead to accuracies of 69.13% for arousal and 67.75% for valence, which are encouraging for further research with a larger training dataset and population. © 2019 IEEE.",Humans; Emotions; emotion; Speech recognition; Emotion recognition; Emotion elicitation; human; Multimodal emotion recognition; Classification (of information); Support vector machines; Electrocardiography; electrocardiography; arousal; heart rate; breathing; support vector machine; Electrodermal activity; Data visualization; Visualization; Support Vector Machine; Arousal; Respiration; Blood volume pulse; Heart Rate; Classification technique; Statistical features; Video visualization,,,emotion,No,Yes
scopus,Multimodal Approach of Speech Emotion Recognition Using Multi-Level Multi-Head Fusion Attention-Based Recurrent Neural Network,"Ho, N.-H.; Yang, H.-J.; Kim, S.-H.; Lee, G.",2020,,8,,10.1109/ACCESS.2020.2984368,"Speech emotion recognition is a challenging but important task in human computer interaction (HCI). As technology and understanding of emotion are progressing, it is necessary to design robust and reliable emotion recognition systems that are suitable for real-world applications both to enhance analytical abilities supporting human decision making and to design human-machine interfaces (HMI) that assist efficient communication. This paper presents a multimodal approach for speech emotion recognition based on Multi-Level Multi-Head Fusion Attention mechanism and recurrent neural network (RNN). The proposed structure has inputs of two modalities: audio and text. For audio features, we determine the mel-frequency cepstrum (MFCC) from raw signals using the OpenSMILE toolbox. Further, we use pre-trained model of bidirectional encoder representations from transformers (BERT) for embedding text information. These features are fed parallelly into the self-attention mechanism base RNNs to exploit the context for each timestamp, then we fuse all representatives using multi-head attention technique to predict emotional states. Our experimental results on the three databases: Interactive Emotional Motion Capture (IEMOCAP), Multimodal EmotionLines Dataset (MELD), and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), reveal that the combination of the two modalities achieves better performance than using single models. Quantitative and qualitative evaluations on all introduced datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods. © 2013 IEEE.",Speech emotion recognition; Speech recognition; Behavioral research; Human computer interaction; Decision making; Recurrent neural networks; RNN; State-of-the-art methods; Human Machine Interface; Recurrent neural network (RNN); audio features; Efficient communications; Human computer interaction (HCI); Mel frequency cepstrum; multi-level multi-head fusion attention; Qualitative evaluations; textual features,,,emotion,No,Yes
scopus,Towards emotion-aided multi-modal dialogue act classification,"Saha, T.; Patra, A.P.; Saha, S.; Bhattacharyya, P.",2020,,,,,"The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants. © 2020 Association for Computational Linguistics",Emotion Recognition; Deep neural networks; Emotional state; Multi-modal; Emotion recognition; Facial Expressions; Classification (of information); Automation; Dialog acts; Communicative intent; Automatic identification; Feature changes; Multi-modality recognition; Non-verbal features,,,emotion,No,Yes
scopus,Multimodal emotion recognition with transformer-based self supervised feature fusion,"Siriwardhana, S.; Kaluarachchi, T.; Billinghurst, M.; Nanayakkara, S.",2020,,8,,10.1109/ACCESS.2020.3026823,"Emotion Recognition is a challenging research area given its complex nature, and humans express emotional cues across various modalities such as language, facial expressions, and speech. Representation and fusion of features are the most crucial tasks in multimodal emotion recognition research. Self Supervised Learning (SSL) has become a prominent and influential research direction in representation learning, where researchers have access to pre-trained SSL models that represent different data modalities. For the first time in the literature, we represent three input modalities of text, audio (speech), and vision with features extracted from independently pre-trained SSL models in this paper. Given the high dimensional nature of SSL features, we introduce a novel Transformers and Attention-based fusion mechanism that can combine multimodal SSL features and achieve state-of-the-art results for the task of multimodal emotion recognition. We benchmark and evaluate our work to show that our model is robust and outperforms the state-of-the-art models on four datasets. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Transformer; Speech recognition; BERT; Emotion recognition; Facial Expressions; Multimodal emotion recognition; State of the art; Self-supervised learning; Self-attention; Fusion mechanism; Input modalities; High-dimensional; Fusion of features,,,emotion,No,No
scopus,Multimodal system for emotion recognition using eeg and customer review,"Panda, D.; Chakladar, D.D.; Dasgupta, T.",2020,,1112,,10.1007/978-981-15-2188-1_32,"Emotion is a basic expression of a human being using which he/she can communicate with the external world. Emotion can be recognized using various media such as movie, image, facial expression, and audio. Emotion recognition can be performed by the brain signal (e.g., Electroencephalogram). A multimodal system of emotion recognition using Electroencephalogram (EEG) and sentiment analysis of customer has been proposed. Four types of emotion, namely: Happy, sad, relaxed and anger have been recognized. The proposed multimodal framework accepts the combination of temporal (EEG signal) and spatial (customer reviews/comments) information as inputs and generates the emotion of user during watching the product on computer screen. The proposed system learns temporal and spatial discriminative features using EEG encoder and text encoder. Both of the encoders transform the features of EEG and text into common feature space. The methodology is being tested on a dataset of 30 users, consisting of EEG and customers’ review data. An accuracy of 98.27% has been recorded. © Springer Nature Singapore Pte Ltd. 2020.",Natural language processing; Machine learning; Speech recognition; Emotion recognition; Facial Expressions; Signal encoding; Sentiment analysis; Electroencephalography; Biomedical signal processing; EEG; Multimodal frameworks; Discriminative features; Multimodal system; Electro-encephalogram (EEG); Computer screens; Sales; Temporal and spatial,,,emotion,No,Yes
scopus,A combined reinforcement regression model based on weighted feedback for multimodal emotion recognition,"Zhang, G.; Yang, G.; Qu, S.; Luo, T.; Han, X.",2019,,,,10.1109/ICIEA.2019.8834030,"Automatic emotion recognition has become more and more important in human-computer interaction. Although several approaches have been presented in emotion recognition, some problems remain open. This paper presents a model structure of automatic emotion recognition based on multimodal (audio and visual) in continuous dimensional emotional space. A combined reinforcement regression model (CRRM) structure is built based on gradient boosting regression (GBR) and support vector regression (SVR) to process visual features and audio features, respectively. And then, two weighted fusion methods are added to deal with outliers in the predicted result of audio and visual. The data set is the LIRIS-ACCEDE database. Finally, the fusion results increased by 15.88% and 6.39% compared the predicted result of audio and visual in arousal space, and increased by 4.03% and 5.94% in valence space, respectively. There are 6.23% and 6.04% increases than linear regression fusion method. The results show that this recognition structure is superior and the fusion results are reliable. © 2019 IEEE.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Human computer interaction; Regression analysis; Automatic emotion recognition; Weighted fusion; Continuous dimensional emotional space; Gradient boosting; Gradient boosting regression (GBR); Industrial electronics; Regression model; Reinforcement; Support vector regression (SVR),,,emotion,No,Yes
scopus,Visual-Texual Emotion Analysis with Deep Coupled Video and Danmu Neural Networks,"Li, C.; Wang, J.; Wang, H.; Zhao, M.; Li, W.; Deng, X.",2020,,22,,10.1109/TMM.2019.2946477,"User emotion analysis toward videos is to automatically recognize the general emotional status of viewers from the multimedia content embedded in the online video stream. Existing works fall into two categories: 1) visual-based methods, which focus on visual content and extract a specific set of features of videos. However, it is generally hard to learn a mapping function from low-level video pixels to high-level emotion space due to great intra-class variance. 2) textual-based methods, which focus on the investigation of user-generated comments associated with videos. The learned word representations by traditional linguistic approaches typically lack emotion information and the global comments usually reflect viewers' high-level understandings rather than instantaneous emotions. To address these limitations, in this paper, we propose to jointly utilize video content and user-generated texts simultaneously for emotion analysis. In particular, we introduce exploiting a new type of user-generated texts, i.e., 'danmu,' which are real-time comments floating on the video and contain rich information to convey viewers' emotional opinions. To enhance the emotion discriminativeness of words in textual feature extraction, we propose Emotional Word Embedding (EWE) to learn text representations by jointly considering their semantics and emotions. Afterward, we propose a novel visual-textual emotion analysis model with Deep Coupled Video and Danmu Neural networks (DCVDN), in which visual and textual features are synchronously extracted and fused to form a comprehensive representation by deep-canonically-correlated-autoencoder-based multi-view learning. Through extensive experiments on a self-crawled real-world video-danmu dataset, we prove that DCVDN significantly outperforms the state-of-the-art baselines. © 1999-2012 IEEE.",Semantics; Learning systems; emotion analysis; Text representation; Textual features; Multimedia contents; Real world videos; Multi-view learning; Danmu; deep multimodal learning; Linguistic approach; Mapping functions; Word representations,,,emotion,No,No
scopus,Dimensional emotion recognition method based on hierarchical attention mechanism,"Tang, Y.; Mao, Q.; Gao, L.",2020,,46,,10.19678/j.issn.1000-3428.0054127,"In continuous dimensional emotion recognition, the part of highlighting emotional expression varies in each modality, and different modalities also have different influence on emotional states. To address the problem, by learning modal features and fusing them in a reasonable way, this paper proposes a multimodal dimensional emotion recognition model based on Hierarchical Attention Mechanism(HAM). Frequency attention mechanism is added to the audio modality to learn the context information in frequency domain, and the video features are fused with the audio features by using the multimodal attention mechanism. Then the problem of missing modalities is relieved by using the improved loss function to improve the robustness and emotion recognition performance. Experimental results on public datasets show that compared with methods such as Convolutional Neural Network(CNN) and Long Short Term Memory(LSTM) networks, this method has improved the Concordance Correlation Coefficient(CCC) index, and has higher recognition efficiency. It is applicable to dimensional emotion recognition of large volumes of data. © 2020, Editorial Office of Computer Engineering. All rights reserved.",Deep learning; Feature fusion; Attention mechanism; Multimodality; Continuous dimensional emotion recognition,,,emotion,No,Yes
scopus,A social emotion classification approach using multi-model fusion,"Xu, G.; Li, W.; Liu, J.",2020,,102,,10.1016/j.future.2019.07.007,"With the proliferation of the online video publishing, the number of multimodal contents on the Internet has exponentially grown. Research of emotion analysis has developed from the traditional single-mode to complex multimode analysis. Most recent studies have paid rare attention to the visual emotion information deriving from merging visual and audio emotional information at the feature or decision level, even though some of them considered the multimodality analysis. In this paper, we extract visual, textual, and audio information from video and propose a multimodal emotional classification framework to capture the emotions of users in social networks. We have designed a 3DCLS (3D Convolutional-Long Short Term Memory) hybrid model that classifies visual emotions as well as a CNN–RNN hybrid model that classifies text-based emotions. Finally, visual, audio and text modes are combined to generate final emotional classification results. Experiments on the MOUD and IEMOCAP emotion datasets show that the proposed framework outperforms existing models in multimodal mood analysis. © 2019 Elsevier B.V.",Modal analysis; Multi-modal fusion; Classification (of information); Emotion analysis; Convolution; Convolutional neural network; Multimodal fusion; Recurrent neural networks; Emotional information; Text processing; Recurrent neural network; Multi-model fusion; Emotional classification; Audio information; 3D convolutional neural network; Multimode analysis,,,emotion,No,No
scopus,Multimodal joint emotion and game context recognition in league of legends livestreams,"Ringer, C.; Walker, J.A.; Nicolaou, M.A.",2019,,2019-August,,10.1109/CIG.2019.8848060,"Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer's emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled ('in-the-wild') conditions. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Secondly, we propose a method that exploits tensor decompositions for high-order fusion of multimodal representations. The proposed method is evaluated on the problem of jointly predicting game context and player affect, compared with a set of baseline fusion approaches such as late and early fusion. Data and code are available at https://github.com/charlieringer/LoLEmoGameRecognition. © 2019 IEEE.",Multi-modal data; Multi-modal fusion; Affective Computing; Human computer interaction; Affective computing; Multimodal fusion; Multi-views; Audio-visual data; Context recognition; Livestreaming; Multi-view fusion; Tensor decomposition,,,emotion,Yes,No
scopus,Multimodal approach for emotion recognition based on simulated flight experiments,"Roza, V.C.C.; Postolache, O.A.",2019,,19,,10.3390/s19245516,"The present work tries to fill part of the gap regarding the pilots’ emotions and their bio-reactions during some flight procedures such as, takeoff, climbing, cruising, descent, initial approach, final approach and landing. A sensing architecture and a set of experiments were developed, associating it to several simulated flights (Nflights = 13) using the Microsoft Flight Simulator Steam Edition (FSX-SE). The approach was carried out with eight beginner users on the flight simulator (Npilots = 8). It is shown that it is possible to recognize emotions from different pilots in flight, combining their present and previous emotions. The cardiac system based on Heart Rate (HR), Galvanic Skin Response (GSR) and Electroencephalography (EEG), were used to extract emotions, as well as the intensities of emotions detected from the pilot face. We also considered five main emotions: happy, sad, angry, surprise and scared. The emotion recognition is based on Artificial Neural Networks and Deep Learning techniques. The Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) were the main methods used to measure the quality of the regression output models. The tests of the produced output models showed that the lowest recognition errors were reached when all data were considered or when the GSR datasets were omitted from the model training. It also showed that the emotion surprised was the easiest to recognize, having a mean RMSE of 0.13 and mean MAE of 0.01; while the emotion sad was the hardest to recognize, having a mean RMSE of 0.82 and mean MAE of 0.08. When we considered only the higher emotion intensities by time, the most matches accuracies were between 55% and 100%. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.",Speech recognition; Emotion recognition; Deep learning; Multimodal sensing; Electroencephalography; Electrophysiology; Multi-modal approach; Mean square error; Galvanic skin response; Neural networks; Errors; Flight simulation; Flight simulators; Microsoft Flight Simulator; Multi-modal sensing; Physiological sensing; Root mean squared errors,,,emotion,No,Yes
scopus,Electroencephalogram Emotion Recognition Based on Empirical Mode Decomposition and Optimal Feature Selection,"Liu, Z.-T.; Xie, Q.; Wu, M.; Cao, W.-H.; Li, D.-Y.; Li, S.-H.",2019,,11,,10.1109/TCDS.2018.2868121,"Electroencephalogram (EEG) emotion recognition based on a hybrid feature extraction method in empirical mode decomposition domain combining with optimal feature selection based on sequence backward selection is proposed, which can reflect subtle information of multiscale components of unstable and nonlinear EEG signals and remove the reductant features to improve the performance of emotion recognition. The proposal is tested on DEAP dataset, in which the emotional states in the Valance dimension and Arousal dimension are classified by both ${K}$ -nearest neighbor and support vector machine, respectively. In the experiments, temporal windows of different length and three kinds of rhythms of EEG signal are taken into account for comparison, from which the results show that EEG signal with 1s temporal window achieves highest recognition accuracy of 86.46% in Valence dimension and 84.90% in Arousal dimension, respectively, which is superior to some state-of-the-art works. The proposed method would be applied to real-time emotion recognition in multimodal emotional communication-based humans-robots interaction system. © 2016 IEEE.",Speech recognition; emotion recognition; Emotion recognition; Human robot interaction; Electroencephalography; Biomedical signal processing; Electrophysiology; Support vector machines; Hybrid features; Electroencephalogram (EEG); Frequency domain analysis; Time domain analysis; Extraction; Feature extraction; Real-time emotion recognition; Nearest neighbor search; Hybrid-feature extraction; Electro-encephalogram (EEG); Empirical Mode Decomposition; empirical mode decomposition (EMD); hybrid feature; Optimal feature selections; Psychology computing; Sequential Backward Selection; sequential backward selection (SBS),,,emotion,No,Yes
scopus,Bimodal emotion recognition based on audio and facial parts using deep convolutional neural networks,"Cornejo, J.; Pedrini, H.",2019,,,,10.1109/ICMLA.2019.00026,"Emotion recognition based on multiple information channels has recently received significant attention. Some examples of multimodal emotion recognition applications include online learning, entertainment, biometric systems, human-computer interactions, interpersonal relations, and behavior prediction. Two natural and effective ways to express emotions in human-human interaction are speech and facial expressions. In this work, we develop and evaluate a hybrid deep convolutional neural network (CNN) to extract audio and visual features from videos. An audio signal is converted into an image representation as input to a 2D convolutional neural network, whereas visual information is extracted from parts of the face. We then fuse both audio and visual features, reducing them through Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). Finally, K-Nearest Neighbor (K-NN), Support Vector Machine (SVM), Logistic Regression (LR) and Gaussian Naive Bayes (GNB) classifiers are employed for emotion recognition. Experiments are conducted on RML, eNTERFACE05 and BAUM-1s datasets. Results show that our model achieved competitive recognition rates compared to approaches available in the literature. © 2019 IEEE.",Deep neural networks; Speech recognition; Face recognition; Multimodal emotion recognition; Learning systems; Human computer interaction; Convolutional neural networks; Biometrics; Convolution; Support vector machines; Principal component analysis; Online systems; Nearest neighbor search; Audio features; Logistic regression; Bimodal emotion recognition; Visual feature; Discriminant analysis; Expression recognition; Linear discriminant analysis; Facial parts; Human-human interactions; Support vector regression; Visual features,,,emotion,No,No
scopus,ElderReact: A multimodal dataset for recognizing emotional response in aging adults,"Ma, K.; Wang, X.; Yang, X.; Zhang, M.; Girard, J.M.; Morency, L.-P.",2019,,,,10.1145/3340555.3353747,"Automatic emotion recognition plays a critical role in technologies such as intelligent agents and social robots and is increasingly being deployed in applied settings such as education and healthcare. Most research to date has focused on recognizing the emotional expressions of young and middleaged adults and, to a lesser extent, children and adolescents. Very few studies have examined automatic emotion recognition in older adults (i.e., elders), which represent a large and growing population worldwide. Given that aging causes many changes in facial shape and appearance and has been found to alter patterns of nonverbal behavior, there is strong reason to believe that automatic emotion recognition systems may need to be developed specifically (or augmented) for the elder population. To promote and support this type of research, we introduce a newly collected multimodal dataset of elders reacting to emotion elicitation stimuli. Specifically, it contains 1323 video clips of 46 unique individuals with human annotations of six discrete emotions: anger, disgust, fear, happiness, sadness, and surprise as well as valence. We present a detailed analysis of the most indicative features for each emotion. We also establish several baselines using unimodal and multimodal features on this dataset. Finally, we show that models trained on dataset of another age group do not generalize well on elders. © 2019 Copyright held by the owner/author(s).",Emotion Recognition; Speech recognition; Behavioral research; Emotion recognition; Multi-modal dataset; Multimodal features; Intelligent robots; Automatic emotion recognition; Interactive computer systems; Emotional expressions; Intelligent agents; Children and adolescents; Elders; Nonverbal behavior; Nonverbal behavior analysis; Pattern recognition systems,,,emotion,No,No
scopus,EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege's Principle,"Mittal, T.; Guhan, P.; Bhattacharya, U.; Chandra, R.; Bera, A.; Manocha, D.",2020,,,,10.1109/CVPR42600.2020.01424,"We present EmotiCon, a learning-based algorithm for context-Aware perceived human emotion recognition from videos and images. Motivated by Frege's Context Principle from psychology, our approach combines three interpretations of context for emotion recognition. Our first interpretation is based on using multiple modalities (e.g.faces and gaits) for emotion recognition. For the second interpretation, we gather semantic context from the input image and use a self-Attention-based CNN to encode this information. Finally, we use depth maps to model the third interpretation related to socio-dynamic interactions and proximity among agents. We demonstrate the efficiency of our network through experiments on EMOTIC, a benchmark dataset. We report an Average Precision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8 over prior methods. We also introduce a new dataset, GroupWalk, which is a collection of videos captured in multiple real-world settings of people walking. We report an AP of 65.83 across 4 categories on GroupWalk, which is also an improvement over prior methods.  © 2020 IEEE.",Semantics; Speech recognition; Behavioral research; Emotion recognition; Multimodal emotion recognition; Human emotion recognition; Learning algorithms; Multiple modalities; Benchmark datasets; Dynamic interaction; Learning-based algorithms; Real world setting,,,emotion,No,No
scopus,Metric Learning-Based Multimodal Audio-Visual Emotion Recognition,"Ghaleb, E.; Popa, M.; Asteriadis, S.",2020,,27,,10.1109/MMUL.2019.2960219,"People express their emotions through multiple channels, such as visual and audio ones. Consequently, automatic emotion recognition can be significantly benefited by multimodal learning. Even-though each modality exhibits unique characteristics; multimodal learning takes advantage of the complementary information of diverse modalities when measuring the same instance, resulting in enhanced understanding of emotions. Yet, their dependencies and relations are not fully exploited in audio-video emotion recognition. Furthermore, learning an effective metric through multimodality is a crucial goal for many applications in machine learning. Therefore, in this article, we propose multimodal emotion recognition metric learning (MERML), learned jointly to obtain a discriminative score and a robust representation in a latent-space for both modalities. The learned metric is efficiently used through the radial basis function (RBF) based support vector machine (SVM) kernel. The evaluation of our framework shows a significant performance, improving the state-of-the-art results on the eNTERFACE and CREMA-D datasets. © 1994-2012 IEEE.",Speech recognition; Emotion recognition; Convolutional neural networks; Convolutional neural network; Support vector machines; Multi-modal learning; Multimodal learning; Metric learning; Radial basis function networks; Audio-video emotion recognition; Fisher vectors,,,emotion,No,Yes
scopus,A VR game-based system for multimodal emotion data collection,"Bassano, C.; Ballestin, G.; Ceccaldi, E.; Larradet, F.I.; Mancini, M.; Volta, E.; Niewiadomsky, R.",2019,,,,10.1145/3359566.3364695,"The rising popularity of learning techniques in data analysis has recently led to an increased need of large-scale datasets. In this study, we propose a system consisting of a VR game and a software platform designed to collect the player’s multimodal data, synchronized with the VR content, with the aim of creating a dataset for emotion detection and recognition. The game was implemented ad-hoc in order to elicit joy and frustration, following the emotion elicitation process described by Roseman’s appraisal theory. In this preliminary study, 5 participants played our VR game along with pre-existing ones and self-reported experienced emotions. © 2019 Copyright held by the owner/author(s).",Emotion elicitation; Emotion detection; Large dataset; Learning techniques; Large-scale datasets; Appraisal theory; Mutimodal; Software platforms; VR interaction,,,emotion,No,No
scopus,Multi-level context extraction and attention-based contextual inter-modal fusion for multimodal sentiment analysis and emotion classification,"Huddar, M.G.; Sannakki, S.S.; Rajpurohit, V.S.",2020,,9,,10.1007/s13735-019-00185-8,"The recent advancements in the Internet technology and its associated services, led the users to post a large amount of multimodal data into social media Web sites, online shopping portals, video repositories, etc. The availability of the huge amount of multimodal content, multimodal sentiment classification, and affective computing has become the most researched topic. The extraction of context among the neighboring utterances and considering the importance of inter-modal utterances before multimodal fusion are the most important research issues in this field. This article presents a novel approach to extract the context at multiple levels and to understand the importance of inter-modal utterances in sentiment and emotion classification. Experiments are conducted on two publically accepted datasets such as CMU-MOSI for sentiment analysis and IEMOCAP for emotion classification. By incorporating the utterance-level contextual information and importance of inter-modal utterances, the proposed model outperforms the standard baselines by over 3% in classification accuracy. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Attention model; Bidirectional recurrent neural network; Inter-modal fusion; Multi-level contextual information,,,emotion,No,Yes
scopus,A transformer-based joint-encoding for emotion recognition and sentiment analysis,"Delbrouck, J.-B.; Tits, N.; Brousmiche, M.; Dupont, S.",2020,,,,,"Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source 1 © 2017 Association for Computational Linguistics",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Signal encoding; Sentiment analysis; Encoding (symbols); Grand Challenge; Modulars; Joint encoding; Open-source,,,emotion,No,No
scopus,Synch-graph: Multisensory emotion recognition through neural synchrony via graph convolutional networks,"Mansouri-Benssassi, E.; Ye, J.",2020,,,,,"Human emotions are essentially multisensory, where emotional states are conveyed through multiple modalities such as facial expression, body language, and non-verbal and verbal signals. Therefore having multimodal or multisensory learning is crucial for recognising emotions and interpreting social signals. Existing multisensory emotion recognition approaches focus on extracting features on each modality, while ignoring the importance of constant interaction and co-learning between modalities. In this paper, we present a novel bio-inspired approach based on neural synchrony in audiovisual multisensory integration in the brain, named Synch-Graph. We model multisensory interaction using spiking neural networks (SNN) and explore the use of Graph Convolutional Networks (GCN) to represent and learn neural synchrony patterns. We hypothesise that modelling interactions between modalities will improve the accuracy of emotion recognition. We have evaluated Synch-Graph on two state-of-the-art datasets and achieved an overall accuracy of 98.3% and 96.82%, which are significantly higher than the existing techniques. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Speech recognition; Convolutional neural networks; Convolution; Multiple modalities; Convolutional networks; Multisensory integration; Extracting features; Biomimetics; Bio-inspired approach; Multi-sensory learning; Multisensory interaction; Spiking neural network(SNN),,,emotion,No,Yes
scopus,Cross-subject multimodal emotion recognition based on hybrid fusion,"Cimtay, Y.; Ekmekcioglu, E.; Caglar-Ozhan, S.",2020,,8,,10.1109/ACCESS.2020.3023871,"Multimodal emotion recognition has gained traction in affective computing research community to overcome the limitations posed by the processing a single form of data and to increase recognition robustness. In this study, a novel emotion recognition system is introduced, which is based on multiple modalities including facial expressions, galvanic skin response (GSR) and electroencephalogram (EEG). This method follows a hybrid fusion strategy and yields a maximum one-subject-out accuracy of 81.2% and a mean accuracy of 74.2% on our bespoke multimodal emotion dataset (LUMED-2) for 3 emotion classes: sad, neutral and happy. Similarly, our approach yields a maximum one-subject-out accuracy of 91.5% and a mean accuracy of 53.8% on the Database for Emotion Analysis using Physiological Signals (DEAP) for varying numbers of emotion classes, 4 in average, including angry, disgust, afraid, happy, neutral, sad and surprised. The presented model is particularly useful in determining the correct emotional state in the case of natural deceptive facial expressions. In terms of emotion recognition accuracy, this study is superior to, or on par with, the reference subject-independent multimodal emotion recognition studies introduced in the literature. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Speech recognition; Emotion recognition; Facial Expressions; Physiological signals; Multimodal emotion recognition; Affective Computing; Electroencephalography; Electrophysiology; Electroencephalogram; Convolutional neural network; Multiple modalities; Galvanic skin response; Multimodal data fusion; Electro-encephalogram (EEG),,,emotion,No,Yes
scopus,Automatic Emotion Recognition Using Temporal Multimodal Deep Learning,"Nakisa, B.; Rastgoo, M.N.; Rakotonirainy, A.; Maire, F.; Chandran, V.",2020,,8,,10.1109/ACCESS.2020.3027026,"Emotion recognition using miniaturised wearable physiological sensors has emerged as a revolutionary technology in various applications. However, detecting emotions using the fusion of multiple physiological signals remains a complex and challenging task. When fusing physiological signals, it is essential to consider the ability of different fusion approaches to capture the emotional information contained within and across modalities. Moreover, since physiological signals consist of time-series data, it becomes imperative to consider their temporal structures in the fusion process. In this study, we propose a temporal multimodal fusion approach with a deep learning model to capture the non-linear emotional correlation within and across electroencephalography (EEG) and blood volume pulse (BVP) signals and to improve the performance of emotion classification. The performance of the proposed model is evaluated using two different fusion approaches - early fusion and late fusion. Specifically, we use a convolutional neural network (ConvNet) long short-term memory (LSTM) model to fuse the EEG and BVP signals to jointly learn and explore the highly correlated representation of emotions across modalities, after learning each modality with a single deep network. The performance of the temporal multimodal deep learning model is validated on our dataset collected from smart wearable sensors and is also compared with results of recent studies. The experimental results show that the temporal multimodal deep learning models, based on early and late fusion approaches, successfully classified human emotions into one of four quadrants of dimensional emotions with an accuracy of 71.61% and 70.17%, respectively. © 2013 IEEE.",Long short-term memory; convolutional neural network; Speech recognition; Emotion recognition; Physiological signals; Deep learning; Learning systems; electroencephalography; Electroencephalography; Convolutional neural networks; Biomedical signal processing; Electrophysiology; Emotion classification; Emotional information; Physiological sensors; Wearable sensors; Automatic emotion recognition; long short-term memory; blood volume pulse; Revolutionary technology; temporal multimodal fusion; Temporal structures,,,emotion,No,Yes
scopus,Pain versus Affect? An investigation in the relationship between observed emotional states and self-reported pain,"Tsai, F.-S.; Weng, Y.-M.; Ng, C.-J.; Lee, C.-C.",2019,,,,10.1109/APSIPAASC47483.2019.9023134,"Painis an internal sensation intricately intertwined with individual affect states resulting in a varied expressive behaviors multimodally. Past research have indicated that emotion is an important factor in shaping one's painful experiences and behavioral expressions. In this work, we present a study into understanding the relationship between individual emotional states and self-reported pain-levels. The analyses show that there is a significant correlation between observed valence state of an individual and his/her own self-reported pain-levels. Furthermore, we propose an emotion-enriched multitask network (EEMN) to improve self-reported pain-level recognition by leveraging the rated emotional states using multimodal expressions computed from face and speech. Our framework achieves accuracy of 70.1% and 52.1% in binary and ternary classes classification. The method improves a relative of 6.6% and 13% over previous work on the same dataset. Further, our analyses not only show that an individual's valence state is negatively correlated to the pain-level reported, but also reveal that asking observers to rate valence attribute could be related more to the self-reported pain than to rate directly on the pain intensity itself. © 2019 IEEE.",Emotional state; Speech recognition; Health; Affect state; Multimodal expressions; Valence state,,,emotion,No,Yes
scopus,Physiological signals-based emotion recognition via high-order correlation learning,"Zhu, J.; Wei, Y.; Feng, Y.; Zhao, X.; Gao, Y.",2019,,15,,10.1145/3332374,"Emotion recognition by physiological signals is an effective way to discern the inner state of human beings and therefore has been widely adopted in many user-centered applications. The majority of current state-of-the-art methods focus on exploring relationship among emotion and physiological signals. Given some particular features of the natural process of emotional expression, it is still a challenging and urgent issue to efficiently combine such high-order correlations among multimodal physiological signals and subjects. To tackle the problem, a novel multi-hypergraph neural networks is proposed, in which one hypergraph is established with one type of physiological signals to formulate inter-subject correlations. Each one of the vertices in a hypergraph stands for one subject with a description of its related stimuli, and the complex correlations among the vertices can be formulated through hyperedges. With the multi-hypergraph structure of the subjects, emotion recognition is translated into classification of vertices in the multi-hypergraph structure. Experimental results with the DEAP dataset and ASCERTAIN dataset demonstrate that the proposed method outperforms the current state-of-the-art methods. © 2019 Association for Computing Machinery.",Speech recognition; Emotion recognition; Physiological signals; Physiology; Multi-modal fusion; Biomedical signal processing; Graph theory; State-of-the-art methods; Emotional expressions; Complex correlation; High order correlation; Hypergraph; Multi-hypergraph neural networks,,,emotion,No,No
scopus,MELD: A multimodal multi-party dataset for emotion recognition in conversations,"Poria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria, E.; Mihalcea, R.",2020,,,,,"Emotion recognition in conversations (ERC) is a challenging task that has recently gained popularity due to its potential applications. Until now, however, there has been no large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue. To address this gap, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual, and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io. © 2019 Association for Computational Linguistics",Multi-modal; Speech recognition; Emotion recognition; Multi-modal information; Computational linguistics,,,emotion,Yes,No
scopus,"Multimodal emotion recognition using facial expressions, body gestures, speech, and text modalities","Huddar, M.G.; Sannakki, S.S.; Rajpurohit, V.S.",2019,,8,,,"Automatic emotion recognition from multimodal content has become an important and growing research field in human-computer interaction. Recent literature has used either audio or facial expression for emotion detection. However, emotion and body gestures are closely related to one another. This paper explores the effectiveness of using text, audio, facial expression and body gesture modalities of multimodal content and machine learning and deep learning based models for building more accurate and robust automatic multimodal emotion recognition systems. First, we get the best accuracy from the individual modalities. Then we use feature level fusion and ensemble based decision level fusion to combine multiple modalities to get better results. Proposed models were tested on IEMOCAP dataset and results show that proposed models with multiple modalities are more accurate compared to unimodal models in classifying emotions. © BEIESP.",Deep learning; Affective computing; LSTM; Multimodal emotion classification; IEMOCAP; Ensemble,,,emotion,No,Yes
scopus,A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition,"Jiang, Y.; Li, W.; Hossain, M.S.; Chen, M.; Alelaiwi, A.; Al-Hammadi, M.",2020,,53,,10.1016/j.inffus.2019.06.019,"With the rapid development of artificial intelligence and mobile Internet, the new requirements for human-computer interaction have been put forward. The personalized emotional interaction service is a new trend in the human-computer interaction field. As a basis of emotional interaction, emotion recognition has also introduced many new advances with the development of artificial intelligence. The current research on emotion recognition mostly focuses on single-modal recognition such as expression recognition, speech recognition, limb recognition, and physiological signal recognition. However, the lack of the single-modal emotional information and vulnerability to various external factors lead to lower accuracy of emotion recognition. Therefore, multimodal information fusion for data-driven emotion recognition has been attracting the attention of researchers in the affective computing filed. This paper reviews the development background and hot spots of the data-driven multimodal emotion information fusion. Considering the real-time mental health monitoring system, the current development of multimodal emotion data sets, the multimodal features extraction, including the EEG, speech, expression, text features, and multimodal fusion strategies and recognition methods are discussed and summarized in detail. The main objective of this work is to present a clear explanation of the scientific problems and future research directions in the multimodal information fusion for data-driven emotion recognition field. © 2019 Elsevier B.V.",Speech recognition; Emotion recognition; Physiological signals; Character recognition; Artificial intelligence; Human computer interaction; Information fusion; Emotional information; Multimodal features; Multimodal information fusion; Emotional interactions; Expression recognition; Future research directions; Data-driven emotion recognition,,,emotion,No,Yes
scopus,"M3ER: Multiplicative multimodal emotion recognition using facial, textual, and speech cues","Mittal, T.; Bhattacharya, U.; Chandra, R.; Bera, A.; Manocha, D.",2020,,,,,"We present M3ER, a learning-based method for emotion recognition from multiple input modalities. Our approach combines cues from multiple co-occurring modalities (such as face, text, and speech) and also is more robust than other methods to sensor noise in any of the individual modalities. M3ER models a novel, data-driven multiplicative fusion method to combine the modalities, which learn to emphasize the more reliable cues and suppress others on a per-sample basis. By introducing a check step which uses Canonical Correlational Analysis to differentiate between ineffective and effective modalities, M3ER is robust to sensor noise. M3ER also generates proxy features in place of the ineffectual modalities. We demonstrate the efficiency of our network through experimentation on two benchmark datasets, IEMOCAP and CMU-MOSEI. We report a mean accuracy of 82.7% on IEMOCAP and 89.0% on CMU-MOSEI, which, collectively, is an improvement of about 5% over prior work. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Speech recognition; Emotion recognition; Face recognition; Multimodal emotion recognition; Artificial intelligence; Multiple inputs; Benchmark datasets; Fusion methods; Correlational analysis; Learning-based methods; Sensor noise,,,emotion,No,Yes
scopus,Deep Learning for Multimodal Emotion Recognition-Attentive Residual Disconnected RNN,"Chandra, E.; Hsu, J.Y.-J.",2019,,,,10.1109/TAAI48200.2019.8959913,"Human communicates using verbal and non-verbal cues. One of the most essential elements that complements the understanding of communication is emotion. Emotion is expressed not only in words, but also facial expressions, body language, tone, etc. Therefore, we formulate the emotion recognition as a multimodal task.Emotions are usually described in a sequence along with the utterances. In recent years, RNN-based models have been known to be good at modeling the entire sequence and capturing long-term dependencies. However, it lacks the ability to extract local key patterns and position-invariant features. Hence, we adopt Deep Attentive Residual Disconnected RNN model which incorporates the concept from both RNN and CNN to enhance the ability to capture spatial and temporal features.We utilize CMU MOSEI dataset comprising of language, visual, and acoustic modalities for training and evaluating our model. The results show that Deep Attentive Residual Disconnected RNN model outperforms the baseline. Besides, the use of multimodal approach also solidifies the recognition better compared to those of single modalities. © 2019 IEEE.",Attention mechanisms; Emotion Recognition; Speech recognition; Emotion recognition; Facial Expressions; Deep learning; Multimodal emotion recognition; Visual languages; Multi-modal approach; Recurrent neural networks; Attention Mechanism; Invariant features; Long-term dependencies; Disconnected Recurrent Neural Network; Essential elements; Residual Network,,,emotion,No,No
scopus,MuSE: A multimodal dataset of stressed emotion,"Jaiswal, M.; Bara, C.-P.; Luo, Y.; Burzo, M.; Mihalcea, R.; Provost, E.M.",2020,,,,,"Endowing automated agents with the ability to provide support, entertainment and interaction with human beings requires sensing of the users' affective state. These affective states are impacted by a combination of emotion inducers, current psychological state, and various contextual factors. Although emotion classification in both singular and dyadic settings is an established area, the effects of these additional factors on the production and perception of emotion is understudied. This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings. The paper also presents several baselines to measure the performance of multimodal features for emotion and stress classification. © European Language Resources Association (ELRA), licensed under CC-BY-NC",Behavioral research; Emotion classification; Multi-modal dataset; Multimodal emotion; Multimodal features; Stress classifications; Contextual factors; Data collection protocols; Natural language; Psychological state; Spontaneus speech; Stressed emotion; Users' affective state,,,emotion,No,Yes
scopus,Attending to Emotional Narratives,"Wu, Z.; Zhang, X.; Zhi-Xuan, T.; Zaki, J.; Ong, D.C.",2019,,,,10.1109/ACII.2019.8925497,"Attention mechanisms in deep neural networks have achieved excellent performance on sequence-prediction tasks. Here, we show that these recently-proposed attention-based mechanisms-in particular, the Transformer with its parallelizable self-attention layers, and the Memory Fusion Network with attention across modalities and time-also generalize well to multimodal time-series emotion recognition. Using a recently-introduced dataset of emotional autobiographical narratives, we adapt and apply these two attention mechanisms to predict emotional valence over time. Our models perform extremely well, in some cases reaching a performance comparable with human raters. We end with a discussion of the implications of attention mechanisms to affective computing. © 2019 IEEE.",Attention; Attention mechanisms; Deep neural networks; Multi-modal; Speech recognition; Emotion recognition; Emotional valences; Deep learning; Deep Learning; Multimodal emotion recognition; Affective Computing; Multimodal Emotion Recognition; Time series; Sequence prediction; Intelligent computing; Time-series Emotion Recognition,,,emotion,No,Yes
scopus,DNN-based Emotion Recognition Based on Bottleneck Acoustic Features and Lexical Features,"Kim, E.; Shin, J.W.",2019,,2019-May,,10.1109/ICASSP.2019.8683077,"In this paper, we propose a novel emotion recognition method to reflect affect salient information using acoustic and lexical features. The acoustic features are extracted from the speech signal by applying statistical functionals of emotionally high-level features derived from Deep Neural Network (DNN). These acoustic features are early fused with two types of lexical features extracted from the text transcription of the speech signal, which are the distributed representation and affective lexicon-based dimensions. The fused features are fed to another DNN for utterance-level emotion classification. Experimental results on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) multimodal dataset showed 75.5% in unweighted accuracy recall, which outperformed the best results reported previously in the multimodal emotion recognition using acoustic and lexical features. © 2019 IEEE.",Deep neural networks; Speech recognition; Emotion recognition; Multimodal emotion recognition; Emotion classification; Multi-modal dataset; Audio signal processing; Speech communication; High-level features; Acoustic features; Lexical features; Acoustic feature; Distributed representation; DNN-based emotion recognition; Lexical feature,,,emotion,No,Yes
scopus,Multimodal Physiological Signal Emotion Recognition Based on Convolutional Recurrent Neural Network,"Liao, J.; Zhong, Q.; Zhu, Y.; Cai, D.",2020,,782,,10.1088/1757-899X/782/3/032005,"In order to solve the problem that the emotion recognition rate of single-mode physiological signals is not high in the physiological signals based emotion recognition, in this paper, we propose a convolutional recurrent neural network based method for multi-modal physiological signal emotion recognition task. The method used convolutional neural network to learn the spatial representations of multi-channel EEG signals and the Long Short-term Memory network to learn the temporal representations of peripheral physiological signals (EOG, EMG, GSR, RSP, BVP, and TMP). The two representations are combined for emotion recognition and classification. In the two emotion dimensions of Arousal and Valence, our experiments conducted on the open source dataset DEAP shows that, this method achieve 89.68% and 89.19% average accuracy in the EEG emotion classification, 63.06% and 62.41% average accuracy in the peripheral physiological signal emotion classification, 93.06% and 91.95% average accuracy in the combined feature emotion classification. The experimental results show that the convolutional recurrent neural network based method that we proposed efficiently extract multi-modal physiological signal feature to improve the emotion recognition performance. © Published under licence by IOP Publishing Ltd.",,,,emotion,No,Yes
scopus,Jointly fine-tuning “BERT-like” self supervised models to improve multimodal speech emotion recognition,"Siriwardhana, S.; Reis, A.; Weerasekera, R.; Nanayakkara, S.",2020,,2020-October,,10.21437/Interspeech.2020-1212,"Multimodal emotion recognition from the speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality specific“BERT-like” pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning “BERT-like” SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT. Copyright © 2020 ISCA",Speech emotion recognition; Transformer; Emotion Recognition; Multi-modal; Speech recognition; BERT; Deep learning; Character recognition; Multimodal emotion recognition; Affective Computing; Transformers; Supervised learning; Speech communication; Learning architectures; Architecture; Fine tuning; Multimodal deep learning; Self supervised learning,,,emotion,Yes,No
scopus,Multimodal Emotion Recognition Based on Ensemble Convolutional Neural Network,"Huang, H.; Hu, Z.; Wang, W.; Wu, M.",2020,,8,,10.1109/ACCESS.2019.2962085,"In recent years, emotional recognition based on Electrophysiological (EEG) signals has become more and more popular. But the researchers ignored the fact that peripheral physiological signals can also reflect changes in mood. We propose an Ensemble Convolutional Neural Network (ECNN) model, which is used to automatically mine the correlation between multi-channel EEG signals and peripheral physiological signals in order to improve the emotion recognition accuracy. First, we design five convolution networks and use global average pooling (GAP) layers instead of fully connected layers; and then the plurality voting strategy is adopted to establish the ensemble model; eventually this model divides emotions into four categories. Based on the simulations on DEAP dataset, the experimental results demonstrate the superiority of the ECNN compared with other methods. © 2013 IEEE.",Speech recognition; emotion recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Biomedical signal processing; Electrophysiology; Physiological models; Convolution; Convolutional neural network; Neural networks; Emotional recognition; physiological signal; Ensemble modeling; ECNN; plurality voting; Plurality voting,,,emotion,No,Yes
scopus,Human emotion recognition using deep belief network architecture,"Hassan, M.M.; Alam, M.G.R.; Uddin, M.Z.; Huda, S.; Almogren, A.; Fortino, G.",2019,,51,,10.1016/j.inffus.2018.10.009,"Recently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques. © 2018 Elsevier B.V.",Speech recognition; Emotion recognition; Physiological signals; Physiology; Deep learning; Classification (of information); Biomedical signal processing; Physiological models; Support vector machines; Fusion model; Network architecture; Electromyography; Extraction; Feature extraction; Man machine systems; Vectors; Image segmentation; Image retrieval; Deep belief networks; Radial basis function networks; Deep belief network; Fine Gaussian support vector machine; Gaussians,,,emotion,No,Yes
scopus,Classification of Five Emotions from EEG and Eye Movement Signals: Discrimination Ability and Stability over Time,"Li, T.-H.; Liu, W.; Zheng, W.-L.; Lu, B.-L.",2019,,2019-March,,10.1109/NER.2019.8716943,"This paper explores the discrimination ability and stability of electroencephalogram (EEG) and eye movement signals over time for classifying five emotions: happy, sad, fear, disgust and neutral. We develop a multimodal emotion dataset called SEED-V with 16 subjects. Two classifiers are trained based on the EEG and eye movement signals. Topographic maps are used to depict the neural patterns of EEG signal. The classification result based on EEG, eye movement, and feature level fusion (FLF) reaches the average accuracies of 70.8%, 59.87% and 75.13%, respectively. The experiment result indicates that: a) the EEG and eye movement signals have good discrimination ability for five emotion classification problem; b) the beta and gamma bands of EEG signal have better discrimination ability than the delta, theta and alpha bands; c) the stable neural patterns of different emotions do exist and are common across sessions; and d) the neural pattern of disgust emotion has high gamma response in the frontal area, while fear emotion has low activation at the top of brain in the gamma band. © 2019 IEEE.",Electroencephalography; Biomedical signal processing; Eye movements; Emotion classification; Classification results; Maps; Feature level fusion; Electro-encephalogram (EEG); Discrimination ability; Low activation; Neural patterns; Topographic map,,,emotion,No,Yes
scopus,Audio-visual emotion recognition using a hybrid deep convolutional neural network based on census transform,"Cornejo, J.Y.R.; Pedrini, H.",2019,,2019-October,,10.1109/SMC.2019.8914193,"Over the last years, recognition of emotions based on multimodal channels has received increasing attention from the scientific community. Many application fields can benefit from multimodal emotion recognition, such as human-computer interactions, educational software, behavior prediction, interpersonal relations. Speech and facial expressions are two natural and effective ways to express emotions in human-human interaction. In this work, we introduce a hybrid deep convolutional neural network to extract audio and visual features from videos. Initially, for extracting audio data, we transform the audio signal into an image representation as input to a 2D-Convolutional Neural Network (CNN). For extracting visual data, we introduce a Census-Transform (CT) based on CNN. Then, we fuse both audio and visual features, reducing them through Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). Finally, K-Nearest Neighbor (K-NN), Support Vector Machine (SVM), Logistic Regression (LR) and Gaussian Naïve Bayes (GNB) classifiers are employed for emotion recognition. Experimental results on RML, eNTERFACE05 and BAUM-1s datasets demonstrated that our model reached competitive recognition rates compared to other state-of-the-art approaches. © 2019 IEEE.",Deep neural networks; Speech recognition; Emotion recognition; Facial expression; Facial Expressions; Data mining; Human computer interaction; Speech; Convolution; Support vector machines; Neural networks; Audio; Principal component analysis; Surveys; Nearest neighbor search; Discriminant analysis; Application programs; Computerized tomography; Feature descriptors; Visual,,,emotion,No,No
scopus,Different Contextual Window Sizes Based RNNs for Multimodal Emotion Detection in Interactive Conversations,"Lai, H.; Chen, H.; Wu, S.",2020,,8,,10.1109/ACCESS.2020.3005664,"Multimodal emotion detection (MED) in interactive conversations is extremely important for improving the overall human-computer interaction experience. Present research methods in this domain do not explicitly distinguish the contexts of a test utterance in a meaningful way while classifying emotions in conversations. In this paper, we propose a model, named different contextual window sizes based recurrent neural networks (DCWS-RNNs), to differentiate the contexts. The model has four recurrent neural networks (RNNs) that use different contextual window sizes. These window sizes can represent the implicit weights of different aspects of contexts. Further, four RNNs are independently to model the different aspects of contexts into memories. Such memories with the test utterance are then merged using attention-based multiple hops. Experiments show DCWS-RNNs outperforms the compared methods on both the IEMOCAP and AVEC datasets. Case studies on the IEMOCAP dataset also demonstrate that our model has excellent performance to capture the emotional dependent utterance that is most relevant to the test utterance and assigned to the highest attention score.  © 2013 IEEE.",Multi-modal; Emotion detection; Human computer interaction; multimodal; Recurrent neural networks; Case-studies; Statistical tests; recurrent neural network; emotion detection; contextual window sizes; Interactive conversations; Multiple hops; Recurrent neural network (RNNs); Window Size,,,emotion,No,No
scopus,Context-dependent domain adversarial neural network for multimodal emotion recognition,"Lian, Z.; Tao, J.; Liu, B.; Huang, J.; Yang, Z.; Li, R.",2020,,2020-October,,10.21437/Interspeech.2020-1705,"Emotion recognition remains a complex task due to speaker variations and low-resource training samples. To address these difficulties, we focus on the domain adversarial neural networks (DANN) for emotion recognition. The primary task is to predict emotion labels. The secondary task is to learn a common representation where speaker identities can not be distinguished. By using this approach, we bring the representations of different speakers closer. Meanwhile, through using the unlabeled data in the training process, we alleviate the impact of low-resource training samples. In the meantime, prior work found that contextual information and multimodal features are important for emotion recognition. However, previous DANN based approaches ignore these information, thus limiting their performance. In this paper, we propose the context-dependent domain adversarial neural network for multimodal emotion recognition. To verify the effectiveness of our proposed method, we conduct experiments on the benchmark dataset IEMOCAP. Experimental results demonstrate that the proposed method shows an absolute improvement of 3.48% over state-of-the-art strategies. © 2020 International Speech Communication Association. All rights reserved.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multimodal features; Contextual information; Speech communication; Neural-networks; Adversarial learning; Context dependent; Domain adversarial learning; Sampling; Speaker independents; Speaker-independent representation; Speaker-independent representations,,,emotion,Yes,Yes
scopus,"MEISD: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations","Firdaus, M.; Chauhan, H.; Ekbal, A.; Bhattacharyya, P.",2020,,,,,"Emotion and sentiment classification in dialogues is a challenging task that has gained popularity in recent times. Humans tend to have multiple emotions with varying intensities while expressing their thoughts and feelings. Emotions in an utterance of dialogue can either be independent or dependent on the previous utterances, making the task complex and interesting. Multi-label emotion detection in conversations is a significant task that provides the ability to the system to understand the various emotions of the users interacting. On the other hand, sentiment analysis in dialogue or conversation helps in understanding the perspective of the user with respect to the ongoing conversation. Besides text, additional information in the form of audio and video assists in identifying the correct emotions with the appropriate intensity and sentiments in an utterance of a dialogue. Lately, quite a few datasets have been made available for emotion and sentiment classification in dialogues. Still, these datasets are imbalanced in representing different emotions and consist of only a single emotion. Hence, we present at first a large-scale balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.",Emotion Recognition; Multi-modal; Emotion recognition; Emotion detection; Sentiment analysis; Classification (of information); Sentiment classification; Large dataset; Emotion classification; Audio and video; Computational linguistics; Multi-labels; Large-scales; Emotion intensity,,,emotion,Yes,No
scopus,Hierarchical Multimodal Transformer with Localness and Speaker Aware Attention for Emotion Recognition in Conversations,"Jin, X.; Yu, J.; Ding, Z.; Xia, R.; Zhou, X.; Tu, Y.",2020,,12431 LNAI,,10.1007/978-3-030-60457-8_4,"Emotion Recognition in Conversations (ERC) aims to predict the emotion of each utterance in a given conversation. Existing approaches for the ERC task mainly suffer from two drawbacks: (1) failing to pay enough attention to the emotional impact of the local context; (2) ignoring the effect of the emotional inertia of speakers. To tackle these limitations, we first propose a Hierarchical Multimodal Transformer as our base model, followed by carefully designing a localness-aware attention mechanism and a speaker-aware attention mechanism to respectively capture the impact of the local context and the emotional inertia. Extensive evaluations on a benchmark dataset demonstrate the superiority of our proposed model over existing multimodal methods for ERC. © 2020, Springer Nature Switzerland AG.",Attention mechanisms; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Natural language processing systems; Benchmark datasets; Local contexts; Base models; Emotional inertia; Hierarchical multimodal transformer; Local context modeling,,,emotion,No,No
scopus,Privacy enhanced multimodal neural representations for emotion recognition,"Jaiswal, M.; Provost, E.M.",2020,,,,,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. To enable this, data are transmitted from users’ devices and stored on central servers. Yet, these data contain sensitive information that could be used by mobile applications without user’s consent or, maliciously, by an eavesdropping adversary. In this work, we show how multimodal representations trained for a primary task, here emotion recognition, can unintentionally leak demographic information, which could override a selected opt-out option by the user. We analyze how this leakage differs in representations obtained from textual, acoustic, and multimodal data. We use an adversarial learning paradigm to unlearn the private information present in a representation and investigate the effect of varying the strength of the adversarial component on the primary task and on the privacy metric, defined here as the inability of an attacker to predict specific demographic information. We evaluate this paradigm on multiple datasets and show that we can improve the privacy metric while not significantly impacting the performance on the primary task. To the best of our knowledge, this is the first work to analyze how the privacy metric differs across modalities and how multiple privacy concerns can be tackled while still maintaining performance on emotion recognition. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Speech recognition; Emotion recognition; Artificial intelligence; Neural representations; Population statistics; Adversarial learning; Conversational agents; Demographic information; Mobile agents; Mobile applications; Mobile computing; Private information; Sensitive informations,,,emotion,No,Yes
scopus,Muse-ing on the Impact of Utterance Ordering on Crowdsourced Emotion Annotations,"Jaiswal, M.; Aldeneh, Z.; Bara, C.-P.; Luo, Y.; Burzo, M.; Mihalcea, R.; Provost, E.M.",2019,,2019-May,,10.1109/ICASSP.2019.8682793,"Emotion recognition algorithms rely on data annotated with high quality labels. However, emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared correct. As a result, annotations are colored by the manner in which they were collected. In this paper, we conduct crowdsourcing experiments to investigate this impact on both the annotations themselves and on the performance of these algorithms. We focus on one critical question: the effect of context. We present a new emotion dataset, Multimodal Stressed Emotion (MuSE), and annotate the dataset using two conditions: randomized, in which annotators are presented with clips in random order, and contextualized, in which annotators are presented with clips in order. We find that contextual labeling schemes result in annotations that are more similar to a speaker's own self-reported labels and that labels generated from randomized schemes are most easily predictable by automated systems. © 2019 IEEE.",emotion; Behavioral research; Emotion recognition; Automation; Audio signal processing; Speech communication; Emotion expression; Classifier performance; annotation; Automated systems; classifier performance; Critical questions; crowdsourcing; Crowdsourcing; emotion perception; Labeling scheme,,,emotion,Yes,Yes
scopus,"Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection","Kanjo, E.; Younis, E.M.G.; Ang, C.S.",2019,,49,,10.1016/j.inffus.2018.09.001,"The detection and monitoring of emotions are important in various applications, e.g., to enable naturalistic and personalised human-robot interaction. Emotion detection often require modelling of various data inputs from multiple modalities, including physiological signals (e.g., EEG and GSR), environmental data (e.g., audio and weather), videos (e.g., for capturing facial expressions and gestures) and more recently motion and location data. Many traditional machine learning algorithms have been utilised to capture the diversity of multimodal data at the sensors and features levels for human emotion classification. While the feature engineering processes often embedded in these algorithms are beneficial for emotion modelling, they inherit some critical limitations which may hinder the development of reliable and accurate models. In this work, we adopt a deep learning approach for emotion classification through an iterative process by adding and removing large number of sensor signals from different modalities. Our dataset was collected in a real-world study from smart-phones and wearable devices. It merges local interaction of three sensor modalities: on-body, environmental and location into global model that represents signal dynamics along with the temporal relationships of each modality. Our approach employs a series of learning algorithms including a hybrid approach using Convolutional Neural Network and Long Short-term Memory Recurrent Neural Network (CNN-LSTM) on the raw sensor data, eliminating the needs for manual feature extraction and engineering. The results show that the adoption of deep-learning approaches is effective in human emotion classification when large number of sensors input is utilised (average accuracy 95% and F-Measure=%95) and the hybrid models outperform traditional fully connected deep neural network (average accuracy 73% and F-Measure=73%). Furthermore, the hybrid models outperform previously developed Ensemble algorithms that utilise feature engineering to train the model average accuracy 83% and F-Measure=82%) © 2018 The Authors",Long short-term memory; Deep neural networks; Emotion recognition; Physiological signals; Deep learning; Data mining; Human computer interaction; Human robot interaction; Classification (of information); Biomedical signal processing; Brain; Convolutional neural network; Learning algorithms; Emotion classification; Feature engineerings; Multiple modalities; Location; Iterative methods; Smartphones; Convoltutional neural network; Long short-term memory mobile sensing; Mobile sensing; Temporal relationships,,,emotion,No,Yes
scopus,Multimodal speech emotion recognition and classification using convolutional neural network techniques,"Christy, A.; Vaithyasubramanian, S.; Jesudoss, A.; Praveena, M.D.A.",2020,,23,,10.1007/s10772-020-09713-y,"Emotion recognition plays a vital role in dealing with day to day interpersonal human interactions. Understanding the feeling of a person from his speech can reveal wonders in shaping social interactions. A persons emotion can be identified with the tone and pitch of his voice. The acoustic speech signal are split into short frames, fast fourier transformation is applied, and relevant features are extracted using mel-frequency cepstrum coefficients (MFCC) and modulation spectral (MS). In this paper, algorithms like linear regression, decision tree, random forest, support vector machine (SVM) and convolutional neural networks (CNN) are used for classification and prediction once relevant features are selected from speech signals. Human emotions like neutral, calm, happy, sad, fearful, disgust and surprise are classified using decision tree, random forest, support vector machine (SVM) and convolutional neural networks (CNN). We have tested our model with RAVDEES dataset and CNN has shown 78.20% accuracy in recognizing emotions compared to decision tree, random forest and SVM. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Speech emotion recognition; Speech recognition; CNN; Emotion recognition; Convolutional neural networks; Convolution; Support vector machines; SVM; Accuracy; Feature extraction; Speech communication; Decision trees; Classification; Random forests; Social interactions; Recognizing emotions; Support vector regression; Fast Fourier transformations; Human interactions; Mel frequency cepstrum coefficients; Relevant features,,,emotion,No,Yes
scopus,Emotion recognition using multimodal residual LSTM network,"Ma, J.; Zheng, W.-L.; Tang, H.; Lu, B.-L.",2019,,,,10.1145/3343031.3350871,"Various studies have shown that the temporal information captured by conventional long-short-term memory (LSTM) networks is very useful for enhancing multimodal emotion recognition using encephalography (EEG) and other physiological signals. However, the dependency among multiple modalities and high-level temporal-feature learning using deeper LSTM networks is yet to be investigated. Thus, we propose a multimodal residual LSTM (MMResLSTM) network for emotion recognition. The MMResLSTM network shares the weights across the modalities in each LSTM layer to learn the correlation between the EEG and other physiological signals. It contains both the spatial shortcut paths provided by the residual network and temporal shortcut paths provided by LSTM for efficiently learning emotion-related high-level features. The proposed network was evaluated using a publicly available dataset for EEG-based emotion recognition, DEAP. The experimental results indicate that the proposed MMResLSTM network yielded a promising result, with a classification accuracy of 92.87% for arousal and 92.30% for valence. © 2019 Association for Computing Machinery.",Long short-term memory; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Electroencephalography; Biomedical signal processing; Electrophysiology; Brain; Multiple modalities; Classification accuracy; High-level features; Temporal information; Short term memory; Long-short-term memory network,,,emotion,No,Yes
scopus,A multimodal LSTM for predicting listener empathic responses over time,"Tan, Z.-X.; Goel, A.; Nguyen, T.-S.; Ong, D.C.",2019,,,,10.1109/FG.2019.8756577,"People naturally understand the emotions of-and often also empathize with-those around them. In this paper, we predict the emotional valence of an empathic listener over time as they listen to a speaker narrating a life story. We use the dataset provided by the OMG-Empathy Prediction Challenge, a workshop held in conjunction with IEEE FG 2019. We present a multimodal LSTM model with feature-level fusion and local attention that predicts empathic responses from audio, text, and visual features. Our best-performing model, which used only the audio and text features, achieved a concordance correlation coefficient (CCC) of. 29 and. 32 on the Validation set for the Generalized and Personalized track respectively, and achieved a CCC of .14 and .14 on the held-out Test set. We discuss the difficulties faced and the lessons learnt tackling this challenge. © 2019 IEEE.",Long short-term memory; Multi-modal; Emotional valences; Forecasting; Gesture recognition; Correlation coefficient; Text feature; Visual feature; Test sets; Feature level fusion; Artificial life,,,empathy,No,Yes
scopus,Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition,"Cimtay, Y.; Ekmekcioglu, E.",2020,,20,,10.3390/s20072034,"The electroencephalogram (EEG) has great attraction in emotion recognition studies due to its resistance to deceptive actions of humans. This is one of the most significant advantages of brain signals in comparison to visual or speech signals in the emotion recognition context. A major challenge in EEG-based emotion recognition is that EEG recordings exhibit varying distributions for different people as well as for the same person at different time instances. This nonstationary nature of EEG limits the accuracy of it when subject independency is the priority. The aim of this study is to increase the subject-independent recognition accuracy by exploiting pretrained state-of-the-art Convolutional Neural Network (CNN) architectures. Unlike similar studies that extract spectral band power features from the EEG readings, raw EEG data is used in our study after applying windowing, pre-adjustments and normalization. Removing manual feature extraction from the training system overcomes the risk of eliminating hidden features in the raw data and helps leverage the deep neural network’s power in uncovering unknown features. To improve the classification accuracy further, a median filter is used to eliminate the false detections along a prediction interval of emotions. This method yields a mean cross-subject accuracy of 86.56% and 78.34% on the Shanghai Jiao Tong University Emotion EEG Dataset (SEED) for two and three emotion classes, respectively. It also yields a mean cross-subject accuracy of 72.81% on the Database for Emotion Analysis using Physiological Signals (DEAP) and 81.8% on the Loughborough University Multimodal Emotion Dataset (LUMED) for two emotion classes. Furthermore, the recognition model that has been trained using the SEED dataset was tested with the DEAP dataset, which yields a mean prediction accuracy of 58.1% across all subjects and emotion classes. Results show that in terms of classification accuracy, the proposed approach is superior to, or on par with, the reference subject-independent EEG emotion recognition studies identified in literature and has limited complexity due to the elimination of the need for feature extraction. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Machine Learning; Humans; Emotions; emotion; Deep neural networks; Speech recognition; Emotion recognition; Physiological signals; human; Neural Networks, Computer; Data mining; machine learning; electroencephalography; Electroencephalography; Convolutional neural networks; Biomedical signal processing; EEG; Convolution; Convolutional neural network; Extraction; Feature extraction; Classification accuracy; Recognition accuracy; Median filters; Prediction accuracy; Electro-encephalogram (EEG); Dataset independency; Dense layer; Filtering on output; Loughborough University; Prediction interval; Pretrained models; Raw data; Subject independency",,,emotion,No,Yes
scopus,Cross-culture Multimodal Emotion Recognition with Adversarial Learning,"Liang, J.; Chen, S.; Zhao, J.; Jin, Q.; Liu, H.; Lu, L.",2019,,2019-May,,10.1109/ICASSP.2019.8683725,"With the development of globalization, automatic emotion recognition has faced a new challenge in the multi-culture scenario - to generalize across different cultures. Previous works mainly rely on multi-cultural datasets to address the cross-culture discrepancy, which are expensive to collect. In this paper, we propose an adversarial learning framework to alleviate the culture influence on multimodal emotion recognition. We treat the emotion recognition and culture recognition as two adversarial tasks. The emotion feature embedding is trained to improve the emotion recognition but to confuse the culture recognition, so that it is more emotion-salient and culture-invariant for cross-culture emotion recognition. Our approach is applicable to both mono-culture and multi-culture emotion datasets. Extensive experiments demonstrate that the proposed method significantly outperforms previous baselines in both cross-culture and multi-culture evaluations. © 2019 IEEE.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Multimodal Emotion Recognition; Audio signal processing; Speech communication; Automatic emotion recognition; Adversarial learning; Emotion feature; Cross culture; Adversarial Learning; Adversarial task; Cross-culture; Culture recognition,,,emotion,No,No
scopus,A Multimodal Music Emotion Classification Method Based on Multifeature Combined Network Classifier,"Chen, C.; Li, Q.",2020,,2020,,10.1155/2020/4606027,"Aiming at the shortcomings of single network classification model, this paper applies CNN-LSTM (convolutional neural networks-long short-term memory) combined network in the field of music emotion classification and proposes a multifeature combined network classifier based on CNN-LSTM which combines 2D (two-dimensional) feature input through CNN-LSTM and 1D (single-dimensional) feature input through DNN (deep neural networks) to make up for the deficiencies of original single feature models. The model uses multiple convolution kernels in CNN for 2D feature extraction, BiLSTM (bidirectional LSTM) for serialization processing and is used, respectively, for audio and lyrics single-modal emotion classification output. In the audio feature extraction, music audio is finely divided and the human voice is separated to obtain pure background sound clips; the spectrogram and LLDs (Low Level Descriptors) are extracted therefrom. In the lyrics feature extraction, the chi-squared test vector and word embedding extracted by Word2vec are, respectively, used as the feature representation of the lyrics. Combining the two types of heterogeneous features selected by audio and lyrics through the classification model can improve the classification performance. In order to fuse the emotional information of the two modals of music audio and lyrics, this paper proposes a multimodal ensemble learning method based on stacking, which is different from existing feature-level and decision-level fusion methods, the method avoids information loss caused by direct dimensionality reduction, and the original features are converted into label results for fusion, effectively solving the problem of feature heterogeneity. Experiments on million song dataset show that the audio classification accuracy of the multifeature combined network classifier in this paper reaches 68%, and the lyrics classification accuracy reaches 74%. The average classification accuracy of the multimodal reaches 78%, which is significantly improved compared with the single-modal. © 2020 Changfeng Chen and Qiang Li.",Long short-term memory; Deep neural networks; Learning systems; Classification (of information); Music emotion classifications; Convolutional neural networks; Convolution; Emotion classification; Emotional information; Audio acoustics; Extraction; Feature extraction; Classification accuracy; Feature representation; Audio feature extraction; Classification performance; Heterogeneous features; Bismuth compounds; Dimensionality reduction,,,emotion,Yes,Yes
scopus,"Deep, dimensional and multimodal emotion recognition using attention mechanisms","Lucas, J.; Ghaleb, E.; Asteriadis, S.",2020,,,,,"Emotion recognition is an increasingly important sub-field in artificial intelligence (AI). Advances in this field could drastically change the way people interact with computers and allow for automation of tasks that currently require a lot of manual work. For example, registering the emotion a subject expresses for a potential advert. Previous work has shown that using multiple modalities, although challenging, is very beneficial. Affective cues in audio and video may not occur simultaneously, and the modalities do not always contribute equally to emotion. This work seeks to apply attention mechanisms to aid in the fusion of audio and video, for the purpose of emotion recognition using state-of-the-art techniques from artificial intelligence and, more specifically, deep neural networks. To achieve this, two forms of attention are used. Embedding attention applies attention on the input of a modality-specific model, allowing recurrent networks to consider multiple input time steps. Bimodal attention fusion applies attention to fuse the output of modality-specific networks. Combining both these attention mechanisms yielded CCCs of 0.62 and 0.72 for arousal and valence respectively on the RECOLA dataset used in AVEC 2016. These results are competitive with the state-of-the-art, underlying the potential of attention mechanisms in multimodal fusion for behavioral signals. © 2020 University of Groningen. All rights reserved.",Attention mechanisms; Emotion Recognition; Deep neural networks; Embeddings; Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Multimodal; Recurrent neural networks; Multiple modalities; Audio and video; Neural-networks; Attention Mechanisms; State-of-the-art techniques; Neural Networks; Sub fields,,,emotion,No,No
scopus,Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition,"Ghaleb, E.; Popa, M.; Asteriadis, S.",2019,,,,10.1109/ACII.2019.8925444,"In Audio-Video Emotion Recognition (AVER), the idea is to have a human-level understanding of emotions from video clips. There is a need to bring these two modalities into a unified framework, to effectively learn multimodal fusion for AVER. In addition, literature studies lack in-depth analysis and utilization of how emotions vary as a function of time. Psychological and neurological studies show that negative and positive emotions are not recognized at the same speed. In this paper, we propose a novel multimodal temporal deep network framework that embeds video clips using their audio-visual content, onto a metric space, where their gap is reduced and their complementary and supplementary information is explored. We address two research questions, (1) how audio-visual cues contribute to emotion recognition and (2) how temporal information impacts the recognition rate and speed of emotions. The proposed method is evaluated on two datasets, CREMA-D and RAVDESS. The study findings are promising, achieving the state-of-the-artperformance on both datasets, and showing a significant impact of multimodal and temporal emotion perception. © 2019 IEEE.",Speech recognition; Behavioral research; Emotion recognition; Deep learning; Incremental learning; Video cameras; Metric learning; Temporal information; Intelligent computing; Audio-video emotion recognition; Audio-visual content; Deep metric learning; Literature studies; Multimodal and incremental learning; Supplementary information; Temporal perception,,,emotion,No,No
scopus,Multimodal attention network for continuous-time emotion recognition using video and EEG signals,"Choi, D.Y.; Kim, D.-H.; Song, B.C.",2020,,8,,10.1109/ACCESS.2020.3036877,"Emotion recognition is a very important technique for ultimate interactions between human beings and artificial intelligence systems. For effective emotion recognition in a continuous-time domain, this article presents a multimodal fusion network which integrates video modality and electroencephalogram (EEG) modality networks. To calculate the attention weights of facial video features and the corresponding EEG features in fusion, a multimodal attention network, that is utilizing bilinear pooling based on low-rank decomposition, is proposed. Finally, continuous domain valence values are computed by using two modality network outputs and attention weights. Experimental results show that the proposed fusion network provides an improved performance of about 6.9% over the video modality network for the MAHNOB human computer interface (MAHNOB-HCI) dataset. Also, we achieved the performance improvement even for our proprietary dataset. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Attention; Speech recognition; Emotion recognition; Multi-modal fusion; Artificial intelligence; Electroencephalography; Biomedical signal processing; EEG; Multimodal fusion; Multimodality; Video; Human computer interfaces; Artificial intelligence systems; Electro-encephalogram (EEG); Continuous domain; Continuous time systems; Continuous-time; Low-rank decomposition,,,emotion,No,Yes
scopus,Design of a multimodal interface based on psychophysiological sensing to identify emotion,"Roza, V.C.C.; Postolache, O.",2017,,2017-September,,,"This work proposes a design of a multimodal interface to classify or estimate emotion states. Thus, 7emotions are considered such as:anger, boredom, disgust, anxiety/fear, happiness, sadness and normal.A couple of sensing technologies such as: galvanic skin response (GSR), heart rate (HR), electrocardiography (ECG), oxygen saturation (SpO2) and electroencephalography (EEG)are used to collect psychophysiological signals in relation with emotion state estimation. The International Affective Picture System (IAPS) dataset is used to design the classifier system. Regarding the classification task, a comparison between artificial neural networks (ANN-MLP) and support vector machine (SVM) is presented. The tests were carried out for 20 healthy volunteers () of both genders with age from 23-50 years old. The proposed classifier presents accuracies of 85.71% when using ANN-MLP and 77.14% when using SVM. © 2018 IMEKO-International Measurement Federation Secretariat. All Rights Reserved.",Classification (of information); Electroencephalography; Electrophysiology; Support vector machines; Electrocardiography; Emotion classification; Galvanic skin response; Interface states; Neural networks; Classification tasks; Interactive computer systems; Signal analysis; Analog to digital conversion; Classifier systems; Electric variables measurement; Healthy volunteers; Multi-modal interfaces; Multimodal interface; Psychophysiological signals; Sensing technology,,,emotion,No,Yes
scopus,Multi-feature based emotion recognition for video clips,"Liu, C.; Tang, T.; Wang, M.; Lv, K.",2018,,,,10.1145/3242969.3264989,"In this paper, we present our latest progress in Emotion Recognition techniques, which combines acoustic features and facial features in both non-temporal and temporal mode. This paper presents the details of our techniques used in the Audio-Video Emotion Recognition subtask in the 2018 Emotion Recognition in the Wild (EmotiW) Challenge. After the multimodal results fusion, our final accuracy in Acted Facial Expression in Wild (AFEW) test dataset achieves 61.87%, which is 1.53% higher than the best results last year. Such improvements prove the effectiveness of our methods. © 2018 Association for Computing Machinery.",Long short-term memory; Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Deep Learning; LSTM; Statistical tests; Interactive computer systems; 3D Face Landmark; 3D faces; DenseNet; EmotiW 2018; Inception Net; SoundNet,,,emotion,No,Yes
scopus,Emotion recognition from speech: A survey,"Drakopoulos, G.; Pikramenos, G.; Spyrou, E.; Perantonis, S.J.",2019,,,,10.5220/0008495004320439,"Emotion recognition from speech signals is an important field in its own right as well as a mainstay of many multimodal sentiment analysis systems. The latter may as well include a broad spectrum of modalities which are strongly associated with consciously or subconsciously communicating human emotional state such as visual cues, gestures, body postures, gait, or facial expressions. Typically, emotion discovery from speech signals not only requires considerably less computational complexity than other modalities, but also at the same time in the overwhelming majority of studies the inclusion of speech modality increases the accuracy of the overall emotion estimation process. The principal algorithmic cornerstones of emotion estimation from speech signals are Hidden Markov Models, time series modeling, cepstrum processing, and deep learning methodologies, the latter two being prime examples of higher order data processing. Additionally, the most known datasets which serve as emotion recognition benchmarks are described. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Deep neural networks; Speech recognition; Emotion recognition; Affective Computing; Sentiment analysis; Speech; Affective computing; Data handling; Audio signal processing; Speech communication; Signal processing; Hidden Markov models; Time series analysis; Information use; Cepstrum; Hidden Markov chains; Higher order data; Higher-order; Information systems; MFCC coefficients,,,emotion,No,Yes
scopus,MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception,"Busso, C.; Parthasarathy, S.; Burmania, A.; Abdelwahab, M.; Sadoughi, N.; Provost, E.M.",2017,,8,,10.1109/TAFFC.2016.2515617,"We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database. © 2010-2012 IEEE.",emotion recognition; Behavioral research; Emotion recognition; Emotion elicitation; Classification (of information); Audio recordings; Audio-visual database; Dyadic interaction; audiovisual emotional dataset; Classification evaluation; Emotional database; emotional evaluation,,,emotion,No,Yes
scopus,Emotion inferring from large-scale internet voice data: A multimodal deep learning approach,"Zhou, S.; Jia, J.; Wang, Y.; Chen, W.; Meng, F.; Li, Y.; Tao, J.",2018,,,,10.1109/ACIIAsia.2018.8470311,"Voice Dialogue Applications(VDAs) increase popularity nowadays. As the same sentence expressed with different emotion may convey different meanings, inferring emotion from users' queries can help give a more humanized response for VDAs. However, the large-scale Internet voice data involving a tremendous amount of users, bring in a great diversity of users' dialects and expression preferences. Therefore, the traditional speech emotion recognition methods mainly targeting at acted corpora cannot handle the massive and diverse data effectively. In this paper, we propose a semi-supervised Emotion-oriented Bimodal Deep Autoencoder (EBDA) to infer emotion from large-scale Internet voice data. Specifically, as the previous research mainly focuses on acoustic features only, we utilize EBDA to fully integrate both acoustic and textual features. Meanwhile, to employ large-scale unlabeled data to enhance the classification performance, we adopt a semi-supervised strategy. The experimental results on 6 emotion categories based on a dataset collected from Sogou Voice Assistant1 containing 7.5 million utterances outperform several alternative baselines (+0.18% in terms of F1 on average). Finally, we show some interesting case studies to further demonstrate the practicability of our model. © 2018 IEEE.",Speech emotion recognition; Speech recognition; Emotion; Deep learning; Auto encoders; Learning approach; Acoustic features; Classification performance; Large scale Internet; Voice data; Intelligent computing; Bimodal Deep Autoencoder; Internet Voice Data,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition Using Deep Neural Networks,"Tang, H.; Liu, W.; Zheng, W.-L.; Lu, B.-L.",2017,,10637 LNCS,,10.1007/978-3-319-70093-9_86,"The change of emotions is a temporal dependent process. In this paper, a Bimodal-LSTM model is introduced to take temporal information into account for emotion recognition with multimodal signals. We extend the implementation of denoising autoencoders and adopt the Bimodal Deep Denoising AutoEncoder modal. Both models are evaluated on a public dataset, SEED, using EEG features and eye movement features as inputs. Our experimental results indicate that the Bimodal-LSTM model outperforms other state-of-the-art methods with a mean accuracy of 93.97%. The Bimodal-LSTM model is also examined on DEAP dataset with EEG and peripheral physiological signals, and it achieves the state-of-the-art results with a mean accuracy of 83.53%. © 2017, Springer International Publishing AG.",Deep neural networks; Speech recognition; Emotion recognition; Physiological signals; Multimodal emotion recognition; Learning systems; State of the art; Electroencephalography; Biomedical signal processing; EEG; Physiological models; Eye movements; LSTM; State-of-the-art methods; Temporal information; Dependent process,,,emotion,No,Yes
scopus,Multimodal Emotion Recognition in Multi-Cultural Conditions,"Chen, S.-Z.; Wang, S.; Jin, Q.",2018,,29,,10.13328/j.cnki.jos.005412,"Automatic emotion recognition is a challenging task with a wide range of applications. This paper addresses the problem of emotion recognition in multi-cultural conditions. Different multi-modal features are extracted from audio and visual modalities, and the emotion recognition performance is compared between hand-crafted features and automatically learned features from deep neural networks. Multimodal feature fusion is also explored to combine different modalities. The CHEAVD Chinese multimodal emotion dataset and AFEW English multimodal emotion dataset are utilized to evaluate the proposed methods. The importance of the culture factor for emotion recognition through cross-culture emotion recognition is demonstrated, and then three different strategies, including selecting corresponding emotion model for different cultures, jointly training with multi-cultural datasets, and embedding features from multi-cultural datasets into the same emotion space, are developed to improve the emotion recognition performance in the multi-cultural environment. The embedding strategy separates the culture influence from original features and can generate more discriminative emotion features, resulting in best performance for acoustic and multimodal emotion recognition. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep neural networks; Speech recognition; Emotion recognition; Facial Expressions; Multi-modal fusion; Multimodal fusion; Emotion feature; Acoustic emotion feature; Cultural conditions; Deepconvolutional neural networks; Facial expression feature; Multi-cultural condition,,,emotion,No,Yes
scopus,A preliminary structured database for Multimodal Measurements and Elicitations of EMOtions: M2E2MO,"Balconi, M.; Fronda, G.; De Filippis, D.; Polsinelli, M.; Placidi, G.",2019,,,,10.7358/neur-2019-026-bal1,"Recent studies show that emotions can be considered as different subjective processes. In this paper we describe M2E2<s/up>MO, a new multimodal database including signals from electroencephalogram (EEG), functional near-infrared spectroscopy (fNIRS) and autonomic data recorded during emotions elicitation by means of different stimulations: visual-stimuli (International Affective Picture System, IAPS), interactional scripts (IS, brief interactional sketches) and memory stimulation on autobiographic basis (autobiographic memories, AM). M2E2MO has been designed to host big-data from heterogeneous sources. Although there are many databases for measuring emotions, the present study used a new measurement dataset for the recording of neurophysiological emotions pathways when they are self-generated by recalling in mind past life experiences (AM) and when external stimulations elicit them (IAPS; IS). M2E2MO will be made publicly available to allow neuroscientists a better insight into brain mechanisms to study new signal processing and classification strategies of emotional signals to be used for studying human-computer interfaces. © 2019 Edizioni Universitarie di Lettere Economia Diritto.",Emotion recognition; Multimodality; Emotional memories; Facial expressions of emotions,,,emotion,No,No
scopus,Deep Mul timodal learning for emotion recognition in spoken language,"Gu, Y.; Chen, S.; Marsic, I.",2018,,2018-April,,10.1109/ICASSP.2018.8462440,"In this paper, we present a novel deep multimodal framework to predict human emotions based on sentence-level spoken language. Our architecture has two distinctive characteristics. First, it extracts the high-level features from both text and audio via a hybrid deep multimodal structure, which considers the spatial information from text, temporal information from audio, and high -level associations from low-level handcrafted features. Second, we fuse all features by using a three-layer deep neural network to learn the correlations across modalities and train the feature extraction and fusion modules together, allowing optimal global fine-tuning of the entire structure. We evaluated the proposed framework on the IEMOCAP dataset. Our result shows promising performance, achieving 60.4% in weighted accnrar-v for five emotion categories. © 2018 IEEE.",Emotion recognition; Deep multimodallearning; Spoken language,,,emotion,No,Yes
scopus,The OMG-Emotion Behavior Dataset,"Barros, P.; Churamani, N.; Lakomkin, E.; Siqueira, H.; Sutherland, A.; Wermter, S.",2018,,2018-July,,10.1109/IJCNN.2018.8489099,"This paper is the basis paper for the accepted IJCNN challenge One-Minute Gradual-Emotion Recognition (OMG-Emotion)1 by which we hope to foster long-emotion classification using neural models for the benefit of the IJCNN community. The proposed corpus has as novelty the data collection and annotation strategy based on emotion expressions which evolve over time into a specific context. Different from other corpora, we propose a novel multimodal corpus for emotion expression recognition, which uses gradual annotations with a focus on contextual emotion expressions. Our dataset was collected from Youtube videos using a specific search strategy based on restricted keywords and filtering which guaranteed that the data follow a gradual emotion expression transition, i.e. Emotion expressions evolve over time in a natural and continuous fashion. We also provide an experimental protocol and a series of unimodal baseline experiments which can be used to evaluate deep and recurrent neural models in a fair and standard manner.1https://www2.informatik.uni-hamburg.de/wtm/OMG-EmotionChallenge/ © 2018 IEEE.",Multi-modal; Emotion recognition; Emotion classification; Data collection; Neural networks; Emotion expression; Experimental protocols; Neural models; Search strategies,,,emotion,Yes,No
scopus,Incomplete Cholesky decomposition based kernel cross modal factor analysis for audiovisual continuous dimensional emotion recognition,"Li, X.; Lu, G.; Yan, J.; Li, H.; Zhang, Z.; Sun, N.; Xie, S.",2019,,13,,10.3837/TIIS.2019.02.018,"Recently, continuous dimensional emotion recognition from audiovisual clues has attracted increasing attention in both theory and in practice. The large amount of data involved in the recognition processing decreases the efficiency of most bimodal information fusion algorithms. A novel algorithm, namely the incomplete Cholesky decomposition based kernel cross factor analysis (ICDKCFA), is presented and employed for continuous dimensional audiovisual emotion recognition, in this paper. After the ICDKCFA feature transformation, two basic fusion strategies, namely feature-level fusion and decision-level fusion, are explored to combine the transformed visual and audio features for emotion recognition. Finally, extensive experiments are conducted to evaluate the ICDKCFA approach on the AVEC 2016 Multimodal Affect Recognition Sub-Challenge dataset. The experimental results show that the ICDKCFA method has a higher speed than the original kernel cross factor analysis with the comparable performance. Moreover, the ICDKCFA method achieves a better performance than other common information fusion methods, such as the Canonical correlation analysis, kernel canonical correlation analysis and cross-modal factor analysis based fusion methods. Copyright © 2019 KSII",Speech recognition; Modal analysis; Information fusion; Audiovisual emotion recognition; Multimodal information fusion; Correlation methods; Audiovisual; Decision level fusion; Kernel canonical correlation analysis; Multivariant analysis; Canonical correlation analysis; Continuous dimensional emotion recognition; Information fusion method; Factor analysis; Feature transformations; Incomplete Cholesky decomposition; Information fusion algorithm; Kernel cross-modal factor analysis,,,emotion,No,Yes
scopus,Learning alignment for multimodal emotion recognition from speech,"Xu, H.; Zhang, H.; Han, K.; Wang, Y.; Peng, Y.; Li, X.",2019,,2019-September,,10.21437/Interspeech.2019-3247,"Speech emotion recognition is a challenging problem because human convey emotions in subtle and complex ways. For emotion recognition on human speech, one can either extract emotion related features from audio signals or employ speech recognition techniques to generate text from speech and then apply natural language processing to analyze the sentiment. Further, emotion recognition will be beneficial from using audio-textual multimodal information, it is not trivial to build a system to learn from multimodality. One can build models for two input sources separately and combine them in a decision level, but this method ignores the interaction between speech and text in the temporal domain. In this paper, we propose to use an attention mechanism to learn the alignment between speech frames and text words, aiming to produce more accurate multimodal feature representations. The aligned multimodal features are fed into a sequential model for emotion recognition. We evaluate the approach on the IEMOCAP dataset and the experimental results show the proposed approach achieves the state-of-the-art performance on the dataset. Copyright © 2019 ISCA",Attention; Emotion Recognition; Multimodal; Alignment,,,emotion,No,Yes
scopus,Exploiting EEG Signals and Audiovisual Feature Fusion for Video Emotion Recognition,"Xing, B.; Zhang, H.; Zhang, K.; Zhang, L.; Wu, X.; Shi, X.; Yu, S.; Zhang, S.",2019,,7,,10.1109/ACCESS.2019.2914872,"External stimulation, mood swing, and physiological arousal are closely related and induced by each other. The exploration of internal relations between these three aspects is interesting and significant. Currently, video is the most popular multimedia stimuli that can express rich emotional semantics by its visual and auditory features. Apart from the video features, human electroencephalography (EEG) features can provide useful information for video emotion recognition, as they are the direct and instant authentic feedback on human perception with individuality. In this paper, we collected a total of 39 participants' EEG data induced by watching emotional video clips and built a fusion dataset of EEG and video features. Subsequently, the machine-learning algorithms, including Liblinear, REPTree, XGBoost, MultilayerPerceptron, RandomTree, and RBFNetwork were applied to obtain the optimal model for video emotion recognition based on a multi-modal dataset. We discovered that using the data fusion of all-band EEG power spectrum density features and video audio-visual features can achieve the best recognition results. The video emotion classification accuracy achieves 96.79% for valence (Positive/Negative) and 97.79% for arousal (High/Low). The study shows that this method can be a potential method of video emotion indexing for video information retrieval. © 2013 IEEE.",Semantics; Machine learning; Multi-modal; Speech recognition; Behavioral research; Affective Computing; multimodal; Electroencephalography; Biomedical signal processing; EEG; Electrophysiology; Affective computing; signal processing; Audio-visual features; Learning algorithms; Emotion classification; Data fusion; video; Signal processing; External stimulation; Power spectrum density; Video information retrieval,,,emotion,No,Yes
scopus,Emo react: A multimodal approach and dataset for recognizing emotional responses in children,"Nojavanasghari, B.; Baltrušaitis, T.; Hughes, C.E.; Morency, L.-P.",2016,,,,10.1145/2993148.2993168,"Automatic emotion recognition plays a central role in the technologies underlying social robots, affect-sensitive human computer interaction design and affect-Aware tutors. Although there has been a considerable amount of research on automatic emotion recognition in adults, emotion recognition in children has been understudied. This problem is more challenging as children tend to fidget and move around more than adults, leading to more self-occlusions and non-frontal head poses. Also, the lack of publicly available datasets for children with annotated emotion labels leads most researchers to focus on adults. In this paper, we introduce a newly collected multimodal emotion dataset of children between the ages of four and fourteen years old. The dataset contains 1102 audio-visual clips annotated for 17 different emotional states: six basic emotions, neutral, valence and nine complex emotions including curiosity, uncertainty and frustration. Our experiments compare unimodal and multimodal emotion recognition baseline models to enable future research on this topic. Finally, we present a detailed analysis of the most indicative behavioral cues for emotion recognition in children. © 2016 ACM.",Speech recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; Human computer interaction; Human robot interaction; Multi-modal approach; Audio-visual; Facial analysis; Automatic emotion recognition; Interactive computer systems; Nonverbal behavior; Nonverbal behavior analysis; Audio- visual sensing; Human computer interaction design; Machine design,,,emotion,Yes,No
scopus,'BioVid Emo DB': A multimodal database for emotion analyses validated by subjective ratings,"Zhang, L.; Walter, S.; Ma, X.; Werner, P.; Al-Hamadi, A.; Traue, H.C.; Gruss, S.",2017,,,,10.1109/SSCI.2016.7849931,"The precondition of productive data mining is having an efficient database to work on. The BioVid Emo DB is a multimodal database recorded for the purpose of analyzing human affective states and data mining related to emotion. Psychophysiological signals such as Skin Conductance Level, Electrocardiogram, Trapezius Electromyogram and also 4 video signals were recorded. 5 discrete emotions (amusement, sadness, anger, disgust and fear) were elicited by 15 standardized film clips. 94 participants watched them, rated them in terms of the experienced emotional level and selected the film clips that evoked the strongest emotion. A preliminary analysis of the subjective ratings made during the experiment is presented. The dataset is available for other researchers. © 2016 IEEE.",emotion recognition; Emotion recognition; Data mining; Artificial intelligence; Database systems; database; Skin conductance; Multimodal database; Psychophysiological signals; data mining; discrete emotions; Electromyo grams; film clips; Preliminary analysis; Subjective rating,,,emotion,No,No
scopus,ICoN: Interactive conversational memory network for multimodal emotion detection,"Hazarika, D.; Poria, S.; Mihalcea, R.; Cambria, E.; Zimmermann, R.",2018,,,,,"Emotion recognition in conversations is crucial for building empathetic machines. Current work in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets. © 2018 Association for Computational Linguistics",Speech recognition; Emotion recognition; Emotion detection; State of the art; Classification (of information); Multimodal features; Memory network; Natural language processing systems; Benchmark datasets; Emotional orientations; Multiple Classification,,,emotion,No,No
scopus,Feature selection and multimodal fusion for estimating emotions evoked by movie clips,"Timar, Y.; Karslioglu, N.; Kaya, H.; Salah, A.A.",2018,,,,10.1145/3206025.3206074,"Perceptual understanding of media content has many applications, including content-based retrieval, marketing, content optimization, psychological assessment, and affect-based learning. In this paper, we model audio visual features extracted from videos via machine learning approaches to estimate the affective responses of the viewers. We use the LIRIS-ACCEDE dataset and the MediaEval 2017 Challenge setting to evaluate the proposed methods. This dataset is composed of movies of professional or amateur origin, annotated with viewers' arousal, valence, and fear scores. We extract a number of audio features, such as Mel-frequency Cepstral Coefficients, and visual features, such as dense SIFT, hue-saturation histogram, and features from a deep neural network trained for object recognition. We contrast two different approaches in the paper, and report experiments with different fusion and smoothing strategies. We demonstrate the benefit of feature selection and multimodal fusion on estimating affective responses to movie segments. © 2018 ACM.",Deep neural networks; Speech recognition; Modal analysis; Affective Computing; Human computer interaction; Affective computing; Audio-visual features; Multimodal interaction; Feature extraction; Object recognition; Motion pictures; Face analysis; Multi-Modal Interactions; Content based retrieval; Emotion estimation; Movie analysis,,,emotion,Yes,No
scopus,Multimodal emotion recognition using the symmetric S-ELM-LUPI paradigm,"Yang, L.; Ban, X.; Mukeshimana, M.; Chen, Z.",2019,,11,,10.3390/sym11040487,"Multimodal emotion recognition has become one of the new research fields of humanmachine interaction. This paper focuses on feature extraction and data fusion in audio-visual emotion recognition, aiming at improving recognition effect and saving storage space. A semi-serial fusion symmetric method is proposed to fuse the audio and visual patterns of emotional recognition, and a method of Symmetric S-ELM-LUPI is adopted (Symmetric Sparse Extreme Learning Machine- Learning Using Privileged Information). The method inherits the generalized high speed of the Extreme Learning Machine, and combines this with the acceleration in the recognition process by the Learning Using Privileged Information and the memory saving of the Sparse Extreme Learning Machine. It is a learning method, which improves the traditional learning methods of examples and targets only. It introduces the role of a teacher in providing additional information to enhance the recognition (test) without complicating the learning process. The proposed method is tested on publicly available datasets and yields promising results. This method regards one pattern as the standard information source, while the other pattern as the privileged information source. Each mode can be treated as privileged information for another mode. The results show that this method is appropriate for multi-modal emotion recognition. For hundreds of samples, the execution time is less than one percent seconds. The sparsity of the proposed method has the advantage of storing memory economy. Compared with other machine learning methods, this method is more accurate and stable. © 2019 by the authors.",Multimodal emotion recognition; Human-machine interaction; Symmetric S-ELM-LUPI paradigm,,,emotion,No,No
scopus,EMOEEG: A new multimodal dataset for dynamic EEG-based emotion recognition with audiovisual elicitation,"Conneau, A.-C.; Hajlaoui, A.; Chetouani, M.; Essid, S.",2017,,2017-January,,10.23919/EUSIPCO.2017.8081305,"EMOEEG is a multimodal dataset where physiological responses to both visual and audiovisual stimuli were recorded, along with videos of the subjects, with a view to developing affective computing systems, especially automatic emotion recognition systems. The experimental setup involves various physiological sensors, among which electroencephalographic sensors. The experiment is performed with 8 participants, 4 from both genders. The stimuli include both sequences of static images from the IAPS dataset, and short video excerpts focusing on negative fear-type emotions. The annotation is obtained by participant self assessment, after a calibration phase. In the case of video stimuli, a novel simplified dynamic annotation strategy is used to enhance the quality and consistency of the self-assessments. This paper also analyses the annotation results and provides a statistical study of inter-annotator agreement. The dataset will continue to grow and will be made publicly available. © 2017 EURASIP.",Speech recognition; Physiology; Multi-modal data; Affective Computing; Human computer interaction; Electroencephalography; Electrophysiology; Physiological models; Affective computing; Multimodal data; Signal processing; Annotation; Arousal; Valence; Electroencephalography (EEG); Fear-type emotions; Inter-annotator agreement,,,emotion,Yes,Yes
scopus,Inferring emotion from conversational voice data: A semi-supervised multi-path generative neural network approach,"Zhou, S.; Jia, J.; Wang, Q.; Dong, Y.; Yin, Y.; Lei, K.",2018,,,,,"To give a more humanized response in Voice Dialogue Applications (VDAs), inferring emotion states from users' queries may play an important role. However, in VDAs, we have tremendous amount of VDA users and massive scale of unlabeled data with high dimension features from multimodal information, which challenge the traditional speech emotion recognition methods. In this paper, to better infer emotion from conversational voice data, we propose a semi-supervised multi-path generative neural network. Specifically, first, we build a novel supervised multi-path deep neural network framework. To avoid high dimensional input, raw features are trained by groups in local classifiers. Then high-level features of each local classifiers are concatenated as input of a global classifier. These two kinds classifiers are trained simultaneously through a single objective function to achieve a more effective and discriminative emotion inferring. To further solve the labeled-data-scarcity problem, we extend the multi-path deep neural network to a generative model based on semi-supervised variational autoencoder(semi-VAE), which is able to train the labeled and unlabeled data simultaneously. Experiment based on a 24,000 real-world dataset collected from Sogou Voice Assistant                             1                             (SVAD13) and a benchmark dataset IEMOCAP show that our method significantly outperforms the existing state-of-the-art results.                          Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Speech emotion recognition; Deep neural networks; Speech recognition; Multi-modal information; Artificial intelligence; Classification (of information); Generative model; Benchmark datasets; High-level features; Global classifiers; Labeled and unlabeled data; Local classifier,,,emotion,Yes,No
scopus,Emotion understanding using multimodal information based on autobiographical memories for Alzheimer’s patients,"Montenegro, J.M.F.; Gkelias, A.; Argyriou, V.",2017,,10116 LNCS,,10.1007/978-3-319-54407-6_17,"Alzheimer Disease (AD) early detection is considered of high importance for improving the quality of life of patients and their families. Amongst all the different approaches for AD detection, significant work has been focused on emotion analysis through facial expressions, body language or speech. Many studies also use the electroencephalogram in order to capture emotions that patients cannot physically express. Our work introduces an emotion recognition approach using facial expression and EEG signal analysis. A novel dataset was created specifically to remark the autobiographical memory deficits of AD patients. This work uses novel EEG features based on quaternions, facial landmarks and the combination of them. Their performance was evaluated in a comparative study with a state of the art methods that demonstrates the proposed approach. © Springer International Publishing AG 2017.",Speech recognition; Emotion recognition; Facial Expressions; Multi-modal information; Electroencephalography; Emotion understanding; Computer vision; State-of-the-art methods; Alzheimer disease; Autobiographical memory; Comparative studies,,,emotion,No,Yes
scopus,Audio-Textual Emotion Recognition Based on Improved Neural Networks,"Cai, L.; Hu, Y.; Dong, J.; Zhou, S.",2019,,2019,,10.1155/2019/2593036,"With the rapid development in social media, single-modal emotion recognition is hard to satisfy the demands of the current emotional recognition system. Aiming to optimize the performance of the emotional recognition system, a multimodal emotion recognition model from speech and text was proposed in this paper. Considering the complementarity between different modes, CNN (convolutional neural network) and LSTM (long short-term memory) were combined in a form of binary channels to learn acoustic emotion features; meanwhile, an effective Bi-LSTM (bidirectional long short-term memory) network was resorted to capture the textual features. Furthermore, we applied a deep neural network to learn and classify the fusion features. The final emotional state was determined by the output of both speech and text emotion analysis. Finally, the multimodal fusion experiments were carried out to validate the proposed model on the IEMOCAP database. In comparison with the single modal, the overall recognition accuracy of text increased 6.70%, and that of speech emotion recognition soared 13.85%. Experimental results show that the recognition accuracy of our multimodal is higher than that of the single modal and outperforms other published multimodal models on the test datasets. © 2019 Linqin Cai et al.",Long short-term memory; Speech emotion recognition; Deep neural networks; Speech recognition; Emotion recognition; Character recognition; Multi-modal fusion; Multimodal emotion recognition; Brain; Convolutional neural network; Emotional recognition; Recognition accuracy; Textual emotion recognition,,,emotion,No,Yes
scopus,A dyadic conversation dataset on moral emotions,"Heron, L.; Kim, J.; Lee, M.; El Haddad, K.; Dupont, S.; Dutoit, T.; Truong, K.",2018,,,,10.1109/FG.2018.00108,"In this paper, we present a dyadic conversation dataset involving topics related to moral emotions which are ethically relevant. To the best of our knowledge, it is the first dataset where the main focus is moral emotions. This dataset also focuses on speaker-listener reactions during a dyadic conversation. Although some of the currently available datasets contain dyadic conversations, they were not conceived with the idea of focusing on the speaker-listener setup. Thus making it difficult to use them to study reactions related to speakers and listeners. Some preliminary analyses of the data are presented as well as our thoughts on future work related to this dataset. © 2018 IEEE.",Multi-modal data; Affective Computing; Affective computing; Dataset; Multimodal data; Gesture recognition; Dyadic interaction; Moral emotions; Non-verbal expressions,,,emotion,No,No
scopus,Multimodal fusion of spatial-temporal features for emotion recognition in the wild,"Wang, Z.; Fang, Y.",2018,,10735 LNCS,,10.1007/978-3-319-77380-3_20,"Making the machine understand human emotion is a challenge to realize artificial intelligence. Considering the temporal correlation widely exists in the video, we present a multimodal fusion of spatial-temporal features system to recognize emotion. For the visual modality, the spatial-temporal features are extracted to represent the dynamic emotional variance along with the facial action in the video. The audio modality is utilized to assist the visual modality. A decision-level fusion approach is presented to make full use of the complementarity between visual modality and audio modality to boost the performance of the emotion recognition system. The experiments on a challenging dataset AFEW4.0 show that the proposed system achieves better generalization performance compared with other state-of-the-art methods. © Springer International Publishing AG, part of Springer Nature 2018.",Computer science; Speech recognition; Emotion recognition; Multi-modal fusion; Artificial intelligence; Multimodal fusion; Visual modalities; Generalization performance; State-of-the-art methods; Decision level fusion; Spatial-temporal features; Computers; Temporal correlations,,,emotion,No,Yes
scopus,Emotion sensing from physiological signals using three defined areas in arousal-valence model,"Wiem, M.B.H.; Lachiri, Z.",2017,,,,10.1109/CADIAG.2017.8075660,"This paper aims to recognize the human emotional states into three defined areas in arousal-valence evaluation: Corresponding to calm, medium aroused, and excited, unpleasant, neutral valence and pleasant. And thanks to the relevance of the peripheral physiological signals in emotion recognition issue, we used in our contribution the multimodal dataset MAHNOB-HCI. In this database, there are emotional bodily responses of twenty four participants after watching twenty affective stimuli videos. In our work, we focused on the ElectroCardioGram (ECG), Galvanic Skin Response (GSR), Skin Temperature (Temp) and Respiration Volume (RESP). To accomplish our purpose, we pre-process the data, extract 169 features and finally, classify the emotional states by using the support vector machine (SVM). As a first step, we classify each signal to know the most relevant physiological signal for emotion assessing, then a level feature fusion is applied to compare our approach to related work. According to previous studies, our obtained results are promising and show that the respiration and electrocardiogram are the most relevant. © 2017 IEEE.",Emotional state; Speech recognition; Emotion recognition; Physiological signals; Physiology; Biomedical signal processing; Electrophysiology; Physiological models; Feature fusion; Support vector machines; Electrocardiography; Multi-modal dataset; Skin temperatures; Galvanic skin response; Support Vector Machine; Related works; Arousal-Valence model; peripheral physiological signals,,,emotion,No,No
scopus,Towards semantic multimodal emotion recognition for enhancing assistive services in ubiquitous robotics,"Ayari, N.; Abdelkawy, H.; Chibani, A.; Amirat, Y.",2017,,FS-17-01 - FS-17-05,,,"In this paper, the problem of endowing ubiquitous robots with cognitive capabilities for recognizing emotions, sentiments, affects and moods of humans, in their context, is studied. A hybrid approach based on multilayer perceptron (MLP) neural network and n-ary ontologies for emotion-aware robotic systems is proposed. In particular, an algorithm based on the hybrid-level fusion, an expressive emotional knowledge representation and reasoning model are introduced to recognize complex and non-observable emotional context of the user. Empirical experiments on real-world dataset corroborate its effectiveness. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Robotics; Semantics; Multimodal emotion recognition; Human computer interaction; Human robot interaction; Intelligent robots; Recognizing emotions; Empirical experiments; Knowledge representation; Ubiquitous robotics; Cognitive capability; Knowledge representation and reasoning; Military applications; Multilayer perceptron neural networks; Public risks; Ubiquitous robots,,,emotion,No,No
scopus,Computational framework for emotional VAD prediction using regularized Extreme Learning Machine,"Guendil, Z.; Lachiri, Z.; Maaoui, C.",2017,,6,,10.1007/s13735-017-0128-9,"With the advancement of Human Computer interaction and affective computing, emotion estimation becomes a very interesting area of research. In literature, the majority of emotion recognition systems presents an insufficiency due to the complexity of processing a huge number of physiological data and analyzing various kind of emotions in one framework. The aim of this paper is to present a rigorous and effective computational framework for humans affect recognition and classification through arousal valence and dominance dimensions. In the proposed algorithm, physiological instances from the multimodal emotion DEAP dataset has been used for the analysis and characterization of emotional pattern. Physiological features were employed to predict VAD levels via Extreme Learning Machine. We adopted a feature-level fusion to exploit the complementary information of some physiological sensors in order to improve the classification performance. The proposed framework was also evaluated in a V–A quadrant by predicting four emotional classes. The obtained results proves the robustness and correctness of our proposed framework compared to other recent studies. We can also confirm the sufficiency of the R-ELM when it was applied for the estimation and recognition of emotional responses. © 2017, Springer-Verlag London.",Physiological data; DEAP; Extreme Learning Machine; VA quadrant; VAD space,,,emotion,No,Yes
scopus,A GAN-Based Data Augmentation Method for Multimodal Emotion Recognition,"Luo, Y.; Zhu, L.-Z.; Lu, B.-L.",2019,,11554 LNCS,,10.1007/978-3-030-22796-8_16,"The lack of training data is an obstacle to build satisfactory multimodal emotion recognition models. Generative adversarial network (GAN) has recently shown great successes in generating realistic-like data. In this paper, we propose a GAN-based data augmentation method for enhancing the performance of multimodal emotion recognition models. We adopt conditional Boundary Equilibrium GAN (cBEGAN) to generate artificial differential entropy features of electroencephalography signal, eye movement data and their direct concatenations. The main advantage of cBEGAN is that it can overcome the instability of conventional GAN and has very quick converge speed. We evaluate our proposed method on two multimodal emotion datasets. The experimental results demonstrate that our proposed method achieves 4.6% and 8.9% improvements of mean accuracies on classifying three and five emotions, respectively. © 2019, Springer Nature Switzerland AG.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Electroencephalography; EEG; Electrophysiology; Eye movement; Eye movements; Data augmentation; Eye movement datum; Training data; Adversarial networks; Boundary equilibrium; Differential entropy; Generative adversarial network,,,emotion,No,Yes
scopus,Perceived psychotherapist's empathy and therapy motivation as determinants of long-term therapy success-results of a cohort study of short term psychodynamic inpatient psychotherapy,"Vitinius, F.; Tieden, S.; Hellmich, M.; Pfaff, H.; Albus, C.; Ommen, O.",2018,,9,,10.3389/fpsyt.2018.00660,"Objective: Outcome predictors and determinants for treatment outcome of inpatient psychotherapy will be assessed in a follow-up-study. Sociodemographic factors and the level of depressiveness at admission, the perceived psychotherapist's empathy rated by patients and the therapy motivation as possible moderators of treatment outcome (reduction of depressive symptoms) are analyzed. Methods: In a cohort study, the outcome of inpatient multimodal psychotherapy was examined with Beck-Depression-Inventory (BDI) at admission (T1), discharge (T2) and at follow-up (1-3 years after treatment) (T3). Inclusion criteria were: Inpatient psychotherapy between 2007 and 2010 with a duration of at least 1 week and complete data set. The influence on therapy success of (1) sociodemographic factors, (2) the perceived psychotherapist's empathy rated by patients using the Consultation and Relational Empathy Measure (CARE), and (3) the therapy motivation of the patients rated by therapists are examined by means of correlation analysis, distribution comparisons and subsequently logistic regression. Results: Ninety-two (64 females, average age 39 yrs.) of 182 eligible patients participated in the follow-up survey. Duration of inpatient psychotherapy lasted 8.7 weeks ± 3.6 [min. 1, max. 33 weeks]. The perceived psychotherapist's empathy, therapy motivation, education level and depression at baseline had a significant impact on therapy success. Gender, age, and partnership were not significant. The length between discharge and follow-up had no influence on the results. Based on these variables a multiple logistic regression explained 42% of the variation (goodness-of-fit). Conclusion: Due to the shown relevance of the psychotherapist's empathy perceived by patients and the therapy motivation of patients for therapy success, both factors should be considered already at the beginning of the therapy. Consequently, they should be recognized in the context of postgraduate training and education. © 2018 Vitinius, Tieden, Hellmich, Pfaff, Albus and Ommen. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.",depression; Depression; empathy; adult; female; human; male; Article; cohort analysis; motivation; clinical effectiveness; age; gender; educational status; Follow-up; hospital patient; Inpatient psychotherapy; long term care; Outcome predictors; psychodynamic psychotherapy; psychotherapist attitude; short term psychotherapy; treatment duration; Treatment outcome,,,empathy,No,No
scopus,Stacked Auto-Encoder Optimized Emotion Recognition in Multimodal Wearable Biosensor Network,"Dai, Y.-X.; Wang, X.; Dai, P.; Zhang, W.-H.; Zhang, P.-B.",2017,,40,,10.11897/SP.J.1016.2017.01750,"Emotional health draws great concern with the enhancement of public health consciousness. Emotional health is closely related to the quality of personal life. Even for some special groups of people, like pilots, soldiers, etc., their emotional states will have impacts on the stability of communities. Traditionally, to evaluate emotional states of human beings relies on the doctors or psychologists to communicate with subjects and give scores based on various questionnaires. This approach is not scientific enough and leads to the difficulties in the emotional health monitoring in daily-life. Emotion recognition enables lifeless sensors and computers to measure and interpret human emotions. It is a procedure consisting of emotion-related bio signals recording, features extraction and emotional states classification, providing scientific evidence for emotional health monitoring and primary diagnosis of potential mental diseases. In the related works concerning emotion recognition, the application scenarios are usually restricted in the hospitals or labs and the common-used classifiers are not suitable for the daily emotion recognition data set. This paper develops a multimodal biosensor network to simplify the sensing framework so that it can finish emotion recognition tasks when subjects are participating in daily tasks without so many disturbances. Several wearable biosensor nodes record multimodal bio signals (electroencephalography, pulse and blood pressure) and transmit them to a body station employing Arduino UNO3 and its expansion boards. The body station works as a web client connecting to a web data center on the Internet by wireless routers or personal hotspots. The web data center is established on NI-PXI 1065 with a static public IP address. The recognition algorithm is carried out in the data center and the results are displayed for authorized web terminals with the assistance of web publishing service supported by LabVIEW. The multimodal wearable biosensor network can provide emotion-related bio signals from which typical features are extracted based on the existing theories. In particular, due to the uncertainties in signal acquisition and feature extraction, a stacked auto-encoder (based on the deep learning theory) optimized emotion recognition method is proposed to improve the recognition process. The stacked auto-encoder helps to pre-learn the feature vectors and with the fine tuning it generates a better scheme for emotion classification phase. There are 9 emotional states for classification according to the Valence-Arousal dimensional model. A two-layer stacked neural network with a softmax classifier is designed to finish the final classification tasks. The experiment convinces that the feature vectors pre-learned by stacked auto-encoder are of higher quality both in centrality and distinguishability based on the similarity evaluation theory. The final recognition rate is also improved approximately 5% compared to related works. The main contributions of this paper are the wearable network-based sensing structure, the stacked auto-encoder optimized multimodal emotion recognition algorithm and the quantitative analysis on 71-day experimental data. This is a novel system for daily emotional health monitoring and can provide scientific suggestions for doctors or guardians. However, in the future, large scale of data should be accumulated. Moreover, the dynamic performance and energy efficiency also need improving. © 2017, Science Press. All right reserved.",Speech recognition; Emotion recognition; Deep learning; Multimodal sensing; Multimodal emotion recognition; Learning systems; Signal encoding; Classification (of information); Auto encoders; Electroencephalography; Biomedical signal processing; Electrophysiology; Network coding; Emotion classification; Extraction; Wearable technology; Health; Diagnosis; Internet of things; Internet of Things; Signal processing; Wearable sensors; Quality control; Network layers; Surveys; Recognition algorithm; Blood pressure; Biosensors; Energy efficiency; Multi-modal sensing; Computer aided diagnosis; Emotional health monitoring; Microcomputers; Routers; Sensor networks; Stacked auto-encoder; Stacked neural networks; Wearable biosensor network,,,emotion,No,Yes
scopus,Multimodal stability-sensitive emotion recognition based on brainwave and physiological signals,"Thammasan, N.; Hagad, J.L.; Fukui, K.-I.; Numao, M.",2017,,2018-January,,10.1109/ACIIW.2017.8272584,"This paper presents a framework for adaptive multimodal emotion recognition based on signal stability as a context. To verify the efficacy of the method, experiments were conducted using a dataset of brainwave and physiological signals (EEG, ECG, GSR) captured from nine subjects listening to music. The proposed method uses a combination of signal-based features as well as accelerometer data to quantify the approximate reliability of each modality. In contrast to existing approaches, unstable modalities are not rejected outright, instead their relative contribution is dynamically adapted based on a corresponding stability index. In the case of EEG, the stability index was calculated using an artifact rejection technique, while for the ECG and GSR modalities it was calculated based on body movement detected through accelerometers. The experimental results show that temporally varying the relative contribution of each modality can improve emotion recognition performance. © 2017 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Physiology; Multimodal emotion recognition; Biomedical signal processing; Electrocardiograms; Accelerometers; Brain wave; Accelerometer data; Relative contribution; Feature data; Signal stability; Stability; Stability indices,,,emotion,No,Yes
scopus,Correlated Attention Networks for Multimodal Emotion Recognition,"Qiu, J.-L.; Li, X.-Y.; Hu, K.",2019,,,,10.1109/BIBM.2018.8621129,"Emotion is a subjective, conscious experience when people face different kinds of stimuli. In this paper, we propose a new model, Correlated Attention Network (CAN), to make multimodal emotion recognition. Correlated Attention Network is an extension of attention based recurrent neural network with correlation calculated of different gated recurrent units to take correlation of EEG and eye movement extracted signals into attention mechanism and takes advantage of coordinated representation with complementary features. In experiments on 3 real world datasets, we find that our model can significantly contribute to higher emotion classification accuracy when higher correlation is acquired. Our experiment results indicate that the Correlated Attention Network model performs better than the state-of-the-art methods with a mean accuracy of of 94.03% on SEED dataset, 87.71% on SEED IV dataset, 88.51% and 85.62% for four classification and two dichotomies on DEAP dataset, respectively. © 2018 IEEE.",Attention mechanisms; Speech recognition; Multimodal emotion recognition; Classification (of information); Electroencephalography; EEG; Eye movements; Recurrent neural networks; Emotion classification; Attention Mechanism; Bioinformatics; State-of-the-art methods; Complementary features; Real-world datasets; Canonical correlation analysis; Recurrent Neural Network; Deep Canonical Correlation Analysis; Network modeling,,,emotion,No,Yes
scopus,Canonical Correlation Analysis for Data Fusion in Multimodal Emotion Recognition,"Nemati, S.",2018,,,,10.1109/ISTEL.2018.8661140,"Multimodal emotion recognition systems aim at classifying emotion data, usually from different natures, into discrete affective categories. These systems fuse different modalities as each modality classifies the data from its own viewpoint and can compensate the limitations of others when combining with them. Existing approaches for multimodal data fusion either use feature-level or decision-level fusion. The former needs different modalities to be synchronized while the latter has not this limitation. In the current study, audio, visual, and users' comments are used as modalities for video emotion recognition. Although the first two modalities are synchronized, users' comments are not synchronized with them and this makes the use of pure feature-level data fusion impossible. In order to exploit the benefits of the feature-level approach, a hybrid method is proposed in the current study which first applies feature-level canonical correlation analysis (CCA) to audio and visual modalities and then combines the outputs with users' comments using a decision-level fusion. The results of applying this proposed method to DEAP dataset shows that using feature-level CCA not only outperforms the baseline feature-level method but also achieves a higher performance than the decision-level fusion. © 2018 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Modal analysis; Multimodal emotion recognition; 'current; Multimodal; Recognition systems; Data fusion; Audio-visual; Correlation methods; Multimodal data fusion; Canonical correlations analysis; Decision level fusion; CCA; Data Fusion; Feature level; Synchronization,,,emotion,No,Yes
scopus,Fusion based emotion recognition system,"Agrawal, A.; Mishra, N.K.",2017,,,,10.1109/CSCI.2016.0142,"The field of Emotion recognition (ER) is a part of human-computer interaction and this field has evolved very rapidly since the last decade. There are several works which have been done on emotion recognition using audio and video, however recently work is being done on fusion of the different modalities. The aim of this paper is to fuse the results of emotion detection obtained using audio and visual modalities to increase the amount of accuracy in emotion detection. Four emotions are addressed in this paper - angry, sad, happy and disgust. The face is extracted and the control points and muscle vectors of the face are calculated. Using this data, the features are extracted and then the SVM classifier is used to classify the emotion present in the video sample. For the audio module, different prosodic features like pitch, formant frequency, energy and MFCC are computed for feature extraction. The global feature vector is formed and then the SVM classifier for audio is used for the final result. Both the results are then fused using the F-score technique. This F-score value is used for the formation of the decision matrix which gives the fused emotion of the two modules. We checked the results of fusion on SAVEE dataset as well as on our own created dataset. We obtained correct results for emotion after fusion even in those cases where either video or audio mode failed to detect the correct emotion. The tool we have developed using the fusion approach is able to recognize emotions in real-time. © 2016 IEEE.",facial expression; Multi-modal; Speech recognition; emotion recognition; Emotion recognition; Facial Expressions; Artificial intelligence; Human computer interaction; multimodal; speech; Speech; Feature extraction; fusion; Fusion reactions; Decision matrices; F-score; Formant frequency; Global feature vectors; Prosodic features,,,emotion,No,Yes
scopus,Adversarial and cooperative correlated domain adaptation based multimodal emotion recognition,"Qiu, J.-L.; Chen, X.; Hu, K.",2019,,2328,,,"In this paper, we propose a new model, Adversarial and Cooperative Correlated Domain Adaptation (ACCDA), to make multimodal emotion recognition. Adversarial and Cooperative Correlated Domain Adaptation (ACCDA) is an extension and unity, which unifies adversarial discriminative domain adaptation and cooperative generative domain adaptation with deep canonical correlation analysis to train highly correlated domains of multiple physiological data (EEG and eye movement signals), which make use of their complentarity and relevance. In experiments on two real world datasets, we find that our model can significantly contribute to higher emotion classification accuracy when higher correlation is acquired. Our experiment results indicate that the Adversarial and Cooperative Correlated Domain Adaptation model performs better than the state-of-the-art methods with a mean accuracy of 88.64% for four emotion classification on SEED IV dataset. It also outperforms than the state-of-the-art results on DEAP dataset with a mean accuracy of 86.15% for two dichotomies. © 2019 CEUR-WS. All rights reserved.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Classification (of information); Electroencephalography; EEG; Eye movement; Eye movements; Domain adaptation; Emotion classification; State-of-the-art methods; Physiological data; Real-world datasets; Canonical correlation analysis,,,emotion,No,Yes
scopus,Audio-visual attention networks for emotion recognition,"Lee, J.; Kim, S.; Kim, S.; Sohn, K.",2018,,,,10.1145/3264869.3264873,"We present a spatiotemporal attention based multimodal deep neural networks for dimensional emotion recognition in multimodal audio-visual video sequence. To learn the temporal attention that discriminatively focuses on emotional sailient parts within speech audios, we formulate the temporal attention network using deep neural networks (DNNs). In addition, to learn the spatiotemporal attention that selectively focuses on emotional sailient parts within facial videos, the spatiotemporal encoder-decoder network is formulated using Convolutional LSTM (ConvLSTM) modules, and learned implicitly without any pixel-level annotations. By leveraging the spatiotemporal attention, the 3D convolutional neural networks (3D-CNNs) is also formulated to robustly recognize the dimensional emotion in facial videos. Furthermore, to exploit multimodal information, we fuse the audio and video features to emotion regression model. The experimental results show that our method can achieve the state-of-the-art results in dimensional emotion recognition with the highest concordance correlation coefficient (CCC) on AV+EC 2017 dataset. © 2018 Association for Computing Machinery.",Long short-term memory; Deep neural networks; Speech recognition; Behavioral research; Emotion recognition; Multi-modal information; Multimodal emotion recognition; State of the art; Convolution; Convolutional neural network; Recurrent neural networks; Regression analysis; Correlation coefficient; Recurrent Neural Network; Regression model; Convolutional Long Short-Term Memory; Spatiotemporal attention,,,emotion,No,No
scopus,Audio-Visual Emotion Recognition with Capsule-like Feature Representation and Model-Based Reinforcement Learning,"Ouyang, X.; Nagisetty, S.; Goh, E.G.H.; Shen, S.; Ding, W.; Ming, H.; Huang, D.-Y.",2018,,,,10.1109/ACIIAsia.2018.8470316,"This paper presents the techniques used in our contribution to Multimodal Emotion Recognition Challenge (MEC 2017). The purpose of the challenge is to classify the eight basic emotions (happy, sad, angry, worried, anxious, surprise, disgust and neutral) from Chinese Natural Audio-Visual Emotion Database (CHEAVD) 2.0 selected from Chinese movies and TV programs. As racial expressions are caused by the movement of racial features such as the mouth and eyebrows, a capsule like feature representation is proposed to captures not only the existences of static racial emotions in video frames but also the instantiation parameters. In order to further improve the performance of emotion classification accuracy, a model based reinforcement learning is proposed for audio-visual fusion method, which exploits feedbacks of submission on challenge testing dataset as rewards to learn the fusion model. The overall accuracy of proposed approach on test dataset is 52.3% and the macro average precision is 39.7%. The performance achieves the top 2 of the MEC2017 audio-visual sub challenge. © 2018 IEEE.",Emotion Recognition; Speech recognition; Emotion recognition; Multimodal emotion recognition; Classification (of information); Convolutional neural network; Reinforcement learning; Statistical tests; Neural networks; Feature representation; Audio-visual emotion recognition; Convolutional Neural Network; Model fusion; Short term memory; Intelligent computing; Long Short Term Memory Network; Model Fusion; Model-based reinforcement learning; Reinforcement Learning,,,emotion,No,Yes
scopus,Wearable biosensor network enabled multimodal daily-life emotion recognition employing reputation-driven imbalanced fuzzy classification,"Dai, Y.; Wang, X.; Zhang, P.; Zhang, W.",2017,,109,,10.1016/j.measurement.2017.06.006,"Daily-life emotion recognition is a new procedure developed from basic emotion recognition. It records and analyzes emotion-related bio-signals to evaluate emotional states of subjects when they are participating in daily tasks instead of receiving specific stimulations. This paper develops a wearable biosensor network to take a step further towards daily-life emotion recognition. Multimodal bio-signals (electroencephalography, pulse, skin temperature and blood pressure) are recorded by the sensor nodes and transmitted to the remote web data center through a body station to realize the web-enabled recognition scheme. In total, a 103-day emotion diary is kept from Jun 4th 2015 to Feb 28th 2016, discontinuously. The remarkably different appearing possibilities of 4 emotional states (horror, happiness, boredom and relaxation) and the noisy sensing environment create an imbalanced and noisy dataset. Thus, a reputation-driven imbalanced fuzzy support vector machine (RI-FSVM) classification method is proposed to reduce the adverse effects caused by both within-class noisy samples and between-class imbalance. The fuzzy membership function is determined by the reputation values (indicating the reliability of samples) and the class-imbalanced ratios. The experiment convinces that the wearable biosensor network works well and successfully extracts efficient features from multimodal bio-signals. These features are convinced to have better performance than the related work in both centrality and distinguishability. The proposed method improves the sensitivity, specificity and Gm of emotion classification compared with the typical classification methods. Eventually, our research achieves a competitive accuracy with a low-cost consumer-grade sensing system. The main contributions of this paper are the quantitative analysis on emotion diary and the imbalanced classification algorithm for daily-life emotion recognition. © 2017 Elsevier Ltd",Speech recognition; Emotion recognition; Electroencephalography; Biomedical signal processing; Electrophysiology; Support vector machines; Emotion classification; Wearable technology; Classification methods; Biosignals; Membership functions; Blood pressure; Biosensors; Wearable biosensor network; Daily-life emotion recognition; Fuzzy classification; Fuzzy membership function; Fuzzy support vector machines; Imbalanced classification; Multimodal bio-signals; Reputation-driven imbalanced fuzzy support vector machine (RI-FSVM); Sensor nodes,,,emotion,No,Yes
scopus,Poster abstract: Multimodal emotion recognition by extracting common and modality-specific information,"Zhang, W.; Gu, W.; Ma, F.; Ni, S.; Zhang, L.; Huang, S.-L.",2018,,,,10.1145/3274783.3275200,"Emotion recognition technologies have been widely used in numerous areas including advertising, healthcare and online education. Previous works usually recognize the emotion from either the acoustic or the visual signal, yielding unsatisfied performances and limited applications. To improve the inference capability, we present a multimodal emotion recognition model, EMOdal. Apart from learning the audio and visual data respectively, EMOdal efficiently learns the common and modality-specific information underlying the two kinds of signals, and therefore improves the inference ability. The model has been evaluated on our large-scale emotional data set. The comprehensive evaluations demonstrate that our model outperforms traditional approaches. © 2018 Association for Computing Machinery.",Multi-modal; Speech recognition; Emotion recognition; Multimodal emotion recognition; Learning systems; Multimodal machine learning; Visual signals; Distance education; Traditional approaches; Embedded systems; Comprehensive evaluation; On-line education; Specific information,,,emotion,No,No
scopus,Towards new mappings between emotion representation models,"Landowska, A.",2018,,8,,10.3390/app8020274,"There are several models for representing emotions in affect-aware applications, and available emotion recognition solutions provide results using diverse emotion models. As multimodal fusion is beneficial in terms of both accuracy and reliability of emotion recognition, one of the challenges is mapping between the models of affect representation. This paper addresses this issue by: proposing a procedure to elaborate new mappings, recommending a set of metrics for evaluation of the mapping accuracy, and delivering new mapping matrices for estimating the dimensions of a Pleasure-Arousal-Dominance model from Ekman's six basic emotions. The results are based on an analysis using three datasets that were constructed based on affect-annotated lexicons. The new mappings were obtained with linear regression learning methods. The proposed mappings showed better results on the datasets in comparison with the state-of-the-art matrix. The procedure, as well as the proposed metrics, might be used, not only in evaluation of the mappings between representation models, but also in comparison of emotion recognition and annotation results. Moreover, the datasets are published along with the paper and new mappings might be created and evaluated using the proposed methods. The study results might be interesting for both researchers and developers, who aim to extend their software solutions with affect recognition techniques. © 2018 by the authors.",Emotion recognition; Affective computing; Ekman's six basic emotions; Emotion mapping; Emotion representation models; Pleasure-Arousal-Dominance model,,,emotion,No,Yes
scopus,Electroencephalography based fusion two-dimensional (2D)-convolution neural networks (CNN) model for emotion recognition system,"Kwon, Y.-H.; Shin, S.-B.; Kim, S.-D.",2018,,18,,10.3390/s18051383,"The purpose of this study is to improve human emotional classification accuracy using a convolution neural networks (CNN) model and to suggest an overall method to classify emotion based on multimodal data. We improved classification performance by combining electroencephalogram (EEG) and galvanic skin response (GSR) signals. GSR signals are preprocessed using by the zero-crossing rate. Sufficient EEG feature extraction can be obtained through CNN. Therefore, we propose a suitable CNN model for feature extraction by tuning hyper parameters in convolution filters. The EEG signal is preprocessed prior to convolution by a wavelet transform while considering time and frequency simultaneously. We use a database for emotion analysis using the physiological signals open dataset to verify the proposed process, achieving 73.4% accuracy, showing significant performance improvement over the current best practice models. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.",Speech recognition; Emotion recognition; Deep learning; Classification (of information); Electroencephalography; Biomedical signal processing; EEG; Electrophysiology; Physiological models; Convolution; Extraction; Feature extraction; Galvanic skin response; GSR; Wavelet transforms; Pattern recognition; Convolution neural network; Emotional classification; Classification performance; Electro-encephalogram (EEG); Convolution neural networks; Hybrid neural network; Hybrid neural networks; Performance improvements,,,emotion,No,Yes
scopus,Multi-modal sequence fusion via recursive attention for emotion recognition,"Beard, R.; Das, R.; Ng, R.W.M.; Gopalakrishnan, P.G.K.; Eerens, L.; Swietojanski, P.; Miksik, O.",2018,,,,10.18653/v1/k18-1025,"Natural human communication is nuanced and inherently multi-modal. Humans possess specialised sensoria for processing vocal, visual, and linguistic, and para-linguistic information, but form an intricately fused percept of the multi-modal data stream to provide a holistic representation. Analysis of emotional content in face-to-face communication is a cognitive task to which humans are particularly attuned, given its sociological importance, and poses a difficult challenge for machine emulation due to the subtlety and expressive variability of cross-modal cues. Inspired by the empirical success of recent so-called End-To-End Memory Networks (Sukhbaatar et al., 2015), we propose an approach based on recursive multi-attention with a shared external memory updated over multiple gated iterations of analysis. We evaluate our model across several large multimodal datasets and show that global contextualised memory with gated memory update can effectively achieve emotion recognition. © 2018 Association for Computational Linguistics.",Speech recognition; Emotion recognition; Modal analysis; Multi-modal data; Linguistics; Large dataset; Human communications; Linguistic information; Multimodal datasets; Data streams; Face-to-face communications; Cognitive task; External memory,,,emotion,No,No
scopus,Multimodal fusion based on information gain for emotion recognition in the wild,"Ghaleb, E.; Popa, M.; Hortal, E.; Asteriadis, S.",2017,,2018-January,,10.1109/IntelliSys.2017.8324224,"In this paper we present a novel approach towards multi-modal emotion recognition on a challenging dataset AFEW'16, composed of video clips labeled with the six basic emotions plus the neutral state. After a preprocessing stage, we employ different feature extraction techniques (CNN, DSIFT on face and facial ROI, geometric and audio based) and encoded frame-based features using Fisher vector representations. Next, we leverage the properties of each modality using different fusion schemes. Apart from the early-level fusion and the decision level fusion approaches, we propose a hierarchical decision level method based on information gain principles and we optimize its parameters using genetic algorithms. The experimental results prove the suitability of our method, as we obtain 53.06% validation accuracy, surpassing by 14% the baseline of 38.81% on a challenging dataset, suitable for emotion recognition in the wild. © 2017 IEEE.",Emotion Recognition; Multi-modal; Speech recognition; Emotion recognition; Multi-modal fusion; multimodal fusion; Genetic algorithms; Feature extraction techniques; Basic emotions; Video-clips; Audio-based; Frame-based; genetic algorithm; information gain; Information gain; Neutral state,,,emotion,Yes,Yes
scopus,Multimodal Speech Emotion Recognition Using Audio and Text,"Yoon, S.; Byun, S.; Jung, K.",2018,,,,10.1109/SLT.2018.8639583,"Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%. © 2018 IEEE.",Natural language processing; Speech emotion recognition; Emotion Recognition; Multi-modal; Speech recognition; deep learning; Deep learning; Character recognition; natural language processing; Recurrent neural networks; Audio acoustics; Language processing; Natural languages; Speech data; Audio features; speech emotion recognition; Paralinguistic; Computational paralinguistic; computational paralinguistics,,,emotion,No,Yes
scopus,A wavelet-based approach to emotion classification using EDA signals,"Feng, H.; Golshan, H.M.; Mahoor, M.H.",2018,,112,,10.1016/j.eswa.2018.06.014,"Emotion is an intense mental experience often manifested by rapid heartbeat, breathing, sweating, and facial expressions. Emotion recognition from these physiological signals is a challenging problem with interesting applications such as developing wearable assistive devices and smart human-computer interfaces. This paper presents an automated method for emotion classification in children using electrodermal activity (EDA) signals. The time-frequency analysis of the acquired raw EDAs provides a feature space based on which different emotions can be recognized. To this end, the complex Morlet (C-Morlet) wavelet function is applied on the recorded EDA signals. The dataset used in this paper includes a set of multimodal recordings of social and communicative behavior as well as EDA recordings of 100 children younger than 30 months old. The dataset is annotated by two experts to extract the time sequence corresponding to three main emotions including “Joy” “Boredom” and “Acceptance”. The annotation process is performed considering the synchronicity between the children's facial expressions and the EDA time sequences. Various experiments are conducted on the annotated EDA signals to classify emotions using a support vector machine (SVM) classifier. The quantitative results show that the emotion classification performance remarkably improves compared to other methods when the proposed wavelet-based features are used. © 2018 Elsevier Ltd",Human computer interaction; Classification (of information); Biomedical signal processing; Support vector machines; Emotion classification; Wearable technology; Feature extraction; Electrodermal activity; Time-frequency analysis; Human computer interfaces; Wearable devices; Wavelet-based approach; Eletrodermal activity; Social and communicative behavior; Time frequency analysis; Wearable assistive devices; Wearable device,,,emotion,Yes,Yes
scopus,ISLA: Temporal segmentation and labeling for audio-visual emotion recognition,"Kim, Y.; Provost, E.M.",2019,,10,,10.1109/TAFFC.2017.2702653,"Emotion is an essential part of human interaction. Automatic emotion recognition can greatly benefit human-centered interactive technology, since extracted emotion can be used to understand and respond to user needs. However, real-world emotion recognition faces a central challenge when a user is speaking: facial movements due to speech are often confused with facial movements related to emotion. Recent studies have found that the use of phonetic information can reduce speech-related variability in the lower face region. However, methods to differentiate upper face movements due to emotion and due to speech have been underexplored. This gap leads us to the proposal of the Informed Segmentation and Labeling Approach (ISLA). ISLA uses speech signals that alter the dynamics of the lower and upper face regions. We demonstrate how pitch can be used to improve estimates of emotion from the upper face, and how this estimate can be combined with emotion estimates from the lower face and speech in a multimodal classification system. Our emotion classification results on the IEMOCAP and SAVEE datasets show that ISLA improves overall classification performance. We also demonstrate how emotion estimates from different modalities correlate with each other, providing insights into the differences between posed and spontaneous expressions. © 2010-2012 IEEE.",emotion; Multi-modal; Classification (of information); multimodal; recognition; speech; Speech; Audio-visual; Continuous speech recognition; face region; Face regions; temporal,,,emotion,No,Yes
scopus,Multimodal dimensional and continuous emotion recognition in dyadic video interactions,"Zhao, J.; Chen, S.; Jin, Q.",2018,,11164 LNCS,,10.1007/978-3-030-00776-8_28,"Automatic emotion recognition is a challenging task which can make great impact on improving natural human computer interactions. In dyadic human-human interactions, a more complex interaction scenario, a person’s emotion state will be influenced by the interlocutor’s behaviors, such as talking style/prosody, speech content, facial expression and body language. Mutual influence, a person’s influence on the interacting partner’s behaviors in a dialog, is shown to be important for predicting the person’s emotion state in previous works. In this paper, we proposed several multimodal interaction strategies to imitate the interactive patterns in the real scenarios for exploring the effect of mutual influence in continuous emotion prediction tasks. Our experiments based on the Audio/Visual Emotion Challenge (AVEC) 2017 dataset used in continuous emotion prediction tasks, and the results show that our proposed multimodal interaction strategy gains 3.82% and 3.26% absolute improvement on arousal and valence respectively. Additionally, we analyse the influence of the correlation between the interactive pairs on both arousal and valence. Our experimental results show that the interactive pairs with strong correlation significantly outperform the pairs with weak correlation on both arousal and valence. © Springer Nature Switzerland AG 2018.",Multi-modal; Speech recognition; Emotion recognition; Human computer interaction; Multimodal; Forecasting; Emotion predictions; Automatic emotion recognition; Interactive computer systems; Natural human computer interactions; Dyadic interaction; Multi-Modal Interactions; Human-human interactions,,,emotion,No,No
scopus,A Hybrid Latent Space Data Fusion Method for Multimodal Emotion Recognition,"Nemati, S.; Rohani, R.; Basiri, M.E.; Abdar, M.; Yen, N.Y.; Makarenkov, V.",2019,,7,,10.1109/ACCESS.2019.2955637,"Multimodal emotion recognition is an emerging interdisciplinary field of research in the area of affective computing and sentiment analysis. It aims at exploiting the information carried by signals of different nature to make emotion recognition systems more accurate. This is achieved by employing a powerful multimodal fusion method. In this study, a hybrid multimodal data fusion method is proposed in which the audio and visual modalities are fused using a latent space linear map and then, their projected features into the cross-modal space are fused with the textual modality using a Dempster-Shafer (DS) theory-based evidential fusion method. The evaluation of the proposed method on the videos of the DEAP dataset shows its superiority over both decision-level and non-latent space fusion methods. Furthermore, the results reveal that employing Marginal Fisher Analysis (MFA) for feature-level audio-visual fusion results in higher improvement in comparison to cross-modal factor analysis (CFA) and canonical correlation analysis (CCA). Also, the implementation results show that exploiting textual users' comments with the audiovisual content of movies improves the performance of the system. © 2013 IEEE.",Speech recognition; emotion recognition; Emotion recognition; Modal analysis; Multi-modal fusion; Multimodal emotion recognition; Affective Computing; Sentiment analysis; Affective computing; Computation theory; multimodal fusion; Data fusion; Canonical correlation analysis; Interdisciplinary fields; latent space model; Latent space models; Marginal fisher analysis,,,emotion,No,Yes
scopus,LSTM for dynamic emotion and group emotion recognition in the wild,"Sun, B.; Wei, Q.; Li, L.; Xu, Q.; He, J.; Yu, L.",2016,,,,10.1145/2993148.2997640,"In this paper, we describe our work in the fourth Emotion Recognition in the Wild (EmotiW 2016) Challenge. For video based emotion recognition sub-challenge, we extract acoustic features, LBPTOP, Dense SIFT and CNN-LSTM features to recognize the emotions of film characters. For group level emotion recognition sub-challenge, we use LSTM and GEM model. We train linear SVM classifiers for these kinds of features on the AFEW6.0 and HAPPEI dataset, and use a fusion network we proposed to combine all the extracted features at the decision level. The final achievements we have gained are 51.54% accuracy on the AFEW testing set and 0.836 RMSE on the HAPPEI testing set. © 2016 ACM.",Speech recognition; Emotion recognition; Classification (of information); Multimodal features; Group emotions; Group level; Interactive computer systems; Acoustic features; Decision levels; Group emotion; Linear SVM; Lstm,,,emotion,No,Yes
scopus,UMEME: University of Michigan emotional mcgurk effect data set,"Provost, E.M.; Shangguan, Y.; Busso, C.",2015,,6,,10.1109/TAFFC.2015.2407898,"Emotion is cto communication; it colors our interpretation of events and social interactions. Emotion expression is generally multimodal, modulating our facial movement, vocal behavior, and body gestures. The method through which this multimodal information is integrated and perceived is not well understood. This knowledge has implications for the design of multimodal classification algorithms, affective interfaces, and even mental health assessment. We present a novel data set designed to support research into the emotion perception process, the University of Michigan Emotional McGurk Effect Data set (UMEME). UMEME has a critical feature that differentiates it from currently existing data sets; it contains not only emotionally congruent stimuli (emotionally matched faces and voices), but also emotionally incongruent stimuli (emotionally mismatched faces and voices). The inclusion of emotionally complex and dynamic stimuli provides an opportunity to study how individuals make assessments of emotion content in the presence of emotional incongruence, or emotional noise. We describe the collection, annotation, and statistical properties of the data and present evidence illustrating how audio and video interact to result in specific types of emotion perception. The results demonstrate that there exist consistent patterns underlying emotion evaluation, even given incongruence, positioning UMEME as an important new tool for understanding emotion perception. © 2015 IEEE.",Affect; Multi-modal; Computer programming; Behavioral research; Multi-modal information; Unimodal; Multimodal; Classification algorithm; Emotion perception; McGurk effect; Statistical properties; University of Michigan,,,emotion,Yes,Yes
scopus,FILTWAM and voice emotion recognition,"Bahreini, K.; Nadolski, R.; Westera, W.",2014,,8605,,10.1007/978-3-319-12157-4_10,"This paper introduces the voice emotion recognition part of our framework for improving learning through webcams and microphones (FILTWAM). This framework enables multimodal emotion recognition of learners during game-based learning. The main goal of this study is to validate the use of microphone data for a real-time and adequate interpretation of vocal expressions into emotional states were the software is calibrated with end users. FILTWAM already incorporates a valid face emotion recognition module and is extended with a voice emotion recognition module. This extension aims to provide relevant and timely feedback based upon learner’s vocal intonations. The feedback is expected to enhance learner’s awareness of his or her own behavior. Six test persons received the same computer-based tasks in which they were requested to mimic specific vocal expressions. Each test person mimicked 82 emotions, which led to a dataset of 492 emotions. All sessions were recorded on video. An overall accuracy of our software based on the requested emotions and the recognized emotions is a pretty good 74.6% for the emotions happy and neutral emotions; but will be improved for the lower values of an extended set of emotions. In contrast with existing software our solution allows to continuously and unobtrusively monitor learners’ intonations and convert these intonations into emotional states. This paves the way for enhancing the quality and efficacy of game-based learning by including the learner’s emotional states, and links these to pedagogical scaffolding. © Springer International Publishing Switzerland 2014.",Emotion recognition; Multimodal emotion recognition; Human computer interaction; Human-computer interaction; Microphones; Game-based learning; Game-based Learning; Microphone; Real-time voice emotion recognition,,,emotion,No,Yes
scopus,Wellness contents recommendation based on human emotional and health status using em,"Jung, Y.; Yoon, Y.I.",2015,,2015-August,,10.1109/ICUFN.2015.7182692,"In smart healthcare age, there are a high demand on physical and mental wellness for improving the quality of life. Wellness generally means a healthy balance of the mind, body and spirit that results in an overall feeling of well-being. For wellness service, we monitor the physical and mental status of object for developing an inspection service middleware using multimodal biosensors. This multimodal sensor can measure EEG (electroencephalography), ECG (electrocardiography), respiration rate, SpO2 and skin temperature. From the measurements, we generated bio-emotional index. We also use ontology model which is enables reasoning, classification of datasets from sensor network and wellness contents recommendation. For this analysis we propose an Expectation Maximization (EM) based object health status prediction model for maximization of the risk rates of object status. © 2015 IEEE.",Health risks; Risk assessment; Classification (of information); Electroencephalography; Electrophysiology; Electrocardiography; Skin temperatures; Human emotion; Multimodal sensor; Ontology; Predictive analytics; Sensor networks; Contents recommendations; Expectation - maximizations; Expectation Maximization; Human emotion detection; Inspection services; Maximum principle; Middleware; Web ontology language; Web Ontology Language; Wellness contents recommendation,,,emotion,No,No
scopus,Emotion recognition from embedded bodily expressions and speech during dyadic interactions,"Muller, P.M.; Amin, S.; Verma, P.; Andriluka, M.; Bulling, A.",2015,,,,10.1109/ACII.2015.7344640,"Previous work on emotion recognition from bodily expressions focused on analysing such expressions in isolation, of individuals or in controlled settings, from a single camera view, or required intrusive motion tracking equipment. We study the problem of emotion recognition from bodily expressions and speech during dyadic (person-person) interactions in a real kitchen instrumented with ambient cameras and microphones. We specifically focus on bodily expressions that are embedded in regular interactions and background activities and recorded without human augmentation to increase naturalness of the expressions. We present a human-validated dataset that contains 224 high-resolution, multi-view video clips and audio recordings of emotionally charged interactions between eight couples of actors. The dataset is fully annotated with categorical labels for four basic emotions (anger, happiness, sadness, and surprise) and continuous labels for valence, activation, power, and anticipation provided by five annotators for each actor. We evaluate vision and audio-based emotion recognition using dense trajectories and a standard audio pipeline and provide insights into the importance of different body parts and audio features for emotion recognition. © 2015 IEEE.",Speech recognition; Visual languages; Affect recognition; Cameras; Body language; Emotional expressions; Audio recordings; body language; Dyadic interaction; Multimodal database; Intelligent computing; affect recognition in dyadic interactions; Motion analysis; multimodal database; recognition of emotional expressions; visual affect recognition,,,emotion,Yes,No
scopus,Improved multimodal emotion recognition for better game-based learning,"Bahreini, K.; Nadolski, R.; Westera, W.",2015,,9221,,10.1007/978-3-319-22960-7_11,"This paper introduces the integration of the face emotion recognition part and the voice emotion recognition part of our FILTWAM framework that uses webcams and microphones. This framework enables real-time multimodal emotion recognition of learners during game-based learning for triggering feedback towards improved learning. The main goal of this study is to validate the integration of webcam and microphone data for a real-time and adequate interpretation of facial and vocal expressions into emotional states where the software modules are calibrated with end users. This integration aims to improve timely and relevant feedback, which is expected to increase learners’ awareness of their own behavior. Twelve test persons received the same computer-based tasks in which they were requested to mimic specific facial and vocal expressions. Each test person mimicked 80 emotions, which led to a dataset of 960 emotions. All sessions were recorded on video. An overall accuracy of Kappa value based on the requested emotions, expert opinions, and the recognized emotions is 0.61, of the face emotion recognition software is 0.76, and of the voice emotion recognition software is 0.58. A multimodal fusion between the software modules can increase the accuracy to 78 %. In contrast with existing software our software modules allow real-time, continuously and unobtrusively monitoring of learners’ face expressions and voice intonations and convert these into emotional states. This inclusion of learner’s emotional states paves the way for more effective, efficient and enjoyable game-based learning. © Springer International Publishing Switzerland 2015.",Speech recognition; Face recognition; Multimodal emotion recognition; Learning systems; Affective Computing; Human computer interaction; Integration; Affective computing; Human-computer interaction; Computer games; Statistical tests; Real-time emotion recognition; Computer software; Microphones; Game-based learning; Game-based Learning; Microphone; Webcam,,,emotion,No,Yes
scopus,Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild,"Sun, B.; Li, L.; Wu, X.; Zuo, T.; Chen, Y.; Zhou, G.; He, J.; Zhu, X.",2016,,10,,10.1007/s12193-015-0203-6,"Emotion recognition in the wild is a very challenging task. In this paper, we investigate a variety of different multimodal features (acoustic and visual) from video clips to evaluate their discriminative abilities in human emotion analysis. For each clip, we extract MSDF BoW, LBP-TOP, PHOG, LPQ-TOP and Audio features. We train different classifiers for every type of feature on the AFEW dataset from the ICMI 2014 EmotiW Challenge, and we propose a novel hierarchical classification framework, which combines the feature-level and decision-level fusion strategy for all of the extracted multimodal features. The final achievement we gain on the AFEW test set is 47.17 %, which is considerably better than the best baseline recognition rate of 33.7 %. Among all of the teams participating in the ICMI 2014 EmotiW challenge, our recognition performance won the first runner-up award. Furthermore, we test our method on FERA and CK datasets, the experimental results also show good performance. © 2015, OpenInterface Association.",Speech recognition; Emotion recognition; Classification (of information); Multimodal features; Multiple kernel learning; Multiple Kernel Learning; Decision level fusion; Feature level fusion; Decision-level fusion; Feature-level fusion; Hierarchical classifier; Hierarchical classifiers,,,emotion,No,Yes
scopus,EmoNets: Multimodal deep learning approaches for emotion recognition in video,"Kahou, S.E.; Bouthillier, X.; Lamblin, P.; Gulcehre, C.; Michalski, V.; Konda, K.; Jean, S.; Froumenty, P.; Dauphin, Y.; Boulanger-Lewandowski, N.; Chandias Ferrari, R.; Mirza, M.; Warde-Farley, D.; Courville, A.; Vincent, P.; Memisevic, R.; Pal, C.; Bengio, Y.",2016,,10,,10.1007/s12193-015-0195-2,"The task of the Emotion Recognition in the Wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based “bag-of-mouths” model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67 % on the 2014 dataset. © 2015, OpenInterface Association.",Speech recognition; Emotion recognition; Deep learning; Learning systems; Convolutional neural network; Multiple modalities; Statistical tests; Multi-modal learning; Multimodal learning; Neural networks; Visual information; Focusing; Model combination; Realistic conditions,,,emotion,Yes,Yes
scopus,A new tool for gestural action recognition to support decisions in emotional framework,"Bevilacqua, V.; Barone, D.; Cipriani, F.; D'Onghia, G.; Mastrandrea, G.; Mastronardi, G.; Suma, M.; D'Ambruoso, D.",2014,,,,10.1109/INISTA.2014.6873616,"Introduction and objective: the purpose of this work is to design and implement an innovative tool to recognize 16 different human gestural actions and use them to predict 7 different emotional states. The solution proposed in this paper is based on RGB and depth information of 2D/3D images acquired from a commercial RGB-D sensor called Kinect. Materials: the dataset is a collection of several human actions made by different actors. Each action is performed by each actor for three times in each video. 20 actors perform 16 different actions, both seated and upright, totalling 40 videos per actor. Methods: human gestural actions are recognized by means feature extractions as angles and distances related to joints of human skeleton from RGB and depth images. Emotions are selected according to the state-of-the-art. Experimental results: despite truly similar actions, the overall-accuracy reached is approximately 80%. Conclusions and future works: the proposed work seems to be back-ground- and speed-independent, and it will be used in the future as part of a multimodal emotion recognition software based on facial expressions and speech analysis as well. © 2014 IEEE.",emotion recognition; Emotion recognition; Facial Expressions; Multimodal emotion recognition; Intelligent systems; Gesture recognition; Action recognition; Image recognition; Motion estimation; Depth sensors; depth sensor; Design and implements; gesture recognition; kinect; Rgb-d cameras; RGB-D cameras,,,emotion,No,Yes
scopus,Multimodal emotion recognition for AVEC 2016 challenge,"Povolný, F.; Matějka, P.; Hradiš, M.; Popková, A.; Otrusina, L.; Smrž, P.",2016,,,,10.1145/2988257.2988268,"This paper describes a systems for emotion recognition and its application on the dataset from the AV+EC 2016 Emotion Recognition Challenge. The realized system was produced and submitted to the AV+EC 2016 evaluation, making use of all three modalities (audio, video, and physiological data). Our work primarily focused on features derived from audio. The original audio features were complement with bottleneck features and also text-based emotion recognition which is based on transcribing audio by an automatic speech recognition system and applying resources such as word embedding models and sentiment lexicons. Our multimodal fusion reached CCC=0.855 on dev set for arousal and 0.713 for valence. CCC on test set is 0.719 and 0.596 for arousal and valence respectively. © 2016 ACM.",Speech recognition; Emotion recognition; Character recognition; Physiological models; Neural networks; Arousal; Valence; Speech transmission; Transcription; Bottleneck features; Regression; Speech transcription word embedding; Speech transcriptions,,,emotion,No,No
scopus,Feature and decision level audio-visual data fusion in emotion recognition problem,"Sidorov, M.; Sopov, E.; Ivanov, I.; Minker, W.",2015,,2,,10.5220/0005527002460251,"The speech-based emotion recognition problem has already been investigated by many authors, and reasonable results have been achieved. This article focuses on applying audio-visual data fusion approach to emotion recognition. Two state-of-the-art classification algorithms were applied to one audio and three visual feature datasets. Feature level data fusion was applied to build a multimodal emotion classification system, which helped increase emotion classification accuracy by 4% compared to the best accuracy achieved by unimodal systems. The class precisions achieved by applying algorithms on unimodal and multimodal datasets helped to reveal that different data-classifier combinations are good at recognizing certain emotions. These data-classifier combinations were fused on the decision level using several approaches, which still helped increase the accuracy by 3% compared to the best accuracy achieved by feature level fusion.",Robotics; Speech recognition; Emotion recognition; Human computer interaction; Classification (of information); Speech; Emotion classification; Data fusion; Neural networks; Classification algorithm; Emotion classification systems; Vision; Neural network; Decision level fusion; Feature level fusion; Agricultural robots; Human computer interaction (HCI); Audio-visual data fusion; Human-computer interaction (HCI); PCA,,,emotion,No,Yes
scopus,Comparative analysis of physiological signals and electroencephalogram (EEG) for multimodal emotion recognition using generative models,"Torres-Valencia, C.A.; Garcia-Arias, H.F.; Lopez, M.A.A.; Orozco-Gutierrez, A.A.",2015,,,,10.1109/STSIVA.2014.7010181,"Multimodal Emotion recognition (MER) is an application of machine learning were different biological signals are used in order to automatically classify a determined affective state. MER systems has been developed for different type of applications from psychological evaluation, anxiety assessment, human-machine interfaces and marketing. There are several spaces of classification proposed in the state of art for the emotion recognition task, the most known are discrete and dimensional spaces were the emotions are described in terms of some basic emotions and latent dimensions respectively. The use of dimensional spaces of classification allows a higher range of emotional states to be analyzed. The most common dimensional space used for this purpose is the Arousal/Valence space were emotions are described in terms of the intensity of the emotion that goes from inactive to active in the arousal dimension, and from unpleasant to pleasant in the valence dimension. The use of physiological signals and the EEG is well suited for emotion recognition due to the fact that an emotional states generates responses from different biological systems of the human body. Since the expression of an emotion is a dynamic process, we propose the use of generative models as Hidden Markov Models (HMM) to capture de dynamics of the signals for further classification of emotional states in terms of arousal and valence. For the development of this work an international database for emotion classification known as Dataset for Emotion Analysis using Physiological signals (DEAP) is used. The objective of this work is to determine which of the physiological and EEG signals brings more relevant information in the emotion recognition task, several experiments using HMMs from different signals and combinations of them are performed, and the results shows that some of those signals brings more discrimination between arousal and valence levels as the EEG and the Galvanic Skin Response (GSR) and the Heart rate (HR). © 2014 IEEE.",Speech recognition; Physiological signals; Multimodal emotion recognition; Classification (of information); Electroencephalography; Biomedical signal processing; Electrophysiology; Physiological models; Emotion classification; Galvanic skin response; Hidden Markov models; Human Machine Interface; Signal analysis; Electro-encephalogram (EEG); International database; Psychological evaluation,,,emotion,No,Yes
scopus,Combining multimodal features with hierarchical classifier fusion for emotion recognition in the wild,"Sun, B.; Li, L.; Zuo, T.; Chen, Y.; Zhou, G.; Wu, X.",2014,,,,10.1145/2663204.2666272,"Emotion recognition in the wild is a very challenging task. In this paper, we investigate a variety of different multimodal features from video and audio to evaluate their discriminative ability to human emotion analysis. For each clip, we extract SIFT, LBP-TOP, PHOG, LPQ-TOP and audio features. We train different classifiers for every kind of features on the dataset from EmotiW 2014 Challenge, and we propose a novel hierarchical classifier fusion method for all the extracted features. The final achievement we gained on the test set is 47.17% which is much better than the best baseline recognition rate of 33.7%. Copyright 2014 ACM.",Multi-modal; Speech recognition; Emotion recognition; Classification (of information); Multimodal; Feature fusion; Support vector machines; Multimodal features; Human emotion; Support vector machine; Audio features; Interactive computer systems; Hierarchical classifier; Hierarchical classifiers; Discriminative ability,,,emotion,No,No
scopus,Multimodal emotional state recognition using sequence-dependent deep hierarchical features,"Barros, P.; Jirak, D.; Weber, C.; Wermter, S.",2015,,72,,10.1016/j.neunet.2015.09.009,"Emotional state recognition has become an important topic for human-robot interaction in the past years. By determining emotion expressions, robots can identify important variables of human behavior and use these to communicate in a more human-like fashion and thereby extend the interaction possibilities. Human emotions are multimodal and spontaneous, which makes them hard to be recognized by robots. Each modality has its own restrictions and constraints which, together with the non-structured behavior of spontaneous expressions, create several difficulties for the approaches present in the literature, which are based on several explicit feature extraction techniques and manual modality fusion. Our model uses a hierarchical feature representation to deal with spontaneous emotions, and learns how to integrate multiple modalities for non-verbal emotion recognition, making it suitable to be used in an HRI scenario. Our experiments show that a significant improvement of recognition accuracy is achieved when we use hierarchical features and multimodal information, and our model improves the accuracy of state-of-theart approaches from 82.5% reported in the literature to 91.3% for a benchmark dataset on spontaneous emotion expressions. © 2015 The Authors.",Humans; Robots; Robotics; Emotions; emotion; facial expression; robotics; Speech recognition; Behavioral research; Emotion recognition; Multi-modal information; Deep learning; human; Human robot interaction; Article; controlled study; learning; Learning; recognition; Convolutional neural network; emotionality; Feature extraction; Neural networks; Man machine systems; task performance; State estimation; Feature extraction techniques; Recognition accuracy; body movement; Convolutional Neural Networks; visual stimulation; priority journal; Human Robot Interaction; State-of-the-art approach; Hierarchical features; stimulus response; mental task; Recognition (Psychology),,,emotion,No,Yes
scopus,Multimodal learning with deep Boltzmann Machine for emotion prediction in user generated videos,"Pang, L.; Ngo, C.-W.",2015,,,,10.1145/2671188.2749400,"Detecting emotions from user-generated videos, such as ""anger"" and ""sadness"", has attracted widespread interest recently. The problem is challenging as effectively representing video data with multi-view information (e.g., audio, video or text) is not trivial. In contrast to the existing works that extract features from each modality (view) separately followed by early or late fusion, we propose to learn a joint density model over the space of multi-modal inputs (including visual, auditory and textual modalities) with Deep Boltzmann Machine (DBM). The model is trained directly on the user-generated Web videos without any labeling effort. More importantly, the deep architecture enlightens the possibility of discovering the highly non-linear relationships that exist between lowlevel features across different modalities. The experiment results show that the DBM model learns joint representation complementary to the hand-crafted visual and auditory features, leading to 7.7% performance improvement in classification accuracy on the recently released VideoEmotion dataset. Copyright © 2015 ACM.",Emotion analysis; Multimodal learning; Deep Boltzmann Machine,,,emotion,Yes,Yes
scopus,Detection of emotions in Parkinson's disease using higher order spectral features from brain's electrical activity,"Yuvaraj, R.; Murugappan, M.; Mohamed Ibrahim, N.; Sundaraj, K.; Omar, M.I.; Mohamad, K.; Palaniappan, R.",2014,,14,,10.1016/j.bspc.2014.07.005,"Objective Non-motor symptoms in Parkinson's disease (PD) involving cognition and emotion have been progressively receiving more attention in recent times. Electroencephalogram (EEG) signals, being an activity of central nervous system, can reflect the underlying true emotional state of a person. This paper presents a computational framework for classifying PD patients compared to healthy controls (HC) using emotional information from the brain's electrical activity. Approach Emotional EEG data were obtained from 20 PD patients and 20 healthy age-, gender- and education level-matched controls by inducing the six basic emotions of happiness, sadness, fear, anger, surprise and disgust using multimodal (audio and visual) stimuli. In addition, participants were asked to report their subjective affect. Because of the nonlinear and dynamic nature of EEG signals, we utilized higher order spectral features (specifically, bispectrum) for analysis. Two different classifiers namely K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) were used to investigate the performance of the HOS based features to classify each of the six emotional states of PD patients compared to HC. Ten-fold cross-validation method was used for testing the reliability of the classifier results. Main results From the experimental results with our EEG data set, we found that (a) classification performance of bispectrum features across ALL frequency bands is better than individual frequency bands in both the groups using SVM classifier; (b) higher frequency band plays a more important role in emotion activities than lower frequency band; and (c) PD patients showed emotional impairments compared to HC, as demonstrated by a lower classification performance, particularly for negative emotions (sadness, fear, anger and disgust). Significance These results demonstrate the effectiveness of applying EEG features with machine learning techniques to classify the each emotional state difference of PD patients compared to HC, and offer a promising approach for detection of emotional impairments associated with other neurological disorders. © 2014 Elsevier Ltd.",affect; emotion; Emotion recognition; adult; female; human; male; Learning systems; Classification (of information); Article; controlled study; recognition; electroencephalography; Electroencephalography; Biomedical signal processing; electroencephalogram; Brain; Electroencephalogram; Support vector machines; algorithm; signal processing; article; anger; disgust; fear; Feature extraction; k nearest neighbor; support vector machine; Nearest neighbor search; Support vector machine; Electroencephalogram signals; clinical article; Machine learning techniques; happiness; sadness; visual stimulation; Classification performance; corpus striatum; priority journal; surprise; K nearest neighbor (KNN); middle aged; educational status; Neurodegenerative diseases; Parkinson's disease; amantadine; auditory stimulation; Bispectrum; brain electrophysiology; carbidopa plus levodopa; catechol methyltransferase inhibitor; cholinergic receptor blocking agent; Cross-validation methods; dopamine 2 receptor stimulating agent; monoamine oxidase B inhibitor; Parkinson disease; reliability; self evaluation,,,emotion,No,Yes
scopus,Use of agreement/disagreement classification in dyadic interactions for continuous emotion recognition,"Khaki, H.; Erzin, E.",2016,,08-12-September-2016,,10.21437/Interspeech.2016-407,"Natural and affective handshakes of two participants define the course of dyadic interaction. Affective states of the participants are expected to be correlated with the nature or type of the dyadic interaction. In this study, we investigate relationship between affective attributes and nature of dyadic interaction. In this investigation we use the JESTKOD database, which consists of speech and full-body motion capture data recordings for dyadic interactions under agreement and disagreement scenarios. The dataset also has affective annotations in activation, valence and dominance (AVD) attributes. We pose the continuous affect recognition problem under agreement and disagreement scenarios of dyadic interactions. We define a statistical mapping using the support vector regression (SVR) from speech and motion modalities to affective attributes with and without the dyadic interaction type (DIT) information. We observe an improvement in estimation of the valence attribute when the DIT is available. Furthermore this improvement sustains even we estimate the DIT from the speech and motion modalities of the dyadic interaction. Copyright © 2016 ISCA.",Speech recognition; Emotion recognition; Human computer interaction; Speech; Speech processing; Human-computer interaction; Speech communication; Affect recognition; Affective state; Dyadic interaction; Support vector regression (SVR); Affective annotations; Agreement/disagreement; Dyadic interaction type; Full-body motions; Multimodal continuous emotion recognition,,,emotion,No,No
scopus,Emotion recognition in the wild: incorporating voice and lip activity in multimodal decision-level fusion,"Ringeval, F.; Amiriparian, S.; Eyben, F.; Scherer, K.; Schuller, B.",2014,,,,10.1145/2663204.2666271,"In this paper, we investigate the relevance of using voice and lip activity to improve performance of audiovisual emotion recognition in unconstrained settings, as part of the 2014 Emotion Recognition in the Wild Challenge (EmotiW14). Indeed, the dataset provided by the organisers contains movie excerpts with highly challenging variability in terms of audiovisual content; e. g., speech and/or face of the subject expressing the emotion can be absent in the data. We therefore propose to tackle this issue by incorporating both voice and lip activity as additional features in a decisionlevel fusion. Results obtained on the blind test set show that the decision-level fusion can improve the best monomodal approach, and that the addition of both voice and lip activity in the feature set leads to the best performance (UAR = 35:27%), with an absolute improvement of 5.36% over the baseline. Copyright 2014 ACM.",Speech recognition; Emotion recognition; Interactive computer systems; Multimedia; Decision level fusion; Decision-level fusion; Activity detection; Lip activity detection; Voice activity detection,,,emotion,No,Yes
scopus,Long short term memory recurrent neural network based encoding method for emotion recognition in video,"Chao, L.; Tao, J.; Yang, M.; Li, Y.; Wen, Z.",2016,,2016-May,,10.1109/ICASSP.2016.7472178,"Human emotion is a temporally dynamic event which can be inferred from both audio and video feature sequences. In this paper we investigate the long short term memory recurrent neural network (LSTM-RNN) based encoding method for category emotion recognition in the video. LSTM-RNN is able to incorporate knowledge about how emotion evolves over long range successive frames and emotion clues from isolated frame. After encoding, each video clip can be represented by a vector for each input feature sequence. The vectors contain both frame level and sequence level emotion information. These vectors are then concatenated and fed into support vector machine (SVM) to get the final prediction result. Extensive evaluations on Emotion Challenge in the Wild (EmotiW2015) dataset show the efficiency of the proposed encoding method and competitive results are obtained. The final recognition accuracy achieves 46.38% for audio-video emotion recognition sub-challenge, where the challenge baseline is 39.33%. © 2016 IEEE.",Facial Expression; Emotion Recognition; Multimodal; Long Short Term Memory Recurrent Neutral Network,,,emotion,No,Yes
scopus,Optimal set of EEG features for emotional state classification and trajectory visualization in Parkinson's disease,"Yuvaraj, R.; Murugappan, M.; Ibrahim, N.M.; Sundaraj, K.; Omar, M.I.; Mohamad, K.; Palaniappan, R.",2014,,94,,10.1016/j.ijpsycho.2014.07.014,"In addition to classic motor signs and symptoms, individuals with Parkinson's disease (PD) are characterized by emotional deficits. Ongoing brain activity can be recorded by electroencephalograph (EEG) to discover the links between emotional states and brain activity. This study utilized machine-learning algorithms to categorize emotional states in PD patients compared with healthy controls (HC) using EEG. Twenty non-demented PD patients and 20 healthy age-, gender-, and education level-matched controls viewed happiness, sadness, fear, anger, surprise, and disgust emotional stimuli while fourteen-channel EEG was being recorded. Multimodal stimulus (combination of audio and visual) was used to evoke the emotions. To classify the EEG-based emotional states and visualize the changes of emotional states over time, this paper compares four kinds of EEG features for emotional state classification and proposes an approach to track the trajectory of emotion changes with manifold learning. From the experimental results using our EEG data set, we found that (a) bispectrum feature is superior to other three kinds of features, namely power spectrum, wavelet packet and nonlinear dynamical analysis; (b) higher frequency bands (alpha, beta and gamma) play a more important role in emotion activities than lower frequency bands (delta and theta) in both groups and; (c) the trajectory of emotion changes can be visualized by reducing subject-independent features with manifold learning. This provides a promising way of implementing visualization of patient's emotional state in real time and leads to a practical system for noninvasive assessment of the emotional impairments associated with neurological disorders. © 2014 Elsevier B.V.",Adult; Female; Humans; Male; Middle Aged; Emotions; emotion; adult; female; human; male; physiology; Article; controlled study; machine learning; electroencephalography; Electroencephalography; procedures; electroencephalogram; Electroencephalogram; anger; disgust; fear; Emotion classification; emotion assessment; classification; psychology; aged; clinical article; happiness; sadness; Manifold learning; disease association; middle aged; Parkinson's disease; auditory stimulation; Parkinson disease; Acoustic Stimulation; alpha rhythm; beta rhythm; delta rhythm; emotional disorder; Feature reduction; gamma rhythm; non invasive measurement; nonlinear system; Parkinson Disease; Photic Stimulation; photostimulation; theta rhythm,,,emotion,No,No
scopus,Enhanced autocorrelation in real world emotion recognition,"Meudt, S.; Schwenker, F.",2014,,,,10.1145/2663204.2666276,"Multimodal emotion recognition in real world environments is still a challenging task of affective computing research. Recognizing the affective or physiological state of an individual is difficult for humans as well as for computer systems, and thus finding suitable discriminative features is the most promising approach in multimodal emotion recognition. In the literature numerous features have been developed or adapted from related signal processing tasks. But still, classifying emotional states in real world scenarios is diffcult and the performance of automatic classifiers is rather limited. This is mainly due to the fact that emotional states can not be distinguished by a well defined set of discriminating features. In this work we present an enhanced autocorrelation feature as a multi pitch detection feature and compare its performance to feature well known, and state-of-the-art in signal and speech processing. Results of the evaluation show that the enhanced autocorrelation outperform other state-of-the-art features in case of the challenge data set. The complexity of this benchmark data set lies in between real world data sets showing naturalistic emotional utterances, and the widely applied and well-understood acted emotional data sets. Copyright 2014 ACM.",Speech recognition; Emotion recognition; Multimodal emotion recognition; Affective Computing; Human computer interaction; Speech processing; Affective computing; Virtual reality; Signal processing; Audio features; Interactive computer systems; Discriminative features; Real world environments; Continuous speech recognition; Autocorrelation; Autocorrelation features; Automatic classifiers; Enhanced autocorrelation,,,emotion,No,Yes
scopus,Predicting evoked emotions in video,"Ellis, J.G.; Lin, W.S.; Lin, C.-Y.; Chang, S.-F.",2015,,,,10.1109/ISM.2014.69,"Understanding how human emotion is evoked from visual content is a task that we as people do every day, but machines have not yet mastered. In this work we address the problem of predicting the intended evoked emotion at given points within movie trailers. Movie Trailers are carefully curated to elicit distinct and specific emotional responses from viewers, and are therefore well-suited for emotion prediction. However, current emotion recognition systems struggle to bridge the 'affective gap', which refers to the difficulty in modeling high-level human emotions with low-level audio and visual features. To address this problem, we propose a mid-level concept feature, which is based on detectable movie shot concepts which we believe to be tied closely to emotions. Examples of these concepts are 'Fight', 'Rock Music', and 'Kiss'. We also create 2 datasets, the first with shot-level concept annotations for learning our concept detectors, and a separate, second dataset with emotion annotations taken throughout the trailers using the two dimensional arousal and valence model for emotion annotation. We report the performance of our concept detectors, and show that by using the output of these detectors as a mid-level representation for the movie shots we are able to more accurately predict the evoked emotion throughout a trailer than by using low-level features. © 2014 IEEE.",Multi-modal; Affective Computing; multimodal; Emotion analysis; signal processing; Audio systems; Forecasting; Feature extraction; Audio signal processing; emotion analysis; Computer vision; Audio processing; Video processing; Video signal processing; Signal processing; affective computing; computer vision; Motion pictures; audio processing; video processing; movie; Movie analysis; movie analysis; mulitmedia,,,emotion,Yes,Yes
scopus,Convolutional MKL based multimodal emotion recognition and sentiment analysis,"Poria, S.; Chaturvedi, I.; Cambria, E.; Hussain, A.",2016,,0,,10.1109/ICDM.2016.178,"Technology has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. Much of the content being posted and consumed online is multimodal. With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of video on the Internet will only continue to increase. It has become increasingly difficult for researchers to keep up with this deluge of multimodal content, let alone organize or make sense of it. Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content. This is particularly important in sentiment analysis, as both service and product reviews are gradually shifting from unimodal to multimodal. We present a novel method to extract features from visual and textual modalities using deep convolutional neural networks. By feeding such features to a multiple kernel learning classifier, we significantly outperform the state of the art of multimodal emotion recognition and sentiment analysis on different datasets. © 2016 IEEE.",Emotion Recognition; Deep neural networks; Multi-modal; Speech recognition; Modal analysis; Deep learning; Multimodal emotion recognition; Data mining; Sentiment analysis; Classification (of information); Convolutional neural networks; Convolution; Convolutional neural network; Multimodal sentiment analyse; Multimodal sentiment analysis; Multiple kernel learning; Multiple Kernel Learning; Google+; Internet connection; Internet wills,,,emotion,No,No
scopus,A deep feature based multi-kernel learning approach for video Emotion Recognition,"Li, W.; Abtahi, F.; Zhu, Z.",2015,,,,10.1145/2818346.2830583,"In this paper, we describe our proposed approach for participating in the Third Emotion Recognition in the Wild Challenge (EmotiW 2015). We focus on the sub-challenge of Audio-Video Based Emotion Recognition using the AFEW dataset. The AFEW dataset consists of 7 emotion groups corresponding to the 7 basic emotions. Each group includes multiple videos from movie clips with people acting a certain emotion. In our approach, we extract LBP-TOP-based video features, openEAR energy/spectral-based audio features, and CNN (convolutional neural network) based deep image features by fine-tuning a pre-trained model with extra emotion images from the web. For each type of features, we run an SVM grid search to find the best RBF kernel. Then multi-kernel learning is employed to combine the RBF kernels to accomplish the feature fusion and generate a fused RBF kernel. Running multi-class SVM classification, we achieve a 45.23% test accuracy on the AFEW dataset. We then apply a decision optimization method to adjust the label distribution closer to the ground truth, by setting offsets for some of the classifiers' prediction confidence score. By applying this modification, the test accuracy increases to 50.46%, which is a significant improvement comparing to the baseline accuracy 39.33%. © 2015 ACM.",Emotion Recognition; Speech recognition; Emotion recognition; Deep learning; Classification (of information); Convolutional neural network; Statistical tests; Multimodal features; Neural networks; Interactive computer systems; Radial basis function networks; Label distribution; Multi kernel learning; Multi-kernel learning; Optimization method; Prediction confidence,,,emotion,Yes,Yes
scopus,Emotion recognition using multimodal deep learning,"Liu, W.; Zheng, W.-L.; Lu, B.-L.",2016,,9948 LNCS,,10.1007/978-3-319-46672-9_58,"To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models with SEED and DEAP datasets to recognize different kinds of emotions. We demonstrate that high level representation features extracted by the Bimodal Deep AutoEncoder (BDAE) are effective for emotion recognition. With the BDAE network, we achieve mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to those of the state-of-the-art approaches. By analysing the confusing matrices, we found that EEG and eye features contain complementary information and the BDAE network could fully take advantage of this complement property to enhance emotion recognition. © Springer International Publishing AG 2016.",Multi-modal; Speech recognition; Emotion recognition; Physiological signals; Deep learning; Learning systems; Auto encoders; Electroencephalography; EEG; Physiological models; Multimodal deep learning; Real-world; Auto-encoder; State-of-the-art approach; Affective model; Information science,,,emotion,No,Yes
scopus,CREMA-D: Crowd-sourced emotional multimodal actors dataset,"Cao, H.; Cooper, D.G.; Keutmann, M.K.; Gur, R.C.; Nenkova, A.; Verma, R.",2014,,5,,10.1109/TAFFC.2014.2336244,"People convey their emotional state in their face and voice. We present an audio-visual dataset uniquely suited for the study of multi-modal emotion expression and perception. The dataset consists of facial and vocal emotional expressions in sentences spoken in a range of basic emotional states (happy, sad, anger, fear, disgust, and neutral). 7,442 clips of 91 actors with diverse ethnicbackgrounds were rated by multiple raters in three modalities: audio, visual, and audio-visual. Categorical emotion labels andreal-value intensity values for the perceived emotion were collected using crowd-sourcing from 2,443 raters. The human recognition of intended emotion for the audio-only, visual-only, and audio-visual data are 40.9, 58.2 and 63.6 percent respectively. Recognition rates are highest for neutral, followed by happy, anger, disgust, fear, and sad. Average intensity levels of emotion are rated highest forvisual-only perception. The accurate recognition of disgust and fear requires simultaneous audio-visual cues, while anger andhappiness can be well recognized based on evidence from a single modality. The large dataset we introduce can be used to probe other questions concerning the audio-visual perception of emotion. © 2010-2012 IEEE.",facial expression; Multi-modal; Behavioral research; Face recognition; Facial Expressions; Emotion expression; Audio-visual data; Emotional expressions; multi-modal recognition; Emotional corpora; Audio-visual perceptions; Human recognition; voice expression,,,emotion,Yes,No
scopus,Identification of human emotions via univariate and multivarite multiscale entropy,"Ahammed, K.",2015,,8,,10.3844/ajeassp.2015.410.416,"This work analyzes the emotions of human in terms of complexity. This analysis is achieved by applying both univariate and multivariate multiscale entropy methods on a multimodal dataset. Most of the contemporary human-computer interaction systems are unable to identify human affective states. So, the benefit of analyzing human emotions is to fill this gap by detecting human affective states. The univariate and multivariate multiscale entropy analysis curves obtained using multimodal dataset show differences in terms of complexity among different affective states, which can be used for emotion detection and classification for machine vision applications. © 2015 Kawser Ahammed.",Emotion classification; Multimodal dataset; Affective states computing; Machine vision applications,,,emotion,No,No
scopus,Combining multimodal features within a fusion network for Emotion recognition in the wild,"Sun, B.; Li, L.; Zhou, G.; Wu, X.; He, J.; Yu, L.; Li, D.; Wei, Q.",2015,,,,10.1145/2823327.2823332,"In this paper, we describe our work in the third Emotion Recognition in the Wild (EmotiW 2015) Challenge. For each video clip, we extract MSDF, LBP-TOP, HOG, LPQ-TOP and acoustic features to recognize the emotions of film characters. For the static facial expression recognition based on video frame, we extract MSDF, DCNN and RCNN features. We train linear SVM classifiers for these kinds of features on the AFEW and SFEW dataset, and we propose a novel fusion network to combine all the extracted features at decision level. The final achievement we gained is 51.02% on the AFEW testing set and 51.08% on the SFEW testing set, which are much better than the baseline recognition rate of 39.33% and 39.13%. © 2015 ACM.",Speech recognition; Emotion recognition; Face recognition; Classification (of information); Facial expression recognition; Multimodal features; Interactive computer systems; Acoustic features; Decision levels; Video clips; Fusion network; Linear SVM; Video frame,,,emotion,No,No
scopus,MEC 2016: The multimodal emotion recognition challenge of CCPR 2016,"Li, Y.; Tao, J.; Schuller, B.; Shan, S.; Jiang, D.; Jia, J.",2016,,663,,10.1007/978-981-10-3005-5_55,"Emotion recognition is a significant research filed of pattern recognition and artificial intelligence. The Multimodal Emotion Recognition Challenge (MEC) is a part of the 2016 Chinese Conference on Pattern Recognition (CCPR). The goal of this competition is to compare multimedia processing and machine learning methods for multimodal emotion recognition. The challenge also aims to provide a common benchmark data set, to bring together the audio and video emotion recognition communities, and to promote the research in multimodal emotion recognition. The data used in this challenge is the Chinese Natural Audio- Visual Emotion Database (CHEAVD), which is selected from Chinese movies and TV programs. The discrete emotion labels are annotated by four experienced assistants. Three sub-challenges are defined: audio, video and multimodal emotion recognition. This paper introduces the baseline audio, visual features, and the recognition results by Random Forests. © Springer Nature Singapore Pte Ltd. 2016.",Speech recognition; Emotion; Multi-modal fusion; Learning systems; Artificial intelligence; Affective Computing; Affective computing; Multimodal fusion; Decision trees; Challenge; Pattern recognition; Features; Audio-visual corpora; Audio-visual corpus,,,emotion,Yes,No
google-scholar,Expression EEG multimodal emotion recognition method based on the bidirectional LSTM and attention mechanism,"Zhao, Yifeng; Chen, Deyun",2021,,2021,,,,,,,emotion,No,No
google-scholar,m_AutNet–A Framework for Personalized Multimodal Emotion Recognition in Autistic Children,"Kurian, Asha; Tripathi, Shikha",2024,,,,,,,,,emotion,No,No
google-scholar,Exploring Hybrid Multi-View Multimodal for Natural Language Emotion Recognition using Multi-Source Information Learning Model,"Rohit, S",,,,,,,,,,emotion,No,No
google-scholar,"Enhancing Automatic Emotion Recognition for Clinical Applications: A Multimodal, Personalized Approach and Quantification of Emotional Reaction Intensity with Transformers","Qian, Yang",2023,,,,,,,,,emotion,No,No
google-scholar,ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and Multimodal Fusion Evaluation,"Frieske, Rita; Shi, Bertrand E",2024,,,,,,,,,emotion,No,No
google-scholar,GCE: An Audio-Visual Dataset for Group Cohesion and Emotion Analysis,"Lim, Eunchae; Ho, Ngoc-Huynh; Pant, Sudarshan; Kang, Young-Shin; Jeon, Seong-Eun; Kim, Seungwon; Kim, Soo-Hyung; Yang, Hyung-Jeong",2024,,14,,,,,,,emotion,No,No
google-scholar,AVaTER: A Multimodal Approach of Recognizing Emotion using Cross-modal Attention Technique,"Das, Avishek; Sarma, Moumita Sen; Hoque, Mohammed Moshiul; Siddique, Nazmul; Dewan, M Ali Akber",2024,,,,,,,,,emotion,No,No
google-scholar,M2fnet: Multi-modal fusion network for emotion recognition in conversation,"Chudasama, Vishal; Kar, Purbayan; Gudmalwar, Ashish; Shah, Nirmesh; Wasnik, Pankaj; Onoe, Naoyuki",2022,,,,,,,,,emotion,No,No
google-scholar,Adaptive multimodal emotion detection architecture for social robots,"Heredia, Juanpablo; Lopes-Silva, Edmundo; Cardinale, Yudith; Diaz-Amado, Jose; Dongo, Irvin; Graterol, Wilfredo; Aguilera, Ana",2022,,10,,,,,,,emotion,No,No
google-scholar,Deep multimodal emotion recognition on human speech: A review,"Koromilas, Panagiotis; Giannakopoulos, Theodoros",2021,,11,,,,,,,emotion,No,No
google-scholar,Multimodal emotion recognition using data augmentation and fusion,"Shoumy, Nusrat Jahan",2022,,,,,,,,,emotion,No,No
google-scholar,"A systematic review on affective computing: Emotion models, databases, and recent advances","Wang, Yan; Song, Wei; Tao, Wei; Liotta, Antonio; Yang, Dawei; Li, Xinlei; Gao, Shuyong; Sun, Yixuan; Ge, Weifeng; Zhang, Wei",2022,,83,,,,,,,emotion,No,No
google-scholar,Building a robust system for multimodal emotion recognition,"Wagner, Johannes; Lingenfelser, Florian; André, Elisabeth",2015,,,,,,,,,emotion,No,No
google-scholar,Emotion based hate speech detection using multimodal learning,"Rana, Aneri; Jha, Sonali",2022,,,,,,,,,emotion,No,No
google-scholar,Multimodal Emotion Recognition using Deep Learning Techniques: A novel system for real-world Emotion Recognition,"Kardakis, Spyridon",2021,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition with multimodal features and temporal models,"Wang, Shuai; Wang, Wenxuan; Zhao, Jinming; Chen, Shizhe; Jin, Qin; Zhang, Shilei; Qin, Yong",2017,,,,,,,,,emotion,No,No
google-scholar,Emotion classification from speech and text in videos using a multimodal approach,"Caschera, Maria Chiara; Grifoni, Patrizia; Ferri, Fernando",2022,,6,,,,,,,emotion,No,No
google-scholar,"Multimodal multitask emotion recognition using images, texts and tags","Pagé Fortin, Mathieu; Chaib-draa, Brahim",2019,,,,,,,,,emotion,No,No
google-scholar,Exploring fusion methods for multimodal emotion recognition with missing data,"Wagner, Johannes; Andre, Elisabeth; Lingenfelser, Florian; Kim, Jonghwa",2011,,2,,,,,,,emotion,No,No
google-scholar,"Muse 2020 challenge and workshop: Multimodal sentiment analysis, emotion-target engagement and trustworthiness detection in real-life media: Emotional car reviews in-the-wild","Stappen, Lukas; Baird, Alice; Rizos, Georgios; Tzirakis, Panagiotis; Du, Xinchen; Hafner, Felix; Schumann, Lea; Mallol-Ragolta, Adria; Schuller, Björn W; Lefter, Iulia",2020,,,,,,,,,emotion,No,No
google-scholar,Emoset: A large-scale visual emotion dataset with rich attributes,"Yang, Jingyuan; Huang, Qirui; Ding, Tingting; Lischinski, Dani; Cohen-Or, Danny; Huang, Hui",2023,,,,,,,,,emotion,No,No
google-scholar,Behavioral and physiological signals-based deep multimodal approach for mobile emotion recognition,"Yang, Kangning; Wang, Chaofan; Gu, Yue; Sarsenbayeva, Zhanna; Tag, Benjamin; Dingler, Tilman; Wadley, Greg; Goncalves, Jorge",2021,,14,,,,,,,emotion,No,No
google-scholar,Few-shot learning for fine-grained emotion recognition using physiological signals,"Zhang, Tianyi; El Ali, Abdallah; Hanjalic, Alan; Cesar, Pablo",2022,,25,,,,,,,emotion,No,No
google-scholar,Multimodal fused emotion recognition about expression-EEG interaction and collaboration using deep learning,"Wu, Di; Zhang, Jianpei; Zhao, Qingchao",2020,,8,,,,,,,emotion,No,No
google-scholar,Multimodal Emotion Recognition Using Bi-LG-GCN for MELD Dataset,"Alsaadawı, Hussein Farooq Tayeb; Daş, Resul",2024,,12,,,,,,,emotion,No,No
google-scholar,Group emotion recognition in the wild by combining deep neural networks for facial expression classification and scene-context analysis,"Abbas, Asad; Chalup, Stephan K",2017,,,,,,,,,emotion,No,No
google-scholar,Automatic movie genre classification & emotion recognition via a Biprojection Multimodal Transformer,"Moreno-Galván, Diego Aarón; López-Santillán, Roberto; González-Gurrola, Luis Carlos; Montes-Y-Gómez, Manuel; Sanchez-Vega, Fernando; López-Monroy, Adrián Pastor",2024,,,,,,,,,emotion,No,No
google-scholar,Automated Multimodal Emotion Recognition,"Fernández Carbonell, Marcos",2020,,,,,,,,,emotion,No,No
google-scholar,An active learning paradigm for online audio-visual emotion recognition,"Kansizoglou, Ioannis; Bampis, Loukas; Gasteratos, Antonios",2019,,13,,,,,,,emotion,No,No
google-scholar,Photogram classification-based emotion recognition,"López-Gil, Juan Miguel; Garay-Vitoria, Nestor",2021,,9,,,,,,,emotion,No,No
google-scholar,Classifying emotions and engagement in online learning based on a single facial expression recognition neural network,"Savchenko, Andrey V; Savchenko, Lyudmila V; Makarov, Ilya",2022,,13,,,,,,,emotion,No,No
google-scholar,Attx: Attentive cross-connections for fusion of wearable signals in emotion recognition,"Bhatti, Anubhav; Behinaein, Behnam; Hungler, Paul; Etemad, Ali",2022,,,,,,,,,emotion,No,No
google-scholar,"Emotion recognition in conversation: Research challenges, datasets, and recent advances","Poria, Soujanya; Majumder, Navonil; Mihalcea, Rada; Hovy, Eduard",2019,,7,,,,,,,emotion,No,No
google-scholar,Advances in multimodal emotion recognition based on brain–computer interfaces,"He, Zhipeng; Li, Zina; Yang, Fuzhou; Wang, Lei; Li, Jingcong; Zhou, Chengju; Pan, Jiahui",2020,,10,,,,,,,emotion,No,No
google-scholar,Using multi-inception CNN for face emotion recognition,"Altaher, Ali; Salekshahrezaee, Zahra; Abdollah Zadeh, Azadeh; Rafieipour, Hoda; Altaher, Ahmed",2020,,3,,,,,,,emotion,No,No
google-scholar,Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism,"Sun, Licai; Lian, Zheng; Tao, Jianhua; Liu, Bin; Niu, Mingyue",2020,,,,,,,,,emotion,No,No
google-scholar,Towards sentiment and emotion aided multi-modal speech act classification in twitter,"Saha, Tulika; Upadhyaya, Apoorva; Saha, Sriparna; Bhattacharyya, Pushpak",2021,,,,,,,,,emotion,No,No
google-scholar,Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition.,"Lian, Zheng; Tao, Jianhua; Liu, Bin; Huang, Jian; Yang, Zhanlei; Li, Rongjun",2020,,,,,,,,,emotion,No,No
google-scholar,Decision-level fusion method for emotion recognition using multimodal emotion recognition information,"Song, Kyu-Seob; Nho, Young-Hoon; Seo, Ju-Hwan; Kwon, Dong-soo",2018,,,,,,,,,emotion,No,No
google-scholar,"A survey of ai-based facial emotion recognition: Features, ml & dl techniques, age-wise datasets and future directions","Dalvi, Chirag; Rathod, Manish; Patil, Shruti; Gite, Shilpa; Kotecha, Ketan",2021,,9,,,,,,,emotion,No,No
google-scholar,Supporting depression screening with multimodal emotion detection,"Francese, Rita; Attanasio, Pasquale",2021,,,,,,,,,emotion,No,No
google-scholar,CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition,"Peng, Cheng; Chen, Ke; Shou, Lidan; Chen, Gang",2024,,38,,,,,,,emotion,No,No
google-scholar,Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning,"Cheng, Zebang; Cheng, Zhi-Qi; He, Jun-Yan; Sun, Jingdong; Wang, Kai; Lin, Yuxiang; Lian, Zheng; Peng, Xiaojiang; Hauptmann, Alexander",2024,,,,,,,,,emotion,No,No
google-scholar,Human‐Computer Interaction with Detection of Speaker Emotions Using Convolution Neural Networks,"Alnuaim, Abeer Ali; Zakariah, Mohammed; Alhadlaq, Aseel; Shashidhar, Chitra; Hatamleh, Wesam Atef; Tarazi, Hussam; Shukla, Prashant Kumar; Ratna, Rajnish",2022,,2022,,,,,,,emotion,No,No
google-scholar,Pairwise emotional relationship recognition in drama videos: Dataset and benchmark,"Gao, Xun; Zhao, Yin; Zhang, Jie; Cai, Longjun",2021,,,,,,,,,emotion,No,No
google-scholar,Weakly-supervised learning for fine-grained emotion recognition using physiological signals,"Zhang, Tianyi; El Ali, Abdallah; Wang, Chen; Hanjalic, Alan; Cesar, Pablo",2022,,14,,,,,,,emotion,No,No
google-scholar,Deep learning-based drivers emotion classification system in time series data for remote applications,"Naqvi, Rizwan Ali; Arsalan, Muhammad; Rehman, Abdul; Rehman, Ateeq Ur; Loh, Woong-Kee; Paul, Anand",2020,,12,,,,,,,emotion,No,No
google-scholar,Dense Graph Convolutional With Joint Cross-Attention Network for Multimodal Emotion Recognition,"Cheng, Cheng; Liu, Wenzhe; Feng, Lin; Jia, Ziyu",2024,,,,,,,,,emotion,No,No
google-scholar,Human emotion recognition with electroencephalographic multidimensional features by hybrid deep neural networks,"Li, Youjun; Huang, Jiajin; Zhou, Haiyan; Zhong, Ning",2017,,7,,,,,,,emotion,No,No
google-scholar,Multimodal Emotion Recognition Using Temporal Convolutional Networks,"Harb, Hussein",2023,,,,,,,,,emotion,No,No
google-scholar,C-GCN: Correlation based graph convolutional network for audio-video emotion recognition,"Nie, Weizhi; Ren, Minjie; Nie, Jie; Zhao, Sicheng",2020,,23,,,,,,,emotion,No,No
google-scholar,A hybrid deep learning neural approach for emotion recognition from facial expressions for socially assistive robots,"Ruiz-Garcia, Ariel; Elshaw, Mark; Altahhan, Abdulrahman; Palade, Vasile",2018,,29,,,,,,,emotion,No,No
google-scholar,"Identifying similarities and differences in emotion recognition with EEG and eye movements among Chinese, German, and French People","Liu, Wei; Zheng, Wei-Long; Li, Ziyi; Wu, Si-Yuan; Gan, Lu; Lu, Bao-Liang",2022,,19,,,,,,,emotion,No,No
google-scholar,Emotion-aware speaker identification with transfer learning,"Noh, Kyoungju; Jeong, Hyuntae",2023,,,,,,,,,emotion,No,No
google-scholar,Multimodal emotion recognition via face and voice,"Griera i Jiménez, Oriol",2022,,,,,,,,,emotion,No,No
google-scholar,BEC-1D: Biosignal-Based Emotions Classification with 1D ConvNet,"Luján-García, Juan Eduardo; Cardoso-Moreno, Marco A; Yáñez-Márquez, Cornelio; Calvo, Hiram",2023,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition by textual tweets classification using voting classifier (LR-SGD),"Yousaf, Anam; Umer, Muhammad; Sadiq, Saima; Ullah, Saleem; Mirjalili, Seyedali; Rupapara, Vaibhav; Nappi, Michele",2020,,9,,,,,,,emotion,No,No
google-scholar,"Revisiting crowd behaviour analysis through deep learning: Taxonomy, anomaly detection, crowd emotions, datasets, opportunities and prospects","Sánchez, Francisco Luque; Hupont, Isabelle; Tabik, Siham; Herrera, Francisco",2020,,64,,,,,,,emotion,No,No
google-scholar,"Human emotion: a survey focusing on languages, ontologies, datasets, and systems","Elkobaisi, Mohammed R; Al Machot, Fadi; Mayr, Heinrich C",2022,,3,,,,,,,emotion,No,No
google-scholar,Multi-label emotion detection via emotion-specified feature extraction and emotion correlation learning,"Deng, Jiawen; Ren, Fuji",2020,,14,,,,,,,emotion,No,No
google-scholar,Emotion dictionary learning with modality attentions for mixed emotion exploration,"Liu, Fang; Yang, Pei; Shu, Yezhi; Yan, Fei; Zhang, Guanhua; Liu, Yong-Jin",2023,,,,,,,,,emotion,No,No
google-scholar,Multimodal physiological signals fusion for online emotion recognition,"Pan, Tongjie; Ye, Yalan; Cai, Hecheng; Huang, Shudong; Yang, Yang; Wang, Guoqing",2023,,,,,,,,,emotion,No,No
google-scholar,Emo2vec: Learning generalized emotion representation by multi-task training,"Xu, Peng; Madotto, Andrea; Wu, Chien-Sheng; Park, Ji Ho; Fung, Pascale",2018,,,,,,,,,emotion,No,No
google-scholar,EEG-based multimodal emotion recognition: a machine learning perspective,"Liu, Huan; Lou, Tianyu; Zhang, Yuzhe; Wu, Yixiao; Xiao, Yang; Jensen, Christian S; Zhang, Dalin",2024,,,,,,,,,emotion,No,No
google-scholar,LSTM-modeling of emotion recognition using peripheral physiological signals in naturalistic conversations,"Zitouni, M Sami; Park, Cheul Young; Lee, Uichin; Hadjileontiadis, Leontios J; Khandoker, Ahsan",2022,,27,,,,,,,emotion,No,No
google-scholar,Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching,"Zhang, Shiqing; Zhang, Shiliang; Huang, Tiejun; Gao, Wen",2017,,20,,,,,,,emotion,No,No
google-scholar,Labeling images with facial emotion and the potential for pediatric healthcare,"Kalantarian, Haik; Jedoui, Khaled; Washington, Peter; Tariq, Qandeel; Dunlap, Kaiti; Schwartz, Jessey; Wall, Dennis P",2019,,98,,,,,,,emotion,No,No
google-scholar,Deep neural networks for acoustic emotion recognition: Raising the benchmarks,"Stuhlsatz, André; Meyer, Christine; Eyben, Florian; Zielke, Thomas; Meier, Günter; Schuller, Björn",2011,,,,,,,,,emotion,No,No
google-scholar,A Neural Network Architecture for Children’s Audio–Visual Emotion Recognition,"Matveev, Anton; Matveev, Yuri; Frolova, Olga; Nikolaev, Aleksandr; Lyakso, Elena",2023,,11,,,,,,,emotion,No,No
google-scholar,Emotion Recognition in Teachers' Teaching Behavior Based on Multimodal Data Analysis,"Lu, Yuanyuan; Chen, Zengzhao; Zheng, Qiuyu; Wang, Mengke",2023,,,,,,,,,emotion,No,No
google-scholar,Spontaneous visual database for detecting learning-centered emotions during online learning,"Xu, Yaping; Li, Yanyan; Chen, Yunshan; Bao, Haogang; Zheng, Yaqian",2023,,136,,,,,,,emotion,No,No
google-scholar,Facial emotion recognition: A survey and real-world user experiences in mixed reality,"Mehta, Dhwani; Siddiqui, Mohammad Faridul Haque; Javaid, Ahmad Y",2018,,18,,,,,,,emotion,No,No
google-scholar,Recognising human emotions from body movement and gesture dynamics,"Castellano, Ginevra; Villalba, Santiago D; Camurri, Antonio",2007,,,,,,,,,emotion,No,No
google-scholar,A multi-modal emotion recognition system based on CNN-transformer deep learning technique,"Karatay, Busra; Bestepe, Deniz; Sailunaz, Kashfia; Ozyer, Tansel; Alhajj, Reda",2022,,,,,,,,,emotion,No,No
google-scholar,"M3GAT: A multi-modal, multi-task interactive graph attention network for conversational sentiment analysis and emotion recognition","Zhang, Yazhou; Jia, Ao; Wang, Bo; Zhang, Peng; Zhao, Dongming; Li, Pu; Hou, Yuexian; Jin, Xiaojia; Song, Dawei; Qin, Jing",2023,,42,,,,,,,emotion,No,No
google-scholar,Wavelet ELM-AE based data augmentation and deep learning for efficient emotion recognition using EEG recordings,"Ari, Berna; Siddique, Kamran; Alçin, Ömer Faruk; Aslan, Muzaffer; Şengür, Abdulkadir; Mehmood, Raja Majid",2022,,10,,,,,,,emotion,No,No
google-scholar,Speech emotion recognition using convolutional neural networks,"Shahsavarani, Somayeh",2018,,,,,,,,,emotion,No,No
google-scholar,A Multimodal Dataset for Mixed Emotion Recognition,"Yang, Pei; Liu, Niqi; Liu, Xinge; Shu, Yezhi; Ji, Wenqi; Ren, Ziqi; Sheng, Jenny; Yu, Minjing; Yi, Ran; Zhang, Dan",2024,,11,,,,,,,emotion,No,No
google-scholar,"A multimodal psychological, physiological and behavioural dataset for human emotions in driving tasks","Li, Wenbo; Tan, Ruichen; Xing, Yang; Li, Guofa; Li, Shen; Zeng, Guanzhong; Wang, Peizhi; Zhang, Bingbing; Su, Xinyu; Pi, Dawei",2022,,9,,,,,,,emotion,No,No
google-scholar,EmpathicStories++: A Multimodal Dataset for Empathy towards Personal Experiences,"Shen, Jocelyn; Kim, Yubin; Hulse, Mohit; Zulfikar, Wazeer; Alghowinem, Sharifa; Breazeal, Cynthia; Park, Hae Won",2024,,,,,,,,,empathy,No,No
google-scholar,A uniform human multimodal dataset for emotion perception and judgment,"Sun, Sai; Cao, Runnan; Rutishauser, Ueli; Yu, Rongjun; Wang, Shuo",2023,,10,,,,,,,emotion,No,No
google-scholar,"BIRAFFE2, a multimodal dataset for emotion-based personalization in rich affective game environments","Kutt, Krzysztof; Drążyk, Dominika; Żuchowska, Laura; Szelążek, Maciej; Bobek, Szymon; Nalepa, Grzegorz J",2022,,9,,,,,,,emotion,No,No
google-scholar,Emotion recognition from multimodal physiological signals for emotion aware healthcare systems,"Ayata, Değer; Yaslan, Yusuf; Kamasak, Mustafa E",2020,,40,,,,,,,emotion,No,No
google-scholar,MEDIC: A multimodal empathy dataset in counseling,"Zhu, Zhouan; Li, Chenguang; Pan, Jicai; Li, Xin; Xiao, Yufei; Chang, Yanan; Zheng, Feiyi; Wang, Shangfei",2023,,,,,,,,,empathy,No,No
google-scholar,Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset,"Liu, Rui; Zuo, Haolin; Lian, Zheng; Xing, Xiaofen; Schuller, Björn W; Li, Haizhou",2024,,,,,,,,,emotion,No,No
google-scholar,A survey on databases for multimodal emotion recognition and an introduction to the VIRI (visible and InfraRed image) database,"Siddiqui, Mohammad Faridul Haque; Dhakal, Parashar; Yang, Xiaoli; Javaid, Ahmad Y",2022,,6,,,,,,,emotion,No,No
google-scholar,WEMAC: Women and emotion multi-modal affective computing dataset,"Miranda, Jose A; Rituerto-González, Esther; Gutiérrez-Martín, Laura; Luis-Mingueza, Clara; Canabal, Manuel F; Bárcenas, Alberto Ramírez; Lanza-Gutiérrez, Jose M; Peláez-Moreno, Carmen; López-Ongil, Celia",2022,,,,,,,,,emotion,No,No
google-scholar,Mgeed: a multimodal genuine emotion and expression detection database,"Wang, Yiming; Yu, Hui; Gao, Weihong; Xia, Yifan; Nduka, Charles",2023,,15,,,,,,,emotion,No,No
google-scholar,A multimodal hierarchical approach to speech emotion recognition from audio and text,"Singh, Prabhav; Srivastava, Ridam; Rana, KPS; Kumar, Vineet",2021,,229,,,,,,,emotion,No,No
google-scholar,Memotion 2: Dataset on sentiment and emotion analysis of memes,"Ramamoorthy, Sathyanarayanan; Gunti, Nethra; Mishra, Shreyash; Suryavardan, S; Reganti, Aishwarya; Patwa, Parth; DaS, Amitava; Chakraborty, Tanmoy; Sheth, Amit; Ekbal, Asif",2022,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition using multimodal deep learning in multiple psychophysiological signals and video,"Wang, Zhongmin; Zhou, Xiaoxiao; Wang, Wenlang; Liang, Chen",2020,,11,,,,,,,emotion,No,No
google-scholar,Emotionmeter: A multimodal framework for recognizing human emotions,"Zheng, Wei-Long; Liu, Wei; Lu, Yifei; Lu, Bao-Liang; Cichocki, Andrzej",2018,,49,,,,,,,emotion,No,No
google-scholar,Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS),"Santamaria-Granados, Luz; Munoz-Organero, Mario; Ramirez-Gonzalez, Gustavo; Abdulhay, Enas; Arunkumar, NJIA",2018,,7,,,,,,,emotion,No,No
google-scholar,Real-time multimodal emotion classification system in e-learning context,"Nandi, Arijit; Xhafa, Fatos; Subirats, Laia; Fort, Santi",2021,,,,,,,,,emotion,No,No
google-scholar,EmoSen: Generating sentiment and emotion controlled responses in a multimodal dialogue system,"Firdaus, Mauajama; Chauhan, Hardik; Ekbal, Asif; Bhattacharyya, Pushpak",2020,,13,,,,,,,emotion,No,No
google-scholar,Vreed: Virtual reality emotion recognition dataset using eye tracking & physiological measures,"Tabbaa, Luma; Searle, Ryan; Bafti, Saber Mirzaee; Hossain, Md Moinul; Intarasisrisawat, Jittrapol; Glancy, Maxine; Ang, Chee Siang",2021,,5,,,,,,,emotion,No,No
google-scholar,Disentangled representation learning for multimodal emotion recognition,"Yang, Dingkang; Huang, Shuai; Kuang, Haopeng; Du, Yangtao; Zhang, Lihua",2022,,,,,,,,,emotion,No,No
google-scholar,A multimodal facial emotion recognition framework through the fusion of speech with visible and infrared images,"Siddiqui, Mohammad Faridul Haque; Javaid, Ahmad Y",2020,,4,,,,,,,emotion,No,No
google-scholar,MDPE: A Multimodal Deception Dataset with Personality and Emotional Characteristics,"Cai, Cong; Liang, Shan; Liu, Xuefei; Zhu, Kang; Wen, Zhengqi; Tao, Jianhua; Xie, Heng; Cui, Jizhou; Ma, Yiming; Cheng, Zhenhua",2024,,,,,,,,,emotion,No,No
google-scholar,A multimodal approach towards emotion recognition of music using audio and lyrical content,"Bhattacharya, Aniruddha; Kadambari, KV",2018,,,,,,,,,emotion,No,No
google-scholar,The senseemotion database: A multimodal database for the development and systematic validation of an automatic pain-and emotion-recognition system,"Velana, Maria; Gruss, Sascha; Layher, Georg; Thiam, Patrick; Zhang, Yan; Schork, Daniel; Kessler, Viktor; Meudt, Sascha; Neumann, Heiko; Kim, Jonghwa",2017,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition using fusion of audio and video features,"Ortega, Juan DS; Cardinal, Patrick; Koerich, Alessandro L",2019,,,,,,,,,emotion,No,No
google-scholar,"Emotion analysis using audio/video, emg and eeg: A dataset and comparison study","Abtahi, Farnaz; Ro, Tony; Li, Wei; Zhu, Zhigang",2018,,,,,,,,,emotion,No,No
google-scholar,The audio-visual arabic dataset for natural emotions,"Shaqra, Ftoon Abu; Duwairi, Rehab; Al-Ayyoub, Mahmoud",2019,,,,,,,,,emotion,No,No
google-scholar,Multimodal cross-and self-attention network for speech emotion recognition,"Sun, Licai; Liu, Bin; Tao, Jianhua; Lian, Zheng",2021,,,,,,,,,emotion,No,No
google-scholar,RAMAS: Russian Multimodal Corpus of Dyadic Interaction for studying emotion recognition,"Perepelkina, Olga; Kazimirova, Eva; Konstantinova, Maria",2018,,6,,,,,,,emotion,No,No
google-scholar,Contextual time-continuous emotion recognition based on multimodal data,"Fedotov, Dmitrii",2022,,,,,,,,,emotion,No,No
google-scholar,Elder emotion classification through multimodal fusion of intermediate layers and cross-modal transfer learning,"Sreevidya, P; Veni, S; Ramana Murthy, OV",2022,,16,,,,,,,emotion,No,No
google-scholar,A dataset of continuous affect annotations and physiological signals for emotion analysis,"Sharma, Karan; Castellini, Claudio; Van Den Broek, Egon L; Albu-Schaeffer, Alin; Schwenker, Friedhelm",2019,,6,,,,,,,emotion,No,No
google-scholar,"A multimodal emotion recognition model integrating speech, video and MoCAP","Jia, Ning; Zheng, Chunjun; Sun, Wei",2022,,81,,,,,,,emotion,No,No
google-scholar,MDEAW: A Multimodal Dataset for Emotion Analysis through EDA and PPG signals from wireless wearable low-cost off-the-shelf Devices,"Nandi, Arijit; Xhafa, Fatos; Subirats, Laia; Fort, Santi",2022,,,,,,,,,emotion,No,No
google-scholar,Emotion classification with multi‐modal physiological signals using multi‐attention‐based neural network,"Zou, Chengsheng; Deng, Zhen; He, Bingwei; Yan, Maosong; Wu, Jie; Zhu, Zhaoju",2024,,,,,,,,,emotion,No,No
google-scholar,AVES: An Audio-Visual Emotion Stream Dataset for Temporal Emotion Detection,"Li, Yan; Gan, Wei; Lu, Ke; Jiang, Dongmei; Jain, Ramesh",2024,,,,,,,,,emotion,No,No
google-scholar,A Real-time Multimodal Intelligent Tutoring Emotion Recognition System (MITERS),"Khediri, Nouha; Ben Ammar, Mohamed; Kherallah, Monji",2024,,83,,,,,,,emotion,No,No
google-scholar,Emotion Recognition from Videos Using Multimodal Large Language Models,"Vaiani, Lorenzo; Cagliero, Luca; Garza, Paolo",2024,,16,,,,,,,emotion,No,No
google-scholar,Multimodal continuous emotion analysis,"Zhang, Su",2023,,,,,,,,,emotion,No,No
google-scholar,Multimodal Emotion Recognition using facial expression and other physiological condition,"Mathisen, Andreas",2022,,,,,,,,,emotion,No,No
google-scholar,Unraveling Emotions: Multimodal Deep Learning for Fine-Grained Emotion Recognition.,"Srivastava, Riktesh; Srivastava, Rajita",2023,,11,,,,,,,emotion,No,No
google-scholar,Multimodal Daily-life Emotional Recognition using Heart Rate and Speech Data from Wearables,"Moon, Eesun; Sagar, ASM Sharifuzzaman; Kim, Hyung Seok",2024,,,,,,,,,emotion,No,No
google-scholar,Icanet: A method of short video emotion recognition driven by multimodal data,"Wu, Xuecheng; Tian, Mengmeng; Zhai, Lanhang",2022,,,,,,,,,emotion,No,No
google-scholar,Decoding Emotions: Integrating EEG Signals and Facial Expressions for Advanced Multimodal Emotion Recognition,"Moussaoui, Khouloud; Farah, Mohamed",2024,,1,,,,,,,emotion,No,No
google-scholar,"Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers","Shayaninasab, Minoo; Babaali, Bagher",2024,,,,,,,,,emotion,No,No
google-scholar,Tensor Correlation Fusion for Multimodal Physiological Signal Emotion Recognition,"Shen, Jian; Zhu, Kexin; Liu, Huakang; Wu, Jinwen; Wang, Kang; Dong, Qunxi",2024,,,,,,,,,emotion,No,No
google-scholar,Emotion detection using noninvasive low cost sensors,"Girardi, Daniela; Lanubile, Filippo; Novielli, Nicole",2017,,,,,,,,,emotion,No,No
google-scholar,CAMEL: Capturing Metaphorical Alignment with Context Disentangling for Multimodal Emotion Recognition,"Zhang, Linhao; Jin, Li; Xu, Guangluan; Li, Xiaoyu; Xu, Cai; Wei, Kaiwen; Liu, Nayu; Liu, Haonan",2024,,38,,,,,,,emotion,No,No
google-scholar,A Novel Multimodal Speech Emotion Recognition System,"Asiya, UA; Kiran, VK",2022,,,,,,,,,emotion,No,No
google-scholar,DECNet: A Non-Contacting Dual-Modality Emotion Classification Network for Driver Health Monitoring,"Dong, Zhekang; Hu, Chenhao; Zhou, Shiqi; Zhu, Liyan; Wang, Junfan; Chen, Yi; Lv, Xudong; Ji, Xiaoyue",2024,,,,,,,,,emotion,No,No
google-scholar,"K-emophone: A mobile and wearable dataset with in-situ emotion, stress, and attention labels","Kang, Soowon; Choi, Woohyeok; Park, Cheul Young; Cha, Narae; Kim, Auk; Khandoker, Ahsan Habib; Hadjileontiadis, Leontios; Kim, Heepyung; Jeong, Yong; Lee, Uichin",2023,,10,,,,,,,emotion,Yes,No
google-scholar,Datasets for Automated Affect and Emotion Recognition from Cardiovascular Signals Using Artificial Intelligence—A Systematic Review,"Jemioło, Paweł; Storman, Dawid; Mamica, Maria; Szymkowski, Mateusz; Żabicka, Wioletta; Wojtaszek-Główka, Magdalena; Ligęza, Antoni",2022,,22,,,,,,,emotion,No,No
google-scholar,Tdfnet: Transformer-based deep-scale fusion network for multimodal emotion recognition,"Zhao, Zhengdao; Wang, Yuhua; Xu, Yuezhu; Zhang, Jiayuan",2023,,,,,,,,,emotion,No,No
google-scholar,EmoLLM: Multimodal Emotional Understanding Meets Large Language Models,"Yang, Qu; Ye, Mang; Du, Bo",2024,,,,,,,,,emotion,No,No
google-scholar,Group emotion detection based on social robot perception,"Quiroz, Marco; Patiño, Raquel; Diaz-Amado, José; Cardinale, Yudith",2022,,22,,,,,,,emotion,No,No
google-scholar,A Multi-modal Approach for Emotion Recognition Through the Quadrants of Valence–Arousal Plane,"Dutta, Stobak; Mishra, Brojo Kishore; Mitra, Anirban; Chakraborty, Amartya",2023,,4,,,,,,,emotion,No,No
google-scholar,Cross-language speech emotion recognition using multimodal dual attention transformers,"Zaidi, Syed Aun Muhammad; Latif, Siddique; Qadir, Junaid",2023,,,,,,,,,emotion,No,No
google-scholar,Towards Generalizable and Robust Multimodal ML models for Video Emotion Recognition,"Devulapally, Naresh Kumar",2024,,,,,,,,,emotion,No,No
google-scholar,EmoWear: Wearable Physiological and Motion Dataset for Emotion Recognition and Context Awareness,"Rahmani, Mohammad Hasan; Symons, Michelle; Sobhani, Omid; Berkvens, Rafael; Weyn, Maarten",2024,,11,,,,,,,emotion,No,No
google-scholar,Learning facial expression and body gesture visual information for video emotion recognition,"Wei, Jie; Hu, Guanyu; Yang, Xinyu; Luu, Anh Tuan; Dong, Yizhuo",2024,,237,,,,,,,emotion,No,No
google-scholar,Deep convolutional neural network for emotion recognition using EEG and peripheral physiological signal,"Lin, Wenqian; Li, Chao; Sun, Shouqian",2017,,,,,,,,,emotion,No,No
google-scholar,Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition.,"Dissanayake, Vipula; Seneviratne, Sachith; Suriyaarachchi, Hussel; Wen, Elliott; Nanayakkara, Suranga",2022,,,,,,,,,emotion,No,No
google-scholar,Research on Image-text Multimodal Emotions Analysis with Fused Emoji,"Bao, Guangbin; Sun, Liangliang; Zhang, Rui; Zhang, Bo; Shen, Zhiming; Chen, Shuang",2024,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition in valence-arousal space from multi-channel EEG data and wavelet based deep learning framework,"Garg, Divya; Verma, Gyanendra K",2020,,171,,,,,,,emotion,No,No
google-scholar,Empirical evidence relating EEG signal duration to emotion classification performance,"Pereira, Eanes Torres; Gomes, Herman Martins; Veloso, Luciana Ribeiro; Mota, Moisés Roberto A",2018,,12,,,,,,,emotion,No,Yes
google-scholar,Multimodal Emotion Recognition Based on Global Information Fusion in Conversations,"Kim, Dae Hyeon; Choi, Young-Seok",2024,,,,,,,,,emotion,No,No
google-scholar,Prediction of emotional empathy in intelligent agents to facilitate precise social interaction,"Alanazi, Saad Awadh; Shabbir, Maryam; Alshammari, Nasser; Alruwaili, Madallah; Hussain, Iftikhar; Ahmad, Fahad",2023,,13,,,,,,,emotion_and_empathy,No,No
google-scholar,Emotion Recognition in Adaptive Virtual Reality Settings: Challenges and Opportunities.,"Mousavi, Seyed Muhammad Hossein; Khaertdinov, Bulat; Jeuris, Pedro; Hortal, Enrique; Andreoletti, Davide; Giordano, Silvia",2023,,,,,,,,,emotion,No,No
google-scholar,EmoPercept: EEG-based emotion classification through perceiver,"Aadam; Tubaishat, Abdallah; Al-Obeidat, Feras; Halim, Zahid; Waqas, Muhammad; Qayum, Fawad",2022,,,,,,,,,emotion,No,No
google-scholar,Empathic: Emulating human-like multimodal personality architecture through thoughtful human-ai conversation,"Devi, Varshini; Oviya, IR; Raja, Kalpana",2024,,,,,,,,,empathy,No,No
google-scholar,Emotion recognition with short-period physiological signals using bimodal sparse autoencoders.,"Lee, Yun-Kyu; Pae, Dong-Sung; Hong, Dae-Ki; Lim, Myo-Taeg; Kang, Tae-Koo",2022,,32,,,,,,,emotion,No,No
google-scholar,Electrocardiogram-based emotion recognition systems and their applications in healthcare—a review,"Hasnul, Muhammad Anas; Aziz, Nor Azlina Ab; Alelyani, Salem; Mohana, Mohamed; Aziz, Azlan Abd",2021,,21,,,,,,,emotion,No,No
google-scholar,Attention-based word-level contextual feature extraction and cross-modality fusion for sentiment analysis and emotion classification,"Huddar, Mahesh G; Sannakki, Sanjeev S; Rajpurohit, Vijay S",2020,,8,,,,,,,emotion,No,No
google-scholar,Research Advanced in Multimodal Emotion Recognition Based on Deep Learning,"Kong, Weiqi",2024,,85,,,,,,,emotion,No,No
google-scholar,Emotion recognition from physiological signals using continuous wavelet transform and deep learning,"Jalal, Lana; Peer, Angelika",2022,,,,,,,,,emotion,No,No
google-scholar,Transformer-based multilingual speech emotion recognition using data augmentation and feature fusion,"Al-onazi, Badriyya B; Nauman, Muhammad Asif; Jahangir, Rashid; Malik, Muhmmad Mohsin; Alkhammash, Eman H; Elshewey, Ahmed M",2022,,12,,,,,,,emotion,No,No
google-scholar,Cross-corpus EEG-based emotion recognition,"Rayatdoost, Soheil; Soleymani, Mohammad",2018,,,,,,,,,emotion,No,No
google-scholar,A multibias-mitigated and sentiment knowledge enriched transformer for debiasing in multimodal conversational emotion recognition,"Wang, Jinglin; Ma, Fang; Zhang, Yazhou; Song, Dawei",2022,,,,,,,,,emotion,No,No
google-scholar,Emotion Recognition Based on Galvanic Skin Response and Photoplethysmography Signals Using Artificial Intelligence Algorithms,"Bamonte, Marcos F; Risk, Marcelo; Herrero, Víctor",2023,,,,,,,,,emotion,No,No
google-scholar,"FAF: A novel multimodal emotion recognition approach integrating face, body and text","Fang, Zhongyu; He, Aoyun; Yu, Qihui; Gao, Baopeng; Ding, Weiping; Zhang, Tong; Ma, Lei",2022,,,,,,,,,emotion,No,No
google-scholar,Conversational transfer learning for emotion recognition,"Hazarika, Devamanyu; Poria, Soujanya; Zimmermann, Roger; Mihalcea, Rada",2021,,65,,,,,,,emotion,No,No
google-scholar,Recognition of emotional states using EEG signals based on time-frequency analysis and SVM classifier.,"George, Fabian Parsia; Shaikat, Istiaque Mannafee; Ferdawoos, Prommy Sultana; Parvez, Mohammad Zavid; Uddin, Jia",2019,,9,,,,,,,emotion,No,No
google-scholar,AV-ITN: A Method of Multimodal Video Emotional Content Analysis,"Fu, Liangyu; Zhang, Qian; Wang, Rui",2022,,,,,,,,,emotion,No,No
google-scholar,"Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, and Refinement Framework","Sun, Haoqin; Zhao, Shiwan; Li, Shaokai; Kong, Xiangyu; Wang, Xuechen; Kong, Aobo; Zhou, Jiaming; Chen, Yong; Zeng, Wenjia; Qin, Yong",2024,,,,,,,,,emotion,No,No
google-scholar,A Review of Emotion Recognition Based on EEG using DEAP Dataset,"Chaudhary, Rama; Jaswal, Ram Avtar",2021,,,,,,,,,emotion,No,No
google-scholar,Deep Learning Approaches for Effective Human Computer Interaction: A Comprehensive Survey on Single and Multimodal Emotion Detection,"Jagadeesh, M; Viswanathan, Shruthi; Varadarajan, Shruthi",2024,,,,,,,,,emotion,No,No
google-scholar,EEG-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network,"Cui, Heng; Liu, Aiping; Zhang, Xu; Chen, Xiang; Wang, Kongqiao; Chen, Xun",2020,,205,,,,,,,emotion,No,No
google-scholar,Personalized emotion recognition by personality-aware high-order learning of physiological signals,"Zhao, Sicheng; Gholaminejad, Amir; Ding, Guiguang; Gao, Yue; Han, Jungong; Keutzer, Kurt",2019,,15,,,,,,,emotion,No,No
google-scholar,A survey on physiological signal-based emotion recognition,"Ahmad, Zeeshan; Khan, Naimul",2022,,9,,,,,,,emotion,No,No
google-scholar,EEG-based emotion recognition with deep convolutional neural networks,"Ozdemir, Mehmet Akif; Degirmenci, Murside; Izci, Elif; Akan, Aydin",2021,,66,,,,,,,emotion,No,No
google-scholar,Analysis of emotion annotation strength improves generalization in speech emotion recognition models,"Palotti, Joao; Narula, Gagan; Raheem, Lekan; Bay, Herbert",2023,,,,,,,,,emotion,No,No
google-scholar,Multimodal Fusion via Hypergraph Autoencoder and Contrastive Learning for Emotion Recognition in Conversation,"Yi, Zijian; Zhao, Ziming; Shen, Zhishu; Zhang, Tiehua",2024,,,,,,,,,emotion,No,No
google-scholar,FedCMD: A Federated Cross-modal Knowledge Distillation for Drivers’ Emotion Recognition,"Bano, Saira; Tonellotto, Nicola; Cassarà, Pietro; Gotta, Alberto",2024,,15,,,,,,,emotion,No,No
google-scholar,Deep learning-based categorical and dimensional emotion recognition for written and spoken text,"Atmaja, Bagus Tris; Akagi, Masato",2019,,,,,,,,,emotion,No,No
google-scholar,MM-SPCL for Multimodal Emotion Recognition in Conversation,"Deviyani, Athiya; Khan, Abuzar; Skarphedinsson, Neil; Varshney, Prasoon",,,,,,,,,,emotion,No,No
google-scholar,Analysis of EEG Signals in the DEAP Dataset for Emotion Recognition using Deep Learning Algortihms,"Gaddanakeri, Ranjita D; Naik, Manavi M; Kulkarni, Sujata; Patil, Prakashgoud",2024,,,,,,,,,emotion,No,No
google-scholar,Residual-based graph convolutional network for emotion recognition in conversation for smart Internet of Things,"Choi, Young-Ju; Lee, Young-Woon; Kim, Byung-Gyu",2021,,9,,,,,,,emotion,No,No
google-scholar,A Review of multi-modal speech emotion recognition and various techniques used to solve emotion recognition on speech data,"Nanduri, Venkata Naga Pavani Sai Suchitra; Sagiri, Chinmai; Manasa, S Satya Siva; Sanvithatesh, Raavi; Ashwin, M",2023,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition based on photoplethysmogram and electroencephalogram,"Tong, Zhongkai; Chen, XianXiang; He, Zhengling; Tong, Kai; Fang, Zhen; Wang, Xianlong",2018,,2,,,,,,,emotion,No,No
google-scholar,Emotion recognition during social interactions using peripheral physiological signals,"Gupta, Priyansh; Balaji, S Ashwin; Jain, Sambhav; Yadav, RK",2022,,,,,,,,,emotion,No,No
google-scholar,EEG-based emotion classification using deep neural network,"Sallout, Mohammed; Mattar, Ebrahim",2021,,2021,,,,,,,emotion,No,No
google-scholar,"Impact of multiple modalities on emotion recognition: investigation into 3d facial landmarks, action units, and physiological data","Fabiano, Diego; Jaishanker, Manikandan; Canavan, Shaun",2020,,,,,,,,,emotion,No,No
google-scholar,Analysis and Emotion Recognition of Educational Network New Media Images Based on Deep Learning.,"Zeng, Yuhan",2024,,41,,,,,,,emotion,No,No
google-scholar,Fine-grained emotion recognition from eeg signal using fast fourier transformation and cnn,"Hasan, Mahmudul; Yasmin, Samina; Pias, Tanmoy Sarkar",2021,,,,,,,,,emotion,No,No
google-scholar,DGNN: Dependency Graph Neural Network for Multimodal Emotion Recognition in Conversation,"Zhang, Zhen; Wang, Xin; Yuan, Lifeng; Miao, Gongxun; Liu, Mengqiu; Yun, Wenhao; Wu, Guohua",2023,,,,,,,,,emotion,No,No
google-scholar,Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition,"Zhang, Su; Tang, Chuangao; Guan, Cuntai",2022,,130,,,,,,,emotion,No,No
google-scholar,Emotion classification using nature based optimization with transformers and transfer learning,"Goel, Swati",2022,,,,,,,,,emotion,No,No
google-scholar,Emvas: End-to-End Multimodal Emotion Visualization Analysis System,"Zhu, Xianxun; Feng, Heyang; Guo, Chaopeng; Fan, Xuhui; Huang, Jingze; Wang, Rui",,,,,,,,,,emotion,No,No
google-scholar,Merged LSTM Model for emotion classification using EEG signals,"Garg, Anumit; Kapoor, Ashna; Bedi, Anterpreet Kaur; Sunkaria, Ramesh K",2019,,,,,,,,,emotion,No,No
google-scholar,DialogueINAB: an interaction neural network based on attitudes and behaviors of interlocutors for dialogue emotion recognition,"Ding, Junyuan; Chen, Xiaoliang; Lu, Peng; Yang, Zaiyan; Li, Xianyong; Du, Yajun",2023,,79,,,,,,,emotion,No,No
google-scholar,A Novel Experiment Setting for Cross-subject Emotion Recognition,"Hu, Hao-Yi; Zhao, Li-Ming; Liu, Yu-Zhong; Li, Hua-Liang; Lu, Bao-Liang",2021,,,,,,,,,emotion,No,No
google-scholar,A new deep convolutional neural network incorporating attentional mechanisms for ECG emotion recognition,"Fan, Tianqi; Qiu, Sen; Wang, Zhelong; Zhao, Hongyu; Jiang, Junhan; Wang, Yongzhen; Xu, Junnan; Sun, Tao; Jiang, Nan",2023,,159,,,,,,,emotion,No,No
google-scholar,Investigating patterns for self-induced emotion recognition from EEG signals,"Zhuang, Ning; Zeng, Ying; Yang, Kai; Zhang, Chi; Tong, Li; Yan, Bin",2018,,18,,,,,,,emotion,No,No
google-scholar,LR-GCN: Latent relation-aware graph convolutional network for conversational emotion recognition,"Ren, Minjie; Huang, Xiangdong; Li, Wenhui; Song, Dan; Nie, Weizhi",2021,,24,,,,,,,emotion,No,No
google-scholar,"A Systematic Literature Review of Modalities, Trends, and Limitations in Emotion Recognition, Affective Computing, and Sentiment Analysis","García-Hernández, Rosa A; Luna-García, Huizilopoztli; Celaya-Padilla, José M; García-Hernández, Alejandra; Reveles-Gómez, Luis C; Flores-Chaires, Luis Alberto; Delgado-Contreras, J Ruben; Rondon, David; Villalba-Condori, Klinge O",2024,,14,,,,,,,emotion,No,No
google-scholar,Online multi-hypergraph fusion learning for cross-subject emotion recognition,"Pan, Tongjie; Ye, Yalan; Zhang, Yangwuyong; Xiao, Kunshu; Cai, Hecheng",2024,,108,,,,,,,emotion,No,No
google-scholar,A deep-learning model for subject-independent human emotion recognition using electrodermal activity sensors,"Al Machot, Fadi; Elmachot, Ali; Ali, Mouhannad; Al Machot, Elyan; Kyamakya, Kyandoghere",2019,,19,,,,,,,emotion,No,No
google-scholar,Emotion Recognition in Conversation Based on a Dynamic Complementary Graph Convolutional Network,"Yang, Zhenyu; Li, Xiaoyang; Cheng, Yuhu; Zhang, Tong; Wang, Xuesong",2024,,,,,,,,,emotion,No,No
google-scholar,EEG-based emotion recognition in an immersive virtual reality environment: From local activity to brain network features,"Yu, Minchang; Xiao, Shasha; Hua, Minlei; Wang, Hui; Chen, Xi; Tian, Feng; Li, Yingjie",2022,,72,,,,,,,emotion,No,No
google-scholar,Automated feature extraction on AsMap for emotion classification using EEG,"Ahmed, Md Zaved Iqubal; Sinha, Nidul; Phadikar, Souvik; Ghaderpour, Ebrahim",2022,,22,,,,,,,emotion,No,No
google-scholar,Evaluation of galvanic skin response (GSR) signals features for emotion recognition,"Kipli, Kuryati; Latip, Aisya Amelia Abdul; Lias, Kasumawati; Bateni, Norazlina; Yusoff, Salmah Mohamad; Suud, Jamaah; Jalil, Muhammad Arif; Ray, Kanad; Kaiser, M Shamim; Mahmud, Mufti",2022,,,,,,,,,emotion,No,No
google-scholar,Automatic annotation of corpora for emotion recognition through facial expressions analysis,"Diamantini, Claudia; Mircoli, Alex; Potena, Domenico; Storti, Emanuele",2021,,,,,,,,,emotion,No,No
google-scholar,Human emotion recognition models using machine learning techniques,"Alam, Aftab; Urooj, Shabana; Ansari, Abdul Quaiyum",2023,,,,,,,,,emotion,No,No
google-scholar,Emotion Recognition Using EEG Signals: Accuracy Comparison Between Methods and Frequency Bands,"Nazemi, Hamed; Taheri, Alireza; Meghdari, Ali; Boroushaki, Mehrdad; Ghazizadeh, Ali",2021,,,,,,,,,emotion,No,Yes
google-scholar,"Survey of deep emotion recognition in dynamic data using facial, speech and textual cues","Zhang, Tao; Tan, Zhenhua",2024,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition based on EEG feature maps through deep learning network,"Topic, Ante; Russo, Mladen",2021,,24,,,,,,,emotion,No,No
google-scholar,Emotion Recognition: An Integration of Different Perspectives,"Stratton, Derek",2022,,,,,,,,,emotion,No,No
google-scholar,EEG brainwave emotion detection using stacked ensembling method,"Jain, Vansh; Parab, Kshitii; Kalgutkar, Sharvari; Sonkusare, Reena",2021,,,,,,,,,emotion,No,No
google-scholar,Survey on Emotion Recognition Databases,"Hong, Juyoung; Hwang, Yujin; Lee, Gwangjin; Choi, Yukyung",2022,,,,,,,,,emotion,No,No
google-scholar,A brief survey of machine learning methods for emotion prediction using physiological data,"Khalid, Maryam; Willis, Emily",2022,,,,,,,,,emotion,No,No
google-scholar,Emotion classification based on biophysical signals and machine learning techniques,"Bălan, Oana; Moise, Gabriela; Petrescu, Livia; Moldoveanu, Alin; Leordeanu, Marius; Moldoveanu, Florica",2019,,12,,,,,,,emotion,No,No
google-scholar,Extending Multimodal Emotion Recognition with Biological Signals: Presenting a Novel Dataset and Recent Findings,"Baird, Alice",2020,,,,,,,,,emotion,No,No
google-scholar,XED: A multilingual dataset for sentiment analysis and emotion detection,"Öhman, Emily; Pàmies, Marc; Kajava, Kaisla; Tiedemann, Jörg",2020,,,,,,,,,emotion,No,No
google-scholar,LSSED: a large-scale dataset and benchmark for speech emotion recognition,"Fan, Weiquan; Xu, Xiangmin; Xing, Xiaofen; Chen, Weidong; Huang, Dongyan",2021,,,,,,,,,emotion,No,No
google-scholar,imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis,"Liu, Xin; Shi, Henglin; Chen, Haoyu; Yu, Zitong; Li, Xiaobai; Zhao, Guoying",2021,,,,,,,,,emotion,No,No
google-scholar,Building a large scale dataset for image emotion recognition: The fine print and the benchmark,"You, Quanzeng; Luo, Jiebo; Jin, Hailin; Yang, Jianchao",2016,,30,,,,,,,emotion,No,No
google-scholar,A comparative analysis of emotion-detecting AI systems with respect to algorithm performance and dataset diversity,"Bryant, De'Aira; Howard, Ayanna",2019,,,,,,,,,emotion,No,No
google-scholar,AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition,"Lian, Zheng; Sun, Haiyang; Sun, Licai; Yi, Jiangyan; Liu, Bin; Tao, Jianhua",2024,,,,,,,,,emotion,No,No
google-scholar,A dataset for emotion recognition using virtual reality and EEG (DER-VREEG): Emotional state classification using low-cost wearable VR-EEG headsets,"Suhaimi, Nazmi Sofian; Mountstephens, James; Teo, Jason",2022,,6,,,,,,,emotion,No,No
google-scholar,Head fusion: Improving the accuracy and robustness of speech emotion recognition on the IEMOCAP and RAVDESS dataset,"Xu, Mingke; Zhang, Fan; Zhang, Wei",2021,,9,,,,,,,emotion,No,Yes
google-scholar,A Review of Advancements in Driver Emotion Detection: Deep Learning Approaches and Dataset Analysis,"Alfaras, Mohammed Shukur; Karan, Oğuz",2024,,,,,,,,,emotion,No,No
google-scholar,Text mining and emotion classification on monkeypox Twitter dataset: A deep learning-natural language processing (NLP) approach,"Olusegun, Ruth; Oladunni, Timothy; Audu, Halima; Houkpati, YAO; Bengesi, Staphord",2023,,11,,,,,,,emotion,No,No
google-scholar,A new Amharic speech emotion dataset and classification benchmark,"Retta, Ephrem Afele; Almekhlafi, Eiad; Sutcliffe, Richard; Mhamed, Mustafa; Ali, Haider; Feng, Jun",2023,,22,,,,,,,emotion,No,No
google-scholar,"Odyssey 2024-Speech Emotion Recognition Challenge: Dataset, Baseline Framework, and Results","Goncalves, Lucas; Salman, Ali N; Naini, Abinay R; Velazquez, Laureano Moro; Thebaud, Thomas; Garcia, Leibny Paola; Dehak, Najim; Sisman, Berrak; Busso, Carlos",2024,,10,,,,,,,emotion,No,No
google-scholar,Emotion classification using 1D-CNN and RNN based on deap dataset,"Zamani, Farhad; Wulansari, Retno",2021,,,,,,,,,emotion,No,No
google-scholar,The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition,"Zhang, Enshi; Trujillo, Rafael; Poellabauer, Christian",2024,,,,,,,,,emotion,No,No
google-scholar,Analysis and recognition of textual emotion for Twitter dataset using transfer learning,"Guleria, Nidhi; Kumar, Rakesh",2022,,2555,,,,,,,emotion,No,No
google-scholar,"Exploring Thermography Technology: A Comprehensive Facial Dataset for Face Detection, Recognition, and Emotion","Abuhussein, Mohamed Fawzi Abdelshafie; Darwish, Ashraf; Hassanien, Aboul Ella",2024,,,,,,,,,emotion,No,No
google-scholar,Emotion recognition on large video dataset based on convolutional feature extractor and recurrent neural network,"Rangulov, Denis; Fahim, Muhammad",2020,,,,,,,,,emotion,No,No
google-scholar,Memotion 3: Dataset on sentiment and emotion analysis of codemixed hindi-english memes,"Mishra, Shreyash; Suryavardan, S; Patwa, Parth; Chakraborty, Megha; Rani, Anku; Reganti, Aishwarya; Chadha, Aman; Das, Amitava; Sheth, Amit; Chinnakotla, Manoj",2023,,,,,,,,,emotion,No,No
google-scholar,Joint domain symmetry and predictive balance for cross-dataset EEG emotion recognition,"Jiang, Haiting; Shen, Fangyao; Chen, Lina; Peng, Yong; Guo, Hongjie; Gao, Hong",2023,,400,,,,,,,emotion,No,No
google-scholar,Optimizing Speech Emotion Recognition with Deep Learning and Grey Wolf Optimization: A Multi-Dataset Approach,"Tyagi, Suryakant; Szénási, Sándor",2024,,17,,,,,,,emotion,No,No
google-scholar,MINDS: A Multi-label Emotion and Sentiment Classification Dataset Related to COVID-19.,"Bhardwaj, Anjali; Abulaish, Muhammad",2022,,,,,,,,,emotion,Yes,No
google-scholar,ML-SocMedEmot: Machine Learning Event-based Social Media Emotion Detection Proactive Framework Addressing Mental Health: A Novel Twitter Dataset and Case Study of COVID-19,"Ismail, Leila; Shahin, Nada; Materwala, Huned; Hennebelle, Alain; Frermann, Lea",2023,,,,,,,,,emotion,No,No
google-scholar,"Machine learning applied in emotion classification: a survey on dataset, techniques, and trends for text based documents","Chavan, Disha; Anvekar, Esha; Dandapat, Megha; Bichave, Vaibhav; Jagdale, Jayashree",2023,,1,,,,,,,emotion,No,No
google-scholar,Emotion action detection and emotion inference: the task and dataset,"Liu, Pengyuan; Du, Chengyu; Zhao, Shuofeng; Zhu, Chenghao",2019,,,,,,,,,emotion,No,No
google-scholar,facial emotion recognition in static and live streaming image dataset using CNN,"Seal, Aishani; Saha, Ranita; Kumar, Rishav; Goenka, Subham; Dey, Lopamudra",2022,,,,,,,,,emotion,No,No
google-scholar,Single Modality and Joint Fusion for Emotion Recognition on RAVDESS Dataset,"Haddad, Syrine; Daassi, Olfa; Belghith, Safya",2024,,5,,,,,,,emotion,No,No
google-scholar,UniC: a Dataset for Emotion Analysis of Videos with Multimodal and Unimodal Labels,"Du, Quanqi; Labat, Sofie; Demeester, Thomas; Hoste, Veronique",2024,,,,,,,,,emotion,Yes,No
google-scholar,Emotion analysis using machine learning model and deep learning model on DEAP dataset,"Hasan, Anita; Abrar, Fahim; Sabur, Eshaan Tanzim; Muntasir, Iftehaj",2021,,,,,,,,,emotion,No,No
google-scholar,Evaluation of Large Tweet Dataset for Emotion Detection Model: A Comparative Study between Various ML and Transformer,"Lee, Sanghyub John; Lim, JongYoon; Paas, Leo; Ahn, Ho Seok",2023,,,,,,,,,emotion,No,No
google-scholar,Cross-dataset facial expression recognition based on arousal-valence emotion model and transfer learning method,"Yang, Yong; Liu, Chuan; Wu, Qingshan",2017,,,,,,,,,emotion,No,No
google-scholar,Speech Emotion Recognition Using Energies in six bands and Multilayer Perceptron on RAVDESS Dataset,"Agrima, Abdellah; Barakat, Aziza; Mounir, Ilham; Farchi, Abdelmajid; ElMazouzi, Laila; Mounir, Badia",2022,,,,,,,,,emotion,No,No
google-scholar,Sentiment Analysis on'HelloTalk'App Reviews Using NRC Emotion Lexicon and GoEmotions Dataset,"Akar, Simay; Kim, Yang Sok; Noh, Mi Jin",2024,,13,,,,,,,emotion,No,No
google-scholar,Human Emotion Classification using KNN Classifier and Recurrent Neural Networks with Seed Dataset,"Satyanarayana, KN V; Tejasri, V; Srujitha, YS Naga; Mounisha, K Nitya Sai; Yerramsetti, Sai Tejasri; Darapu, Gayathri Devi",2022,,,,,,,,,emotion,No,No
google-scholar,Exploring Emotion Recognition with a Multi-Scale fNIRS Dataset: A Novel Approach Integrating Statistical Information and Cross-Channel Attention,"Shen, Qiao; Jin, Jianxiu; Tie, Qianfeng; Zeng, Zhejun; Shu, Lin; Xu, Xiangmin",2024,,,,,,,,,emotion,No,No
google-scholar,SFEW Dataset Based Facial Emotion Classification Using BiDirectional Neural Network and Convolutional Neural Network,"Wu, Chujie",,,,,,,,,,emotion,No,No
google-scholar,Comparative Wavelet and MFCC Speech Emotion Recognition Experiments on the RAVDESS Dataset,"Bajaj, Aayush; Jha, Abhishek; Vashisth, Lakshay; Tripathi, KC",2022,,71,,,,,,,emotion,No,No
google-scholar,"Multilingual, Cross-lingual, and Monolingual Speech Emotion Recognition on EmoFilm Dataset","Atmaja, Bagus Tris; Sasou, Akira",2023,,,,,,,,,emotion,No,No
google-scholar,Enhancing Speech Emotion Recognition Through Advanced Feature Extraction and Deep Learning: A Study Using the RAVDESS Dataset,"Ankitha, SM; AshwinKumar, UM",2024,,,,,,,,,emotion,No,No
google-scholar,Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning,"Wang, Kaipeng; Jing, Zhi; Su, Yongye; Han, Yikun",2024,,,,,,,,,emotion,No,No
google-scholar,RideKE: Leveraging Low-resource Twitter User-generated Content for Sentiment and Emotion Detection on Code-switched RHS Dataset.,"Etori, Naome; Gini, Maria",2024,,,,,,,,,emotion,No,No
google-scholar,Speech Emotion Recognition Using Fully Convolutional Network and Augmented RAVDESS Dataset,"Singh, Vandana; Prasad, Swati",2023,,,,,,,,,emotion,No,No
google-scholar,Emotion classification of restaurant and laptop review dataset: Semeval 2014 task 4,"Kirange, DK; Deshmukh, Ratnadeep R",2015,,113,,,,,,,emotion,No,No
google-scholar,A Comparative Study of Content Dependent and Independent Emotion Recognition using Convolutional Neural Network Based on DEAP Dataset,"Huang, Zhiying; Guo, Ao; Ma, Jianhua",2024,,,,,,,,,emotion,No,No
google-scholar,Emotion Recognition based on PPG and GSR Signals using DEAP Dataset,"Shubha, B; Poornima, N; Gowda, Vachana M; Sushma, U; Meghana, YR; Bhoomika, TS",2023,,,,,,,,,emotion,No,No
google-scholar,A Novel Machine Learning Approach for Classification of Emotion and Polarity in Sentiment140 Dataset,"Behera, Rabi Narayan; Roy, Manan; Dash, Sujata",2016,,,,,,,,,emotion,No,No
google-scholar,Emotion Classification Using Optimized Features and Ensemble Learning Techniques for EEG Dataset,"Bharkavi, S Dhivya; Kavitha, S; Harini, M; Kumar, MV Harish",2023,,,,,,,,,emotion,No,No
google-scholar,Emotion Classification and Input Encoding Technique Analysis using SFEW Dataset,"Fu, Chunze",,,,,,,,,,emotion,No,No
google-scholar,Transformation of EEG signal for emotion analysis and dataset construction for DNN learning,"Kwon, Yeahoon; Nan, Yiyan; Kim, Shin-Dug",2018,,,,,,,,,emotion,No,No
google-scholar,Emotion Recognition in Virtual Reality: Creation and validation of a VR-based multi-modal emotion recognition dataset,"Regmi, Bishwas",2024,,,,,,,,,emotion,No,No
google-scholar,RESEARCH OF EMOTION DETECT PROCESS IN THE CONCEPT OF PREPROCESSING OF DATASET AND CLASSIFICATION BASED ON THE FEATURES,"Неуймин, ЕВ; Александров, АС; Багрянцев, АА; Максименко, ДМ; Богданов, СС",,,,,,,,,,emotion,No,No
google-scholar,Introducing a Novel Dataset for Facial Emotion Recognition and Demonstrating Significant Enhancements in Deep Learning Performance Through Preprocessing Techniques,"Yalcin, Nursel; Alisawi, Muthana",,,,,,,,,,emotion,No,Yes
proquest,"Affective Human-Machine Interfaces: Towards Multi-lingual, Environment-Robust Emotion Detection from Speech","Kshirsagar, Shruti Rajendra",2022,,,,,,,,,emotion,No,No
proquest,"COVID-19 Twitter dataset with latent topics, sentiments and emotions attributes","Gupta, Raj Kumar; Vishwanath, Ajay; Yang, Yinping",2020,,,,,,,,,emotion,No,No
proquest,Altering emotions near the hand: Approach–avoidance swipe interactions modulate the perceived valence of emotional pictures.,"Cervera-Torres, Sergio; Ruiz Fernández, Susana; Lachmair, Martin; Riekert, Matthias; Gerjets, Peter",2021,,21,,,,,,,emotion,No,No
proquest,Reliability generalization of tasks and recommendations for assessing the ability to perceive facial expressions of emotion.,"Olderbak, Sally; Riggenmann, Olesia; Wilhelm, Oliver; Doebler, Philipp",2021,,33,,,,,,,emotion,No,No
proquest,UMEME: University of Michigan Emotional,"Set, McGurk Effect Data",2015,,,,,,,,,emotion,No,No
proquest,Emotions in Spoken Language-Do we need acoustics?,"Probol, Nadine; Mieskes, Margot",2023,,,,,,,,,emotion,No,No
proquest,Multimodal emotion recognition by extracting common and modality-specific information,"Zhang, Wei; Gu, Weixi; Ma, Fei; Ni, Shiguang; Zhang, Lin; Huang, Shao-Lun",2018,,,,,,,,,emotion,No,No
proquest,A Method of Multimodal Emotion Recognition in Video Learning Based on Knowledge Enhancement.,"Ye, Hanmin; Zhou, Yinghui; Tao, Xiaomei",2023,,47,,,,,,,emotion,No,No
proquest,[Retracted] Analyzing the Role of Emotional Intelligence on the Performance of Small and Medium Enterprises (SMEs) Using AI‐Based Convolutional Neural Networks (CNNs),"Serbaya, Suhail H",2022,,2022,,,,,,,emotion,No,No
proquest,Evaluating significant features in context‐aware multimodal emotion recognition with XAI methods,"Khalane, Aaishwarya; Makwana, Rikesh; Shaikh, Talal; Ullah, Abrar",2023,,,,,,,,,emotion,No,No
acl,{K}orean {T}witter Emotion Classification Using Automatically Built Emotion Lexicons and Fine-Grained Features,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Distant-supervised Language Model for Detecting Emotional Upsurge on {T}witter,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SWAT}-{CMW}: Classification of {T}witter Emotional Polarity using a Multiple-Classifier Decision Schema and Enhanced Emotion Tagging,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sentence-level Emotion Classification with Label and Context Dependence,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Corpus Fusion for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Identification and Classification of Emotional Key Phrases from Psychological Texts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Linguistic Template Extraction for Recognizing Reader-Emotion and Emotional Resonance Writing Assistance,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Computational Analysis of Affect and Emotion in Language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Segment-based Fine-grained Emotion Detection for {C}hinese Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Real Time Early-stage Influenza Detection with Emotion Factors from Sina Microblog,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,結合非線性動態特徵之語音情緒辨識(Speech Emotion Recognition via Nonlinear Dynamical Features)[In {C}hinese],,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{JEAM}: A Novel Model for Cross-Domain Sentiment Classification Based on Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Detection from text: A Survey,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,The (Un)Predictability of Emotional Hashtags in {T}witter,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,台灣情緒語料庫建置與辨識 (An Emotional Speech Database in {T}aiwan: Collection and Recognition) [In {C}hinese],,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Annotating Events in an Emotion Corpus,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Comparison of Gender- and Speaker-adaptive Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Speech-Based Emotion Recognition: Feature Selection by Self-Adaptive Multi-Criteria Genetic Algorithm,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EMOVO} Corpus: an {I}talian Emotional Speech Database,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Exploiting Community Emotion for Microblog Event Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Classifying Idiomatic and Literal Expressions Using Topic Models and Intensity of Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Expanding the Range of Automatic Emotion Detection in Microblogging Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Fine-Grained Emotion Recognition in Olympic Tweets Based on Human Computation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,On the Impact of Sentiment and Emotion Based Features in Detecting Online Sexual Predators,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Portable Features for Classifying Emotional Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Mining Sentiment Words from Microblogs for Predicting Writer-Reader Emotion Transition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mpa{T}weet: Annotating and Detecting Emotions on {T}witter,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Predicting and Eliciting Addressee{'}s Emotion in Online Dialogue,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Multilingual Natural Stress Emotion Database,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Parallel Corpus of Music and Lyrics Annotated with Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Building a Multimodal Laughter Database for Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards Emotion and Affect Detection in the Multimodal {LAST} {MINUTE} Corpus,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,D{\'e}tection d{'}{\'e}motions dans la voix de patients en interaction avec un agent conversationnel anim{\'e} (Emotions detection in the voice of patients interacting with an animated conversational agent) [in {F}rench],,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Experimenting with Distant Supervision for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A hierarchical approach with feature selection for emotion recognition from speech,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Extending the {E}moti{N}et Knowledge Base to Improve the Automatic Detection of Implicitly Expressed Emotions from Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Automatic Emotion Classification for Interpersonal Communication,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,$F^2$ - New Technique for Recognition of User Emotional States in Spoken Dialogue Systems,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Evaluation of Unsupervised Emotion Models to Textual Affect Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Building a System for Emotions Detection from Speech to Control an Affective Avatar,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Hybrid Approach to Emotional Sentence Polarity and Intensity Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Detection in Email Customer Care,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Text-driven Rule-based System for Emotion Cause Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Hierarchical versus Flat Classification of Emotions in Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Cause Events: Corpus Construction and Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CINEMO} {---} A {F}rench Spoken Language Resource for Complex Emotions: Facts and Baselines,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Automatic Annotation of Word Emotion in Sentences Based on {R}en-{CEC}ps,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Cause Detection with Linguistic Constructions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Exploration of Features for Recognizing Word Emotion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Classification of Emotion Words in {R}ussian and {R}omanian Languages,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Speech Emotion Recognition With {TGI}+.2 Classifier,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Recognition from Speech: Stress Experiment,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Coding Emotional Events in Audiovisual Corpora,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Real-World Emotional Speech Corpus for {M}odern {G}reek,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Classification Using Massive Examples Extracted from the Web,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UA}-{ZBSA}: A Headline Emotion Classification through Web Information,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Manifolds Based Emotion Recognition in Speech,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Subjective Evaluation of an Emotional Speech Database for {B}asque,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Using {R}oget{'}s Thesaurus for Fine-grained Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Recognition from Speech Using {IG}-Based Feature Compensation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,以部落格文本進行情緒分類之研究 (A Study of Emotion Classification Using Blog Articles) [In {C}hinese],,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Detecting Emotion in Speech: Experiments in Three Domains,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Improving Automatic Emotion Recognition from Speech via Gender Differentiaion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Creation of a corpus of multimodal spontaneous expressions of emotions in Human-Machine Interaction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotional Recognition Using a Compensation Transformation in Speech Signal,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Fear-type emotions of the {SAFE} Corpus: annotation issues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotions from Text: Machine Learning for Text-based Emotion Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Co-training for Predicting Emotions with Spoken Dialogue Data,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-Modal Emotion Recognition from Speech and Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Detecting Emotions in {M}andarin Speech,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Recognition and Evaluation of {M}andarin Speech Using Weighted {D}-{KNN} Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Predicting Student Emotions in Computer-Human Tutoring Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Curtin {OCAI} at {WASSA} 2023 Empathy, Emotion and Personality Shared Task: Demographic-Aware Prediction Using Multiple Transformers",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,{NCUEE}-{NLP} at {WASSA} 2023 Shared Task 1: Empathy and Emotion Prediction Using Sentiment-Enhanced {R}o{BERT}a Transformers,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"Domain Transfer for Empathy, Distress, and Personality Prediction",,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,"Converge at {WASSA} 2023 Empathy, Emotion and Personality Shared Task: A Transformer-based Approach for Multi-Label Emotion Classification",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"Team Bias Busters at {WASSA} 2023 Empathy, Emotion and Personality Shared Task: Emotion Detection with Generative Pretrained Transformers",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"Leveraging Empathy, Distress, and Emotion for Accurate Personality Subtyping from Complex Human Textual Responses",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"{S}oul{C}hat: Improving {LLM}s{'} Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations",,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Global-Local Modeling with Prompt-Based Knowledge Enhancement for Emotion Inference in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Empathy Intent Drives Empathy Detection,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,A Unified Framework for Emotion Identification and Generation in Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{HIT}-{SCIR} at {WASSA} 2023: Empathy and Emotion Analysis at the Utterance-Level and the Essay-Level,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Investigating Stylistic Profiles for the Task of Empathy Classification in Medical Narrative Essays,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Predicting Empathic Accuracy from User-Designer Interviews,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Empathy and Distress Prediction using Transformer Multi-output Regression and Emotion Analysis with an Ensemble of Supervised and Zero-Shot Learning Models,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Transformer-based Architecture for Empathy Prediction and Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,The uncivil empathy: Investigating the relation between empathy and toxicity in online mental health support forums,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,"{WASSA} 2022 Shared Task: Predicting Empathy, Emotion and Personality in Reaction to News Stories",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,{IUCL} at {WASSA} 2022 Shared Task: A Text-only Approach to Empathy and Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Continuing Pre-trained Model with Multiple Training Strategies for Emotional Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Team {IITP}-{AINLPML} at {WASSA} 2022: Empathy Detection, Emotion Classification and Personality Detection",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Empathic Machines: Using Intermediate Features as Levers to Emulate Emotions in Text-To-Speech Systems,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Enriching Deep Learning with Frame Semantics for Empathy Classification in Medical Narrative Essays,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,"{SURREY}-{CTS}-{NLP} at {WASSA}2022: An Experiment of Discourse and Sentiment Analysis for the Prediction of Empathy, Distress and Emotion",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,An Ensemble Approach to Detect Emotions at an Essay Level,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CAISA} at {WASSA} 2022: Adapter-Tuning for Empathy Prediction,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,{MMDAG}: Multimodal Directed Acyclic Graph Network for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Calibrating Student Models for Emotion-related Tasks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Using Extracted Emotion Cause to Improve Content-Relevance for Empathetic Conversation Generation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Emotion-based {K}orean Multimodal Empathetic Dialogue System,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{P}et{K}az at {S}em{E}val-2024 Task 3: Advancing Emotion Classification with an {LLM} for Emotion-Cause Pair Extraction in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{V}erba{N}ex{AI} Lab at {S}em{E}val-2024 Task 10: Emotion recognition and reasoning in mixed-coded conversations based on an {NRC} {VAD} approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UCSC} {NLP} at {S}em{E}val-2024 Task 10: Emotion Discovery and Reasoning its Flip in Conversation ({ED}i{R}e{F}),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{AIMA} at {S}em{E}val-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{L}ast{R}esort at {S}em{E}val-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IASBS} at {S}em{E}val-2024 Task 10: Delving into Emotion Discovery and Reasoning in Code-Mixed Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{V}erba{N}ex{AI} Lab at {S}em{E}val-2024 Task 3: Deciphering emotional causality in conversations using multimodal analysis approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UIC} {NLP} {GRADS} at {S}em{E}val-2024 Task 3: Two-Step Disjoint Modeling for Emotion-Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NUS}-Emo at {S}em{E}val-2024 Task 3: Instruction-Tuning {LLM} for Multimodal Emotion-Cause Analysis in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IITK} at {S}em{E}val-2024 Task 10: Who is the speaker? Improving Emotion Recognition and Flip Reasoning in Conversations via Speaker Embeddings,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}em{E}val-2024 Task 3: Multimodal Emotion Cause Analysis in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emo-Gen {BART} - A Multitask Emotion-Informed Dialogue Generation Framework,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Automated Emotion Annotation of {F}innish Parliamentary Speeches Using {GPT}-4,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{AIMA} at {S}em{E}val-2024 Task 10: History-Based Emotion Recognition in {H}indi-{E}nglish Code-Mixed Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}em{E}val 2024 - Task 10: Emotion Discovery and Reasoning its Flip in Conversation ({ED}i{R}e{F}),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural Language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{``}You are an expert annotator{''}: Automatic Best{--}Worst-Scaling Annotations for Emotion Intensity Modeling,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,How to Annotate Emotions in Historical {I}talian Novels: A Case Study on {I} Promessi Sposi,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}el{ME}: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{A}cn{E}mpathize: A Dataset for Understanding Empathy in Dermatology Conversations,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,{ASEM}: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Beyond Linguistic Cues: Fine-grained Conversational Emotion Recognition via Belief-Desire Modelling,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CEPT}: A Contrast-Enhanced Prompt-Tuning Framework for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Deciphering Emotional Landscapes in the {I}liad: A Novel {F}rench-Annotated Dataset for Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,{T}artu{NLP} at {E}va{L}atin 2024: Emotion Polarity Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{PVG} at {WASSA} 2021: A Multi-Input, Multi-Task, Transformer-Based Architecture for Empathy and Distress Prediction",,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Distilling Knowledge for Empathy Detection,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,{WASSA} 2021 Shared Task: Predicting Empathy and Emotion in Reaction to News Stories,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,{WASSA}@{IITK} at {WASSA} 2021: Multi-task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"{E}mp{N}a at {WASSA} 2021: A Lightweight Model for the Prediction of Empathy, Distress and Emotions from Reactions to News Stories",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Team Phoenix at {WASSA} 2021: Emotion Analysis on News Stories with Pre-Trained Language Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mpath{BERT}: A {BERT}-based Framework for Demographic-aware Empathy Prediction,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Enhancing Cognitive Models of Emotions with Representation Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Supporting Cognitive and Emotional Empathic Writing of Students,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Relation between Degree of Empathy for Narrative Speech and Type of Responsive Utterance in Attentive Listening,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{MMGCN}: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{D}ialogue{CRN}: Contextual Reasoning Networks for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Contextualized Emotion Recognition in Conversation as Sequence Tagging,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Learning Word Ratings for Empathy and Distress from Document-Level User Responses,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Annotating Errors and Emotions in Human-Chatbot Interactions in {I}talian,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{G}o{E}motions: A Dataset of Fine-Grained Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{ICON}: Interactive Conversational Memory Network for Multimodal Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modeling Empathy and Distress in Reaction to News Stories,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,"{Z}ara: A Virtual Interactive Dialogue System Incorporating Emotion, Sentiment and Personality Recognition",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,How Interlocutors Coordinate with each other within Emotional Segments?,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Understanding and Predicting Empathic Behavior in Counseling Therapy,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Social and linguistic behavior and its correlation to trait empathy,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Appraisal Framework for Clinical Empathy: A Novel Application to Breaking Bad News Conversations,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,{L}ead{E}mpathy: An Expert Annotated {G}erman Dataset of Empathy in Written Leadership Communication,,,,,,,,Auto Downloaded; Python Script,,,empathy,Yes,No
acl,{LLM}-{GE}m: Large Language Model-Guided Prediction of People{'}s Empathy Levels towards Newspaper Article,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,Causality Reasoning for Empathy-Enriched and Personality-Conditioned Spoken Dialogue System,,,,,,,,Auto Downloaded; Python Script,,,empathy,No,No
acl,"Findings of {WASSA} 2023 Shared Task on Empathy, Emotion and Personality Detection in Conversation and Reactions to News Articles",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"{T}eam{\_}{H}awk at {WASSA} 2023 Empathy, Emotion, and Personality Shared Task: Multi-tasking Multi-encoder based transformers for Empathy and Emotion Prediction in Conversations",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"{PICT}-{CLRL} at {WASSA} 2023 Empathy, Emotion and Personality Shared Task: Empathy and Distress Detection using Ensembles of Transformer Models",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{YNU}-{HPCC} at {WASSA}-2023 Shared Task 1: Large-scale Language Model with {L}o{RA} Fine-Tuning for Empathy Detection and Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,No
acl,"{A}ditya{P}atkar at {WASSA} 2023 Empathy, Emotion, and Personality Shared Task: {R}o{BERT}a-Based Emotion Classification of Essays, Improving Performance on Imbalanced Data",,,,,,,,Auto Downloaded; Python Script,,,emotion_and_empathy,No,Yes
acl,Emotion Analysis of Tweets Banning Education in {A}fghanistan,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sentiment and Emotion Classification in Low-resource Settings,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Transformer-based Prediction of Emotional Reactions to Online Social Network Posts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Adapting Emotion Detection to Analyze Influence Campaigns on Social Media,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,The Paradox of Multilingual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,[RETRACTED] Findings of {WASSA} 2023 Shared Task: Multi-Label and Multi-Class Emotion Classification on Code-Mixed Text Messages,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion classification on code-mixed text messages via soft prompt tuning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{VISU} at {WASSA} 2023 Shared Task: Detecting Emotions in Reaction to News Stories Using Transformers and Stacked Embeddings,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{P}recog{IIITH}@{WASSA}2023: Emotion Detection for {U}rdu-{E}nglish Code-mixed Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{YNU}-{HPCC} at {WASSA} 2023: Using Text-Mixed Data Augmentation for Emotion Classification on Code-Mixed Text Message,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Generative Pretrained Transformers for Emotion Detection in a Code-Switching Setting,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Learning More from Mixed Emotions: A Label Refinement Method for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Emotion-Enriched and Psycholinguistics Features-Based Approach for Rumor Detection on Online Social Media,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{B}p{H}igh at {WASSA} 2023: Using Contrastive Learning to build Sentence Transformer models for Multi-Class Emotion Classification in Code-mixed {U}rdu,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion-Conditioned Text Generation through Automatic Prompt Optimization,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modeling Emotion Dynamics in Song Lyrics with State Space Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{SINAI} at {S}em{E}val-2023 Task 10: Leveraging Emotions, Sentiments, and Irony Knowledge for Explainable Detection of Online Sexism",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Impact of Emojis on Automatic Analysis of Individual Emotion Categories,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-Task Learning for Emotion Recognition in Conversation with Emotion Shift,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Unlocking Emotions in Text: A Fusion of Word Embeddings and Lexical Knowledge for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Transformer-based {B}engali Textual Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{P}rogress: Cumulated Emotion Progression Analysis in Dreams and Customer Service Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Dialogue Quality and Emotion Annotations for Customer Support Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,The {PEACE}-Reviews dataset: Modeling Cognitive Appraisals in Emotion Text Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Recognition in Conversation via Dynamic Personality,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Exploring the Emotional Dimension of {F}rench Online Toxic Content,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IDEM}: The {ID}ioms with {EM}otions Dataset for Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Estimating the Emotion of Disgust in {G}reek Parliament Records,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{T}rans: Emotional Transition-based Model for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Enhancing Emotion Prediction in News Headlines: Insights from {C}hat{GPT} and {S}eq2{S}eq Models for Free-Text Generation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{ESCP}: Enhancing Emotion Recognition in Conversation with Speech and Contextual Prefixes,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Topic Bias in Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{M}ix-3{L}: A Code-Mixed Dataset for {B}angla-{E}nglish-{H}indi for Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Ice and Fire: Dataset on Sentiment, Emotions, Toxicity, Sarcasm, Hate speech, Sympathy and More in {I}celandic Blog Comments",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Identifying Emotional and Polar Concepts via Synset Translation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sociolinguistically Informed Interpretability: A Case Study on {H}inglish Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}rans{M}istral at {S}em{E}val-2024 Task 10: Using Mistral 7{B} for Emotion Discovery and Reasoning its Flip in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{TW}-{NLP} at {S}em{E}val-2024 Task10: Emotion Recognition and Emotion Reversal Inference in Multi-Party Dialogues.,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Offensiveness, Hate, Emotion and {GPT}: Benchmarking {GPT}3.5 and {GPT}4 as Classifiers on {T}witter-specific Datasets",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{P}ersian{E}mo: Enhancing {F}arsi-{D}ari Emotion Analysis with a Hybrid Transformer and Recurrent Neural Network Model,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NCL} Team at {S}em{E}val-2024 Task 3: Fusing Multimodal Pre-training Embeddings for Emotion Cause Prediction in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UWBA} at {S}em{E}val-2024 Task 3: Dialogue Representation and Multimodal Fusion for Emotion Cause Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{GAV}x at {S}em{E}val-2024 Task 10: Emotion Flip Reasoning via Stacked Instruction Finetuning of {LLM}s,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Hidetsune at {S}em{E}val-2024 Task 3: A Simple Textual Approach to Emotion Classification and Emotion Cause Analysis in Conversations Using Machine Learning and Next Sentence Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CLT}eam1 at {S}em{E}val-2024 Task 10: Large Language Model based ensemble for Emotion Detection in {H}inglish,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Hidetsune at {S}em{E}val-2024 Task 10: An {E}nglish Based Approach to Emotion Recognition in {H}indi-{E}nglish code-mixed Conversations Using Machine Learning and Machine Translation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{L}inguis{T}ech at {S}em{E}val-2024 Task 10: Emotion Discovery and Reasoning its Flip in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}amsung Research {C}hina-{B}eijing at {S}em{E}val-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{MIPS} at {S}em{E}val-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SSN}{\_}{S}emeval10 at {S}em{E}val-2024 Task 10: Emotion Discovery and Reasoning its Flip in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Innovators at {S}em{E}val-2024 Task 10: Revolutionizing Emotion Recognition and Flip Analysis in Code-Mixed Texts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{ISDS}-{NLP} at {S}em{E}val-2024 Task 10: Transformer based neural networks for emotion recognition in conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SSN}{\_}{ARMM} at {S}em{E}val-2024 Task 10: Emotion Detection in Multilingual Code-Mixed Conversations using {L}inear{SVC} and {TF}-{IDF},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{F}eed{F}orward at {S}em{E}val-2024 Task 10: Trigger and sentext-height enriched emotion analysis in multi-party conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{YNU}-{HPCC} at {S}em{E}val-2024 Task10: Pre-trained Language Model for Emotion Discovery and Reasoning its Flip in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UMUT}eam at {S}em{E}val-2024 Task 10: Discovering and Reasoning about Emotions in Conversation using Transformers,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UIR}-{ISC} at {S}em{E}val-2024 Task 3: Textual Emotion-Cause Pair Extraction in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{BITS} Pilani at {S}em{E}val-2024 Task 10: Fine-tuning {BERT} and Llama 2 for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{TECHSSN}1 at {S}em{E}val-2024 Task 10: Emotion Classification in {H}indi-{E}nglish Code-Mixed Dialogue using Transformer-based Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{RACAI} at {S}em{E}val-2024 Task 10: Combining algorithms for code-mixed Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{PWEITINLP} at {S}em{E}val-2024 Task 3: Two Step Emotion Cause Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}ent{EMO}: A Multilingual Adaptive Platform for Aspect-based Sentiment and Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Can Emotion Carriers Explain Automatic Sentiment Prediction? A Study on Personal Narratives,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{XLM}-{EMO}: Multilingual Emotion Prediction in Social Media Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Identifying Emotions in Code Mixed {H}indi-{E}nglish Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,On the Complementarity of Images and Text for the Expression of Emotions in Social Media,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Analysis of Writers and Readers of {J}apanese Tweets on Vaccinations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}nglish-{M}alay Word Embeddings Alignment for Cross-lingual Emotion Classification with Hierarchical Attention Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Leveraging Emotion-Specific features to improve Transformer performance for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,Yes
acl,Transformer based ensemble for emotion detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Baseline {E}nglish and {M}altese-{E}nglish Classification Models for Subjectivity Detection, Sentiment Analysis, Emotion Analysis, Sarcasm Detection, and Irony Detection",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{YNU}-{HPCC} at {S}em{E}val-2022 Task 5: Multi-Modal and Multi-label Emotion Classification Based on {LXMERT},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Do Multimodal Emotion Recognition Models Tackle Ambiguity?,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Improving the Generalizability of Text-Based Emotion Detection by Leveraging Transformers with Psycholinguistic Features,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}tud{E}mo: A Non-aggregated Review Dataset for Personalized Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Experiencer-Specific Emotion and Appraisal Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Static and Dynamic Speaker Modeling based on Graph Neural Network for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Empirical Study on Multiple Knowledge from {C}hat{GPT} for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Enhancing Emotion Recognition in Conversation via Multi-view Feature Alignment and Memorization,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Semi-Supervised Domain Adaptation for Emotion-Related Tasks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Misery Loves Complexity: Exploring Linguistic Complexity in the Context of Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Mitigating Linguistic Artifacts in Emotion Recognition for Conversations from {TV} Scripts to Daily Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Social Convos: Capturing Agendas and Emotions on Social Media,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,n{EMO}: Dataset of Emotional Speech in {P}olish,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{SM}-{FEEL}-{BG} - the First {B}ulgarian Datasets and Classifiers for Detecting Feelings, Emotions, and Sentiments of {B}ulgarian Social Media Text",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}opic{D}iff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotional Toll and Coping Strategies: Navigating the Effects of Annotating Hate Speech Data,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,Predicting Client Emotions and Therapist Interventions in Psychotherapy Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Improving Contrastive Learning in Emotion Recognition in Conversation via Data Augmentation and Decoupled Neutral Emotion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Annotating Emotions in Acquired Brain Injury Patients{'} Narratives,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}ensory{T}5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Automatic Annotation of Dream Report{'}s Emotional Content with Large Language Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Prevalent Frequency of Emotional and Physical Symptoms in Social Anxiety using Zero Shot Classification: An Observational Study,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Analysing Emotions in Cancer Narratives: A Corpus-Driven Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modelling Emotions in Task-Oriented Dialogue,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Understanding Emotion Valence is a Joint Deep Learning Task,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}em{E}val-2019 Task 3: {E}mo{C}ontext Contextual Emotion Detection in Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CECL} at {S}em{E}val-2019 Task 3: Using Surface Learning for Detecting Emotion in Textual Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CX}-{ST}-{RNM} at {S}em{E}val-2019 Task 3: Fusion of Recurrent Neural Networks Based on Contextualized and Static Word Representations for Contextual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{P}arallel{D}ots at {S}em{E}val-2019 Task 3: Domain Adaptation with feature embeddings for Contextual Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EL}i{RF}-{UPV} at {S}em{E}val-2019 Task 3: Snapshot Ensemble of Hierarchical Convolutional Neural Networks for Contextual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{D}et at {S}em{E}val-2019 Task 3: Emotion Detection in Text using Deep Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CL}a{C} Lab at {S}em{E}val-2019 Task 3: Contextual Emotion Detection Using a Combination of Neural Networks and {SVM},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CLARK} at {S}em{E}val-2019 Task 3: Exploring the Role of Context to Identify Emotion in a Short Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CLP} at {S}em{E}val-2019 Task 3: Multi-Encoder in Hierarchical Attention Networks for Contextual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{C}on{SSED} at {S}em{E}val-2019 Task 3: Configurable Semantic and Sentiment Emotion Detector,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}-{LSTM} at {S}em{E}val-2019 Task 3: Semantic and Sentimental Features Retention for Emotion Detection in Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EMOMINER} at {S}em{E}val-2019 Task 3: A Stacked {B}i{LSTM} Architecture for Contextual Emotion Detection in Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EPITA}-{ADAPT} at {S}em{E}val-2019 Task 3: Detecting emotions in textual conversations using deep learning models combination,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Figure Eight at {S}em{E}val-2019 Task 3: Ensemble of Transfer Learning Methods for Contextual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{G}en{SMT} at {S}em{E}val-2019 Task 3: Contextual Emotion Detection in tweets using multi task generic approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IIT} {G}andhinagar at {S}em{E}val-2019 Task 3: Contextual Emotion Detection Using Deep Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{KSU} at {S}em{E}val-2019 Task 3: Hybrid Features for Emotion Recognition in Textual Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{S}ense at {S}em{E}val-2019 Task 3: Bidirectional {LSTM} Network for Contextual Emotion Detection in Textual Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{GWU} {NLP} Lab at {S}em{E}val-2019 Task 3 : {E}mo{C}ontext: Effectiveness of{C}ontextual Information in Models for Emotion Detection in{S}entence-level at Multi-genre Corpus,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{KGPC}hamps at {S}em{E}val-2019 Task 3: A deep learning approach to detect emotions in the dialog utterances.,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{LIRMM}-Advanse at {S}em{E}val-2019 Task 3: Attentive Conversation Modeling for Emotion Detection and Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{M}oon{G}rad at {S}em{E}val-2019 Task 3: Ensemble {B}i{RNN}s for Contextual Emotion Detection in Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,ntuer at {S}em{E}val-2019 Task 3: Emotion Classification with Word and Sentence Representations in {RCNN},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NL}-{FIIT} at {S}em{E}val-2019 Task 3: Emotion Detection From Conversational Triplets Using Hierarchical Encoders,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{PKUSE} at {S}em{E}val-2019 Task 3: Emotion Detection with Emotion-Oriented Neural Attention Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SINAI} at {S}em{E}val-2019 Task 3: Using affective features for emotion classification in textual conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SSN}{\_}{NLP} at {S}em{E}val-2019 Task 3: Contextual Emotion Identification from Textual Conversation using {S}eq2{S}eq Deep Neural Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}ymanto{R}esearch at {S}em{E}val-2019 Task 3: Combined Neural Models for Emotion Classification in Human-Chatbot Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{TDB}ot at {S}em{E}val-2019 Task 3: Context Aware Emotion Detection Using A Conditioned Classification Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{THU}{\_}{NGN} at {S}em{E}val-2019 Task 3: Dialog Emotion Classification using Attentional {LSTM}-{CNN},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{THU}-{HCSI} at {S}em{E}val-2019 Task 3: Hierarchical Ensemble Classification of Contextual Emotion in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sentiment and Emotion Based Representations for Fake Reviews Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{SWAP} at {S}em{E}val-2019 Task 3: Emotion detection in conversations through Tweets, {CNN} and {LSTM} deep neural networks",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}okyo{T}ech{\_}{NLP} at {S}em{E}val-2019 Task 3: Emotion-related Symbols in Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{COGMEN}: {CO}ntextualized {GNN} based Multimodal Emotion recognitio{N},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{C}o{MPM}: Context Modeling with Speaker{'}s Pre-trained Memory Tracking for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,How Language-Dependent is Emotion Detection? Evidence from Multilingual {BERT},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Neural Feature Extraction for Contextual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Understanding the role of Emojis for emotion detection in {T}amil,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{BERT} 4{EVER}@{LT}-{EDI}-{ACL}2022-Detecting signs of Depression from Social Media:Detecting Depression in Social Media using Prompt-Learning and Word-Emotion Cluster,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Aspect-Based Emotion Analysis and Multimodal Coreference: A Case Study of Customer Comments on Adidas {I}nstagram Posts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Dataset for Speech Emotion Recognition in {G}reek Theatrical Plays,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,x-en{VENT}: A Corpus of Event Descriptions with Experiencer-specific Emotion and Appraisal Annotations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{RED} v2: Enhancing {RED} Dataset for Multi-Label Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,A Study on the Ambiguity in Human Annotation of {G}erman Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{WOZ}: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{I}n{H}indi: A Multi-label Emotion and Intensity Annotated Dataset in {H}indi for Emotion Recognition in Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,Emotion analysis and detection during {COVID}-19,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{RELATE}: Generating a linguistically inspired Knowledge Graph for fine-grained emotion classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Cross-lingual Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion-guided Cross-domain Fake News Detection using Adversarial Domain Adaptation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}uni{SER}: Toward a {T}unisian Speech Emotion Recognition System,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Evaluating Large-Language Models for Dimensional Music Emotion Prediction from Social Media Discourse,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Sentiment and Emotion Annotated Dataset for Bitcoin Price Forecasting Based on {R}eddit Posts,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,Teaching Interactively to Learn Emotions in Natural Language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{C}aps: Emotion Capsule based Model for Conversational Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Efficient Nearest Neighbor Emotion Classification with {BERT}-whitening,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{U}ni{MSE}: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Textless Speech Emotion Conversion using Discrete {\&} Decomposed Representations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{MMM}: An Emotion and Novelty-aware Approach for Multilingual Multimodal Misinformation Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{I} miss you babe: Analyzing Emotion Dynamics During {COVID}-19 Pandemic,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Identifying Worry in {T}witter: Beyond Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{MPDD}: A Multi-Party Dialogue Dataset for Analysis of Emotions and Interpersonal Relationships,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EDA}: Enriching Emotional Dialogue Acts using an Ensemble of Neural Annotators,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{E}vent: A Multilingual Emotion Corpus based on different Events,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Measuring {Emotions} in the {COVID}-19 {Real} {World} {Worry} {Dataset},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An {A}lgerian Corpus and an Annotation Platform for Opinion and Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Annotation of Emotion Carriers in Personal Narratives,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IIIT}-{H} {TEMD} Semi-Natural Emotional Speech Database from Professional Actors and Non-Actors,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{G}ood{N}ews{E}veryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{K}orean-Specific Emotion Annotation Procedure Using {N}-Gram-Based Distant Supervision and {K}orean-Specific-Feature-Based Distant Supervision,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{CEASE}, a Corpus of Emotion Annotated Suicide notes in {E}nglish",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Event-comment Social Media Corpus for Implicit Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Emotional Mess! Deciding on a Framework for Building a {D}utch Emotion-Annotated Corpus,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{PO}-{EMO}: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in {G}erman and {E}nglish Poetry",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Semi-Automatic Construction and Refinement of an Annotated Corpus for a Deep Learning Framework for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Annotated Corpus of Tweets in {E}nglish from Various Domains for Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EST}e{R}: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards a Multi-Dataset for Complex Emotions Learning Based on Deep Neural Networks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Conditional Causal Relationships between Emotions and Causes in Texts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-modal Multi-label Emotion Detection with Modality and Label Dependence,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Hashtags, Emotions, and Comments: A Large-Scale Dataset to Understand Fine-Grained Social Emotions to Online Topics",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Classification by Jointly Learning to Lexiconize and Classify,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{C}ancer{E}mo: A Dataset for Fine-Grained Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Appraisal Theories for Emotion Classification in Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Symmetric Local Search Network for Emotion-Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Unified Sequence Labeling Model for Emotion Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{H}i{T}rans: A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Lost in Back-Translation: Emotion Preservation in Neural Machine Translation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{MEISD}: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations",,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,Exploiting Narrative Context and A Priori Knowledge of Categories in Textual Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Cross-Lingual Emotion Lexicon Induction using Representation Alignment in Low-Resource Settings,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Iterative Emotion Interaction Network for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Knowledge Aware Emotion Recognition in Textual Conversations via Multi-Task Incremental Transformer,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sentiment Analysis for Emotional Speech Synthesis in a News Dialogue System,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{XED}: A Multilingual Dataset for Sentiment Analysis and Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CAN}-{GRU}: a Hierarchical Model for Emotion Recognition in Dialogue,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Attending the Emotions to Detect Online Abusive Language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Convolutional and Recurrent Neural Networks for Spoken Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{ECPE}-2{D}: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Creation of Corpus and analysis in Code-Mixed {K}annada-{E}nglish {T}witter data for Emotion Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multilogue-Net: A Context-Aware {RNN} for Multi-modal Emotion Detection and Sentiment Analysis in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Joint Modelling of Emotion and Abusive Language Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Detecting Perceived Emotions in Hurricane Disasters,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modality-Transferable Emotion Embeddings for Low-Resource Multimodal Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{FERN}et: Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{ANA} at {S}em{E}val-2019 Task 3: Contextual Emotion detection in Conversations through hierarchical {LSTM}s and {BERT},,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{B}rain{EE} at {S}em{E}val-2019 Task 3: Ensembling Linear Classifiers for Emotion Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CA}i{RE}{\_}{HKUST} at {S}em{E}val-2019 Task 3: Hierarchical Attention for Dialogue Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Exploring Fine-Tuned Embeddings that Model Intensifiers for Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Analyzing Incorporation of Emotion in Emoji Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UMUT}eam@{T}amil{NLP}-{ACL}2022: Emotional Analysis in {T}amil,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{GJG}@{T}amil{NLP}-{ACL}2022: Emotion Analysis and Classification in {T}amil using Transformers,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{J}udith{J}eyafreeda{A}ndrew@{T}amil{NLP}-{ACL}2022:{CNN} for Emotion Analysis in {T}amil,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SSNCSE}{\_}{NLP}@{T}amil{NLP}-{ACL}2022: Transformer based approach for Emotion analysis in {T}amil language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{MUCS}@{D}ravidian{L}ang{T}ech@{ACL}2022: Ensemble of Logistic Regression Penalties to Identify Emotions in {T}amil Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CUET}-{NLP}@{T}amil{NLP}-{ACL}2022: Multi-Class Textual Emotion Detection from Social Media using Transformer,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{O}ptimize{\_}{P}rime@{D}ravidian{L}ang{T}ech-{ACL}2022: Emotion Analysis in {T}amil,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}ynt{A}ct: A Synthesized Database of Basic Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{D}ialogue{EIN}: Emotion Interaction Network for Dialogue Affective Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Enriched Retrofitted Word Embeddings,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{TSAM}: A Two-Stream Attention Model for Causal Emotion Entailment,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{M}u{CDN}: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{COMMA}-{DEER}: {CO}mmon-sense Aware Multimodal Multitask Approach for Detection of Emotion and Emotional Reasoning in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{M}ent: An Emotion Annotated Mental Health Corpus from Two {S}outh {A}sian Countries,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Comparing emotion feature extraction approaches for predicting depression and anxiety,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,基于关系图注意力网络和宽度学习的负面情绪识别方法(Negative Emotion Recognition Method Based on Rational Graph Attention Network and Broad Learning),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Context or Knowledge is Not Always Necessary: A Contrastive Learning Framework for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards Distribution-shift Robust Text Classification of Emotional Content,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{QAP}: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction ({DICE}) in Multimodal Online Posts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Federated Meta-Learning for Emotion and Sentiment Aware Multi-modal Complaint Identification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Analysis from Texts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Conversational Emotion-Cause Pair Extraction with Guided Mixture of Experts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Training-Free Debiasing Framework with Counterfactual Reasoning for Conversational Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimoda Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,How people talk about each other: Modeling Generalized Intergroup Bias and Emotion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Social Media Data Analysis for {M}alayalam {Y}ou{T}ube Comments: Sentiment Analysis and Emotion Detection using {ML} and {DL} Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,基于互信息最大化和对比损失的多模态对话情绪识别模型(Multimodal Emotion Recognition in Conversation with Mutual Information Maximization and Contrastive Loss),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Automatic Emotion Experiencer Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{BE}mo{L}ex{BERT}: A Hybrid Model for Multilabel Textual Emotion Classification in {B}angla by Combining Transformers with Lexicon Features,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Unveiling Emotional Landscapes in Plautus and Terentius Comedies: A Computational Approach for Qualitative Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{D}ual{GAT}s: Dual Graph Attention Networks for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Unsupervised Extractive Summarization of Emotion Triggers,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Emotional Journey: Detecting Emotion Trajectories in {D}utch Customer Service Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{M}ulti{EMO}: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}motion{X}-{DLC}: Self-Attentive {B}i{LSTM} for Detecting Sequential Emotions in Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}motion{X}-{S}mart{D}ubai{\_}{NLP}: Detecting User Emotions In Social Media Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}motion{X}-Area66: Predicting Emotions in Dialogues using Hierarchical Attention Network with Sequence Labeling,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}motion{X}-{JTML}: Detecting emotions with Attention,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Recognizing Emotions in Video Using Multimodal {DNN} Feature Fusion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}ocial{NLP} 2018 {E}motion{X} Challenge Overview: Recognizing Emotions in Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}motion{X}-{AR}: {CNN}-{DCNN} autoencoder based Emotion Classifier,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multimodal Relational Tensor Network for Sentiment and Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{ASR}-based Features for Emotion Recognition: A Transfer Learning Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Enabling Deep Learning of Emotion With First-Person Seed Expressions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{W}ord{N}et: Automatic Expansion of Emotion Lexicon Using {E}nglish {W}ord{N}et,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UWB} at {S}em{E}val-2018 Task 1: Emotion Intensity Detection in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{A}ttn{C}onvnet at {S}em{E}val-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}i{TAKA} at {S}em{E}val-2018 Task 1: An Ensemble of N-Channels {C}onv{N}et and {XG}boost Regressors for Emotion Analysis of Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{LT}3 at {S}em{E}val-2018 Task 1: A classifier chain to detect emotions in tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SINAI} at {S}em{E}val-2018 Task 1: Emotion Recognition in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Tw-{S}t{AR} at {S}em{E}val-2018 Task 1: Preprocessing Impact on Multi-label Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Yuan at {S}em{E}val-2018 Task 1: Tweets Emotion Intensity Prediction using Ensemble Recurrent Neural Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{C}rystal{F}eel at {S}em{E}val-2018 Task 1: Understanding and Detecting Emotion Intensity using Affective Lexicons,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{DMCB} at {S}em{E}val-2018 Task 1: Transfer Learning of Sentiment Classification Using Group {LSTM} for Emotion Intensity prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{ECNU} at {S}em{E}val-2018 Task 1: Emotion Intensity Prediction Using Effective Features and Machine Learning Models,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EMA} at {S}em{E}val-2018 Task 1: Emotion Mining for {A}rabic,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UG}18 at {S}em{E}val-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in {S}panish,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{D}eep{M}iner at {S}em{E}val-2018 Task 1: Emotion Intensity Recognition Using Deep Representation Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Personal Bias in Prediction of Emotions Elicited by Textual Opinions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,e{MLM}: A New Pre-training Objective for Emotion Related Tasks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Demonstrating the Reliability of Self-Annotated Emotion Data,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,{U}yghur Metaphor Detection Via Considering Emotional Consistency,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Classification of {COVID}-19 {C}hinese Microblogs based on the Emotion Category Description,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Directed Acyclic Graph Network for Conversational Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Distributed Representations of Emotion Categories in Emotion Space,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Studying The Effect of Emotional and Moral Language on Information Contagion during the Charlottesville Event,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Token Sequence Labeling vs. Clause Classification for {E}nglish Emotion Stimulus Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Speech-Emotion Detection in an {I}ndonesian Movie,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Challenges in Emotion Style Transfer: An Exploration with a Lexical Substitution Pipeline,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}urkish Emotion Voice Database ({T}ur{EV}-{DB}),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NUAA}-{QMUL} at {S}em{E}val-2020 Task 8: Utilizing {BERT} and {D}ense{N}et for {I}nternet Meme Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{DSC} {IIT}-{ISM} at {S}em{E}val-2020 Task 8: Bi-Fusion Techniques for Deep Meme Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{BERT} at {S}em{E}val-2020 Task 8: Using {BERT} to Analyse Meme Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Hitachi at {S}em{E}val-2020 Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{B}ham{NLP} at {S}em{E}val-2020 Task 12: An Ensemble of Different Word Embeddings and Emotion Transfer Learning for {A}rabic Offensive Language Identification in Social Media,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{M}emo{SYS} at {S}em{E}val-2020 Task 8: Multimodal Emotion Analysis in Memes,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}oc{C}og{C}om at {S}em{E}val-2020 Task 11: Characterizing and Detecting Propaganda Using Sentence-Level Emotional Salience Features,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Contextual Augmentation of Pretrained Language Models for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Cross-lingual Emotion Intensity Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"The {L}i{L}a{H} Emotion Lexicon of {C}roatian, {D}utch and {S}lovene",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Systematic Evaluation of a Framework for Unsupervised Emotion Recognition for Narrative Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in {T}witter,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{COIN}: Conversational Interactive Networks for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Classification in {G}erman Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Recognition under Consideration of the Emotion Component Process Model,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,以遷移學習改善深度神經網路模型於中文歌詞情緒辨識 (Using Transfer Learning to Improve Deep Neural Networks for Lyrics Emotion Recognition in {C}hinese),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Meta-learning for Classifying Previously Unseen Data Source into Previously Unseen Emotional Categories,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,M{\'e}ta-apprentissage : classification de messages en cat{\'e}gories {\'e}motionnelles inconnues en entra{\^\i}nement (Meta-learning : Classifying Messages into Unseen Emotional Categories),,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Retrofitting of Pre-trained Emotion Words with {VAD}-dimensions and the {P}lutchik Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Discourse-Aware Graph Neural Network for Emotion Recognition in Multi-Party Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Boundary Detection with {BERT} for Span-level Emotion Cause Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Past, Present, and Future: Conversational Emotion Recognition through Structural Modeling of Psychological Knowledge",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Knowledge-Interactive Network with Sentiment Polarity Intensity-Aware Multi-Task Learning for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Exploring the Role of Context in Utterance-level Emotion, Act and Intent Classification in Conversations: An Empirical Study",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Dimensional Emotion Detection from Categorical Emotion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards Label-Agnostic Emotion Embeddings,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}pan{E}mo: Casting Multi-label Emotion Classification as Span-prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{PHASE}: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{T}ag {--} Towards an Emotion-Based Analysis of Emojis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Crowdsourcing and Validating Event-focused Emotion Corpora for {G}erman and {E}nglish,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Impacts Speech Recognition Performance,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,Yes
acl,{MELD}: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{H}i{GRU}: {H}ierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Detecting Depression in Social Media using Fine-Grained Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Frowning {F}rodo, Wincing {L}eia, and a Seriously Great Friendship: Learning to Classify Emotional Relationships of Fictional Characters",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-Channel Convolutional Neural Network for {T}witter Emotion and Sentiment Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Interpretable Relevant Emotion Ranking with Event-Driven Attention,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{D}ialogue{GCN}: A Graph Convolutional Neural Network for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modelling the interplay of metaphor and emotion through multitask learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Detection with Neural Personal Discrimination,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{DENS}: A Dataset for Multi-class Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Converting Sentiment Annotated Data to Emotion Annotated Data,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,{IEST}: {WASSA}-2018 Implicit Emotions Shared Task,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IIIDYT} at {IEST} 2018: Implicit Emotion Classification With Deep Contextualized Word Representations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{HUMIR} at {IEST}-2018: Lexicon-Sensitive and Left-Right Context-Sensitive {B}i{LSTM} for Implicit Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{NLP} at {IEST} 2018: An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{D}ata{SEARCH} at {IEST} 2018: Multiple Word Embedding based Models for Implicit Emotion Classification of Tweets with Deep Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NTUA}-{SLP} at {IEST} 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Leveraging Writing Systems Change for Deep Learning Based {C}hinese Emotion Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,The Role of Emotions in Native Language Identification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NLP} at {IEST} 2018: {B}i{LSTM}-Attention and {LSTM}-Attention via Soft Voting in Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SINAI} at {IEST} 2018: Neural Encoding of Emotional External Knowledge for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{HGSGNLP} at {IEST} 2018: An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NL}-{FIIT} at {IEST}-2018: Emotion Recognition utilizing Neural Networks and Multi-level Preprocessing,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{UWB} at {IEST} 2018: Emotion Prediction in Tweets with Bidirectional Long Short-Term Memory Neural Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{USI}-{IR} at {IEST} 2018: Sequence Modeling and Pseudo-Relevance Feedback for Implicit Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}moti{KLUE} at {IEST} 2018: Topic-Informed Classification of Implicit Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{B}rain{T} at {IEST} 2018: Fine-tuning Multiclass Perceptron For Implicit Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Disney at {IEST} 2018: Predicting Emotions using an Ensemble,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo2{V}ec: Learning Generalized Emotion Representation by Multi-task Training,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Sentylic at {IEST} 2018: Gated Recurrent Neural Network and Capsule Network Based Approach for Implicit Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Predicting Emotion in Spoken Dialogue from Multiple Knowledge Sources,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{``}You Stupid Tin Box{''} - Children Interacting with the {AIBO} Robot: A Cross-linguistic Emotional Speech Corpus,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{F}olksonomication: Predicting Tags for Movies from Plot Synopses using Emotion Flow Encoded Neural Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards Emotion Prediction in Spoken Tutoring Dialogues,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Analysis of Annotated Corpora for Emotion Classification in Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Public Apologies in {I}ndia - Semantics, Sentiment and Emotion",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{WASSA}-2017 Shared Task on Emotion Intensity,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{IMS} at {E}mo{I}nt-2017: Emotion Intensity Prediction with Affective Norms, Automatically Extended Resources and Deep Learning",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{G}rad{A}scent at {E}mo{I}nt-2017: Character and Word Level Recurrent Neural Network Models for Tweet Emotion Intensity Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NUIG} at {E}mo{I}nt-2017: {B}i{LSTM} and {SVR} Ensemble to Detect Emotion Intensity,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{PLN}-{PUCRS} at {E}mo{I}nt-2017: Psycholinguistic features for emotion intensity prediction in tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Prayas at {E}mo{I}nt 2017: An Ensemble of Deep Neural Architectures for Emotion Intensity Prediction in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Understanding human values and their emotional effect,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modeling Temporal Progression of Emotional Status in Mental Health Forum: A Recurrent Neural Net Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IITP} at {E}mo{I}nt-2017: Measuring Intensity of Emotions using Sentence Embeddings and Optimized Features,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{NSE}mo at {E}mo{I}nt-2017: An Ensemble to Predict Emotion Intensity in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{YZU}-{NLP} at {E}mo{I}nt-2017: Determining Emotion Intensity Using a Bi-directional {LSTM}-{CNN} Model,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"{UW}at-Emote at {E}mo{I}nt-2017: Emotion Intensity Detection using Affect Clues, Sentiment Polarity and Word Embeddings",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,deep{C}yb{E}r{N}et at {E}mo{I}nt-2017: Deep Emotion Intensities in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Video Highlights Detection and Summarization with Lag-Calibration based on Concept-Emotion Mapping of Crowdsourced Time-Sync Comments,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{A}tt at {E}mo{I}nt-2017: Inner attention sentence embedding for Emotion Intensity,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Predicting News Values from Headline Text and Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Investigating the Relationship between Literary Genres and Emotional Plot Development,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Gradient Emotional Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{N}et: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Using Teacher-Student Model For Emotional Speech Recognition[In {C}hinese],,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Intensities in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards the Improvement of Automatic Emotion Pre-annotation with Polarity and Subjective Information,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Novel Trajectory-based Spatial-Temporal Spectral Features for Speech Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Amplifying a Sense of Emotion toward Drama-Long Short-Term Memory Recurrent Neural Network for dynamic emotion recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Automatically augmenting an emotion dataset improves classification using audio,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Predicting Emotional Word Ratings using Distributional Representations and Signed Clustering,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Modelling Representation Noise in Emotion Analysis using {G}aussian Processes,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Reusing Neural Speech Representations for Auditory Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Event Based Emotion Classification for News Articles,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{M}ood{S}wipe: A Soft Keyboard that Suggests {M}essage{B}ased on User-Specified Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Question Answering Approach for Emotion Cause Extraction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multiple Emotions Detection in Conversation Transcripts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,The Effect of Gender and Age Differences on the Recognition of Emotions from Facial Expressions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Distant supervision for emotion detection using {F}acebook reactions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Innovative Semi-Automatic Methodology to Annotate Emotional Corpora,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Crowdsourcing-based Annotation of Emotions in {F}ilipino and {E}nglish Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Support Super-Vector Machines in Automatic Speech Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Microblog Emotion Classification by Computing Similarity in Text, Time, and Space",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Metaphor Detection with Topic Transition, Emotion and Cognition in Context",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Linguistic Template Extraction for Recognizing Reader-Emotion,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"以多層感知器辨識情緒於國台客語料庫 (Use Multilayer Perceptron To Recognize Emotion in {M}andarin,{T}aiwanese and {H}akka Database) [In {C}hinese]",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,標記對於類神經語音情緒辨識系統辨識效果之影響(Effects of Label in Neural Speech Emotion Recognition System)[In {C}hinese],,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Could Speaker, Gender or Age Awareness be beneficial in Speech-based Emotion Recognition?",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Corpus Construction Based on Selection from Hashtags,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Exploring Fine-Grained Emotion Detection in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{T}weet-28: A Fine-Grained Emotion Corpus for Sentiment Analysis,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Construction of {J}apanese Audio-Visual Emotion Database and Its Application in Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}ensing Emotions in Text Messages: An Application and Deployment Study of {E}motion{P}ush,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Selective Co-occurrences for Word-Emotion Association,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Analysis on {T}witter: The Hidden Challenge,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Bilingual Attention Network for Code-switched Emotion Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Two-View Label Propagation to Semi-supervised Reader Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{SSN} {MLRG}1 at {S}em{E}val-2018 Task 1: Emotion and Sentiment Intensity Detection Using Rule Based Feature Selection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{IIT} {D}elhi at {S}em{E}val-2018 Task 1 : Emotion Intensity Prediction,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}eam{UNCC} at {S}em{E}val-2018 Task 1: Emotion Detection in {E}nglish and {A}rabic Tweets using Deep Learning,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{T}eam{CEN} at {S}em{E}val-2018 Task 1: Global Vectors Representation in Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Mutux at {S}em{E}val-2018 Task 1: Exploring Impacts of Context Information On Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Corpus Creation and Emotion Prediction for {H}indi-{E}nglish Code-Mixed Social Media Text,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Letting Emotions Flow: Success Prediction by Modeling the Flow of Emotions in Books,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Relevant Emotion Ranking from Text Constrained with Emotion Relationships,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Sentence and Clause Level Emotion Annotation, Detection, and Classification in a Multi-Genre Corpus",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{EMO}{\&}{LY} ({EMO}tion and {A}noma{LY}) : A new corpus for anomaly detection in an audiovisual stream with emotional context.,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Joint Learning for Emotion Classification and Emotion Cause Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Fine-Grained Emotion Detection in Health-Related Online Posts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{CARER}: Contextualized Affect Representations for Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Learning Emotion-enriched Word Representations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles of Emotions,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{M}3{ED}: Multi-modal Multi-scene Multi-label Emotional Dialogue Database,,,,,,,,Auto Downloaded; Python Script,,,emotion,Yes,No
acl,{E}mo{N}o{B}a: A Dataset for Analyzing Fine-Grained Emotions on Noisy {B}angla Texts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}+{PAGE}: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Prediction of People{'}s Emotional Response towards Multi-modal News,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Meta-Learning based Deferred Optimisation for Sentiment and Emotion aware Multi-modal Dialogue Act Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Emotion Ratings: How Intensity, Annotation Confidence and Agreements are Entangled",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Universal Joy A Data Set and Results for Classifying Emotions Across Languages,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{FEEL}-{IT}: Emotion and Sentiment Classification for the {I}talian Language,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,"Emotion-Aware, Emotion-Agnostic, or Automatic: Corpus Creation Strategies to Obtain Cognitive Event Appraisal Annotations",,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Towards Emotion Recognition in {H}indi-{E}nglish Code-Mixed Data: A Transformer Based Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Nearest neighbour approaches for Emotion Detection in Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-Emotion Classification for Song Lyrics,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotional {R}ob{BERT} and Insensitive {BERT}je: Combining Transformers and Affect Lexica for {D}utch Emotion Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Classifying Emotional Utterances by Employing Multi-modal Speech Emotion Recognition,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{E}mo{P}ars: A Collection of 30{K} Emotion-Annotated {P}ersian Social Media Texts,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,A Study on Using Transfer Learning to Improve {BERT} Model for Emotional Classification of {C}hinese Lyrics,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Speech Emotion Recognition Based on {CNN}+{LSTM} Model,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Probabilistic Ensembles of Zero- and Few-Shot Learning Models for Emotion Classification,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{RED}: A Novel Dataset for {R}omanian Emotion Detection from Tweets,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Exploring Reliability of Gold Labels for Emotion Detection in {T}witter,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion Classification in a Resource Constrained Language Using Transformer-based Approach,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{S}eq2{E}mo: A Sequence to Multi-Label Emotion Classification Model,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,An Emotional Comfort Framework for Improving User Satisfaction in {E}-Commerce Customer Service Chatbots,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,{MUSER}: {MU}ltimodal Stress detection using Emotion Recognition as an Auxiliary Task,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acl,Emotion-Infused Models for Explainable Psychological Stress Detection,,,,,,,,Auto Downloaded; Python Script,,,emotion,No,No
acm,Ubiquitous emotion-aware computing,"Broek, Egon L.",2013,,17,,10.1007/s00779-011-0479-9,"Emotions are a crucial element for personal and ubiquitous computing. What to sense and how to sense it, however, remain a challenge. This study explores the rare combination of speech, electrocardiogram, and a revised Self-Assessment Mannequin to assess people's emotions. 40 people watched 30 International Affective Picture System pictures in either an office or a living-room environment. Additionally, their personality traits neuroticism and extroversion and demographic information (i.e., gender, nationality, and level of education) were recorded. The resulting data were analyzed using both basic emotion categories and the valence–arousal model, which enabled a comparison between both representations. The combination of heart rate variability and three speech measures (i.e., variability of the fundamental frequency of pitch (F0), intensity, and energy) explained 90% (p &lt; .001) of the participants' experienced valence–arousal, with 88% for valence and 99% for arousal (ps &lt; .001). The six basic emotions could also be discriminated (p &lt; .001), although the explained variance was much lower: 18—20%. Environment (or context), the personality trait neuroticism, and gender proved to be useful when a nuanced assessment of people's emotions was needed. Taken together, this study provides a significant leap toward robust, generic, and ubiquitous emotion-aware computing.",Emotion; Speech features; Heart rate variability; Ubiquitous computing; Personality; Unobtrusive sensing,,,emotion,No,No
acm,Driver Emotion Recognition for Intelligent Vehicles: A Survey,"Zepf, Sebastian; Hernandez, Javier; Schmitt, Alexander; Minker, Wolfgang; Picard, Rosalind W.",2020,,53,,10.1145/3388790,"Driving can occupy a large portion of daily life and often can elicit negative emotional states like anger or stress, which can significantly impact road safety and long-term human health. In recent decades, the arrival of new tools to help recognize human affect has inspired increasing interest in how to develop emotion-aware systems for cars. To help researchers make needed advances in this area, this article provides a comprehensive literature survey of work addressing the problem of human emotion recognition in an automotive context. We systematically review the literature back to 2002 and identify 63 peer-review published articles on this topic. We overview each study’s methodology to measure and recognize emotions in the context of driving. Across the literature, we find a strong preference toward studying emotional states associated with high arousal and negative valence, monitoring the different states with cardiac, electrodermal activity, and speech signals, and using supervised machine learning to automatically infer the underlying human affective states. This article summarizes the existing work together with publicly available resources (e.g.,&nbsp;datasets and tools) to help new researchers get started in this field. We also identify new research opportunities to help advance progress for improving driver emotion recognition.",machine learning; Affective computing; emotion measurement; intelligent user sensing; literature survey; road safety,,,emotion,No,No
acm,A Personality-based Model of Emotional Contagion and Control in Crowd Queuing Simulations,"Xue, Junxiao; Zhang, Mingchuang; Yin, Hui",2023,,33,,10.1145/3577589,"Queuing is a frequent daily activity. However, long waiting lines equate to frustration and potential safety hazards. We present a novel, personality-based model of emotional contagion and control for simulating crowd queuing. Our model integrates the influence of individual personalities and interpersonal relationships. Through the interaction between the agents and the external environment parameters, the emotional contagion model based on well-known theories in psychology is used to complete the agents’ behavior planning and path planning function. We combine the epidemiological SIR model with the cellular automaton model to capture various emotional modelling for multi-agent simulations. The overall formulation involves different emotional parameters, such as patience, urgency, and friendliness, closely related to crowd queuing. In addition, to manage the order of the queue, governing agents are added to prevent the emotional outbreak. We perform qualitative and quantitative comparisons between our simulation results and real-world observations on various scenarios. Numerous experiments show that reasonably increasing the queue channel and adding governing agents can effectively improve the quality of queues.",Crowd simulation; emotional contagion; queue management,,,emotion,No,No
acm,Speech Emotion Recognition Considering Nonverbal Vocalization in Affective Conversations,"Hsu, Jia-Hao; Su, Ming-Hsiang; Wu, Chung-Hsien; Chen, Yi-Hsuan",2021,,29,,10.1109/TASLP.2021.3076364,"In real-life communication, nonverbal vocalization such as laughter, cries or other emotion interjections, within an utterance play an important role for emotion expression. In previous studies, only few emotion recognition systems consider nonverbal vocalization, which naturally exists in our daily conversation. In this work, both verbal and nonverbal sounds within an utterance are considered for emotion recognition of real-life affective conversations. Firstly, a support vector machine (SVM)-based verbal and nonverbal sound detector is developed. A prosodic phrase auto-tagger is further employed to extract the verbal/nonverbal sound segments. For each segment, the emotion and sound feature embeddings are respectively extracted using the deep residual networks (ResNets). Finally, a sequence of the extracted feature embeddings for the entire dialog turn are fed to an attentive long short-term memory (LSTM)-based sequence-to-sequence model to output an emotional sequence as recognition result. The NNIME corpus (The NTHU-NTUA Chinese interactive multimodal emotion corpus), which consists of verbal and nonverbal sounds, was adopted for system training and testing. 4766 single speaker dialogue turns in the audio data of the NNIME corpus were selected for evaluation. The experimental results showed that nonverbal vocalization was helpful for speech emotion recognition. For comparison, the proposed method based on decision-level fusion achieved an accuracy of 61.92% for speech emotion recognition outperforming the traditional methods as well as the feature-level and model-level fusion approaches.",,,,emotion,No,Yes
acm,ISNet: Individual Standardization Network for Speech Emotion Recognition,"Fan, Weiquan; Xu, Xiangmin; Cai, Bolun; Xing, Xiaofen",2022,,30,,10.1109/TASLP.2022.3171965,"Speech emotion recognition plays an essential role in human-computer interaction. However, cross-individual representation learning and individual-agnostic systems are challenging due to the distribution deviation caused by individual differences. The existing related approaches mostly use the auxiliary task of speaker recognition to eliminate individual differences. Unfortunately, although these methods can reduce interindividual voiceprint differences, it is difficult to dissociate interindividual expression differences since each individual has its unique expression habits. In this paper, we propose an individual standardization network (ISNet) for speech emotion recognition to alleviate the problem of interindividual emotion confusion caused by individual differences. Specifically, we model individual benchmarks as representations of nonemotional neutral speech, and ISNet realizes individual standardization using the automatically generated benchmark, which improves the robustness of individual-agnostic emotion representations. In response to individual differences, we also propose more comprehensive and meaningful individual-level evaluation metrics. In addition, we continue our previous work to construct a challenging large-scale speech emotion dataset (LSSED). We propose a more reasonable division method of the training set and testing set to prevent individual information leakage. Experimental results on datasets of both large and small scales have proven the effectiveness of ISNet, and the new state-of-the-art performance is achieved under the same experimental conditions on IEMOCAP and LSSED.",,,,emotion,No,Yes
acm,Emotion Prediction Oriented Method With Multiple Supervisions for Emotion-Cause Pair Extraction,"Hu, Guimin; Zhao, Yi; Lu, Guangming",2023,,31,,10.1109/TASLP.2023.3250833,"Emotion-cause pair extraction (ECPE) task aims to extract all the pairs of emotions and their causes from an unannotated emotion text. The previous works usually extract the emotion-cause pairs from two perspectives of emotion and cause. However, emotion extraction is more crucial to the ECPE task than cause extraction. Motivated by this analysis, we propose an end-to-end emotion-cause extraction approach oriented toward emotion prediction (EPO-ECPE), aiming to fully exploit the potential of emotion prediction to enhance emotion-cause pair extraction. Considering the strong dependence between emotion prediction and emotion-cause pair extraction, we propose a synchronization mechanism to share their improvement in the training process. That is, the improvement of emotion prediction can facilitate the emotion-cause pair extraction, and then the results of emotion-cause pair extraction can also be used to improve the accuracy of emotion prediction simultaneously. For the emotion-cause pair extraction, we divide it into genuine pair supervision and fake pair supervision, where the genuine pair supervision learns from the pairs with more possibility to be emotion-cause pairs. In contrast, fake pair supervision learns from other pairs. In this way, the emotion-cause pairs can be extracted directly from the genuine pair, thereby reducing the difficulty of extraction. Experimental results show that our approach outperforms the 13 compared systems and achieves new state-of-the-art performance.",,,,emotion,No,Yes
acm,Empathic Robot for Group Learning: A Field Study,"Alves-Oliveira, Patrícia; Sequeira, Pedro; Melo, Francisco S.; Castellano, Ginevra; Paiva, Ana",2019,,8,,10.1145/3300188,"This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we developed an autonomous robot with empathic competencies that is able to interact with a group of students in a learning activity about sustainable development. Two studies were conducted. The first study compares learning outcomes in children across three conditions: learning with an empathic robot; learning with a robot without empathic capabilities; and learning without a robot. The results show that the autonomous robot with empathy fosters meaningful discussions about sustainability, which is a learning outcome in sustainability education. The second study features groups of students who interact with the robot in a school classroom for 2 months. The long-term educational interaction did not seem to provide significant learning gains, although there was a change in game-actions to achieve more sustainability during game-play. This result reflects the need to perform more long-term research in the field of educational robots for group learning.",empathy; Social robotics; human-robot interaction; collaborative learning; education; group learning; learning gains,,,empathy,No,No
acm,The Effects of Healthcare Robot Empathy Statements and Head Nodding on Trust and Satisfaction: A Video Study,"Johanson, Deborah; Ahn, Ho Seok; Goswami, Rishab; Saegusa, Kazuki; Broadbent, Elizabeth",2023,,12,,10.1145/3549534,"Clinical empathy has been associated with many positive outcomes, including patient trust and satisfaction. Physicians can demonstrate clinical empathy through verbal statements and non-verbal behaviors, such as head nodding. The use of verbal and non-verbal empathy behaviors by healthcare robots may also positively affect patient outcomes. The current study examined whether the use of robot verbal empathy statements and head nodding during a video recorded interaction between a healthcare robot and patient improved participant trust and satisfaction. One hundred participants took part in the experiment, online through Amazon Mechanical Turk. They were randoimnized to watch one of four videos depicting an interaction with a `patient' and a Nao robot that (1) either made empathetic or neutral statements, and (2) either nodded its head when listening to the patient or did not. Results showed that the use of empathetic statements by the healthcare robot significantly increased participant perceptions of robot empathy, trust and satisfaction, and reduced robot distrust. No significant findings were revealed in relation to robot head nodding. The positive effects of empathy statements support the model of Robot-Patient Communication, which theorizes that robot use of recommended clinical empathy behaviors can improve patient outcomes. The effects of healthcare robot nodding behavior needs to be further investigated.",,,,empathy,No,No
acm,Virtual Empathy: Usability and Immersivity of a VR System for Enhancing Social Cohesion through Cultural Heritage,"Lucifora, Chiara; Schembri, Massimiliano; Asprino, Luigi; Follo, Anna; Gangemi, Aldo",2024,,,,10.1145/3679019,"In this paper we describe and evaluate a VR application, designed with a novel approach that allows personalized interaction and a more immersive virtual experience. This VR system uses both a user-based methodology in which the user expresses his/her emotions by interacting directly with the environment; and an artificial intelligence-driven methodology that is based on automated language systems able to detect user emotion and moral values from verbal speech. In this study we evaluated the usability and immersivity of our system on a sample of 30 museum visitors. The results indicate that our application is easy and pleasant to use for visitors, and that positive assessments are significantly associated with the level of immersion and realism experienced within the virtual environment. Additionally, we found a low incidence of motion sickness among users.",Usability; Virtual Reality; Cultural Heritage; Social Cohesion,,,empathy,No,No
acm,Empathetic Response Generation Based on Plug-and-Play Mechanism With Empathy Perturbation,"Hsu, Jia-Hao; Chang, Jeremy; Kuo, Min-Hsueh; Wu, Chung-Hsien",2023,,31,,10.1109/TASLP.2023.3277274,"Spoken dialogue systems have rapidly developed but are often viewed as inhumane because they lack empathetic communication skills. In this study, a transformer-based language model (DialoGPT fine-tuned on the EmpatheticDialogues dataset) was combined with two proposed attribute models for affective and cognitive empathy to improve its performance. The affective empathy model ensures that the user sentence and system response have similar emotional valence, and the cognitive empathy model ensures that the system response is relevant to the user's input by using a DialoGPT-based reverse generation model to calculate the cross-entropy loss. A plug-and-play structure with these empathy attribute models was used to perturb the language generation model to increase response empathy without fine-tuning or retraining the generation model. Experiments indicated that the proposed model responses had substantially higher affective empathy, cognitive empathy, and BLEU scores than did the baseline model. Subjective evaluations also indicated that the responses of the proposed model had greater empathy, relevance, and fluency than did the baseline model. Moreover, the proposed model outperformed other similar models in A/B tests.",,,,empathy,No,Yes
acm,Troi: Towards Understanding Users Perspectives to Mobile Automatic Emotion Recognition System in Their Natural Setting,"Dissanayake, Vipula; Tang, Vanessa; Elvitigala, Don Samitha; Wen, Elliott; Wu, Michelle; Nanayakkara, Suranga",2022,,6,,10.1145/3546738,"Emotional Self-Awareness (ESA) plays a vital role in physical and mental well-being. Recent advancements in artificial intelligence technologies have shown promising emotion recognition results, opening new opportunities to build systems to support ESA. However, little research has been done to understand users' perspectives on artificial-intelligence-based emotion recognition systems. We introduce Troi, an automatic emotion recognition mobile app using wearable signals. With Troi, we ran a multi-day user study with 12 users to understand user preference parameters, such as perceived accuracy, confidence, preferred emotion representations, effect of self-awareness of emotions, and real-time use cases. Further, we extend our study to evaluate the machine learning model in-the-wild to understand behaviours in-the-wild. We found that users perceived accuracy of the emotion recognition model is higher than the actual model prediction accuracy; there was no strong preference for one specific emotion representation, and users' self-awareness of emotions improved over time.",physiological signals; mobile application; automatic emotion recognition; in-the-wild study,,,emotion,No,Yes
acm,Multi-Modal Attentive Prompt Learning for Few-shot Emotion Recognition in Conversations,"Liang, Xingwei; Tu, Geng; Du, Jiachen; Xu, Ruifeng",2024,,79,,10.1613/jair.1.15301,"Emotion recognition in conversations (ERC) has emerged as an important research area in Natural Language Processing and Affective Computing, focusing on accurately identifying emotions within the conversational utterance. Conventional approaches typically rely on labeled training samples for fine-tuning pre-trained language models (PLMs) to enhance classification performance. However, the limited availability of labeled data in real-world scenarios poses a significant challenge, potentially resulting in diminished model performance. In response to this challenge, we present the Multi-modal Attentive Prompt (MAP) learning framework, tailored specifically for few-shot emotion recognition in conversations. The MAP framework consists of four integral modules: multi-modal feature extraction for the sequential embedding of text, visual, and acoustic inputs; a multi-modal prompt generation module that creates six manually-designed multi-modal prompts; an attention mechanism for prompt aggregation; and an emotion inference module for emotion prediction. To evaluate our proposed model’s efficacy, we conducted extensive experiments on two widely recognized benchmark datasets, MELD and IEMOCAP. Our results demonstrate that the MAP framework outperforms state-of-the-art ERC models, yielding notable improvements of 3.5% and 0.4% in micro F1 scores. These findings highlight the MAP learning framework’s ability to effectively address the challenge of limited labeled data in emotion recognition, offering a promising strategy for improving ERC model performance.",,,,emotion,Yes,Yes
acm,Patent Applications as Glimpses into the Sociotechnical Imaginary: Ethical Speculation on the Imagined Futures of Emotion AI for Mental Health Monitoring and Detection,"Karizat, Nadia; Vinson, Alexandra H.; Parthasarathy, Shobita; Andalibi, Nazanin",2024,,8,,10.1145/3637383,"Patent applications provide insight into how inventors imagine and legitimize uses of their imagined technologies; as part of this imagining they envision social worlds and produce sociotechnical imaginaries. Examining sociotechnical imaginaries is important for emerging technologies in high-stakes contexts such as the case of emotion AI to address mental health care. We analyzed emotion AI patent applications (N=58) filed in the U.S. concerned with monitoring and detecting emotions and/or mental health. We examined the described technologies' imagined uses and the problems they were positioned to address. We found that inventors justified emotion AI inventions as solutions to issues surrounding data accuracy, care provision and experience, patient-provider communication, emotion regulation, and preventing harms attributed to mental health causes. We then applied an ethical speculation lens to anticipate the potential implications of the promissory emotion AI-enabled futures described in patent applications. We argue that such a future is one filled with mental health conditions' (or 'non-expected' emotions') stigmatization, equating mental health with propensity for crime, and lack of data subjects' agency. By framing individuals with mental health conditions as unpredictable and not capable of exercising their own agency, emotion AI mental health patent applications propose solutions that intervene in this imagined future: intensive surveillance, an emphasis on individual responsibility over structural barriers, and decontextualized behavioral change interventions. Using ethical speculation, we articulate the consequences of these discourses, raising questions about the role of emotion AI as positive, inherent, or inevitable in health and care-related contexts. We discuss our findings' implications for patent review processes, and advocate for policy makers, researchers and technologists to refer to patent (applications) to access, evaluate and (re)consider potentially harmful sociotechnical imaginaries before they become our reality.",emotion recognition; mental health; healthcare; ai ethics; data subjects; emotion ai; emotion artificial intelligence; ethical speculation; patents; sociotechnical imaginary,,,emotion,No,No
acm,Recent Trends in Deep Learning Based Textual Emotion Cause Extraction,"Su, Xinxin; Huang, Zhen; Zhao, Yunxiang; Chen, Yifan; Dou, Yong; Pan, Hengyue",2023,,31,,10.1109/TASLP.2023.3254166,"Emotion Cause Extraction Field (ECEF) focuses on the cause that triggers an emotion in a document. Traditional ECEF aims to extract the cause based on a given emotion while recent ECEF focuses more on extracting both the emotion and its corresponding cause. ECEF has attracted a lot of attention due to the significant developments in deep learning techniques, especially machine reading comprehension and neural network-based information retrieval. However, a comprehensive review of existing approaches and recent trends in ECEF is lacking. In this paper, we present a thorough survey to summarise existing methods for ECEF, including those for Emotion Cause Extraction (ECE), Emotion Cause Pair Extraction (ECPE), and Conversational Emotion Cause Extraction Field (CECEF). We also detail the widely used public datasets and discuss the limitations and prospects of existing methods in ECEF.",,,,emotion,No,No
acm,Understanding the Performance of AI Algorithms in Text-Based Emotion Detection for Conversational Agents,"Kusal, Sheetal D.; Patil, Shruti G.; Choudrie, Jyoti; Kotecha, Ketan V.",2024,,23,,10.1145/3643133,"Current industry trends demand automation in every aspect, where machines could replace humans. Recent advancements in conversational agents have grabbed a lot of attention from industries, markets, and businesses. Building conversational agents that exhibit human communication characteristics is a need in today's marketplace. Thus, by accumulating emotions, we can build emotionally aware conversational agents. Emotion detection in text-based dialogues has turned into a pivotal component of conversational agents, enhancing their ability to understand and respond to users’ emotional states. This article extensively compares various artificial intelligence techniques adapted to text-based emotion detection for conversational agents. The study covers a wide range of methods, from machine learning models to cutting-edge pre-trained models and deep learning models. We evaluate the performance of these techniques on the benchmark unbalanced Topical-Chat and balanced Empathetic Dialogue datasets. This article offers an overview of the practical implications of emotion detection techniques in conversational systems and their impact on user response. The outcomes of this work contribute to the ongoing development of empathetic conversational agents, emphasizing natural human-machine interactions.",deep learning; Artificial intelligence; machine learning; natural language processing; pre-trained models; text-based emotion detection,,,emotion,No,No
acm,Values in Emotion Artificial Intelligence Hiring Services: Technosolutions to Organizational Problems,"Roemmich, Kat; Rosenberg, Tillie; Fan, Serena; Andalibi, Nazanin",2023,,7,,10.1145/3579543,"Despite debates about emotion artificial intelligence's (EAI) validity, legality, and social consequences, EAI is increasingly present in the high stakes context of hiring, with potential to shape the future of work and the workforce. The values laden in technology play a significant role in its societal impact.We conducted qualitative content analysis on the public-facing websites (N=229) of EAI hiring services. We identify the organizational problems that EAI hiring services claim to solve and reveal the values emerging in desired EAI uses as promoted by EAI hiring services to solve organizational problems. Our findings show that EAI hiring services market their technologies as technosolutions to three purported organizational hiring problems: 1) hiring (in)accuracy, 2) hiring (mis)fit, and 3) hiring (in)authenticity. We unpack these problems to expose how these desired uses of EAI are legitimized by the corporate ideals of data-driven decision making, continuous improvement, precision, loyalty, and stability. We identify the unfair and deceptive mechanisms by which EAI hiring services claim to solve the purported organizational hiring problems, suggesting that they unfairly exclude and exploit job candidates through EAI's creation, extraction, and affective commodification of a candidate's affective value through pseudoscientific approaches. Lastly, we interrogate EAI hiring service claims to reveal the core values that underpin their stated desired use: techno-omnipresence, techno-omnipotence, and techno-omniscience. We show how EAI hiring services position desired use of their technology as a moral imperative for hiring organizations with supreme capabilities to solve organizational hiring problems, then discuss implications for fairness, ethics, and policy in EAI-enabled hiring within the US policy landscape.",emotion recognition; affective computing; values; future of work; ai ethics; emotion ai; ai; algorithmic decision-making; artificial emotional intelligence; employment assessments; labor; psychometrics; recruiting software; talent; talent acquisition,,,emotion,No,No
acm,Decoding Functional Brain Data for Emotion Recognition: A Machine Learning Approach,"Tülay, Emine Elif; Balli, Tugçe",2024,,21,,10.1145/3657638,"The identification of emotions is an open research area and has a potential leading role in the improvement of socio-emotional skills such as empathy, sensitivity, and emotion recognition in humans. The current study aimed at using Event Related Potential (ERP) components (N100, N200, P200, P300, early Late Positive Potential (LPP), middle LPP, and late LPP) of EEG data for the classification of emotional states (positive, negative, neutral). EEG data were collected from 62 healthy individuals over 18 electrodes. An emotional paradigm with pictures from the International Affective Picture System (IAPS) was used to record the EEG data. A linear Support Vector Machine (C = 0.1) was used to classify emotions, and a forward feature selection approach was used to eliminate irrelevant features. The early LPP component, which was the most discriminative among all ERP components, had the highest classification accuracy (70.16%) for identifying negative and neutral stimuli. The classification of negative versus neutral stimuli had the best accuracy (79.84%) when all ERP components were used as a combined feature set, followed by positive versus negative stimuli (75.00%) and positive versus neutral stimuli (68.55%). Overall, the combined ERP component feature sets outperformed single ERP component feature sets for all stimulus pairings in terms of accuracy. These findings are promising for further research and development of EEG-based emotion recognition systems.",emotion classification; support vector machine (SVM); Event-related potentials (ERP); sequential forward selection,,,emotion,No,Yes
acm,Latent Discriminative Models for Social Emotion Detection with Emotional Dependency,"Quan, Xiaojun; Wang, Qifan; Zhang, Ying; Si, Luo; Wenyin, Liu",2015,,34,,10.1145/2749459,"Sentiment analysis of such opinionated online texts as reviews and comments has received increasingly close attention, yet most of the work is intended to deal with the detection of authors’ emotion. In contrast, this article presents our study of the social emotion detection problem, the objective of which is to identify the evoked emotions of readers by online documents such as news articles. A novel Latent Discriminative Model (LDM) is proposed for this task. LDM works by introducing intermediate hidden variables to model the latent structure of input text corpora. To achieve this, it defines a joint distribution over emotions and latent variables, conditioned on the observed text documents. Moreover, we assume that social emotions are not independent but correlated with one another, and the dependency of them is capable of providing additional guidance to LDM in the training process. The inclusion of this emotional dependency into LDM gives rise to a new Emotional Dependency-based LDM (eLDM). We evaluate the proposed models through a series of empirical evaluations on two real-world corpora of news articles. Experimental results verify the effectiveness of LDM and eLDM in social emotion detection.",Discriminative model; opinion mining and sentiment analysis; social emotion detection,,,emotion,No,No
acm,Open Questions for Empathy-Building Interventions for Inclusive Software Development,"L-Messerle, Kyle; Malachowsky, Samuel; Krutz, Daniel E.",2023,,38,,,"Research has demonstrated that much of the software being created today is not sufficiently inclusive, unbiased and equitable. This has been found to frequently result in real-world implications such as prejudice against women or people of color, and software that is inaccessible to people with disabilities. Preliminary research has found that empathy-focused experiential educational activities can be beneficial for not only creating empathy, but in advancing the participant's interest and knowledge retention over traditional non empathy-building interventions. This work will provide a foundational background on the current research in the intersection of experiential learning and empathy-building interventions in computing education. We will also present several important questions that still must be explored, thus serving as the foundation for future work in this area.",,,,empathy,No,No
acm,Empathy in Virtual Agents and Robots: A Survey,"Paiva, Ana; Leite, Iolanda; Boukricha, Hana; Wachsmuth, Ipke",2017,,7,,10.1145/2912150,"This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user’s or another agent’s emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent’s situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future.",empathy; human-computer interaction; human-robot interaction; affective computing; social robots; virtual agents,,,empathy,No,No
acm,Social-Emotional-Sensory Design Map for Affective Computing Informed by Neurodivergent Experiences,"Zolyomi, Annuska; Snyder, Jaime",2021,,5,,10.1145/3449151,"One of the grand challenges of artificial intelligence and affective computing is for technology to become emotionally-aware and thus, more human-like. Modeling human emotions is particularly complicated when we consider the lived experiences of people who are on the autism spectrum. To understand the emotional experiences of autistic adults and their attitudes towards common representations of emotions, we deployed a context study as the first phase of a Grounded Design research project. Based on community observations and interviews, this work contributes empirical evidence of how the emotional experiences of autistic adults are entangled with social interactions as well as the processing of sensory inputs. We learned that (1) the emotional experiences of autistic adults are embodied and co-constructed within the context of physical environments, social relationships, and technology use, and (2) conventional approaches to visually representing emotion in affective education and computing systems fail to accurately represent the experiences and perceptions of autistic adults. We contribute a social-emotional-sensory design map to guide designers in creating more diverse and nuanced affective computing interfaces that are enriched by accounting for neurodivergent users.",interpersonal communication; emotions; autism; accessibility; social-emotional learning,,,emotion,No,No
acm,The Empathetic Car: Exploring Emotion Inference via Driver Behaviour and Traffic Context,"Liu, Shu; Koch, Kevin; Zhou, Zimu; Föll, Simon; He, Xiaoxi; Menke, Tina; Fleisch, Elgar; Wortmann, Felix",2021,,5,,10.1145/3478078,"An empathetic car that is capable of reading the driver's emotions has been envisioned by many car manufacturers. Emotion inference enables in-vehicle applications to improve driver comfort, well-being, and safety. Available emotion inference approaches use physiological, facial, and speech-related data to infer emotions during driving trips. However, existing solutions have two major limitations: Relying on sensors that are not built into the vehicle restricts emotion inference to those people leveraging corresponding devices (e.g., smartwatches). Relying on modalities such as facial expressions and speech raises privacy concerns. By contrast, researchers in mobile health have been able to infer affective states (e.g., emotions) based on behavioral and contextual patterns decoded in available sensor streams, e.g., obtained by smartphones. We transfer this rationale to an in-vehicle setting by analyzing the feasibility of inferring driver emotions by passively interpreting the data streams of the control area network (CAN-bus) and the traffic context (inferred from the front-view camera). Therefore, our approach does not rely on particularly privacy-sensitive data streams such as the driver facial video or driver speech, but is built based on existing CAN-bus data and traffic information, which is available in current high-end or future vehicles. To assess our approach, we conducted a four-month field study on public roads covering a variety of uncontrolled daily driving activities. Hence, our results were generated beyond the confines of a laboratory environment. Ultimately, our proposed approach can accurately recognise drivers' emotions and achieve comparable performance as the medical-grade physiological sensor-based state-of-the-art baseline method.",Emotion recognition; Control area network (CAN); Driving behaviours; Intelligent vehicle; Traffic contexts,,,emotion,No,No
acm,An Autonomous Cognitive Empathy Model Responsive to Users’ Facial Emotion Expressions,"Bagheri, Elahe; Esteban, Pablo G.; Cao, Hoang-Long; Beir, Albert De; Lefeber, Dirk; Vanderborght, Bram",2020,,10,,10.1145/3341198,"Successful social robot services depend on how robots can interact with users. The effective service can be obtained through smooth, engaged, and humanoid interactions in which robots react properly to a user’s affective state. This article proposes a novel Automatic Cognitive Empathy Model, ACEM, for humanoid robots to achieve longer and more engaged human-robot interactions (HRI) by considering humans’ emotions and replying to them appropriately. The proposed model continuously detects the affective states of a user based on facial expressions and generates desired, either parallel or reactive, empathic behaviors that are already adapted to the user’s personality. Users’ affective states are detected using a stacked autoencoder network that is trained and tested on the RAVDESS dataset.The overall proposed empathic model is verified throughout an experiment, where different emotions are triggered in participants and then empathic behaviors are applied based on proposed hypothesis. The results confirm the effectiveness of the proposed model in terms of related social and friendship concepts that participants perceived during interaction with the robot.",Empathy; social robots; adaptive interaction; facial emotion detection; human robot interaction; non-verbal behavior,,,emotion_and_empathy,No,No
acm,The Language of Situational Empathy,"Zhou, Ke; Aiello, Luca Maria; Scepanovic, Sanja; Quercia, Daniele; Konrath, Sara",2021,,5,,10.1145/3449087,"Empathy is the tendency to understand and share others' thoughts and feelings. Literature in psychology has shown through surveys potential beneficial implications of empathy. Prior psychology literature showed that a particular type of empathy called ""situational empathy"" — an immediate empathic response to a triggering situation (e.g., a distressing situation) — is reflected in the language people use in response to the situation. However, this has not so far been properly measured at scale. In this work, we collected 4k textual reactions (and corresponding situational empathy labels) to different stories. Driven by theoretical concepts, we developed computational models to predict situational empathy from text and, in so doing, we built and made available a list of empathy-related words. When applied to Reddit posts and movie transcripts, our models produced results that matched prior theoretical findings, offering evidence of external validity and suggesting its applicability to unstructured data. The capability of measuring proxies for empathy at scale might benefit a variety of areas such as social media, digital healthcare, and workplace well-being.",empathy; computational model; lexicon,,,empathy,Yes,No
acm,Emotion Recognition with Conversational Generation Transfer,"Ma, Hongchao; Wang, Zhongqing; Zhou, Xiabing; Zhou, Guodong; Zhou, Qinglei",2022,,21,,10.1145/3494532,"Emotion recognition in conversation is one of the essential tasks of natural language processing. However, this task’s annotation data is insufficient since such data is hard to collect and annotate. Meanwhile, there is large-scale data for conversational generation, and this data does not need annotation manually. But, whether the vector space between different datasets is similar will be a problem. Therefore, we utilize a same dataset to train the conversational generator and the classifier, and transfer knowledge between them. In particular, we propose an Emotion Recognition with Conversational Generation Transfer (ERCGT) framework to model the interaction among utterances by transfer learning. First, we train a conversational generator. In the second step, a transfer learning model is used to transfer the knowledge of generator to the emotion recognition model. Empirical studies illustrate the effectiveness of the proposed framework over several strong baselines on three benchmark emotion classification datasets.",Emotion recognition in conversation; transfer learning; conversational generation,,,emotion,Yes,No
acm,An interdisciplinary approach to detecting empathy through emotional analytics and eye tracking,"Cotler, Jami L.; Villa, Luis; Burshteyn, Dmitry; Bult, Zachary; Grant, Garrison; Tanski, Michael; Parente, Anthony",2020,,35,,,"The aim of this interdisciplinary study was to bring together different perspectives to discover if detecting empathetic emotional reactions is possible. This area of research has received recent attention from the computer science, human-computer interaction and psychological research communities. The research team consisted of three students; a computer science, sociology and marketing major. The team worked to understand the complexities of detecting emotions based on facial movement. The team collected time stamped facial emotional data from 210 participants as they watched a video clip from the popular movie depicting bullying behavior towards a disabled person. The results demonstrated significant before-and- after mean differences in emotions that are characterized as empathic towards the main character for the bullying events, which is a promising start to detecting empathic reactions. Each student brought a different perspective from their majors resulting in an educational experience that transcended learning about emotional analytics.",,,,emotion_and_empathy,No,No
acm,From Digital Media to Empathic Spaces: A Systematic Review of Empathy Research in Extended Reality Environments,"Paananen, Ville; Kiarostami, Mohammad Sina; Lik-Hang, Lee; Braud, Tristan; Hosio, Simo",2023,,56,,10.1145/3626518,"Recent advances in extended reality (XR) technologies have enabled new and increasingly realistic empathy tools and experiences. In XR, all interactions take place in different spatial contexts, all with different features, affordances, and constraints. We present a systematic literature survey of recent work on empathy in XR. As a result, we contribute a research roadmap with three future opportunities and six open questions in XR-enabled empathy research across both physical and virtual spaces.",empathy; Extended reality (XR); human-computer interaction (HCI); metaverse; spatiality,,,empathy,No,No
acm,Emotion Ontology Studies: A Framework for Expressing Feelings Digitally and its Application to Sentiment Analysis,"Park, Eun Hee; Storey, Veda C.",2023,,55,,10.1145/3555719,"Emotion ontologies have been developed to capture affect, a concept that encompasses discrete emotions and feelings, especially for research on sentiment analysis, which analyzes a customer's attitude towards a company or a product. However, there have been limited efforts to adapt and employ these ontologies. This research surveys and synthesizes emotion ontology studies to develop a Framework of Emotion Ontologies that can be used to help a user select or design an appropriate emotion ontology to support sentiment analysis and increase the user's understanding of the roles of affect, context, and behavioral information with respect to sentiment. The framework, which is derived from research on emotion ontologies, psychology, and sentiment analysis, classifies emotion ontologies as discrete emotion or one of two hybrid ontologies that are combinations of the discrete, dimensional, or componential process emotion paradigms. To illustrate its usefulness, the framework is applied to the development of an emotion ontology for a sentiment analysis application.",affect; emotion; sentiment analysis; Ontology; componential process ontology; dimensional emotion ontology; discrete emotion ontology; Framework of Emotion Ontologies,,,emotion,No,No
acm,Technical Design Space Analysis for Unobtrusive Driver Emotion Assessment Using Multi-Domain Context,"Bethge, David; Coelho, Luis Falconeri; Kosch, Thomas; Murugaboopathy, Satiyabooshan; Zadow, Ulrich von; Schmidt, Albrecht; Grosse-Puppendahl, Tobias",2023,,6,,10.1145/3569466,"Driver emotions play a vital role in driving safety and performance. Consequently, regulating driver emotions through empathic interfaces have been investigated thoroughly. However, the prerequisite - driver emotion sensing - is a challenging endeavor: Body-worn physiological sensors are intrusive, while facial and speech recognition only capture overt emotions. In a user study (N=27), we investigate how emotions can be unobtrusively predicted by analyzing a rich set of contextual features captured by a smartphone, including road and traffic conditions, visual scene analysis, audio, weather information, and car speed. We derive a technical design space to inform practitioners and researchers about the most indicative sensing modalities, the corresponding impact on users' privacy, and the computational cost associated with processing this data. Our analysis shows that contextual emotion recognition is significantly more robust than facial recognition, leading to an overall improvement of 7% using a leave-one-participant-out cross-validation.",Affective Computing; Automotive; Emotion Sensing; Empathic Interfaces; In-the-Wild Analysis; Remote Sensors,,,emotion,No,Yes
acm,Prototyping a Zoomorphic Interactive Robot Companion with Emotion Recognition and Affective Voice Interaction for Elderly People,"Schnitzer, Benjamin; Vural, Umut Can; Schnitzer, Bastian; Sardar, Muhammad Usman; Fuerst, Oren; Korn, Oliver",2024,,8,,10.1145/3660244,"An aging society paired with a skilled labor shortage, particularly in European countries, requires a rethinking of deprecated structures. Intelligent assistive technologies, specifically socially assistive robots, addressing the gap between caretakers and elderly people in need of care have moved into the focus of debate due to their potentials to reduce costs, improve independence, and eventually raise quality of life. In this work, we outline the potentials of zoomorphic robot companions combining intelligent conversational abilities and emotion recognition. We then describe the prototyping of an emotion-sensing zoomorphic interactive robot companion including the development and implementation of a multimodal emotion recognition framework. This framework uses speech emotion recognition, sentiment analysis, and affective voice interaction based on a large language model. The prototyping has been accompanied by two studies on elderly peoples' design preferences regarding the proposed feature set as well as different embodiments to find the appropriate casing for the robot companion. This work provides valuable insights into the prototyping and can thus support future research endeavors in this area.",Affective Computing; Health; Speech Emotion Recognition; Elderly People; Intelligent Assistive Technology; Socially Assistive Robots; Zoomorphic Embodiment,,,emotion,No,No
acm,FAtiMA Toolkit: Toward an Accessible Tool for the Development of Socio-emotional Agents,"Mascarenhas, Samuel; Guimarães, Manuel; Prada, Rui; Santos, Pedro A.; Dias, João; Paiva, Ana",2022,,12,,10.1145/3510822,"More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this article, we describe the FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers, and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects, and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, the FAtiMA Toolkit’s library-based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components, and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of the FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed.",affective computing; emotions; social robots; cognitive architecture; Embodied agents,,,emotion,No,No
acm,Automated Emotion Recognition in the Workplace: How Proposed Technologies Reveal Potential Futures of Work,"Boyd, Karen L.; Andalibi, Nazanin",2023,,7,,10.1145/3579528,"Emotion recognition technologies, while critiqued for bias, validity, and privacy invasion, continue to be developed and applied in a range of domains including in high-stakes settings like the workplace. We set out to examine emotion recognition technologies proposed for use in the workplace, describing the input data and training, outputs, and actions that these systems take or prompt. We use these design features to reflect on these technologies' implications using the ethical speculation lens. We analyzed patent applications that developed emotion recognition technologies to be used in the workplace (N=86). We found that these technologies scope data collection broadly; claim to reveal not only targets' emotional expressions, but also their internal states; and take or prompt a wide range of actions, many of which impact workers' employment and livelihoods. Technologies described in patent applications frequently violated existing guidelines for ethical automated emotion recognition technology. We demonstrate the utility of using patent applications for ethical speculation. In doing so, we suggest that 1) increasing the visibility of claimed emotional states has the potential to create additional emotional labor for workers (a burden that is disproportionately distributed to low-power and marginalized workers) and contribute to a larger pattern of blurring boundaries between expectations of the workplace and a worker's autonomy, and more broadly to the data colonialism regime; 2) Emotion recognition technology's failures can be invisible, may inappropriately influence high-stakes workplace decisions and can exacerbate inequity. We discuss the implications of making emotions and emotional data visible in the workplace and submit for consideration implications for designers of emotion recognition, employers who use them, and policymakers.",emotion recognition; AI ethics; patents; AI and the future of work; emotion AI,,,emotion,No,No
acm,Personality-affected Emotion Generation in Dialog Systems,"Wen, Zhiyuan; Cao, Jiannong; Shen, Jiaxing; Yang, Ruosong; Liu, Shuaiqi; Sun, Maosong",2024,,42,,10.1145/3655616,"Generating appropriate emotions for responses is essential for dialogue systems to provide human-like interaction in various application scenarios. Most previous dialogue systems tried to achieve this goal by learning empathetic manners from anonymous conversational data. However, emotional responses generated by those methods may be inconsistent, which will decrease user engagement and service quality. Psychological findings suggest that the emotional expressions of humans are rooted in personality traits. Therefore, we propose a new task, Personality-affected Emotion Generation, to generate emotion based on the personality given to the dialogue system and further investigate a solution through the personality-affected mood transition. Specifically, we first construct a daily dialogue dataset, Personality EmotionLines Dataset (PELD), with emotion and personality annotations. Subsequently, we analyze the challenges in this task, i.e., (1) heterogeneously integrating personality and emotional factors and (2) extracting multi-granularity emotional information in the dialogue context. Finally, we propose to model the personality as the transition weight by simulating the mood transition process in the dialogue system and solve the challenges above. We conduct extensive experiments on PELD for evaluation. Results suggest that by adopting our method, the emotion generation performance is improved by 13% in macro-F1 and 5% in weighted-F1 from the BERT-base model.",emotion; dialogue systems; personality,,,emotion,No,Yes
acm,Data Subjects' Conceptualizations of and Attitudes Toward Automatic Emotion Recognition-Enabled Wellbeing Interventions on Social Media,"Roemmich, Kat; Andalibi, Nazanin",2021,,5,,10.1145/3476049,"Automatic emotion recognition (ER)-enabled wellbeing interventions use ER algorithms to infer the emotions of a data subject (i.e., a person about whom data is collected or processed to enable ER) based on data generated from their online interactions, such as social media activity, and intervene accordingly. The potential commercial applications of this technology are widely acknowledged, particularly in the context of social media. Yet, little is known about data subjects' conceptualizations of and attitudes toward automatic ER-enabled wellbeing interventions. To address this gap, we interviewed 13 US adult social media data subjects regarding social media-based automatic ER-enabled wellbeing interventions. We found that participants' attitudes toward automatic ER-enabled wellbeing interventions were predominantly negative. Negative attitudes were largely shaped by how participants compared their conceptualizations of Artificial Intelligence (AI) to the humans that traditionally deliver wellbeing support. Comparisons between AI and human wellbeing interventions were based upon human attributes participants doubted AI could hold: 1) helpfulness and authentic care; 2) personal and professional expertise; 3) morality; and 4) benevolence through shared humanity. In some cases, participants' attitudes toward automatic ER-enabled wellbeing interventions shifted when participants conceptualized automatic ER-enabled wellbeing interventions' impact on others, rather than themselves. Though with reluctance, a minority of participants held more positive attitudes toward their conceptualizations of automatic ER-enabled wellbeing interventions, citing their potential to benefit others: 1) by supporting academic research; 2) by increasing access to wellbeing support; and 3) through egregious harm prevention. However, most participants anticipated harms associated with their conceptualizations of automatic ER-enabled wellbeing interventions for others, such as re-traumatization, the spread of inaccurate health information, inappropriate surveillance, and interventions informed by inaccurate predictions. Lastly, while participants had qualms about automatic ER-enabled wellbeing interventions, we identified three development and delivery qualities of automatic ER-enabled wellbeing interventions upon which their attitudes toward them depended: 1) accuracy; 2) contextual sensitivity; and 3) positive outcome. Our study is not motivated to make normative statements about whether or how automatic ER-enabled wellbeing interventions should exist, but to center voices of the data subjects affected by this technology. We argue for the inclusion of data subjects in the development of requirements for ethical and trustworthy ER applications. To that end, we discuss ethical, social, and policy implications of our findings, suggesting that automatic ER-enabled wellbeing interventions imagined by participants are incompatible with aims to promote trustworthy, socially aware, and responsible AI technologies in the current practical and regulatory landscape in the US.",social media; emotion recognition; affective computing; ethics; AI ethics; fairness; emotion ai; artificial emotional intelligence; affect recognition; algorithmic accountability; wellbeing interventions,,,emotion,No,Yes
acm,Music Theory-Inspired Acoustic Representation for Speech Emotion Recognition,"Li, Xingfeng; Shi, Xiaohan; Hu, Desheng; Li, Yongwei; Zhang, Qingchen; Wang, Zhengxia; Unoki, Masashi; Akagi, Masato",2023,,31,,10.1109/TASLP.2023.3289312,"This research presents a music theory-inspired acoustic representation (hereafter, MTAR) to address improved speech emotion recognition. The recognition of emotion in speech and music is developed in parallel, yet a relatively limited understanding of MTAR for interpreting speech emotions is involved. In the present study, we use music theory to study representative acoustics associated with emotion in speech from vocal emotion expressions and auditory emotion perception domains. In experiments assessing the role and effectiveness of the proposed representation in classifying discrete emotion categories and predicting continuous emotion dimensions, it shows promising performance compared with extensively used features for emotion recognition based on the spectrogram, Mel-spectrogram, Mel-frequency cepstral coefficients, VGGish, and the large baseline feature sets of the INTERSPEECH challenges. This proposal opens up a novel research avenue in developing a computational acoustic representation of speech emotion via music theory.",,,,emotion,No,Yes
acm,Target-guided Emotion-aware Chat Machine,"Wei, Wei; Liu, Jiayi; Mao, Xianling; Guo, Guibing; Zhu, Feida; Zhou, Pan; Hu, Yuchong; Feng, Shanshan",2021,,39,,10.1145/3456414,"The consistency of a response to a given post at the semantic level and emotional level is essential for a dialogue system to deliver humanlike interactions. However, this challenge is not well addressed in the literature, since most of the approaches neglect the emotional information conveyed by a post while generating responses. This article addresses this problem and proposes a unified end-to-end neural architecture, which is capable of simultaneously encoding the semantics and the emotions in a post and leveraging target information to generate more intelligent responses with appropriately expressed emotions. Extensive experiments on real-world data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of both content coherence and emotion appropriateness.",Dialogue generation; emotional chatbot; emotional conversation,,,emotion,No,No
acm,An Emotional Analysis of False Information in Social Media and News Articles,"Ghanem, Bilal; Rosso, Paolo; Rangel, Francisco",2020,,20,,10.1145/3381750,"Fake news is risky, since it has been created to manipulate readers’ opinions and beliefs. In this work, we compared the language of false news to the real one of real news from an emotional perspective, considering a set of false information types (propaganda, hoax, clickbait, and satire) from social media and online news article sources. Our experiments showed that false information has different emotional patterns in each of its types, and emotions play a key role in deceiving the reader. Based on that, we proposed an LSTM neural network model that is emotionally infused to detect false news.",Fake news; emotional analysis; false information; suspicious news,,,emotion,No,No
acm,EmoInt-Trans: A Multimodal Transformer for Identifying Emotions and Intents in Social Conversations,"Singh, Gopendra Vikram; Firdaus, Mauajama; Ekbal, Asif; Bhattacharyya, Pushpak",2022,,31,,10.1109/TASLP.2022.3224287,"In the natural language processing community, open-domain conversational agents, also known as chatbots, are gaining popularity. One of the difficulties is getting them to communicate in an emotionally intelligent manner. To generate dialogues, current neural response generation methods depend solely on end-to-end learning from large scale conversation data. Therefore, we introduce a large-scale multi Emotion and Intent guided Multimodal Dialogue (EmoInt-MD) dataset labelled with 32 emotions and 15 empathetic intents having 32 k dialogues taken from different movie genres. We propose a novel multi-task multimodal contextual Transformer framework for simultaneously identifying the emotions and intents in a given utterance utilizing audio and visual features in addition to the textual information. Experimental analysis proves that the proposed framework outperforms several unimodal and multimodal baselines on the &lt;italic&gt;EmoInt-MD&lt;/italic&gt; dataset. This dataset along with our baseline and proposed framework implementations will be made publicly available for research purposes.",,,,emotion,Yes,No
acm,Robots’ “Woohoo” and “Argh” Can Enhance Users’ Emotional and Social Perceptions: An Exploratory Study on Non-lexical Vocalizations and Non-linguistic Sounds,"Liu, Xiaozhen; Dong, Jiayuan; Jeon, Myounghoon",2023,,12,,10.1145/3626185,"As robots have become more pervasive in our everyday life, social aspects of robots have attracted researchers’ attention. Because emotions play a crucial role in social interactions, research has been conducted on conveying emotions via speech. Our study sought to investigate the synchronization of multimodal interaction in human-robot interaction (HRI). We conducted a within-subjects exploratory study with 40 participants to investigate the effects of non-speech sounds (natural voice, synthesized voice, musical sound, and no sound) and basic emotions (anger, fear, happiness, sadness, and surprise) on user perception with emotional body gestures of an anthropomorphic robot (Pepper). While listening to a fairytale with the participant, a humanoid robot responded to the story with recorded emotional non-speech sounds and gestures. Participants showed significantly higher emotion recognition accuracy from the natural voice than from other sounds. The confusion matrix showed that happiness and sadness had the highest emotion recognition accuracy, which is in line with previous research. The natural voice also induced higher trust, naturalness, and preference compared to other sounds. Interestingly, the musical sound mostly showed lower perception ratings, even compared to no sound. Results are discussed with design guidelines for emotional cues from social robots and future research directions.",Human-robot interaction; emotion perception; non-speech sounds; robot voice,,,emotion,No,Yes
acm,PEACE: A Model of Key Social and Emotional Qualities of Conversational Chatbots,"Svikhnushina, Ekaterina; Pu, Pearl",2022,,12,,10.1145/3531064,"Open-domain chatbots engage with users in natural conversations to socialize and establish bonds. However, designing and developing an effective open-domain chatbot is challenging. It is unclear what qualities of a chatbot most correspond to users’ expectations and preferences. Even though existing work has considered a wide range of aspects, some key components are still missing. For example, the role of chatbots’ ability to communicate with humans at the emotional level remains an open subject of study. Furthermore, these trait qualities are likely to cover several dimensions. It is crucial to understand how the different qualities relate and interact with each other and what the core aspects would be. For this purpose, we first designed an exploratory user study aimed at gaining a basic understanding of the desired qualities of chatbots with a special focus on their emotional intelligence. Using the findings from the first study, we constructed a model of the desired traits by carefully selecting a set of features. With the help of a large-scale survey and structural equation modeling, we further validated the model using data collected from the survey. The final outcome is called the PEACE model (Politeness, Entertainment, Attentive Curiosity, and Empathy). By analyzing the dependencies between the different PEACE constructs, we shed light on the importance of and interplay between the chatbots’ qualities and the effect of users’ attitudes and concerns on their expectations of the technology. Not only PEACE defines the key ingredients of the social qualities of a chatbot, it also helped us derive a set of design implications useful for the development of socially adequate and emotionally aware open-domain chatbots.",Chatbots; interviews; user study; conversational agents; adoption; emotional intelligence; SEM; social intelligence,,,emotion,No,No
acm,What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?,"Gao, Yuan; Bianchi-Berthouze, Nadia; Meng, Hongying",2012,,19,,10.1145/2395131.2395138,"The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players’ emotional states. This prospect would not only be a valuable evaluation indicator for game designers, but also for real-time personalization of the game experience. Psychology studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emotional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69% and 77% for the four emotional states, and higher results ( 89%) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.",Automatic emotion recognition; affective touch; touch behavior; touch-based computer games,,,emotion,No,No
acm,Chinese Emotional Dialogue Response Generation via Reinforcement Learning,"Lan, Rushi; Wang, Jing; Huang, Wenming; Deng, Zhenrong; Sun, Xiyan; Chen, Zhuo; Luo, Xiaonan",2021,,21,,10.1145/3446390,"In an open-domain dialogue system, recognition and expression of emotions are the key factors for success. Most of the existing research related to Chinese dialogue systems aims at improving the quality of content but ignores the expression of human emotions. In this article, we propose a Chinese emotional dialogue response generation algorithm based on reinforcement learning that can generate responses not only according to content but also according to emotion. In the proposed method, a multi-emotion classification model is first used to add emotion labels to the corpus of post-response pairs. Then, with the help of reinforcement learning, the reward function is constructed based on two aspects, namely, emotion and content. Among the generated candidates, the system selects the one with long-term success as the best reply. At the same time, to avoid safe responses and diversify dialogue, a diversity beam search algorithm is applied in the decoding process. The comparative experiments demonstrate that the proposed model achieves satisfactory results according to both automatic and human evaluations.",reinforcement learning; emotion classification; Dialogue generation; reward function; safe response,,,emotion,No,No
acm,Crowdsourcing Empathetic Intelligence: The Case of the Annotation of EMMA Database for Emotion and Mood Recognition,"Katsimerou, Christina; Albeda, Joris; Huldtgren, Alina; Heynderickx, Ingrid; Redi, Judith A.",2016,,7,,10.1145/2897369,"Unobtrusive recognition of the user's mood is an essential capability for affect-adaptive systems. Mood is a subtle, long-term affective state, often misrecognized even by humans. The challenge to train a machine to recognize it from, for example, a video of the user, is significant, and already begins with the lack of ground truth for supervised learning. Existing affective databases consist mainly of short videos, annotated in terms of expressed emotions rather than mood. In very few cases, we encounter perceived mood annotations, of questionable reliability, however, due to the subjectivity of mood estimation and the small number of coders involved. In this work, we introduce a new database for mood recognition from video. Our database contains 180 long, acted videos, depicting typical daily scenarios, and subtle facial and bodily expressions. The videos cover three visual modalities (face, body, Kinect data), and are annotated in terms of emotions (via G-trace) and mood (via the Self-Assessment Manikin and the AffectButton). To annotate the database exhaustively, we exploit crowdsourcing to reach out to an extensive number of nonexpert coders. We validate the reliability of our crowdsourced annotations by (1) adopting a number of criteria to filter out unreliable coders, and (2) comparing the annotations of a subset of our videos with those collected in a controlled lab setting.",emotion recognition; Multimodal database; crowdsourcing; affective annotation; mood recognition,,,emotion,Yes,No
acm,Multimodal Emotion-Cause Pair Extraction with Holistic Interaction and Label Constraint,"Li, Bobo; Fei, Hao; Li, Fei; Chua, Tat-seng; Ji, Donghong",2024,,,,10.1145/3689646,"The multimodal emotion-cause pair extraction (MECPE) task aims to detect the emotions, causes, and emotion-cause pairs from multimodal conversations. Existing methods for this task typically concatenate representations of each utterance from distinct modalities and then predict emotion-cause pairs directly. This approach struggles to effectively integrate multimodal features and capture the subtleties of emotion transitions, which are crucial for accurately identifying causes—thereby limiting overall performance. To address these challenges, we propose a novel model that captures holistic interaction and label constraint (HiLo) features for the MECPE task. HiLo facilitates cross-modality and cross-utterance feature interaction with various attention mechanisms, establishing a robust foundation for precise cause extraction. Notably, our model innovatively leverages emotion transition features as pivotal cues to enhance causal inference within conversations. The experimental results demonstrate the superior performance of HiLo, evidenced by an increase of more than 2% in the F1 score compared to existing benchmarks. Further analysis reveals that our approach adeptly utilizes multimodal and dialogue features, making a significant contribution to the field of emotion-cause analysis. Our code is publicly available at .",Emotion Recognition; Multimodal Learning,,,emotion,No,Yes
acm,I Can Help You Change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions,"Lisetti, Christine; Amini, Reza; Yasavur, Ugan; Rishe, Naphtali",2013,,4,,10.1145/2544103,"We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user’s during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change—the Drinker’s Check-Up (DCU)—whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users’ evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers.",affective computing; alcohol interventions; behavior change; brief motivational interviewing intervention; computer-based interventions; embodied conversational agent; empathy modeling; health informatics; healthy lifestyles; information systems; Intelligent virtual agent; multimodal communication,,,empathy,No,No
acm,Better Feedback from Nicer People: Narrative Empathy and Ingroup Framing Improve Feedback Exchange,"Wu, Y. Wayne; Bailey, Brian P.",2021,,4,,10.1145/3432935,"Online feedback exchange platforms enable content creators to collect a diverse set of design feedback quickly. However, creators can experience low quality and harsh feedback when using such platforms. In this paper, we leverage the empathy of the feedback provider to address both these issues. Specifically, we tested two narrative-based empathy arousal interventions: a negative experience and a design process narrative. We also examined whether ingroup framing further enhances the effects of empathy arousal. In a 3x2 online experiment, participants (n=205) wrote feedback on a poster design after experiencing one of the intervention conditions or a control condition. Our results show both the design process narrative and ingroup framing conditions significantly increased the feedback quality and effort invested in the task. The negative experience narrative condition had similar effects and participants reported significantly increased disapproval towards harsh feedback. We discuss the implications of our results for the design of feedback exchange platforms.",empathy; design feedback; ingroup framing,,,empathy,No,No
acm,Total VREcall: Using Biosignals to Recognize Emotional Autobiographical Memory in Virtual Reality,"Gupta, Kunal; Chan, Sam W. T.; Pai, Yun Suen; Strachan, Nicholas; Su, John; Sumich, Alexander; Nanayakkara, Suranga; Billinghurst, Mark",2022,,6,,10.1145/3534615,"Our memories and past experiences contribute to guiding our perception and action of future affective experiences. Virtual Reality (VR) experiences are more vividly memorized and recalled than non-VR ones, but there is little research on how to detect this recall in VR. We investigate the feasibility of recognizing autobiographical memory (AM) recall in VR using physiological cues: skin conductance, heart-rate variability, eye gaze, and pupillary response. We devised a methodology replicating an existing AM Test in VR. We conducted a user study with 20 participants recalling AM using three valence categories cue words: positive, negative, and neutral. We found a significant effect of AM recalls on EDA peak, and eye blink rate, with a generalized recognition accuracy of 77.1% and person dependent accuracy of up to 95.1%. This shows a promising approach for detecting AM recall in VR and we discuss the implications for VR experience design.",Emotion; Physiological Signals; Biosignals; Eye-tracking; Virtual Reality; Electrodermal Activity; Autobiographical Memory; Memory,,,emotion,No,Yes
acm,Towards a small set of robust acoustic features for emotion recognition: challenges,"Tahon, Marie; Devillers, Laurence",2016,,24,,10.1109/TASLP.2015.2487051,"The search of a small acoustic feature set for emotion recognition faces three main challenges. Such a feature set must be robust to large diversity of contexts in real-life applications; model parameters must also be optimized for reduced subsets; finally, the result of feature selection must be evaluated in cross-corpus condition. The goal of the present study is to select a consensual set of acoustic features for valence recognition using classification and non-classification based feature ranking and cross-corpus experiments, and to optimize emotional models simultaneously. Five realistic corpora are used in this study: three of them were collected in the framework of the French project on robotics ROMEO, one is a game corpus (JEMO) and one is the well-known AIBO corpus. Combinations of features found with nonclassification based methods (information gain and Gaussian mixture models with Bhattacharyya distance) through multicorpora experiments are tested under cross-corpus conditions, simultaneously with SVM parameters optimization. Reducing the number of features goes in pair with optimizing model parameters. Experiments carried on randomly selected features from two acoustic feature sets show that a feature space reduction is needed to avoid over-fitting. Since a Grid search tends to find non-standard values with small feature sets, the authors propose a multicorpus optimization method based on different corpora and acoustic feature subsets which ensures more stability. The results show that acoustic families selected with both feature ranking methods are not relevant in cross-corpus experiments. Promising results have been obtained with a small set of 24 voiced cepstral coefficients while this family was ranked in the 2nd and 5th positions with both ranking methods. The proposed optimization method is more robust than the usual Grid search for cross-corpus experiments with small feature sets.",emotion recognition; information gain; acoustic features; Bhattacharyya distance; crosscorpus; SVM parameter optimization,,,emotion,No,Yes
acm,Accountability and Empathy by Design: Encouraging Bystander Intervention to Cyberbullying on Social Media,"Taylor, Samuel Hardman; DiFranzo, Dominic; Choi, Yoon Hyung; Sannon, Shruti; Bazarova, Natalya N.",2019,,3,,10.1145/3359220,"Bystander intervention can reduce the amount of cyberbullying victimization on social media, but bystanders often fail to act. Limited accountability for their behavior and a lack of empathy for the victim are frequently cited as reasons for why bystanders do not act against cyberbullying. We developed design interventions that aimed to increase accountability and empathy among bystanders. In Study 1, participants were experimentally exposed to three social media posts with different types of empathy nudges. Empathy nudges embedded into social media posts displayed the potential to motivate empathy. In Study 2, participants took part in a 3-day experiment that simulated a social media experience. Results suggested that increased social transparency on social media promoted accountability through heightened self-presentation concerns, but empathy nudges did not encourage greater bystander empathy. Both accountability and empathy predicted bystander intervention, but the types of bystander actions promoted by each mechanism differed. We consider how these results contribute to theories of bystander behavior and designing social media to promote prosocial behaviors.",empathy; social media; cyberbullying; accountability,,,empathy,No,No
acm,Emotional Biosensing: Exploring Critical Alternatives,"Howell, Noura; Chuang, John; De Kosnik, Abigail; Niemeyer, Greg; Ryokai, Kimiko",2018,,2,,10.1145/3274338,"Emotional biosensing is rising in daily life: Data and categories claim to know how people feel and suggest what they should do about it, while CSCW explores new biosensing possibilities. Prevalent approaches to emotional biosensing are too limited, focusing on the individual, optimization, and normative categorization. Conceptual shifts can help explore alternatives: toward materiality, from representation toward performativity, inter-action to intra-action, shifting biopolitics, and shifting affect/desire. We contribute (1) synthesizing wide-ranging conceptual lenses, providing analysis connecting them to emotional biosensing design, (2) analyzing selected design exemplars to apply these lenses to design research, and (3) offering our own recommendations for designers and design researchers. In particular we suggest humility in knowledge claims with emotional biosensing, prioritizing care and affirmation over self-improvement, and exploring alternative desires. We call for critically questioning and generatively re-imagining the role of data in configuring sensing, feeling, 'the good life,' and everyday experience.",biosensing; critical alternatives,,,emotion,No,No
acm,Embodying Care in Matilda: An Affective Communication Robot for Emotional Wellbeing of Older People in Australian Residential Care Facilities,"Khosla, Rajiv; Chu, Mei-Tai",2013,,4,,10.1145/2544104,"Ageing population is at the center of the looming healthcare crisis in most parts of the developed and developing world. Australia, like most of the western world, is bracing up for the looming ageing population crisis, spiraling healthcare costs, and expected serious shortage of healthcare workers. Assistive service and companion (social) robots are being seen as one of the ways for supporting aged care facilities to meet this challenge and improve the quality of care of older people including mental and physical health outcomes, as well as to support healthcare workers in personalizing care. In this article, the authors report on the design and implementation of first-ever field trials of Matilda, a human-like assistive communication (service and companion) robot for improving the emotional well-being of older people in three residential care facilities in Australia involving 70 participants. The research makes several unique contributions including Matilda’s ability to break technology barriers, positively engage older people in group and one-to-one activities, making these older people productive and useful, helping them become resilient and cope better through personalization of care, and finally providing them sensory enrichment through Matilda’s multimodal communication capabilities.",Affective communication robot; aged care; nursing home personhood; personalization of care; well-being,,,emotion,No,No
acm,Positive Emotion Elicitation in Chat-Based Dialogue Systems,"Lubis, Nurul; Sakti, Sakriani; Yoshino, Koichiro; Nakamura, Satoshi",2019,,27,,10.1109/TASLP.2019.2900910,"We aim to draw on an important overlooked potential of affective dialogue systems—their application to promote positive emotional states, similar to that of emotional support between humans. This can be achieved by eliciting a more positive emotional valence throughout a dialogue system interaction, i.e., positive emotion elicitation. Existing works on emotion elicitation have not yet paid attention to the emotional benefit for the users. Moreover, a positive emotion elicitation corpus does not yet exist despite the growing number of emotion-rich corpora. Towards this goal, first, we propose a response retrieval approach for positive emotion elicitation by utilizing examples of emotion appraisal from a dialogue corpus. Second, we efficiently construct a corpus using the proposed retrieval method, by replacing responses in a dialogue with those that elicit a more positive emotion. We validate the corpus through crowdsourcing to ensure its quality. Finally, we propose a novel neural network architecture for an emotion-sensitive neural chat-based dialogue system, optimized on the constructed corpus to elicit positive emotion. Objective and subjective evaluations show that the proposed methods result in dialogue responses that are more natural and elicit a more positive emotional response. Further analyses of the results are discussed in this paper.",,,,emotion,No,No
acm,Sonification of Emotion in Social Media: Affect and Accessibility in Facebook Reactions,"Cantrell, Stanley J.; Winters, R. Michael; Kaini, Prakriti; Walker, Bruce N.",2022,,6,,10.1145/3512966,"Facebook Reactions are a collection of animated icons that enable users to share and express their emotions when interacting with Facebook content. The current design of Facebook Reactions utilizes visual stimuli (animated graphics and text) to convey affective information, which presents usability and accessibility barriers for visually-impaired Facebook users. In this paper, we investigate the use of sonification as a universally-accessible modality to aid in the conveyance of affect for blind and sighted social media users. We discuss the design and evaluation of 48 sonifications, leveraging Facebook Reactions as a conceptual framework. We conducted an online sound-matching study with 75 participants (11 blind, 64 sighted) to evaluate the performance of these sonifications. We found that sonification is an effective tool for conveying emotion for blind and sighted participants, and we highlight sonification design strategies that contribute to improved efficacy. Finally, we contextualize these findings and discuss the implications of this research with respect to HCI and the accessibility of online communities and platforms.",music; emotion; social media; affective computing; accessibility; computer-mediated communication; design and evaluation; sonification; universal design,,,emotion,No,No
acm,Emotional Labor in Everyday Resilience: Class-Based Experiences of Navigating Unemployment Amid the COVID-19 Pandemic in the U.S.,"Lu, Alex Jiahong; Gilhool, Anna; Hsiao, Joey Chiao-Yin; Dillahunt, Tawanna R.",2022,,6,,10.1145/3555113,"During the COVID-19 global health crisis, institutions, policymakers, and academics alike have called for practicing resilience to overcome its ongoing disruptions. This paper contributes a comparative study of the job search experiences of working-class and upper-middle-class job seekers, particularly in relation to their resilience practices during the pandemic. Drawing from in-depth interviews with 12 working-class and 11 upper-middle-class job seekers in the U.S., we unpack challenges resulting from both the pandemic and unemployment and job seekers' novel practices of navigating these challenges in their everyday disrupted life. Job seekers' ongoing negotiation with their resources, situations, and surroundings gives practical meanings to building everyday resilience, which we theorize as an ongoing process of becoming resilient. While job seekers across classes experienced similar challenges, working-class job seekers took on additional emotional labor in their everyday resilience due to their limited experience in the digital job search space, competition with higher-degree holding job seekers applying for the same jobs, limited social support networks, and at times, isolation. By foregrounding the uneven distribution of emotional labor in realizing the promise of resilience along class lines, this work cautions against the romanticization of resilience and calls for a more critical and nuanced understanding of resilience in CSCW.",invisible work; labor; emotional labor; everyday resilience; neoliberalism,,,emotion,No,No
acm,Chatbot-based Emotion Management for Distributed Teams: A Participatory Design Study,"Benke, Ivo; Knierim, Michael Thomas; Maedche, Alexander",2020,,4,,10.1145/3415189,"Fueled by the pervasion of tools like Slack or Microsoft Teams, the usage of text-based communication in distributed teams has grown massively in organizations. This brings distributed teams many advantages, however, a critical shortcoming in these setups is the decreased ability of perceiving, understanding and regulating emotions. This is problematic because better team members? abilities of emotion management positively impact team-level outcomes like team cohesion and team performance, while poor abilities diminish communication flow and well-being. Leveraging chatbot technology in distributed teams has been recognized as a promising approach to reintroduce and improve upon these abilities. In this article we present three chatbot designs for emotion management for distributed teams. In order to develop these designs, we conducted three participatory design workshops which resulted in 153 sketches. Subsequently, we evaluated the designs following an exploratory evaluation with 27 participants. Results show general stimulating effects on emotion awareness and communication efficiency. Further, they report emotion regulation and increased compromise facilitation through social and interactive design features, but also perceived threats like loss of control. With some design features adversely impacting emotion management, we highlight design implications and discuss chatbot design recommendations for enhancing emotion management in teams.",participatory design; chatbot; emotion management; team communication,,,emotion,No,No
acm,Affective Automotive User Interfaces–Reviewing the State of Driver Affect Research and Emotion Regulation in the Car,"Braun, Michael; Weber, Florian; Alt, Florian",2021,,54,,10.1145/3460938,"Affective technology offers exciting opportunities to improve road safety by catering to human emotions. Modern car interiors enable the contactless detection of user states, paving the way for a systematic promotion of safe driver behavior through emotion regulation. We review the current literature regarding the impact of emotions on driver behavior and analyze the state of emotion regulation approaches in the car. We summarize challenges for affective interaction in the form of technological hurdles and methodological considerations, as well as opportunities to improve road safety by reinstating drivers into an emotionally balanced state. The purpose of this review is to outline the community’s combined knowledge for interested researchers, to provide a focussed introduction for practitioners, raise awareness for cultural aspects, and to identify future directions for affective interaction in the car.",Affective computing; emotion regulation; automotive user interfaces,,,emotion,No,No
acm,Emotion Work in Caregiving: The Role of Technology to Support Informal Caregivers of Persons Living With Dementia,"Smriti, Diva; Wang, Lu; Huh-Yoo, Jina",2024,,8,,10.1145/3637325,"The CSCW and HCI community has increasingly examined ways to support informal caregivers through technology, given the increasing burden and its consequences on caregivers' emotional and physical health. In this paper, we interviewed 12 informal caregivers of Persons Living With Dementia (PLWD) to understand their needs and current coping strategies for overcoming caregiving burden, specifically around emotion work. The caregivers associated different personal meanings with caregiving. Participants wanted technology to take over some of the utilitarian, mundane caregiving tasks. At the same time, they did not want technology to take over the tasks that fostered personal connections with PLWD. As the disease progressed, caregivers started to lose their perceived bond with PLWD, making it more challenging to juggle between rewarding, positive emotions with negative experiences. We discuss such complexity of emotion work using the notion of Invisible Work to understand when technology should or should not support the emotion work engaged in informal caregiving.",dementia; informal caregivers; alzheimer's; emotion work; family caregivers; technology,,,emotion,No,No
acm,Unobtrusive Assessment of Students' Emotional Engagement during Lectures Using Electrodermal Activity Sensors,"Di Lascio, Elena; Gashi, Shkurta; Santini, Silvia",2018,,2,,10.1145/3264913,"Modern wearable devices enable the continuous and unobtrusive monitoring of human physiological parameters, including heart rate and electrodermal activity. Through the definition of adequate models these parameters allow to infer the wellbeing, empathy, or engagement of humans in different contexts. In this paper, we show that off-the-shelf wearable devices can be used to unobtrusively monitor the emotional engagement of students during lectures. We propose the use of several novel features to capture students' momentary engagement and use existing methods to characterize the general arousal of students and their physiological synchrony with the teacher. To evaluate our method we collect a data set that – after data cleaning – contains data from 24 students, 9 teachers, and 41 lectures. Our results show that non-engaged students can be identified with high reliability. Using a Support Vector Machine, for instance, we achieve a recall of 81% – which is a 25 percentage points improvement with respect to a Biased Random classifier. Overall, our findings may inform the design of systems that allow students to self-monitor their engagement and act upon the obtained feedback. Teachers could profit of information about non-engaged students too to perform self-reflection and to devise and evaluate methods to (re-)engage students.",Students; Engagement; Wearable; Activity; Electrodermal,,,emotion,No,No
acm,Happiness and Fear: Using Emotions as a Lens to Disentangle How Users Felt About the Launch of Facebook Reactions,"Wisniewski, Pamela; Badillo-Urquiola, Karla; Ashtorab, Zahra; Vitak, Jessica",2020,,3,,10.1145/3414825,"In February 2016, Facebook launched Reactions, an interactive feature expanding the Like button to include five additional emotional responses: Love, Sadness, Anger, Wow, and Haha. In this article, we examine users’ feedback about this new feature and identify important design implications of this significant modification of Facebook's interface. We did this by applying theories of human emotion and emotion-specific influences on cognitive appraisals to conduct a heuristic evaluation of Facebook Reactions and a thematic content analysis of the 3,000 “top” comments posted by Facebook users on the official pre- and post-launch announcements about Reactions. Prior to launch, many users were concerned that the addition of a Dislike button would lead to abuse; thus, they favored the more nuanced design of Reactions. After launch, users were more positive about the feature as many of their misconceptions were clarified through actual use. Overall, we identified several design constraints of this new feature, including users’ inability to express conflicting emotions. We conclude the article by discussing the implications of our findings and the challenges around research and design for sociotechnical systems that involve complex human emotions.",emotion; social media; Facebook Reactions; heuristic evaluation; sociotechnical design,,,emotion,No,No
acm,A Consistent Dual-MRC Framework for Emotion-cause Pair Extraction,"Cheng, Zifeng; Jiang, Zhiwei; Yin, Yafeng; Wang, Cong; Ge, Shiping; Gu, Qing",2023,,41,,10.1145/3558548,"Emotion-cause pair extraction (ECPE) is a recently proposed task that aims to extract the potential clause pairs of emotions and its corresponding causes in a document. In this article, we propose a new paradigm for the ECPE task. We cast the task as a two-turn machine reading comprehension (MRC) task, i.e., the extraction of emotions and causes is transformed to the task of identifying answer clauses from the input document specific to a query. This two-turn MRC formalization brings several key advantages: First, the QA manner provides an explicit pairing way to identify causes specific to the target emotion; second, it provides a natural way of jointly modeling the emotion extraction, the cause extraction, and the pairing of emotion and cause; and third, it allows us to exploit the well-developed MRC models. Based on the two-turn MRC formalization, we propose a dual-MRC framework to extract emotion-cause pairs in a dual-direction way, which enables a more comprehensive coverage of all pairing cases. Furthermore, we propose a consistent training strategy for the second-turn query, so the model is able to filter the errors produced by the first turn at inference. Experiments on two benchmark datasets demonstrate that our method outperforms previous methods and achieves state-of-the-art performance. All the code and data of this work can be obtained at .",sentiment analysis; Emotion-cause pair extraction; machine reading comprehension,,,emotion,No,No
acm,Emotional body language displayed by artificial agents,"Beck, Aryel; Stevens, Brett; Bard, Kim A.; Cañamero, Lola",2012,,2,,10.1145/2133366.2133368,"Complex and natural social interaction between artificial agents (computer-generated or robotic) and humans necessitates the display of rich emotions in order to be believable, socially relevant, and accepted, and to generate the natural emotional responses that humans show in the context of social interaction, such as engagement or empathy. Whereas some robots use faces to display (simplified) emotional expressions, for other robots such as Nao, body language is the best medium available given their inability to convey facial expressions. Displaying emotional body language that can be interpreted whilst interacting with the robot should significantly improve naturalness. This research investigates the creation of an affect space for the generation of emotional body language to be displayed by humanoid robots. To do so, three experiments investigating how emotional body language displayed by agents is interpreted were conducted. The first experiment compared the interpretation of emotional body language displayed by humans and agents. The results showed that emotional body language displayed by an agent or a human is interpreted in a similar way in terms of recognition. Following these results, emotional key poses were extracted from an actor's performances and implemented in a Nao robot. The interpretation of these key poses was validated in a second study where it was found that participants were better than chance at interpreting the key poses displayed. Finally, an affect space was generated by blending key poses and validated in a third study. Overall, these experiments confirmed that body language is an appropriate medium for robots to display emotions and suggest that an affect space for body expressions can be used to improve the expressiveness of humanoid robots.",emotional body language; Human computer interactions; human robot interactions,,,emotion,No,No
anthology+abstracts,Disambiguating Emotional Connotations of Words Using Contextualized Word Representations,,2024,,,,10.18653/v1/2024.starsem-1.21,"Understanding emotional nuances in written content is crucial for effective communication; however, the context-dependent nature of language poses challenges in precisely discerning emotions in text. This study contributes to the understanding of how the emotional connotations of a word are influenced by the sentence context in which it appears. Leveraging the contextual understanding embedded in contextualized word representations, we conduct an empirical investigation to (i) evaluate the varying abilities of these representations in distinguishing the diverse emotional connotations evoked by the same word across different contexts, (ii) explore potential biases in these representations toward specific emotions of a word, and (iii) assess the capability of these representations in estimating the number of emotional connotations evoked by a word in diverse contexts. Our experiments, utilizing four popular models{---}BERT, RoBERTa, XLNet, and GPT-2{---}and drawing on the GoEmotions and SemEval 2018 datasets, demonstrate that these models effectively discern emotional connotations of words. RoBERTa, in particular, shows superior performance and greater resilience against biases. Our further analysis reveals that disambiguating the emotional connotations of words significantly enhances emotion identification at the sentence level.",,https://aclanthology.org/2024.starsem-1.21,,emotion,No,No
anthology+abstracts,nicolay-r at {S}em{E}val-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause in Conversations with Chain-of-Thought on Emotion States,,2024,,,,10.18653/v1/2024.semeval-1.4,"Emotion expression is one of the essential traits of conversations. It may be self-related or caused by another speaker. The variety of reasons may serve as a source of the further emotion causes: conversation history, speaker{'}s emotional state, etc. Inspired by the most recent advances in Chain-of-Thought, in this work, we exploit the existing three-hop reasoning approach (THOR) to perform large language model instruction-tuning for answering: emotion states (THOR-state), and emotion caused by one speaker to the other (THOR-cause). We equip THORcause with the reasoning revision (RR) for devising a reasoning path in fine-tuning. In particular, we rely on the annotated speaker emotion states to revise reasoning path. Our final submission, based on Flan-T5-base (250M) and the rule-based span correction technique, preliminary tuned with THOR-state and fine-tuned with THOR-cause-rr on competition training data, results in 3rd and 4th places (F1-proportional) and 5th place (F1-strict) among 15 participating teams. Our THOR implementation fork is publicly available: https://github.com/nicolay-r/THOR-ECAC",,https://aclanthology.org/2024.semeval-1.4,,emotion,Yes,No
anthology+abstracts,{QFNU}{\_}{CS} at {S}em{E}val-2024 Task 3: A Hybrid Pre-trained Model based Approach for Multimodal Emotion-Cause Pair Extraction Task,,2024,,,,10.18653/v1/2024.semeval-1.53,"This article presents the solution of Qufu Normal University for the Multimodal Sentiment Cause Analysis competition in SemEval2024 Task 3.The competition aims to extract emotion-cause pairs from dialogues containing text, audio, and video modalities. To cope with this task, we employ a hybrid pre-train model based approach. Specifically, we first extract and fusion features from dialogues based on BERT, BiLSTM, openSMILE and C3D. Then, we adopt BiLSTM and Transformer to extract the candidate emotion-cause pairs. Finally, we design a filter to identify the correct emotion-cause pairs. The evaluation results show that, we achieve a weighted average F1 score of 0.1786 and an F1 score of 0.1882 on CodaLab.",,https://aclanthology.org/2024.semeval-1.53,,emotion,No,No
anthology+abstracts,{TECHSSN} at {S}em{E}val-2024 Task 10: {LSTM}-based Approach for Emotion Detection in Multilingual Code-Mixed Conversations,,2024,,,,10.18653/v1/2024.semeval-1.109,"Emotion Recognition in Conversation (ERC) in the context of code-mixed Hindi-English interactions is a subtask addressed in SemEval-2024 as Task 10. We made our maiden attempt to solve the problem using natural language processing, machine learning and deep learning techniques, that perform well in properly assigning emotions to individual utterances from a predefined collection. The use of well-proven classifier such as Long Short Term Memory networks improve the model{'}s efficacy than the BERT and Glove based models. How-ever, difficulties develop in the subtle arena of emotion-flip reasoning in multi-party discussions, emphasizing the importance of specialized methodologies. Our findings shed light on the intricacies of emotion dynamics in code-mixed languages, pointing to potential areas for further research and refinement in multilingual understanding.",,https://aclanthology.org/2024.semeval-1.109,,emotion,No,No
anthology+abstracts,{M}orphing{M}inds at {S}em{E}val-2024 Task 10: Emotion Recognition in Conversation in {H}indi-{E}nglish Code-Mixed Conversations,,2024,,,,10.18653/v1/2024.semeval-1.177,"The research focuses on emotion detection in multilingual conversations, particularly in Romanized Hindi and English, with applications in sentiment analysis and mental health assessments. The study employs Machine learning, deep learning techniques, including Transformer-based models like XLM-RoBERTa, for feature extraction and emotion classification. Various experiments are conducted to evaluate model performance, including fine-tuning, data augmentation, and addressing dataset imbalances. The findings highlight challenges and opportunities in emotion detection across languages and emphasize culturally sensitive approaches. The study contributes to advancing emotion analysis in multilingual contexts and provides practical guidance for developing more accurate emotion detection systems.",,https://aclanthology.org/2024.semeval-1.177,,emotion,No,Yes
anthology+abstracts,{L}y{S} at {S}em{E}val-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing,,2024,,,,10.18653/v1/2024.semeval-1.182,"This paper describes our participation in SemEval 2024 Task 3, which focused on Multimodal Emotion Cause Analysis in Conversations. We developed an early prototype for an end-to-end system that uses graph-based methods from dependency parsing to identify causal emotion relations in multi-party conversations. Our model comprises a neural transformer-based encoder for contextualizing multimodal conversation data and a graph-based decoder for generating the adjacency matrix scores of the causal graph. We ranked 7th out of 15 valid and official submissions for Subtask 1, using textual inputs only. We also discuss our participation in Subtask 2 during post-evaluation using multi-modal inputs.",,https://aclanthology.org/2024.semeval-1.182,,emotion,No,No
anthology+abstracts,{D}eep{P}avlov at {S}em{E}val-2024 Task 3: Multimodal Large Language Models in Emotion Reasoning,,2024,,,,10.18653/v1/2024.semeval-1.249,"This paper presents the solution of the DeepPavlov team for the Multimodal Sentiment Cause Analysis competition in SemEval-2024 Task 3, Subtask 2 (Wang et al., 2024). In the evaluation leaderboard, our approach ranks 7th with an F1-score of 0.2132. Large Language Models (LLMs) are transformative in their ability to comprehend and generate human-like text. With recent advancements, Multimodal Large Language Models (MLLMs) have expanded LLM capabilities, integrating different modalities such as audio, vision, and language. Our work delves into the state-of-the-art MLLM Video-LLaMA, its associated modalities, and its application to the emotion reasoning downstream task, Multimodal Emotion Cause Analysis in Conversations (MECAC). We investigate the model{'}s performance in several modes: zero-shot, few-shot, individual embeddings, and fine-tuned, providing insights into their limits and potential enhancements for emotion understanding.",,https://aclanthology.org/2024.semeval-1.249,,emotion,No,Yes
anthology+abstracts,{S}arc{E}mp - Fine-tuning {D}ialo{GPT} for Sarcasm and Empathy,,2024,,,,,"Conversational models often face challenges such as a lack of emotional temperament and a limited sense of humor when interacting with users. To address these issues, we have selected relevant data and fine-tuned the model to (i) humanize the chatbot based on the user{'}s emotional response and the context of the conversation using a dataset based on empathy and (ii) enhanced conversations while incorporating humor/sarcasm for better user engagement. We aspire to achieve more personalized and enhanced user-computer interactions with the help of varied datasets involving sarcasm together with empathy on top of already available state-of-the-art conversational systems.",,https://aclanthology.org/2024.scichat-1.6,,empathy,No,No
anthology+abstracts,"Multi-Dimensional Insights: Annotated Dataset of Stance, Sentiment, and Emotion in {F}acebook Comments on {T}unisia{'}s {J}uly 25 Measures",,2024,,,,,"On July 25, 2021, Tunisian President Kais Saied announced the suspension of parliament and dismissal of Prime Minister Hichem Mechichi, a move that sparked intense public debate. This study investigates Tunisian public opinion regarding these events by analyzing a corpus of 7,535 Facebook comments collected from the official Tunisian presidency page, specifically the post announcing the July 25 measures. A team of three annotators labeled a subset of 5,000 comments, categorizing each comment{'}s political stance (supportive, opposing, or neutral), sentiment (positive, negative, or neutral), emotions, presence of hate speech, aggressive tone, and racism. The inter-annotator agreement, measured by Cohen{'}s kappa, was 0.61, indicating substantial consensus. The analysis reveals that a majority of commenters supported President Saied{'}s actions, outnumbering those who opposed or took a neutral stance. Moreover, the overall sentiment expressed in the comments was predominantly positive. This study provides valuable insights into the complex landscape of public opinion in Tunisia during a crucial moment in the country{'}s ongoing political transformation, highlighting the role of social media as a platform for political discourse and engagement.",,https://aclanthology.org/2024.politicalnlp-1.3,,emotion,Yes,No
anthology+abstracts,The Colorful Future of {LLM}s: Evaluating and Improving {LLM}s as Emotional Supporters for Queer Youth,,2024,,,,10.18653/v1/2024.naacl-long.113,"Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM{'}s interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.*https://github.com/nitaytech/LGBTeenDataset",,https://aclanthology.org/2024.naacl-long.113,,emotion,Yes,No
anthology+abstracts,Fluid Dynamics-Inspired Emotional Analysis in {S}hakespearean Tragedies: A Novel Computational Linguistics Methodology,,2024,,,,,"This study introduces an innovative method for analyzing emotions in texts, drawing inspiration from the principles of fluid dynamics, particularly the Navier-Stokes equations. It applies this framework to analyze Shakespeare{'}s tragedies {``}Hamlet{''} and {``}Romeo and Juliet{''}, treating emotional expressions as entities akin to fluids. By mapping linguistic characteristics onto fluid dynamics components, this approach provides a dynamic perspective on how emotions are expressed and evolve in narrative texts. The results, when compared with conventional sentiment analysis methods, reveal a more detailed and subtle grasp of the emotional arcs within these works. This interdisciplinary strategy not only enriches emotion analysis in computational linguistics but also paves the way for potential integrations with machine learning in NLP.",,https://aclanthology.org/2024.mathnlp-1.2,,emotion,No,No
anthology+abstracts,Early {M}odern {D}utch Comedies and Farces in the Spotlight: Introducing {E}m{DC}om{F} and Its Emotion Framework,,2024,,,,,"As computational drama studies are developing rapidly, the Dutch dramatic tradition is in need of centralisation still before it can benefit from state-of-the-art methodologies. This paper presents and evaluates EmDComF, a historical corpus of both manually curated and automatically digitised early modern Dutch comedies and farces authored between 1650 and 1725, and describes the refinement of a historically motivated annotation framework exploring sentiment and emotions in these two dramatic subgenres. Originating from Lodewijk Meyer{'}s philosophical writings on passions in the dramatic genre ({\mbox{$\pm$}}1670), published in Naauwkeurig onderwys in de tooneel-po{\""e}zy (Thorough instruction in the Poetics of Drama) by the literary society Nil Volentibus Arduum in 1765, a historical and genre-specific emotion framework is tested and operationalised for annotating emotions in the domain of early modern Dutch comedies and farces. Based on a frequency and cluster analysis of 782 annotated sentences by 2 expert annotators, the initial 38 emotion labels were restructured to a hierarchical label set of the 5 emotions Hatred, Anxiety, Sadness, Joy and Desire.",,https://aclanthology.org/2024.lt4hala-1.17,,emotion,No,No
anthology+abstracts,{CTSM}: Combining Trait and State Emotions for Empathetic Response Model,,2024,,,,,"Empathetic response generation endeavors to empower dialogue systems to perceive speakers{'} emotions and generate empathetic responses accordingly. Psychological research demonstrates that emotion, as an essential factor in empathy, encompasses trait emotions, which are static and context-independent, and state emotions, which are dynamic and context-dependent. However, previous studies treat them in isolation, leading to insufficient emotional perception of the context, and subsequently, less effective empathetic expression. To address this problem, we propose Combining Trait and State emotions for Empathetic Response Model (CTSM). Specifically, to sufficiently perceive emotions in dialogue, we first construct and encode trait and state emotion embeddings, and then we further enhance emotional perception capability through an emotion guidance module that guides emotion representation. In addition, we propose a cross-contrastive learning decoder to enhance the model{'}s empathetic expression capability by aligning trait and state emotions between generated responses and contexts. Both automatic and manual evaluation results demonstrate that CTSM outperforms state-of-the-art baselines and can generate more empathetic responses. Our code is available at https://github.com/wangyufeng-empty/CTSM",,https://aclanthology.org/2024.lrec-main.376,,emotion,No,No
anthology+abstracts,{E}mo{P}rompt-{ECPE}: Emotion Knowledge-aware Prompt-tuning for Emotion-Cause Pair Extraction,,2024,,,,,"Emotion-cause pair extraction (ECPE) main focus is on extracting all potential emotion clauses and corresponding cause clauses from unannotated documents. Existing methods achieve promising results with the help of fine-tuning and prompt paradigms, but they present three downsides. First, most approaches cannot distinguish between the emotion-cause pairs that belong to different types of emotions, limiting the existing approaches{'} applicability. Second, existing prompt methods utilize a one-to-one mapping relation to achieve label words to category mapping, which brings considerable bias to the results. Third, existing methods achieve the cause extraction task supported by explicit semantic understanding or basic prompt templates, ignoring the implicit information contained in the cause clauses themselves. To solve these issues, we propose an Emotion knowledge-aware Prompt-tuning for Emotion-Cause Pair Extraction (EmoPrompt-ECPE) method, which integrate the knowledge of emotion categories in the ECPE task and mine the implicit knowledge of cause clauses. Specifically, we inject the latent knowledge of the cause clauses and the emotion types into the prompt template. Besides, we extend the emotion labels for many-to-one mapping of label words to categories with an external emotion word base. Furthermore, we utilize the cosine similarity filtering of the label word base to reduce the noise caused by knowledge introduction. Experiments on both Chinese and English benchmark datasets show that our approach can achieve state-of-the-art results. Our code and data can be found at: https://github.com/xy-xiaotudou/EmoPrompt-ECPE.",,https://aclanthology.org/2024.lrec-main.504,,emotion,Yes,No
anthology+abstracts,"Emotion Analysis in {NLP}: Trends, Gaps and Roadmap for Future Directions",,2024,,,,,"Emotions are a central aspect of communication. Consequently, emotion analysis (EA) is a rapidly growing field in natural language processing (NLP). However, there is no consensus on scope, direction, or methods. In this paper, we conduct a thorough review of 154 relevant NLP publications from the last decade. Based on this review, we address four different questions: (1) How are EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and which emotions are modeled? (3) Is the subjectivity of emotions considered in terms of demographics and cultural factors? and (4) What are the primary NLP applications for EA? We take stock of trends in EA and tasks, emotion frameworks used, existing datasets, methods, and applications. We then discuss four lacunae: (1) the absence of demographic and cultural aspects does not account for the variation in how emotions are perceived, but instead assumes they are universally experienced in the same manner; (2) the poor fit of emotion categories from the two main emotion theories to the task; (3) the lack of standardized EA terminology hinders gap identification, comparison, and future goals; and (4) the absence of interdisciplinary research isolates EA from insights in other fields. Our work will enable more focused research into EA and a more holistic approach to modeling emotions in NLP.",,https://aclanthology.org/2024.lrec-main.506,,emotion,No,No
anthology+abstracts,Emstremo: Adapting Emotional Support Response with Enhanced Emotion-Strategy Integrated Selection,,2024,,,,,"To provide effective support, it is essential for a skilled supporter to emotionally resonate with the help-seeker{'}s current emotional state. In conversational interactions, this emotional alignment is further influenced by the comforting strategies employed by the supporter. Different strategies guide the interlocutors to align their emotions in nuanced patterns. However, the incorporation of strategy into emotional alignment in the context of emotional support agents remains underexplored. To address this limitation, we propose an improved emotional support agent called Emstremo. Emstremo aims to achieve strategic control of emotional alignment by perceiving and responding to the user{'}s emotions. Our system{'}s state-of-the-art performance emphasizes the importance of integrating emotions and strategies in modeling conversations that provide emotional support.",,https://aclanthology.org/2024.lrec-main.514,,emotion,No,No
anthology+abstracts,{FUSE} - {F}r{U}stration and Surprise Expressions: A Subtle Emotional Multimodal Language Corpus,,2024,,,,,"This study introduces a novel multimodal corpus for expressive task-based spoken language and dialogue, focused on language use under frustration and surprise, elicited from three tasks motivated by prior research and collected in an IRB-approved experiment. The resource is unique both because these are understudied affect states for emotion modeling in language, and also because it provides both individual and dyadic multimodally grounded language. The study includes a detailed analysis of annotations and performance results for multimodal emotion inference in language use.",,https://aclanthology.org/2024.lrec-main.666,,emotion,No,No
anthology+abstracts,{K}az{E}mo{TTS}: A Dataset for {K}azakh Emotional Text-to-Speech Synthesis,,2024,,,,,"This study focuses on the creation of the KazEmoTTS dataset, designed for emotional Kazakh text-to-speech (TTS) applications. KazEmoTTS is a collection of 54,760 audio-text pairs, with a total duration of 74.85 hours, featuring 34.23 hours delivered by a female narrator and 40.62 hours by two male narrators. The list of the emotions considered include {``}neutral{''}, {``}angry{''}, {``}happy{''}, {``}sad{''}, {``}scared{''}, and {``}surprised{''}. We also developed a TTS model trained on the KazEmoTTS dataset. Objective and subjective evaluations were employed to assess the quality of synthesized speech, yielding an MCD score within the range of 6.02 to 7.67, alongside a MOS that spanned from 3.51 to 3.57. To facilitate reproducibility and inspire further research, we have made our code, pre-trained model, and dataset accessible in our GitHub repository.",,https://aclanthology.org/2024.lrec-main.841,,emotion,No,No
anthology+abstracts,Multi-stream Information Fusion Framework for Emotional Support Conversation,,2024,,,,,"Emotional support conversation (ESC) task aims to relieve the emotional distress of users who have high-intensity of negative emotions. However, due to the ignorance of emotion intensity modelling which is essential for ESC, previous methods fail to capture the transition of emotion intensity effectively. To this end, we propose a Multi-stream information Fusion Framework (MFF-ESC) to thoroughly fuse three streams (text semantics stream, emotion intensity stream, and feedback stream) for the modelling of emotion intensity, based on a designed multi-stream fusion unit. As the difficulty of modelling subtle transitions of emotion intensity and the strong emotion intensity-feedback correlations, we use the KL divergence between feedback distribution and emotion intensity distribution to further guide the learning of emotion intensities. Experimental results on automatic and human evaluations indicate the effectiveness of our method.",,https://aclanthology.org/2024.lrec-main.1046,,emotion,No,No
anthology+abstracts,User Guide for {KOTE}: {K}orean Online That-gul Emotions Dataset,,2024,,,,,"Despite the lack of comprehensive exploration of emotional connotations, sentiment analysis, which categorizes data as positive or negative, has been widely employed to identify emotional aspects in texts. Recently, corpora labeled with more than just valence or polarity have been built to surpass this limitation. However, most Korean emotion corpora are limited by their small size and narrow range of emotions covered. In this paper, we introduce the KOTE dataset. The KOTE dataset comprises 50,000 Korean online comments, totaling 250,000 cases, each manually labeled for 43 emotions and NO EMOTION through crowdsourcing. The taxonomy for the 43 emotions was systematically derived through cluster analysis of Korean emotion concepts within the word embedding space. After detailing the development of KOTE, we further discuss the results of fine-tuning, as well as analysis for social discrimination within the corpus.",,https://aclanthology.org/2024.lrec-main.1499,,emotion,Yes,No
anthology+abstracts,"{E}motion{A}rcs: Emotion Arcs for 9,000 Literary Texts",,2024,,,,,"We introduce EmotionArcs, a dataset comprising emotional arcs from over 9,000 English novels, assembled to understand the dynamics of emotions represented in text and how these emotions may influence a novel ́s reception and perceived quality. We evaluate emotion arcs manually, by comparing them to human annotation and against other similar emotion modeling systems to show that our system produces coherent emotion arcs that correspond to human interpretation. We present and make this resource available for further studies of a large collection of emotion arcs and present one application, exploring these arcs for modeling reader appreciation. Using information-theoretic measures to analyze the impact of emotions on literary quality, we find that emotional entropy, as well as the skewness and steepness of emotion arcs correlate with two proxies of literary reception. Our findings may offer insights into how quality assessments relate to emotional complexity and could help with the study of affect in literary novels.",,https://aclanthology.org/2024.latechclfl-1.7,,emotion,Yes,No
anthology+abstracts,Emotion and Modifier in Henry Rider Haggard{'}s Novels,,2023,,,,10.18653/v1/2023.wnu-1.2,"In recent years, there has been a growing scholarly interest in employing quantitative methods to analyze literary texts, as they offer unique insights, theories, and interpretations. In light of this, the current study employs quantitative analysis to examine the fiction written by the renowned British adventure novelist, Sir Henry Rider Haggard. Specifically, the study aims to investigate the affective content and prevalence of distinctive linguistic features in six of Haggard{'}s most distinguished works. We evaluate dominant emotional states at the sentence level as well as investigate the deployment of specific linguistic features such as modifiers and deontic modals, and collocated terms. Through sentence-level emotion analysis the findings reveal a notable prevalence of {``}joy{''}-related emotions across the novels. Furthermore, the study observes that intensifiers are employed more commonly than the mitigators as modifiers and the collocated terms of modifiers exhibit high similarity across the novels. By integrating quantitative analyses with qualitative assessments, this study presents a novel perspective on the patterns of emotion and specialized grammatical features in some of Haggard{'}s most celebrated literary works.",,https://aclanthology.org/2023.wnu-1.2,,emotion,No,No
anthology+abstracts,Emotion and Sentiment Guided Paraphrasing,,2023,,,,10.18653/v1/2023.wassa-1.7,"Paraphrase generation, a.k.a. paraphrasing, is a common and important task in natural language processing. Emotional paraphrasing, which changes the emotion embodied in a piece of text while preserving its meaning, has many potential applications, including moderating online dialogues and preventing cyberbullying. We introduce a new task of fine-grained emotional paraphrasing along emotion gradients, that is, altering the emotional intensities of the paraphrases in fine-grained settings following smooth variations in affective dimensions while preserving the meaning of the original text. We reconstruct several widely used paraphrasing datasets by augmenting the input and target texts with their fine-grained emotion labels. Then, we propose a framework for emotion and sentiment guided paraphrasing by leveraging pre-trained language models for conditioned text generation. Extensive evaluation of the fine-tuned models suggests that including fine-grained emotion labels in the paraphrase task significantly improves the likelihood of obtaining high-quality paraphrases that reflect the desired emotions while achieving consistently better scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.",,https://aclanthology.org/2023.wassa-1.7,,emotion,No,No
anthology+abstracts,Multilingual Language Models are not Multicultural: A Case Study in Emotion,,2023,,,,10.18653/v1/2023.wassa-1.19,"Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.",,https://aclanthology.org/2023.wassa-1.19,,emotion,No,No
anthology+abstracts,Utterance Emotion Dynamics in Children{'}s Poems: Emotional Changes Across Age,,2023,,,,10.18653/v1/2023.wassa-1.35,"Emerging psychopathology studies are showing that patterns of changes in emotional state {---} emotion dynamics {---} are associated with overall well-being and mental health. More recently, there has been some work in tracking emotion dynamics through one{'}s utterances, allowing for data to be collected on a larger scale across time and people. However, several questions about how emotion dynamics change with age, especially in children, and when determined through children{'}s writing, remain unanswered. In this work, we use both a lexicon and a machine learning based approach to quantify characteristics of emotion dynamics determined from poems written by children of various ages. We show that both approaches point to similar trends: consistent increasing intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing valence with age. We also find increasing emotional variability, rise rates (i.e., emotional reactivity), and recovery rates (i.e., emotional regulation) with age. These results act as a useful baselines for further research in how patterns of emotions expressed by children change with age, and their association with mental health.",,https://aclanthology.org/2023.wassa-1.35,,emotion,No,No
anthology+abstracts,{F}eeling{B}lue: A Corpus for Understanding the Emotional Connotation of Color in Context,,2023,Transactions of the Association for Computational Linguistics,11,,10.1162/tacl_a_00540,"While the link between color and emotion has been widely studied, how context-based changes in color impact the intensity of perceived emotions is not well understood. In this work, we present a new multimodal dataset for exploring the emotional connotation of color as mediated by line, stroke, texture, shape, and language. Our dataset, FeelingBlue, is a collection of 19,788 4-tuples of abstract art ranked by annotators according to their evoked emotions and paired with rationales for those annotations. Using this corpus, we present a baseline for a new task: Justified Affect Transformation. Given an image I, the task is to 1) recolor I to enhance a specified emotion e and 2) provide a textual justification for the change in e. Our model is an ensemble of deep neural networks which takes I, generates an emotionally transformed color palette p conditioned on I, applies p to I, and then justifies the color transformation in text via a visual-linguistic model. Experimental results shed light on the emotional connotation of color in context, demonstrating both the promise of our approach on this challenging task and the considerable potential for future investigations enabled by our corpus.1",,https://aclanthology.org/2023.tacl-1.11,,emotion,No,No
anthology+abstracts,Temporal Tides of Emotional Resonance: A Novel Approach to Identify Mental Health on Social Media,,2023,,,,10.18653/v1/2023.socialnlp-1.1,,,https://aclanthology.org/2023.socialnlp-1.1,,emotion,No,No
anthology+abstracts,Eliciting Rich Positive Emotions in Dialogue Generation,,2023,,,,10.18653/v1/2023.sicon-1.1,"Positive emotion elicitation aims at evoking positive emotion states in human users in open-domain dialogue generation. However, most work focuses on inducing a single-dimension of positive sentiment using human annotated datasets, which limits the scale of the training dataset. In this paper, we propose to model various emotions in large unannotated conversations, such as joy, trust and anticipation, by leveraging a latent variable to control the emotional intention of the response. Our proposed emotion-eliciting-Conditional-Variational-AutoEncoder (EE-CVAE) model generates more diverse and emotionally-intelligent responses compared to single-dimension baseline models in human evaluation.",,https://aclanthology.org/2023.sicon-1.1,,emotion,Yes,No
anthology+abstracts,Improving Cross-Domain Hate Speech Generalizability with Emotion Knowledge,,2023,,,,,,,https://aclanthology.org/2023.paclic-1.29,,emotion,No,No
anthology+abstracts,Emotion-based Morality in {T}agalog and {E}nglish Scenarios ({EM}o{TES}-3{K}): A Parallel Corpus for Explaining (Im)morality of Actions,,2023,,,,,"Grasping morality is vital in AI systems, particularly as they become more prevalent in human-focused applications. Yet, research is scarce on this topic. This study presents the Emotion-based Morality in Tagalog and English Scenarios (EMoTES-3K), a collection that shows commonsense morality in both Filipino and English. This dataset is instrumental for analyzing moral decisions in various situations and their justifications. Our tests show that EMoTES-3K is effective for moral text categorization, with the fine-tuned RoBERTa model scoring 94.95{\%} accuracy in English and 88.53{\%} in Filipino. The dataset also excels in text generation tasks, as shown by fine-tuning the FLAN-T5 model to produce clear moral explanations. However, the model faces challenges when dealing with actions that have mixed moral implications. This work not only bridges the gap in moral reasoning datasets for languages like Filipino but also sets the stage for future research in commonsense moral reasoning in artificial intelligence.",,https://aclanthology.org/2023.nlp4dh-1.1,,emotion,No,Yes
anthology+abstracts,Challenges of Human vs Machine Translation of Emotion-Loaded {C}hinese Microblog Texts,,2023,,,,,"This paper attempts to identify challenges professional translators face when translating emotion-loaded texts as well as errors machine translation (MT) makes when translating this content. We invited ten Chinese-English translators to translate thirty posts of a Chinese microblog, and interviewed them about the challenges encountered during translation and the problems they believe MT might have. Further, we analysed more than five-thousand automatic translations of microblog posts to observe problems in MT outputs. We establish that the most challenging problem for human translators is emotion-carrying words, which translators also consider as a problem for MT. Analysis of MT outputs shows that this is also the most common source of MT errors. We also find that what is challenging for MT, such as non-standard writing, is not necessarily an issue for humans. Our work contributes to a better understanding of the challenges for the translation of microblog posts by humans and MT, caused by different forms of expression of emotion.",,https://aclanthology.org/2023.mtsummit-users.21,,emotion,No,No
anthology+abstracts,Unifying Emotion Analysis Datasets using Valence Arousal Dominance ({VAD}),,2023,,,,,,,https://aclanthology.org/2023.ldk-1.19,,emotion,No,No
anthology+abstracts,Grumpiness ambivalently relates to negative and positive emotions in ironic {A}ustrian {G}erman text data,,2023,,,,,,,https://aclanthology.org/2023.ldk-1.28,,emotion,No,No
anthology+abstracts,{HS}-{EMO}: Analyzing Emotions in Hate Speech,,2023,,,,,,,https://aclanthology.org/2023.konvens-main.17,,emotion,No,No
anthology+abstracts,{KMD}: A New {K}urdish Multilabel Emotional Dataset For the {K}urdish {S}orani Dialect,,2023,,,,,,,https://aclanthology.org/2023.icnlsp-1.33,,emotion,No,No
anthology+abstracts,Mixing It Up: Inducing Empathy and Politeness using Multiple Behaviour-aware Generators for Conversational Systems,,2023,,,,10.18653/v1/2023.findings-ijcnlp.30,,,https://aclanthology.org/2023.findings-ijcnlp.30,,empathy,No,No
anthology+abstracts,Generative Emotion Cause Triplet Extraction in Conversations with Commonsense Knowledge,,2023,,,,10.18653/v1/2023.findings-emnlp.260,"Emotion Cause Triplet Extraction in Conversations (ECTEC) aims to simultaneously extract emotion utterances, emotion categories, and cause utterances from conversations. However, existing studies mainly decompose the ECTEC task into multiple subtasks and solve them in a pipeline manner. Moreover, since conversations tend to contain many informal and implicit expressions, it often requires external knowledge and reasoning-based inference to accurately identify emotional and causal clues implicitly mentioned in the context, which are ignored by previous work. To address these limitations, in this paper, we propose a commonSense knowledge-enHanced generAtive fRameworK named SHARK, which formulates the ECTEC task as an index generation problem and generates the emotion-cause-category triplets in an end-to-end manner with a sequence-to-sequence model. Furthermore, we propose to incorporate both retrieved and generated commonsense knowledge into the generative model via a dual-view gate mechanism and a graph attention layer. Experimental results show that our SHARK model consistently outperforms several competitive systems on two benchmark datasets. Our source codes are publicly released at https://github.com/NUSTM/SHARK.",,https://aclanthology.org/2023.findings-emnlp.260,,emotion,No,No
anthology+abstracts,Exploiting Emotion-Semantic Correlations for Empathetic Response Generation,,2023,,,,10.18653/v1/2023.findings-emnlp.320,"Empathetic response generation aims to generate empathetic responses by understanding the speaker{'}s emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses. Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression.",,https://aclanthology.org/2023.findings-emnlp.320,,emotion,No,No
anthology+abstracts,{EMO}-{KNOW}: A Large Scale Dataset on Emotion-Cause,,2023,,,,10.18653/v1/2023.findings-emnlp.737,"Emotion-Cause analysis has attracted the attention of researchers in recent years. However, most existing datasets are limited in size and number of emotion categories. They often focus on extracting parts of the document that contain the emotion cause and fail to provide more abstractive, generalizable root cause. To bridge this gap, we introduce a large-scale dataset of emotion causes, derived from 9.8 million cleaned tweets over 15 years. We describe our curation process, which includes a comprehensive pipeline for data gathering, cleaning, labeling, and validation, ensuring the dataset{'}s reliability and richness. We extract emotion labels and provide abstractive summarization of the events causing emotions. The final dataset comprises over 700,000 tweets with corresponding emotion-cause pairs spanning 48 emotion classes, validated by human evaluators. The novelty of our dataset stems from its broad spectrum of emotion classes and the abstractive emotion cause that facilitates the development of an emotion-cause knowledge graph for nuanced reasoning. Our dataset will enable the design of emotion-aware systems that account for the diverse emotional responses of different people for the same event.",,https://aclanthology.org/2023.findings-emnlp.737,,emotion,Yes,No
anthology+abstracts,Conditioning on Dialog Acts improves Empathy Style Transfer,,2023,,,,10.18653/v1/2023.findings-emnlp.884,"We explore the role of dialog acts in style transfer, specifically empathy style transfer {--} rewriting a sentence to make it more empathetic without changing its meaning. Specifically, we use two novel few-shot prompting strategies: target prompting, which only uses examples of the target style (unlike traditional prompting with source/target pairs), and dialog-act-conditioned prompting, which first estimates the dialog act of the source sentence and then makes it more empathetic using few-shot examples of the same dialog act. Our study yields two key findings: (1) Target prompting typically improves empathy more effectively while maintaining the same level of semantic similarity; (2) Dialog acts matter. Dialog-act-conditioned prompting enhances empathy while preserving both semantics and the dialog-act type. Different dialog acts benefit differently from different prompting methods, highlighting the need for further investigation of the role of dialog acts in style transfer.",,https://aclanthology.org/2023.findings-emnlp.884,,empathy,No,No
anthology+abstracts,Best Practices in the Creation and Use of Emotion Lexicons,,2023,,,,10.18653/v1/2023.findings-eacl.136,"Words play a central role in how we express ourselves. Lexicons of word{--}emotion associations are widely used in research and real-world applications for sentiment analysis, tracking emotions associated with products and policies, studying health disorders, tracking emotional arcs of stories, and so on. However, inappropriate and incorrect use of these lexicons can lead to not just sub-optimal results, but also inferences that are directly harmful to people. This paper brings together ideas from Affective Computing and AI Ethics to present, some of the practical and ethical considerations involved in the creation and use of emotion lexicons {--} best practices. The goal is to provide a comprehensive set of relevant considerations, so that readers (especially those new to work with emotions) can find relevant information in one place. We hope this work will facilitate more thoughtfulness when one is deciding on what emotions to work on, how to create an emotion lexicon, how to use an emotion lexicon, how to draw meaningful inferences, and how to judge success.",,https://aclanthology.org/2023.findings-eacl.136,,emotion,No,No
anthology+abstracts,{PAL}: Persona-Augmented Emotional Support Conversation Generation,,2023,,,,10.18653/v1/2023.findings-acl.34,"Due to the lack of human resources for mental health support, there is an increasing demand for employing conversational agents for support. Recent work has demonstrated the effectiveness of dialogue models in providing emotional support. As previous studies have demonstrated that seekers{'} persona is an important factor for effective support, we investigate whether there are benefits to modeling such information in dialogue models for support. In this paper, our empirical analysis verifies that persona has an important impact on emotional support. Therefore, we propose a framework for dynamically inferring and modeling seekers{'} persona. We first train a model for inferring the seeker{'}s persona from the conversation history. Accordingly, we propose PAL, a model that leverages persona information and, in conjunction with our strategy-based controllable generation method, provides personalized emotional support. Automatic and manual evaluations demonstrate that PAL achieves state-of-the-art results, outperforming the baselines on the studied benchmark. Our code and data are publicly available at \url{https://github.com/chengjl19/PAL}.",,https://aclanthology.org/2023.findings-acl.34,,emotion,No,No
anthology+abstracts,Emotion Cause Extraction on Social Media without Human Annotation,,2023,,,,10.18653/v1/2023.findings-acl.94,"In social media, there is a vast amount of information pertaining to people{'}s emotions and the corresponding causes. The emotion cause extraction (ECE) from social media data is an important research area that has not been thoroughly explored due to the lack of fine-grained annotations. Early studies referred to either unsupervised rule-based methods or supervised machine learning methods using a number of manually annotated data in specific domains. However, the former suffers from limitations in extraction performance, while the latter is constrained by the availability of fine-grained annotations and struggles to generalize to diverse domains. To address these issues, this paper proposes a new ECE framework on Chinese social media that achieves high extraction performance and generalizability without relying on human annotation. Specifically, we design a more dedicated rule-based system based on constituency parsing tree to discover causal patterns in social media. This system enables us to acquire large amounts of fine-grained annotated data. Next, we train a neural model on the rule-annotated dataset with a specific training strategy to further improve the model{'}s generalizability. Extensive experiments demonstrate the superiority of our approach over other methods in unsupervised and weakly-supervised settings.",,https://aclanthology.org/2023.findings-acl.94,,emotion,Yes,No
anthology+abstracts,{A}ug{ESC}: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,,2023,,,,10.18653/v1/2023.findings-acl.99,"Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models{'} generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks.",,https://aclanthology.org/2023.findings-acl.99,,emotion,No,No
anthology+abstracts,{T}rans{ESC}: Smoothing Emotional Support Conversation via Turn-Level State Transition,,2023,,,,10.18653/v1/2023.findings-acl.420,"Emotion Support Conversation (ESC) is an emerging and challenging task with the goal of reducing the emotional distress of people. Previous attempts fail to maintain smooth transitions between utterances in ESC because they ignoring to grasp the fine-grained transition information at each dialogue turn. To solve this problem, we propose to take into account turn-level state Transitions of ESC (TransESC) from three perspectives, including semantics transition, strategy transition and emotion transition, to drive the conversation in a smooth and natural way. Specifically, we construct the state transition graph with a two-step way, named transit-then-interact, to grasp such three types of turn-level transition information. Finally, they are injected into the transition aware decoder to generate more engaging responses. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of TransESC to generate more smooth and effective supportive responses. Our source code will be publicly available.",,https://aclanthology.org/2023.findings-acl.420,,emotion,No,No
anthology+abstracts,Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers,,2023,,,,10.18653/v1/2023.emnlp-main.188,"Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time{---}emotion dynamics{---}are indicators of one{'}s mental health. One{'}s patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one{'}s everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user{'}s self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but not PPD. Rise and recovery rates of valence also exhibited significant differences from the control. This work provides important early evidence for how linguistic cues pertaining to emotion dynamics can play a crucial role as biosocial markers for mental illnesses and aid in the understanding, diagnosis, and management of mental health disorders.",,https://aclanthology.org/2023.emnlp-main.188,,emotion,No,No
anthology+abstracts,Modeling Empathic Similarity in Personal Narratives,,2023,,,,10.18653/v1/2023.emnlp-main.383,"The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others{'} experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.",,https://aclanthology.org/2023.emnlp-main.383,,empathy,Yes,No
anthology+abstracts,{E}-{CORE}: Emotion Correlation Enhanced Empathetic Dialogue Generation,,2023,,,,10.18653/v1/2023.emnlp-main.653,"Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion correlation in dialogues, resulting in inaccurate emotion perception and unsuitable response generation. In this paper, we propose a novel emotion correlation enhanced empathetic dialogue generation framework, which comprehensively realizes emotion correlation learning, utilization, and supervising. Specifically, a multi-resolution emotion graph is devised to capture context-based emotion interactions from different resolutions, further modeling emotion correlation. Then we propose an emotion correlation enhanced decoder, with a novel correlation-aware aggregation and soft/hard strategy, respectively improving the emotion perception and response generation. Experimental results on the benchmark dataset demonstrate the superiority of our model in both empathetic perception and expression.",,https://aclanthology.org/2023.emnlp-main.653,,emotion,Yes,No
anthology+abstracts,Countering Misinformation via Emotional Response Generation,,2023,,,,10.18653/v1/2023.emnlp-main.703,"The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread {--} often in good faith {--} misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities.",,https://aclanthology.org/2023.emnlp-main.703,,emotion,No,No
anthology+abstracts,Evaluation of {C}hinese-{E}nglish Machine Translation of Emotion-Loaded Microblog Texts: A Human Annotated Dataset for the Quality Assessment of Emotion Translation,,2023,,,,,"In this paper, we focus on how current Machine Translation (MT) engines perform on the translation of emotion-loaded texts by evaluating outputs from Google Translate according to a framework proposed in this paper. We propose this evaluation framework based on the Multidimensional Quality Metrics (MQM) and perform detailed error analyses of the MT outputs. From our analysis, we observe that about 50{\%} of MT outputs are erroneous in preserving emotions. After further analysis of the erroneous examples, we find that emotion carrying words and linguistic phenomena such as polysemous words, negation, abbreviation etc., are common causes for these translation errors.",,https://aclanthology.org/2023.eamt-1.13,,emotion,Yes,No
anthology+abstracts,Analysing Mistranslation of Emotions in Multilingual Tweets by Online {MT} Tools,,2023,,,,,"It is common for websites that contain User-Generated Text (UGT) to provide an automatic translation option to reach out to their linguistically diverse users. In such scenarios, the process of translating the users{'} emotions is entirely automatic with no human intervention, neither for post-editing, nor for accuracy checking. In this paper, we assess whether automatic translation tools can be a successful real-life utility in transferring emotion in multilingual tweets. Our analysis shows that the mistranslation of the source tweet can lead to critical errors where the emotion is either completely lost or flipped to an opposite sentiment. We identify linguistic phenomena specific to Twitter data which pose a challenge in translation of emotions and show how frequent these features are in different language pairs. We also show that commonly-used quality metrics can lend false confidence in the performance of online MT tools specifically when the source emotion is distorted in telegraphic messages such as tweets.",,https://aclanthology.org/2023.eamt-1.27,,emotion,No,No
anthology+abstracts,Empathy Identification Systems are not Accurately Accounting for Context,,2023,,,,10.18653/v1/2023.eacl-main.123,"Understanding empathy in text dialogue data is a difficult, yet critical, skill for effective human-machine interaction. In this work, we ask whether systems are making meaningful progress on this challenge. We consider a simple model that checks if an input utterance is similar to a small set of empathetic examples. Crucially, the model does not look at what the utterance is a response to, i.e., the dialogue context. This model performs comparably to other work on standard benchmarks and even outperforms state-of-the-art models for empathetic rationale extraction by 16.7 points on T-F1 and 4.3 on IOU-F1. This indicates that current systems rely on the surface form of the response, rather than whether it is suitable in context. To confirm this, we create examples with dialogue contexts that change the interpretation of the response and show that current systems continue to label utterances as empathetic. We discuss the implications of our findings, including improvements for empathetic benchmarks and how our model can be an informative baseline.",,https://aclanthology.org/2023.eacl-main.123,,empathy,Yes,No
anthology+abstracts,Using Word Embeddings for Identifying Emotions Relating to the Body in a {N}eo-{A}ssyrian Corpus,,2023,,,,,"Research into emotions is a developing field within Assyriology, and NLP tools for Akkadian texts offers a new perspective on the data. In this submission, we use PMI-based word embeddings to explore the relationship between parts of the body and emotions. Using data downloaded from Oracc, we ask which parts of the body were semantically linked to emotions. We do this through examining which of the top 10 results for a body part could be used to express emotions. After identifying two words for the body that have the most emotion words in their results list (\textit{libbu} and \textit{kabattu}), we then examine whether the emotion words in their results lists were indeed used in this manner in the Neo-Assyrian textual corpus. The results indicate that of the two body parts, \textit{kabattu} was semantically linked to happiness and joy, and had a secondary emotional field of anger.",,https://aclanthology.org/2023.alp-1.22,,emotion,No,No
anthology+abstracts,Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach,,2023,,,,10.18653/v1/2023.acl-long.96,"Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one{'}s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., \textit{question}), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy{'}s learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.",,https://aclanthology.org/2023.acl-long.96,,emotion,No,No
anthology+abstracts,Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations,,2023,,,,10.18653/v1/2023.acl-long.225,"Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimental results on two ESC datasets show the superiority of KEMI in both content-preserving evaluation and mixed initiative related analyses.",,https://aclanthology.org/2023.acl-long.225,,emotion,No,No
anthology+abstracts,{PAL} to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent,,2023,,,,10.18653/v1/2023.acl-long.685,"The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients{'} trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client{'}s emotions and ensure client{'}s welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL to lay down the counseling support to substance addict and crime victims. To have client{'}s emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client{'}s emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcase the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need.",,https://aclanthology.org/2023.acl-long.685,,emotion,No,No
anthology+abstracts,{``}splink{''} is happy and {``}phrouth{''} is scary: Emotion Intensity Analysis for Nonsense Words,,2022,,,,10.18653/v1/2022.wassa-1.4,"People associate affective meanings to words - {``}death{''} is scary and sad while {``}party{''} is connotated with surprise and joy. This raises the question if the association is purely a product of the learned affective imports inherent to semantic meanings, or is also an effect of other features of words, e.g., morphological and phonological patterns. We approach this question with an annotation-based analysis leveraging nonsense words. Specifically, we conduct a best-worst scaling crowdsourcing study in which participants assign intensity scores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense words and, for comparison of the results to previous work, to 68 real words. Based on this resource, we develop character-level and phonology-based intensity regressors. We evaluate them on both nonsense words and real words (making use of the NRC emotion intensity lexicon of 7493 words), across six emotion categories. The analysis of our data reveals that some phonetic patterns show clear differences between emotion intensities. For instance, s as a first phoneme contributes to joy, sh to surprise, p as last phoneme more to disgust than to anger and fear. In the modelling experiments, a regressor trained on real words from the NRC emotion intensity lexicon shows a higher performance (r = 0.17) than regressors that aim at learning the emotion connotation purely from nonsense words. We conclude that humans do associate affective meaning to words based on surface patterns, but also based on similarities to existing words ({``}juy{''} to {``}joy{''}, or {``}flike{''} to {``}like{''}).",,https://aclanthology.org/2022.wassa-1.4,,emotion,Yes,No
anthology+abstracts,Invited talk: From Data to Meaning in Representation of Emotions,,2022,,,,,"Historically, now we have an unprecedentedly large amount of data available in various systems, and the growth of data volumes is rapid and continuous. The numbers of scientific papers published per year are higher than ever before. While it is desirable to have the context of the users of a social system known and represented in a machine-readable form, capturing this context is notoriously complex (as social context is more difficult to measure with simple sensors, unlike some physical characteristics). This complexity applies especially to the domain of emotions, but also to other context information relevant for social systems and social sciences (for example, in case of experimental study set up in sociology or marketing, detailed user profiles, exact background and experimental settings need to be recorded in a precise manner). Which data and scientific findings get shared, for which purposes, and how? How to address open and closed data, and reproducibility crisis? How to convert Big Data into Smart Data, which is interpretable by both machine and human? And how to make sure that the resulting Smart Data is trustworthy and appropriately handling biases? In my talk, I discuss these questions from the technical perspective, and give examples for relevant solutions implemented with Semantic Web technology, linked data, knowledge graphs and FAIR (Findable, Accessible, Interoperable, Reusable) data management. Specifically, I will be discussing experiences with combining machine learning and knowledge graphs for semantic representation of emotions. Further, I will talk about research data infrastructures and tools for social sciences that can facilitate semantic interoperability and bring more meaning with sharing semantic representation of context, such as one about emotions. Such semantic representations and infrastructures can serve as a basis for industrial applications, including recommender systems, personal assistants and chatbots, and also serve to improve research data management in social sciences.",,https://aclanthology.org/2022.salld-1.1,,emotion,No,No
anthology+abstracts,Emotions Running High? A Synopsis of the state of {T}urkish Politics through the {P}arla{M}int Corpus,,2022,,,,,"We present the initial results of our quantitative study on emotions (Anger, Disgust, Fear, Happiness, Sadness and Surprise) in Turkish parliament (2011{--}2021). We use machine learning models to assign emotion scores to all speeches delivered in the parliament during this period, and observe any changes to them in relation to major political and social events in Turkey. We highlight a number of interesting observations, such as anger being the dominant emotion in parliamentary speeches, and the ruling party showing more stable emotions compared to the political opposition, despite its depiction as a populist party in the literature.",,https://aclanthology.org/2022.parlaclarin-1.10,,emotion,No,No
anthology+abstracts,"Are Emoji, Sentiment, and Emotion {F}riends? A Multi-task Learning for Emoji, Sentiment, and Emotion Analysis",,2022,,,,,,,https://aclanthology.org/2022.paclic-1.19,,emotion,No,No
anthology+abstracts,Variation in the Expression and Annotation of Emotions: A {W}izard of {O}z Pilot Study,,2022,,,,,"This pilot study employs the Wizard of Oz technique to collect a corpus of written human-computer conversations in the domain of customer service. The resulting dataset contains 192 conversations and is used to test three hypotheses related to the expression and annotation of emotions. First, we hypothesize that there is a discrepancy between the emotion annotations of the participant (the experiencer) and the annotations of our external annotator (the observer). Furthermore, we hypothesize that the personality of the participants has an influence on the emotions they expressed, and on the way they evaluated (annotated) these emotions. We found that for an external, trained annotator, not all emotion labels were equally easy to work with. We also noticed that the trained annotator had a tendency to opt for emotion labels that were more centered in the valence-arousal space, while participants made more {`}extreme{'} annotations. For the second hypothesis, we discovered a positive correlation between the personality trait extraversion and the emotion dimensions valence and dominance in our sample. Finally, for the third premise, we observed a positive correlation between the internal-external agreement on emotion labels and the personality traits conscientiousness and extraversion. Our insights and findings will be used in future research to conduct a larger Wizard of Oz experiment.",,https://aclanthology.org/2022.nlperspectives-1.9,,emotion,Yes,No
anthology+abstracts,Emotion Conditioned Creative Dialog Generation,,2022,,,,,"We present a DialGPT based model for generating creative dialog responses that are conditioned based on one of the following emotions: anger, disgust, fear, happiness, pain, sadness and surprise. Our model is capable of producing a contextually apt response given an input sentence and a desired emotion label. Our model is capable of expressing the desired emotion with an accuracy of 0.6. The best performing emotions are neutral, fear and disgust. When measuring the strength of the expressed emotion, we find that anger, fear and disgust are expressed in the most strong fashion by the model.",,https://aclanthology.org/2022.nlp4dh-1.20,,emotion,No,Yes
anthology+abstracts,Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding,,2022,,,,10.18653/v1/2022.naacl-main.108,"Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED, the first multi-modal and multi-task sentiment, emotion and desire dataset, which contains 9,190 text-image pairs, with English text. Each multi-modal sample is annotated with six desires, three sentiments and six emotions. We also propose the state-of-the-art baselines to evaluate the potential of MSED and show the importance of multi-task and multi-modal clues for desire understanding. We hope this study provides a benchmark for human desire analysis. MSED will be publicly available for research.",,https://aclanthology.org/2022.naacl-main.108,,emotion,Yes,No
anthology+abstracts,Mitigating Toxic Degeneration with Empathetic Data: Exploring the Relationship Between Toxicity and Empathy,,2022,,,,10.18653/v1/2022.naacl-main.363,"Large pre-trained neural language models have supported the effectiveness of many NLP tasks, yet are still prone to generating toxic language hindering the safety of their use. Using empathetic data, we improve over recent work on controllable text generation that aims to reduce the toxicity of generated text. We find we are able to dramatically reduce the size of fine-tuning data to 7.5-30k samples while at the same time making significant improvements over state-of-the-art toxicity mitigation of up to 3.4{\%} absolute reduction (26{\%} relative) from the original work on 2.3m samples, by strategically sampling data based on empathy scores. We observe that the degree of improvements is subject to specific communication components of empathy. In particular, the more cognitive components of empathy significantly beat the original dataset in almost all experiments, while emotional empathy was tied to less improvement and even underperforming random samples of the original data. This is a particularly implicative insight for NLP work concerning empathy as until recently the research and resources built for it have exclusively considered empathy as an emotional concept.",,https://aclanthology.org/2022.naacl-main.363,,empathy,No,No
anthology+abstracts,Angry or Sad ? Emotion Annotation for Extremist Content Characterisation,,2022,,,,,"This paper examines the role of emotion annotations to characterize extremist content released on social platforms. The analysis of extremist content is important to identify user emotions towards some extremist ideas and to highlight the root cause of where emotions and extremist attitudes merge together. To address these issues our methodology combines knowledge from sociological and linguistic annotations to explore French extremist content collected online. For emotion linguistic analysis, the solution presented in this paper relies on a complex linguistic annotation scheme. The scheme was used to annotate extremist text corpora in French. Data sets were collected online by following semi-automatic procedures for content selection and validation. The paper describes the integrated annotation scheme, the annotation protocol that was set-up for French corpora annotation and the results, e.g. agreement measures and remarks on annotation disagreements. The aim of this work is twofold: first, to provide a characterization of extremist contents; second, to validate the annotation scheme and to test its capacity to capture and describe various aspects of emotions.",,https://aclanthology.org/2022.lrec-1.21,,emotion,Yes,No
anthology+abstracts,A (Psycho-)Linguistically Motivated Scheme for Annotating and Exploring Emotions in a Genre-Diverse Corpus,,2022,,,,,"This paper presents a scheme for emotion annotation and its manual application on a genre-diverse corpus of texts written in French. The methodology introduced here emphasizes the necessity of clarifying the main concepts implied by the analysis of emotions as they are expressed in texts, before conducting a manual annotation campaign. After explaining whatentails a deeply linguistic perspective on emotion expression modeling, we present a few NLP works that share some common points with this perspective and meticulously compare our approach with them. We then highlight some interesting quantitative results observed on our annotated corpus. The most notable interactions are on the one hand between emotion expression modes and genres of texts, and on the other hand between emotion expression modes and emotional categories. These observation corroborate and clarify some of the results already mentioned in other NLP works on emotion annotation.",,https://aclanthology.org/2022.lrec-1.64,,emotion,No,No
anthology+abstracts,A Comparative Cross Language View On Acted Databases Portraying Basic Emotions Utilising Machine Learning,,2022,,,,,"Since several decades emotional databases have been recorded by various laboratories. Many of them contain acted portrays of Darwin{'}s famous {``}big four{''} basic emotions. In this paper, we investigate in how far a selection of them are comparable by two approaches: on the one hand modeling similarity as performance in cross database machine learning experiments and on the other by analyzing a manually picked set of four acoustic features that represent different phonetic areas. It is interesting to see in how far specific databases (we added a synthetic one) perform well as a training set for others while some do not. Generally speaking, we found indications for both similarity as well as specificiality across languages.",,https://aclanthology.org/2022.lrec-1.204,,emotion,No,No
anthology+abstracts,Conversational Analysis of Daily Dialog Data using Polite Emotional Dialogue Acts,,2022,,,,,"Many socio-linguistic cues are used in conversational analysis, such as emotion, sentiment, and dialogue acts. One of the fundamental social cues is politeness, which linguistically possesses properties such as social manners useful in conversational analysis. This article presents findings of polite emotional dialogue act associations, where we can correlate the relationships between the socio-linguistic cues. We confirm our hypothesis that the utterances with the emotion classes Anger and Disgust are more likely to be impolite. At the same time, Happiness and Sadness are more likely to be polite. A less expectable phenomenon occurs with dialogue acts Inform and Commissive which contain more polite utterances than Question and Directive. Finally, we conclude on the future work of these findings to extend the learning of social behaviours using politeness.",,https://aclanthology.org/2022.lrec-1.256,,emotion,No,No
anthology+abstracts,{Tweet Emotion Dynamics}: Emotion Word Usage in Tweets from {US} and {C}anada,,2022,,,,,"Over the last decade, Twitter has emerged as one of the most influential forums for social, political, and health discourse. In this paper, we introduce a massive dataset of more than 45 million geo-located tweets posted between 2015 and 2021 from US and Canada (TUSC), especially curated for natural language analysis. We also introduce Tweet Emotion Dynamics (TED) {---} metrics to capture patterns of emotions associated with tweets over time. We use TED and TUSC to explore the use of emotion-associated words across US and Canada; across 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the second year of the pandemic); and across individual tweeters. We show that Canadian tweets tend to have higher valence, lower arousal, and higher dominance than the US tweets. Further, we show that the COVID-19 pandemic had a marked impact on the emotional signature of tweets posted in 2020, when compared to the adjoining years. Finally, we determine metrics of TED for 170,000 tweeters to benchmark characteristics of TED metrics at an aggregate level. TUSC and the metrics for TED will enable a wide variety of research on studying how we use language to express ourselves, persuade, communicate, and influence, with particularly promising applications in public health, affective science, social science, and psychology.",,https://aclanthology.org/2022.lrec-1.442,,emotion,No,No
anthology+abstracts,{ELAL}: An Emotion Lexicon for the Analysis of {A}lsatian Theatre Plays,,2022,,,,,"In this work, we present a novel and manually corrected emotion lexicon for the Alsatian dialects, including graphical variants of Alsatian lexical items. These High German dialects are spoken in the North-East of France. They are used mainly orally, and thus lack a stable and consensual spelling convention. There has nevertheless been a continuous literary production since the middle of the 17th century and, in particular, theatre plays. A large sample of Alsatian theatre plays is currently being encoded according to the Text Encoding Initiative (TEI) Guidelines. The emotion lexicon will be used to perform automatic emotion analysis in this corpus of theatre plays. We used a graph-based approach to deriving emotion scores and translations, relying only on bilingual lexicons, cognates and spelling variants. The source lexicons for emotion scores are the NRC Valence Arousal and Dominance and NRC Emotion Intensity lexicons.",,https://aclanthology.org/2022.lrec-1.534,,emotion,No,No
anthology+abstracts,{HADREB}: Human Appraisals and ({E}nglish) Descriptions of Robot Emotional Behaviors,,2022,,,,,"Humans sometimes anthropomorphize everyday objects, but especially robots that have human-like qualities and that are often able to interact with and respond to humans in ways that other objects cannot. Humans especially attribute emotion to robot behaviors, partly because humans often use and interpret emotions when interacting with other humans, and they apply that capability when interacting with robots. Moreover, emotions are a fundamental part of the human language system and emotions are used as scaffolding for language learning, making them an integral part of language learning and meaning. However, there are very few datasets that explore how humans perceive the emotional states of robots and how emotional behaviors relate to human language. To address this gap we have collected HADREB, a dataset of human appraisals and English descriptions of robot emotional behaviors collected from over 30 participants. These descriptions and human emotion appraisals are collected using the Mistyrobotics Misty II and the Digital Dream Labs Cozmo (formerly Anki) robots. The dataset contains English descriptions and emotion appraisals of more than 500 descriptions and graded valence labels of 8 emotion pairs for each behavior and each robot. In this paper we describe the process of collecting and cleaning the data, give a general analysis of the data, and evaluate the usefulness of the dataset in two experiments, one using a language model to map descriptions to emotions, the other maps robot behaviors to emotions.",,https://aclanthology.org/2022.lrec-1.617,,emotion,Yes,No
anthology+abstracts,Towards a contextualised spatial-diachronic history of literature: mapping emotional representations of the city and the country in {P}olish fiction from 1864 to 1939,,2022,,,,,"In this article, we discuss the conditions surrounding the building of historical and literary corpora. We describe the assumptions and method of making the original corpus of the Polish novel (1864-1939). Then, we present the research procedure aimed at demonstrating the variability of the emotional value of the concept of {``}the city{''} and {``}the country{''} in the texts included in our corpus. The proposed method considers the complex socio-political nature of Central and Eastern Europe, especially the fact that there was no unified Polish state during this period. The method can be easily replicated in studies of the literature of countries with similar specificities.",,https://aclanthology.org/2022.latechclfl-1.14,,emotion,No,No
anthology+abstracts,Plug-and-Play Controller for Story Completion: A Pilot Study toward Emotion-aware Story Writing Assistance,,2022,,,,10.18653/v1/2022.in2writing-1.6,"Emotions are essential for storytelling and narrative generation, and as such, the relationship between stories and emotions has been extensively studied. The authors of this paper, including a professional novelist, have examined the use of natural language processing to address the problems of novelists from the perspective of practical creative writing. In particular, the story completion task, which requires understanding the existing unfinished context, was studied from the perspective of creative support for human writers, to generate appropriate content to complete the unfinished parts. It was found that unsupervised pre-trained large neural models of the sequence-to-sequence type are useful for this task. Furthermore, based on the plug-and-play module for controllable text generation using GPT-2, an additional module was implemented to consider emotions. Although this is a preliminary study, and the results leave room for improvement before incorporating the model into a practical system, this effort is an important step in complementing the emotional trajectory of the story.",,https://aclanthology.org/2022.in2writing-1.6,,emotion,No,No
anthology+abstracts,Beyond calories: evaluating how tailored communication reduces emotional load in diet-coaching,,2022,,,,10.18653/v1/2022.humeval-1.5,"Dieting is a behaviour change task that is difficult for many people to conduct successfully. This is due to many factors, including stress and cost. Mobile applications offer an alternative to traditional coaching. However, previous work on apps evaluation only focused on dietary outcomes, ignoring users{'} emotional state despite its influence on eating habits. In this work, we introduce a novel evaluation of the effects that tailored communication can have on the emotional load of dieting. We implement this by augmenting a traditional diet-app with affective NLG, text-tailoring and persuasive communication techniques. We then run a short 2-weeks experiment and check dietary outcomes, user feedback of produced text and, most importantly, its impact on emotional state, through PANAS questionnaire. Results show that tailored communication significantly improved users{'} emotional state, compared to an app-only control group.",,https://aclanthology.org/2022.humeval-1.5,,emotion,No,No
anthology+abstracts,Design Considerations for an {NLP}-Driven Empathy and Emotion Interface for Clinician Training via Telemedicine,,2022,,,,10.18653/v1/2022.hcinlp-1.3,"As digital social platforms and mobile technologies become more prevalent and robust, the use of Artificial Intelligence (AI) in facilitating human communication will grow. This, in turn, will encourage development of intuitive, adaptive, and effective empathic AI interfaces that better address the needs of socially and culturally diverse communities. In this paper, we present several design considerations of an intelligent digital interface intended to guide the clinicians toward more empathetic communication. This approach allows various communities of practice to investigate how AI, on one side, and human communication and healthcare needs, on the other, can contribute to each other{'}s development.",,https://aclanthology.org/2022.hcinlp-1.3,,emotion_and_empathy,No,No
anthology+abstracts,The Secret of Metaphor on Expressing Stronger Emotion,,2022,,,,10.18653/v1/2022.flp-1.6,"Metaphors are proven to have stronger emotional impact than literal expressions. Although this conclusion is shown to be promising in benefiting various NLP applications, the reasons behind this phenomenon are not well studied. This paper conducts the first study in exploring how metaphors convey stronger emotion than their literal counterparts. We find that metaphors are generally more specific than literal expressions. The more specific property of metaphor can be one of the reasons for metaphors{'} superiority in emotion expression. When we compare metaphors with literal expressions with the same specificity level, the gap of emotion expressing ability between both reduces significantly. In addition, we observe specificity is crucial in literal language as well, as literal language can express stronger emotion by making it more specific.",,https://aclanthology.org/2022.flp-1.6,,emotion,No,No
anthology+abstracts,A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing,,2022,,,,10.18653/v1/2022.findings-emnlp.157,"We review the state of research on empathy in natural language processing and identify the following issues: (1) empathy definitions are absent or abstract, which (2) leads to low construct validity and reproducibility. Moreover, (3) emotional empathy is overemphasized, skewing our focus to a narrow subset of simplified tasks. We believe these issues hinder research progress and argue that current directions will benefit from a clear conceptualization that includes operationalizing cognitive empathy components. Our main objectives are to provide insight and guidance on empathy conceptualization for NLP research objectives and to encourage researchers to pursue the overlooked opportunities in this area, highly relevant, e.g., for clinical and educational sectors.",,https://aclanthology.org/2022.findings-emnlp.157,,empathy,No,No
anthology+abstracts,Empathetic and Emotionally Positive Conversation Systems with an Emotion-specific Query-Response Memory,,2022,,,,10.18653/v1/2022.findings-emnlp.475,"Emotional conversation systems generate responses for the input queries considering the speaker{'}s emotions in a conversation. Existing emotional conversation systems output emotional responses according to either a given emotion or the user{'}s emotion reflected in the input queries. Following a given emotion may lead to an emotional drift between the given emotion and the conversation state, and following only the user{'}s emotion may aggravate the user{'}s negative feelings if users suffer from a negative mood. In this paper, we propose to generate empathetic responses catering to the user{'}s emotions while leading the conversation to be emotionally positive. Particularly, by abstracting the conversation corpus, we extract and store the different responding strategies for different users{'} emotions and conversational topics into a memory. We encourage positive emotions in conversation via a sentiment evaluator. We model the memory outputs with a Gaussian mixture distribution and sample a final responding strategy from the distribution. The strategy acts as a condition to a transformer model to generate responses. The experiments verify our model surpasses the baseline methods in appropriateness, diversity, and generating emotionally positive responses.",,https://aclanthology.org/2022.findings-emnlp.475,,emotion,No,No
anthology+abstracts,Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction,,2022,,,,10.18653/v1/2022.findings-acl.95,"The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents. We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset. Existing methods have set a fixed size window to capture relations between neighboring clauses. However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data. To alleviate the problem, we propose a novel $\textbf{M}$ulti-$\textbf{G}$ranularity $\textbf{S}$emantic $\textbf{A}$ware $\textbf{G}$raph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation. In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representations. Besides, a clause graph is also established to model coarse-grained semantic relations between clauses. Experimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models. Especially, MGSAG outperforms other models significantly in the condition of position-insensitive data.",,https://aclanthology.org/2022.findings-acl.95,,emotion,No,No
anthology+abstracts,Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning,,2022,,,,10.18653/v1/2022.emnlp-main.195,"Providing Emotional Support (ES) to soothe people in emotional distress is an essential capability in social interactions. Most existing researches on building ES conversation systems only considered single-turn interactions with users, which was over-simplified. In comparison, multi-turn ES conversation systems can provide ES more effectively, but face several new technical challenges, including: (1) how to adopt appropriate support strategies to achieve the long-term dialogue goal of comforting the user{'}s emotion; (2) how to dynamically model the user{'}s state. In this paper, we propose a novel system MultiESC to address these issues. For strategy planning, drawing inspiration from the A* search algorithm, we propose lookahead heuristics to estimate the future user feedback after using particular strategies, which helps to select strategies that can lead to the best long-term effects. For user state modeling, MultiESC focuses on capturing users{'} subtle emotional expressions and understanding their emotion causes. Extensive experiments show that MultiESC significantly outperforms competitive baselines in both dialogue generation and strategy planning.",,https://aclanthology.org/2022.emnlp-main.195,,emotion,No,No
anthology+abstracts,Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis,,2022,,,,10.18653/v1/2022.emnlp-main.219,"Aspect-level multimodal sentiment analysis, which aims to identify the sentiment of the target aspect from multimodal data, recently has attracted extensive attention in the community of multimedia and natural language processing. Despite the recent success in textual aspect-based sentiment analysis, existing models mainly focused on utilizing the object-level semantic information in the image but ignore explicitly using the visual emotional cues, especially the facial emotions. How to distill visual emotional cues and align them with the textual content remains a key challenge to solve the problem. In this work, we introduce a face-sensitive image-to-emotional-text translation (FITE) method, which focuses on capturing visual sentiment cues through facial expressions and selectively matching and fusing with the target aspect in textual modality. To the best of our knowledge, we are the first that explicitly utilize the emotional information from images in the multimodal aspect-based sentiment analysis task. Experiment results show that our method achieves state-of-the-art results on the Twitter-2015 and Twitter-2017 datasets. The improvement demonstrates the superiority of our model in capturing aspect-level sentiment in multimodal data with facial expressions.",,https://aclanthology.org/2022.emnlp-main.219,,emotion,No,No
anthology+abstracts,Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction,,2022,,,,10.18653/v1/2022.emnlp-main.358,"Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and corresponding cause clauses, which have recently received growing attention. Previous methods sequentially encode features with a specified order. They first encode the emotion and cause features for clause extraction and then combine them for pair extraction. This lead to an imbalance in inter-task feature interaction where features extracted later have no direct contact with the former. To address this issue, we propose a novel **P**air-**B**ased **J**oint **E**ncoding (**PBJE**) network, which generates pairs and clauses features simultaneously in a joint feature encoding manner to model the causal relationship in clauses. PBJE can balance the information flow among emotion clauses, cause clauses and pairs. From a multi-relational perspective, we construct a heterogeneous undirected graph and apply the Relational Graph Convolutional Network (RGCN) to capture the multiplex relationship between clauses and the relationship between pairs and clauses. Experimental results show that PBJE achieves state-of-the-art performance on the Chinese benchmark corpus.",,https://aclanthology.org/2022.emnlp-main.358,,emotion,No,No
anthology+abstracts,{A}rt{EL}ingo: A Million Emotion Annotations of {W}iki{A}rt with Emphasis on Diversity over Language and Culture,,2022,,,,10.18653/v1/2022.emnlp-main.600,"This paper introduces ArtELingo, a new benchmark and dataset, designed to encourage work on diversity across languages and cultures. Following ArtEmis, a collection of 80k artworks from WikiArt with 0.45M emotion labels and English-only captions, ArtELingo adds another 0.79M annotations in Arabic and Chinese, plus 4.8K in Spanish to evaluate {``}cultural-transfer{''} performance. 51K artworks have 5 annotations or more in 3 languages. This diversity makes it possible to study similarities and differences across languages and cultures. Further, we investigate captioning tasks, and find diversity improves the performance of baseline models. ArtELingo is publicly available at {`}www.artelingo.org{`} with standard splits and baseline models. We hope our work will help ease future research on multilinguality and culturally-aware AI.",,https://aclanthology.org/2022.emnlp-main.600,,emotion,Yes,No
anthology+abstracts,{PANDAS}@{T}amil{NLP}-{ACL}2022: Emotion Analysis in {T}amil Text using Language Agnostic Embeddings,,2022,,,,10.18653/v1/2022.dravidianlangtech-1.17,"As the world around us continues to become increasingly digital, it has been acknowledged that there is a growing need for emotion analysis of social media content. The task of identifying the emotion in a given text has many practical applications ranging from screening public health to business and management. In this paper, we propose a language-agnostic model that focuses on emotion analysis in Tamil text. Our experiments yielded an F1-score of 0.010.",,https://aclanthology.org/2022.dravidianlangtech-1.17,,emotion,No,No
anthology+abstracts,{V}arsini{\_}and{\_}{K}irthanna@{D}ravidian{L}ang{T}ech-{ACL}2022-Emotional Analysis in {T}amil,,2022,,,,10.18653/v1/2022.dravidianlangtech-1.26,"In this paper, we present our system for the task of Emotion analysis in Tamil. Over 3.96 million people use these platforms to send messages formed using texts, images, videos, audio or combinations of these to express their thoughts and feelings. Text communication on social media platforms is quite overwhelming due to its enormous quantity and simplicity. The data must be processed to understand the general feeling felt by the author. We present a lexicon-based approach for the extraction emotion in Tamil texts. We use dictionaries of words labelled with their respective emotions. The process of assigning an emotional label to each text, and then capture the main emotion expressed in it. Finally, the F1-score in the official test set is 0.0300 and our method ranks 5th.",,https://aclanthology.org/2022.dravidianlangtech-1.26,,emotion,Yes,No
anthology+abstracts,Findings of the Shared Task on Emotion Analysis in {T}amil,,2022,,,,10.18653/v1/2022.dravidianlangtech-1.42,"This paper presents the overview of the shared task on emotional analysis in Tamil. The result of the shared task is presented at the workshop. This paper presents the dataset used in the shared task, task description, and the methodology used by the participants and the evaluation results of the submission. This task is organized as two Tasks. Task A is carried with 11 emotions annotated data for social media comments in Tamil and Task B is organized with 31 fine-grained emotion annotated data for social media comments in Tamil. For conducting experiments, training and development datasets were provided to the participants and results are evaluated for the unseen data. Totally we have received around 24 submissions from 13 teams. For evaluating the models, Precision, Recall, micro average metrics are used.",,https://aclanthology.org/2022.dravidianlangtech-1.42,,emotion,Yes,No
anthology+abstracts,"{COMMA}: Modeling Relationship among Motivations, Emotions and Actions in Language-based Human Activities",,2022,,,,,"Motivations, emotions, and actions are inter-related essential factors in human activities. While motivations and emotions have long been considered at the core of exploring how people take actions in human activities, there has been relatively little research supporting analyzing the relationship between human mental states and actions. We present the first study that investigates the viability of modeling motivations, emotions, and actions in language-based human activities, named COMMA (Cognitive Framework of Human Activities). Guided by COMMA, we define three natural language processing tasks (emotion understanding, motivation understanding and conditioned action generation), and build a challenging dataset Hail through automatically extracting samples from Story Commonsense. Experimental results on NLP applications prove the effectiveness of modeling the relationship. Furthermore, our models inspired by COMMA can better reveal the essential relationship among motivations, emotions and actions than existing methods.",,https://aclanthology.org/2022.coling-1.15,,emotion,No,No
anthology+abstracts,"{CHAE}: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions",,2022,,,,,"Story generation has emerged as an interesting yet challenging NLP task in recent years. Some existing studies aim at generating fluent and coherent stories from keywords and outlines; while others attempt to control the global features of the story, such as emotion, style and topic. However, these works focus on coarse-grained control on the story, neglecting control on the details of the story, which is also crucial for the task. To fill the gap, this paper proposes a model for fine-grained control on the story, which allows the generation of customized stories with characters, corresponding actions and emotions arbitrarily assigned. Extensive experimental results on both automatic and human manual evaluations show the superiority of our method. It has strong controllability to generate stories according to the fine-grained personalized guidance, unveiling the effectiveness of our methodology. Our code is available at \url{https://github.com/victorup/CHAE}.",,https://aclanthology.org/2022.coling-1.559,,emotion,No,No
anthology+abstracts,A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction,,2022,,,,,"Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause analysis, which extracts potential emotion-cause pairs from an emotional document. Most recent studies use end-to-end methods to tackle the ECPE task. However, these methods either suffer from a label sparsity problem or fail to model complicated relations between emotions and causes. Furthermore, they all do not consider explicit semantic information of clauses. To this end, we transform the ECPE task into a document-level machine reading comprehension (MRC) task and propose a Multi-turn MRC framework with Rethink mechanism (MM-R). Our framework can model complicated relations between emotions and causes while avoiding generating the pairing matrix (the leading cause of the label sparsity problem). Besides, the multi-turn structure can fuse explicit semantic information flow between emotions and causes. Extensive experiments on the benchmark emotion cause corpus demonstrate the effectiveness of our proposed framework, which outperforms existing state-of-the-art methods.",,https://aclanthology.org/2022.coling-1.584,,emotion,No,No
anthology+abstracts,{UECA}-Prompt: Universal Prompt for Emotion Cause Analysis,,2022,,,,,"Emotion cause analysis (ECA) aims to extract emotion clauses and find the corresponding cause of the emotion. Existing methods adopt fine-tuning paradigm to solve certain types of ECA tasks. These task-specific methods have a deficiency of universality. And the relations among multiple objectives in one task are not explicitly modeled. Moreover, the relative position information introduced in most existing methods may make the model suffer from dataset bias. To address the first two problems, this paper proposes a universal prompt tuning method to solve different ECA tasks in the unified framework. As for the third problem, this paper designs a directional constraint module and a sequential learning module to ease the bias. Considering the commonalities among different tasks, this paper proposes a cross-task training method to further explore the capability of the model. The experimental results show that our method achieves competitive performance on the ECA datasets.",,https://aclanthology.org/2022.coling-1.613,,emotion,No,No
anthology+abstracts,基于知识迁移的情感-原因对抽取(Emotion-Cause Pair Extraction Based on Knowledge-Transfer),,2022,,,,,"{``}现有的情感瘭原因对抽取模型均没有通过加入外部知识来提升情感瘭原因对的抽取效果。本文提出基于知识迁移的情感瘭原因对抽取模型瘨癅癃癐癅瘭癋癔瘩,采用知识库获取文本的显性知识编码;随后引入外部情感分类语料库迁移得到子句的隐性知识编码;最后拼接两个知识编码,加入情感瘨原因瘩子句预测概率及相对位置,搭配癔癲癡癮癳癦癯癲癭癥癲机制融合上下文,并采用窗口机制优化计算压力,实现情感瘭原因对抽取。在癅癃癐癅数据集上的实验结果显示,本文提出的方法超过当前最先进的模型癅癃癐癅瘭瘲癄。{''}",,https://aclanthology.org/2022.ccl-1.45,,emotion,No,No
anthology+abstracts,{MISC}: A Mixed Strategy-Aware Model integrating {COMET} for Emotional Support Conversation,,2022,,,,10.18653/v1/2022.acl-long.25,"Applying existing methods to emotional support conversation{---}which provides valuable assistance to people who are in need{---}has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user{'}s instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user{'}s distress. To address the problems, we propose a novel model $\textbf{MISC}$, which firstly infers the user{'}s fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling.",,https://aclanthology.org/2022.acl-long.25,,emotion,Yes,No
anthology+abstracts,Emotional Intensity Estimation based on Writer{'}s Personality,,2022,,,,,"We propose a method for personalized emotional intensity estimation based on a writer{'}s personality test for Japanese SNS posts. Existing emotion analysis models are difficult to accurately estimate the writer{'}s subjective emotions behind the text. We personalize the emotion analysis using not only the text but also the writer{'}s personality information. Experimental results show that personality information improves the performance of emotional intensity estimation. Furthermore, a hybrid model combining the existing personalized method with ours achieved state-of-the-art performance.",,https://aclanthology.org/2022.aacl-srw.1,,emotion,No,No
anthology+abstracts,Hell Hath No Fury? Correcting Bias in the {NRC} Emotion Lexicon,,2021,,,,10.18653/v1/2021.woah-1.11,"There have been several attempts to create an accurate and thorough emotion lexicon in English, which identifies the emotional content of words. Of the several commonly used resources, the NRC emotion lexicon (Mohammad and Turney, 2013b) has received the most attention due to its availability, size, and its choice of Plutchik{'}s expressive 8-class emotion model. In this paper we identify a large number of troubling entries in the NRC lexicon, where words that should in most contexts be emotionally neutral, with no affect (e.g., {`}lesbian{'}, {`}stone{'}, {`}mountain{'}), are associated with emotional labels that are inaccurate, nonsensical, pejorative, or, at best, highly contingent and context-dependent (e.g., {`}lesbian{'} labeled as Disgust and Sadness, {`}stone{'} as Anger, or {`}mountain{'} as Anticipation). We describe a procedure for semi-automatically correcting these problems in the NRC, which includes disambiguating POS categories and aligning NRC entries with other emotion lexicons to infer the accuracy of labels. We demonstrate via an experimental benchmark that the quality of the resources is thus improved. We release the revised resource and our code to enable other researchers to reproduce and build upon results.",,https://aclanthology.org/2021.woah-1.11,,emotion,No,No
anthology+abstracts,An End-to-End Network for Emotion-Cause Pair Extraction,,2021,,,,,"The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all potential clause-pairs of emotions and their corresponding causes in a document. Unlike the more well-studied task of Emotion Cause Extraction (ECE), ECPE does not require the emotion clauses to be provided as annotations. Previous works on ECPE have either followed a multi-stage approach where emotion extraction, cause extraction, and pairing are done independently or use complex architectures to resolve its limitations. In this paper, we propose an end-to-end model for the ECPE task. Due to the unavailability of an English language ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline for the ECPE task on this dataset. On this dataset, the proposed method produces significant performance improvements (∼ 6.5{\%} increase in F1 score) over the multi-stage approach and achieves comparable performance to the state-of-the-art methods.",,https://aclanthology.org/2021.wassa-1.9,,emotion,No,No
anthology+abstracts,Construction of {MBTI} Personality Estimation Model Considering Emotional Information,,2021,,,,,,,https://aclanthology.org/2021.paclic-1.28,,emotion,No,No
anthology+abstracts,Are Metal Fans Angrier than Jazz Fans? A Genre-Wise Exploration of the Emotional Language of Music Listeners on {R}eddit,,2021,,,,,,,https://aclanthology.org/2021.nlp4musa-1.7,,emotion,No,No
anthology+abstracts,Emotion Stimulus Detection in {G}erman News Headlines,,2021,,,,,,,https://aclanthology.org/2021.konvens-1.7,,emotion,No,No
anthology+abstracts,{SEPRG}: Sentiment aware Emotion controlled Personalized Response Generation,,2021,,,,10.18653/v1/2021.inlg-1.39,"Social chatbots have gained immense popularity, and their appeal lies not just in their capacity to respond to the diverse requests from users, but also in the ability to develop an emotional connection with users. To further develop and promote social chatbots, we need to concentrate on increasing user interaction and take into account both the intellectual and emotional quotient in the conversational agents. Therefore, in this work, we propose the task of sentiment aware emotion controlled personalized dialogue generation giving the machine the capability to respond emotionally and in accordance with the persona of the user. As sentiment and emotions are highly co-related, we use the sentiment knowledge of the previous utterance to generate the correct emotional response in accordance with the user persona. We design a Transformer based Dialogue Generation framework, that generates responses that are sensitive to the emotion of the user and corresponds to the persona and sentiment as well. Moreover, the persona information is encoded by a different Transformer encoder, along with the dialogue history, is fed to the decoder for generating responses. We annotate the PersonaChat dataset with sentiment information to improve the response quality. Experimental results on the PersonaChat dataset show that the proposed framework significantly outperforms the existing baselines, thereby generating personalized emotional responses in accordance with the sentiment that provides better emotional connection and user satisfaction as desired in a social chatbot.",,https://aclanthology.org/2021.inlg-1.39,,emotion,No,No
anthology+abstracts,{H}is{N}et: A Polarity Lexicon based on {W}ord{N}et for Emotion Analysis,,2021,,,,,"Dictionary-based methods in sentiment analysis have received scholarly attention recently, the most comprehensive examples of which can be found in English. However, many other languages lack polarity dictionaries, or the existing ones are small in size as in the case of SentiTurkNet, the first and only polarity dictionary in Turkish. Thus, this study aims to extend the content of SentiTurkNet by comparing the two available WordNets in Turkish, namely KeNet and TR-wordnet of BalkaNet. To this end, a current Turkish polarity dictionary has been created relying on 76,825 synsets matching KeNet, where each synset has been annotated with three polarity labels, which are positive, negative and neutral. Meanwhile, the comparison of KeNet and TR-wordnet of BalkaNet has revealed their weaknesses such as the repetition of the same senses, lack of necessary merges of the items belonging to the same synset and the presence of redundant narrower versions of synsets, which are discussed in light of their potential to the improvement of the current lexical databases of Turkish.",,https://aclanthology.org/2021.gwc-1.18,,emotion,No,No
anthology+abstracts,Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction,,2021,,,,10.18653/v1/2021.findings-emnlp.51,"Emotion cause extraction (ECE) aims to extract the causes behind the certain emotion in text. Some works related to the ECE task have been published and attracted lots of attention in recent years. However, these methods neglect two major issues: 1) pay few attentions to the effect of document-level context information on ECE, and 2) lack of sufficient exploration for how to effectively use the annotated emotion clause. For the first issue, we propose a bidirectional hierarchical attention network (BHA) corresponding to the specified candidate cause clause to capture the document-level context in a structured and dynamic manner. For the second issue, we design an emotional filtering module (EF) for each layer of the graph attention network, which calculates a gate score based on the emotion clause to filter the irrelevant information. Combining the BHA and EF, the EF-BHA can dynamically aggregate the contextual information from two directions and filters irrelevant information. The experimental results demonstrate that EF-BHA achieves the competitive performances on two public datasets in different languages (Chinese and English). Moreover, we quantify the effect of context on emotion cause extraction and provide the visualization of the interactions between candidate cause clauses and contexts.",,https://aclanthology.org/2021.findings-emnlp.51,,emotion,No,No
anthology+abstracts,Uncovering the Limits of Text-based Emotion Detection,,2021,,,,10.18653/v1/2021.findings-emnlp.219,"Identifying emotions from text is crucial for a variety of real world tasks. We consider the two largest now-available corpora for emotion classification: GoEmotions, with 58k messages labelled by readers, and Vent, with 33M writer-labelled messages. We design a benchmark and evaluate several feature spaces and learning algorithms, including two simple yet novel models on top of BERT that outperform previous strong baselines on GoEmotions. Through an experiment with human participants, we also analyze the differences between how writers express emotions and how readers perceive them. Our results suggest that emotions expressed by writers are harder to identify than emotions that readers perceive. We share a public web interface for researchers to explore our models.",,https://aclanthology.org/2021.findings-emnlp.219,,emotion,No,No
anthology+abstracts,{D}ialogue{TRM}: Exploring Multi-Modal Emotional Dynamics in a Conversation,,2021,,,,10.18653/v1/2021.findings-emnlp.229,"Emotion dynamics formulates principles explaining the emotional fluctuation during conversations. Recent studies explore the emotion dynamics from the self and inter-personal dependencies, however, ignoring the temporal and spatial dependencies in the situation of multi-modal conversations. To address the issue, we extend the concept of emotion dynamics to multi-modal settings and propose a Dialogue Transformer for simultaneously modeling the intra-modal and inter-modal emotion dynamics. Specifically, the intra-modal emotion dynamics is to not only capture the temporal dependency but also satisfy the context preference in every single modality. The inter-modal emotional dynamics aims at handling multi-grained spatial dependency across all modalities. Our models outperform the state-of-the-art with a margin of 4{\%}-16{\%} for most of the metrics on three benchmark datasets.",,https://aclanthology.org/2021.findings-emnlp.229,,emotion,No,No
anthology+abstracts,Constructing Emotional Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation,,2021,,,,10.18653/v1/2021.findings-emnlp.268,"Researches on dialogue empathy aim to endow an agent with the capacity of accurate understanding and proper responding for emotions. Existing models for empathetic dialogue generation focus on the emotion flow in one direction, that is, from the context to response. We argue that conducting an empathetic conversation is a bidirectional process, where empathy occurs when the emotions of two interlocutors could converge on the same point, i.e., reaching an emotional consensus. Besides, we also find that the empathetic dialogue corpus is extremely limited, which further restricts the model performance. To address the above issues, we propose a dual-generative model, Dual-Emp, to simultaneously construct the emotional consensus and utilize some external unpaired data. Specifically, our model integrates a forward dialogue model, a backward dialogue model, and a discrete latent variable representing the emotional consensus into a unified architecture. Then, to alleviate the constraint of paired data, we extract unpaired emotional data from open-domain conversations and employ Dual-Emp to produce pseudo paired empathetic samples, which is more efficient and low-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses.",,https://aclanthology.org/2021.findings-emnlp.268,,emotion,Yes,Yes
anthology+abstracts,{NICE}: Neural Image Commenting with Empathy,,2021,,,,10.18653/v1/2021.findings-emnlp.380,"Emotion and empathy are examples of human qualities lacking in many human-machine interactions. The goal of our work is to generate engaging dialogue grounded in a user-shared image with increased emotion and empathy while minimizing socially inappropriate or offensive outputs. We release the Neural Image Commenting with Empathy (NICE) dataset consisting of almost two million images and the corresponding human-generated comments, a set of human annotations, and baseline performance on a range of models. In-stead of relying on manually labeled emotions, we also use automatically generated linguistic representations as a source of weakly supervised labels. Based on these annotations, we define two different tasks for the NICE dataset. Then, we propose a novel pre-training model - Modeling Affect Generation for Image Comments (MAGIC) - which aims to generate comments for images, conditioned on linguistic representations that capture style and affect, and to help generate more empathetic, emotional, engaging and socially appropriate comments. Using this model we achieve state-of-the-art performance on one of our NICE tasks. The experiments show that the approach can generate more human-like and engaging image comments.",,https://aclanthology.org/2021.findings-emnlp.380,,empathy,Yes,Yes
anthology+abstracts,Jointly Identifying Rhetoric and Implicit Emotions via Multi-Task Learning,,2021,,,,10.18653/v1/2021.findings-acl.123,,,https://aclanthology.org/2021.findings-acl.123,,emotion,No,No
anthology+abstracts,Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction,,2021,,,,10.18653/v1/2021.findings-acl.348,,,https://aclanthology.org/2021.findings-acl.348,,emotion,No,No
anthology+abstracts,Modulating Language Models with Emotions,,2021,,,,10.18653/v1/2021.findings-acl.379,,,https://aclanthology.org/2021.findings-acl.379,,emotion,No,No
anthology+abstracts,Automatically Select Emotion for Response via Personality-affected Emotion Transition,,2021,,,,10.18653/v1/2021.findings-acl.444,,,https://aclanthology.org/2021.findings-acl.444,,emotion,No,No
anthology+abstracts,Guilt by Association: Emotion Intensities in Lexical Representations,,2021,,,,10.18653/v1/2021.emnlp-main.781,"What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.",,https://aclanthology.org/2021.emnlp-main.781,,emotion,Yes,No
anthology+abstracts,"Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions",,2021,,,,10.18653/v1/2021.eacl-main.165,"Computational modelling of political discourse tasks has become an increasingly important area of research in the field of natural language processing. Populist rhetoric has risen across the political sphere in recent years; however, due to its complex nature, computational approaches to it have been scarce. In this paper, we present the new Us vs. Them dataset, consisting of 6861 Reddit comments annotated for populist attitudes and the first large-scale computational models of this phenomenon. We investigate the relationship between populist mindsets and social groups, as well as a range of emotions typically associated with these. We set a baseline for two tasks associated with populist attitudes and present a set of multi-task learning models that leverage and demonstrate the importance of emotion and group identification as auxiliary tasks.",,https://aclanthology.org/2021.eacl-main.165,,emotion,Yes,No
anthology+abstracts,Towards Low-Resource Real-Time Assessment of Empathy in Counselling,,2021,,,,10.18653/v1/2021.clpsych-1.22,"Gauging therapist empathy in counselling is an important component of understanding counselling quality. While session-level empathy assessment based on machine learning has been investigated extensively, it relies on relatively large amounts of well-annotated dialogue data, and real-time evaluation has been overlooked in the past. In this paper, we focus on the task of low-resource utterance-level binary empathy assessment. We train deep learning models on heuristically constructed empathy vs. non-empathy contrast in general conversations, and apply the models directly to therapeutic dialogues, assuming correlation between empathy manifested in those two domains. We show that such training yields poor performance in general, probe its causes, and examine the actual effect of learning from empathy contrast in general conversation.",,https://aclanthology.org/2021.clpsych-1.22,,empathy,Yes,No
anthology+abstracts,Multi-level Emotion Cause Analysis by Multi-head Attention Based Multi-task Learning,,2021,,,,,Emotion cause analysis (ECA) aims to identify the potential causes behind certain emotions intext. Lots of ECA models have been designed to extract the emotion cause at the clause level. However in many scenarios only extracting the cause clause is ambiguous. To ease the problemin this paper we introduce multi-level emotion cause analysis which focuses on identifying emotion cause clause (ECC) and emotion cause keywords (ECK) simultaneously. ECK is a more challenging task since it not only requires capturing the specific understanding of the role of eachword in the clause but also the relation between each word and emotion expression. We observethat ECK task can incorporate the contextual information from the ECC task while ECC taskcan be improved by learning the correlation between emotion cause keywords and emotion fromthe ECK task. To fulfill the goal of joint learning we propose a multi-head attention basedmulti-task learning method which utilizes a series of mechanisms including shared and privatefeature extractor multi-head attention emotion attention and label embedding to capture featuresand correlations between the two tasks. Experimental results show that the proposed method consistently outperforms the state-of-the-art methods on a benchmark emotion cause dataset.,,https://aclanthology.org/2021.ccl-1.83,,emotion,Yes,No
anthology+abstracts,Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction,,2021,,,,10.18653/v1/2021.acl-long.261,"The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.",,https://aclanthology.org/2021.acl-long.261,,emotion,Yes,Yes
anthology+abstracts,Towards Emotional Support Dialog Systems,,2021,,,,10.18653/v1/2021.acl-long.269,"Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.",,https://aclanthology.org/2021.acl-long.269,,emotion,Yes,No
anthology+abstracts,Empathy-driven {A}rabic Conversational Chatbot,,2020,,,,,"Conversational models have witnessed a significant research interest in the last few years with the advancements in sequence generation models. A challenging aspect in developing human-like conversational models is enabling the sense of empathy in bots, making them infer emotions from the person they are interacting with. By learning to develop empathy, chatbot models are able to provide human-like, empathetic responses, thus making the human-machine interaction close to human-human interaction. Recent advances in English use complex encoder-decoder language models that require large amounts of empathetic conversational data. However, research has not produced empathetic bots for Arabic. Furthermore, there is a lack of Arabic conversational data labeled with empathy. To address these challenges, we create an Arabic conversational dataset that comprises empathetic responses. However, the dataset is not large enough to develop very complex encoder-decoder models. To address the limitation of data scale, we propose a special encoder-decoder composed of a Long Short-Term Memory (LSTM) Sequence-to-Sequence (Seq2Seq) with Attention. The experiments showed success of our proposed empathy-driven Arabic chatbot in generating empathetic responses with a perplexity of 38.6, an empathy score of 3.7, and a fluency score of 3.92.",,https://aclanthology.org/2020.wanlp-1.6,,empathy,Yes,No
anthology+abstracts,Learning Word Groundings from Humans Facilitated by Robot Emotional Displays,,2020,,,,10.18653/v1/2020.sigdial-1.13,"In working towards accomplishing a human-level acquisition and understanding of language, a robot must meet two requirements: the ability to learn words from interactions with its physical environment, and the ability to learn language from people in settings for language use, such as spoken dialogue. In a live interactive study, we test the hypothesis that emotional displays are a viable solution to the cold-start problem of how to communicate without relying on language the robot does not{--}indeed, cannot{--}yet know. We explain our modular system that can autonomously learn word groundings through interaction and show through a user study with 21 participants that emotional displays improve the quantity and quality of the inputs provided to the robot.",,https://aclanthology.org/2020.sigdial-1.13,,emotion,No,No
anthology+abstracts,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",,2020,,,,,,,https://aclanthology.org/2020.peoples-1.0,,emotion,No,No
anthology+abstracts,Topic and Emotion Development among {D}utch {COVID}-19 {T}witter Communities in the early Pandemic,,2020,,,,,"The paper focuses on a large collection of Dutch tweets from the Netherlands to get an insight into the perception and reactions of users during the early months of the COVID-19 pandemic. We focused on five major user communities of users: government and health organizations, news media, politicians, the general public and conspiracy theory supporters, investigating differences among them in topic dominance and the expressions of emotions. Through topic modeling we monitor the evolution of the conversation about COVID-19 among these communities. Our results indicate that the national focus on COVID-19 shifted from the virus itself to its impact on the economy between February and April. Surprisingly, the overall emotional public response appears to be substantially positive and expressing trust, although differences can be observed in specific group of users.",,https://aclanthology.org/2020.peoples-1.9,,emotion,No,No
anthology+abstracts,"Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine Learning to Infer the Emotions?",,2020,,,,,"Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: {``}who is described to feel the emotion?{''} (experiencer), {``}what causes this emotion?{''} (stimulus), and at which entity is it directed?{''} (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.",,https://aclanthology.org/2020.peoples-1.12,,emotion,No,No
anthology+abstracts,Learning Emotion from 100 Observations: Unexpected Robustness of Deep Learning under Strong Data Limitations,,2020,,,,,"One of the major downsides of Deep Learning is its supposed need for vast amounts of training data. As such, these techniques appear ill-suited for NLP areas where annotated data is limited, such as less-resourced languages or emotion analysis, with its many nuanced and hard-to-acquire annotation formats. We conduct a questionnaire study indicating that indeed the vast majority of researchers in emotion analysis deems neural models inferior to traditional machine learning when training data is limited. In stark contrast to those survey results, we provide empirical evidence for English, Polish, and Portuguese that commonly used neural architectures can be trained on surprisingly few observations, outperforming n-gram based ridge regression on only 100 data points. Our analysis suggests that high-quality, pre-trained word embeddings are a main factor for achieving those results.",,https://aclanthology.org/2020.peoples-1.13,,emotion,Yes,No
anthology+abstracts,Emotion Arcs of Student Narratives,,2020,,,,10.18653/v1/2020.nuse-1.12,"This paper studies emotion arcs in student narratives. We construct emotion arcs based on event affect and implied sentiments, which correspond to plot elements in the story. We show that student narratives can show elements of plot structure in their emotion arcs and that properties of these arcs can be useful indicators of narrative quality. We build a system and perform analysis to show that our arc-based features are complementary to previously studied sentiment features in this area.",,https://aclanthology.org/2020.nuse-1.12,,emotion,No,No
anthology+abstracts,Emotional Speech Corpus for Persuasive Dialogue System,,2020,,,,,"Expressing emotion is known as an efficient way to persuade one{'}s dialogue partner to accept one{'}s claim or proposal. Emotional expression in speech can express the speaker{'}s emotion more directly than using only emotion expression in the text, which will lead to a more persuasive dialogue. In this paper, we built a speech dialogue corpus in a persuasive scenario that uses emotional expressions to build a persuasive dialogue system with emotional expressions. We extended an existing text dialogue corpus by adding variations of emotional responses to cover different combinations of broad dialogue context and a variety of emotional states by crowd-sourcing. Then, we recorded emotional speech consisting of of collected emotional expressions spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system{'}s emotion to users.",,https://aclanthology.org/2020.lrec-1.62,,emotion,No,No
anthology+abstracts,{M}u{SE}: a Multimodal Dataset of Stressed Emotion,,2020,,,,,"Endowing automated agents with the ability to provide support, entertainment and interaction with human beings requires sensing of the users{'} affective state. These affective states are impacted by a combination of emotion inducers, current psychological state, and various conversational factors. Although emotion classification in both singular and dyadic settings is an established area, the effects of these additional factors on the production and perception of emotion is understudied. This paper presents a new dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings. The paper also presents several baselines to measure the performance of multimodal features for emotion and stress classification.",,https://aclanthology.org/2020.lrec-1.187,,emotion,No,Yes
anthology+abstracts,L{'}expression des {\'e}motions dans les textes pour enfants : constitution d{'}un corpus annot{\'e} (Expressing emotions in texts for children: constitution of an annotated corpus),,2020,,,,,"Cet article pr{\'e}sente une typologie de divers modes d{'}expression linguistique des {\'e}motions, le sch{\'e}ma d{'}annotation sous Glozz qui impl{\'e}mente cette typologie et un corpus de textes journalistiques pour enfants annot{\'e} {\`a} l{'}aide de ce sch{\'e}ma. Ces travaux pr{\'e}liminaires s{'}ins{\`e}rent dans le contexte d{'}une {\'e}tude relative au d{\'e}veloppement des capacit{\'e}s langagi{\`e}res des enfants, en particulier de leur capacit{\'e} {\`a} comprendre un texte selon des crit{\`e}res {\'e}motionnels.",,https://aclanthology.org/2020.jeptalnrecital-taln.19,,emotion,No,No
anthology+abstracts,Condolence and Empathy in Online Communities,,2020,,,,10.18653/v1/2020.emnlp-main.45,"Offering condolence is a natural reaction to hearing someone{'}s distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal{---}trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",,https://aclanthology.org/2020.emnlp-main.45,,empathy,No,No
anthology+abstracts,Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme,,2020,,,,10.18653/v1/2020.emnlp-main.289,"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26{\%} (p{\textless}0.001) in F1 measure.",,https://aclanthology.org/2020.emnlp-main.289,,emotion,No,No
anthology+abstracts,End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning,,2020,,,,10.18653/v1/2020.emnlp-main.290,"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering. However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step. To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL). The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move. Finally, CMLL and EMLL are integrated to obtain the final result. We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.",,https://aclanthology.org/2020.emnlp-main.290,,emotion,No,No
anthology+abstracts,Modeling Protagonist Emotions for Emotion-Aware Storytelling,,2020,,,,10.18653/v1/2020.emnlp-main.426,"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.",,https://aclanthology.org/2020.emnlp-main.426,,emotion,No,No
anthology+abstracts,{E}mo{T}ag1200: Understanding the Association between Emojis and Emotions,,2020,,,,10.18653/v1/2020.emnlp-main.720,"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality word-level information is available.",,https://aclanthology.org/2020.emnlp-main.720,,emotion,No,No
anthology+abstracts,{MIME}: {MIM}icking Emotions for Empathetic Response Generation,,2020,,,,10.18653/v1/2020.emnlp-main.721,"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at \url{https://github.com/declare-lab/MIME}.",,https://aclanthology.org/2020.emnlp-main.721,,emotion,No,No
anthology+abstracts,End-to-End Emotion-Cause Pair Extraction with Graph Convolutional Network,,2020,,,,10.18653/v1/2020.coling-main.17,"Emotion-cause pair extraction (ECPE), which aims at simultaneously extracting emotion-cause pairs that express emotions and their corresponding causes in a document, plays a vital role in understanding natural languages. Considering that most emotions usually have few causes mentioned in their contexts, we present a novel end-to-end Pair Graph Convolutional Network (PairGCN) to model pair-level contexts so that to capture the dependency information among local neighborhood candidate pairs. Moreover, in the graphical network, contexts are grouped into three types and each type of contexts is propagated by its own way. Experiments on a benchmark Chinese emotion-cause pair extraction corpus demonstrate the effectiveness of the proposed model.",,https://aclanthology.org/2020.coling-main.17,,emotion,No,No
anthology+abstracts,Summarize before Aggregate: A Global-to-local Heterogeneous Graph Inference Network for Conversational Emotion Recognition,,2020,,,,10.18653/v1/2020.coling-main.367,"Conversational Emotion Recognition (CER) is a crucial task in Natural Language Processing (NLP) with wide applications. Prior works in CER generally focus on modeling emotion influences solely with utterance-level features, with little attention paid on phrase-level semantic connection between utterances. Phrases carry sentiments when they are referred to emotional events under certain topics, providing a global semantic connection between utterances throughout the entire conversation. In this work, we propose a two-stage Summarization and Aggregation Graph Inference Network (SumAggGIN), which seamlessly integrates inference for topic-related emotional phrases and local dependency reasoning over neighbouring utterances in a global-to-local fashion. Topic-related emotional phrases, which constitutes the global topic-related emotional connections, are recognized by our proposed heterogeneous Summarization Graph. Local dependencies, which captures short-term emotional effects between neighbouring utterances, are further injected via an Aggregation Graph to distinguish the subtle differences between utterances containing emotional phrases. The two steps of graph inference are tightly-coupled for a comprehensively understanding of emotional fluctuation. Experimental results on three CER benchmark datasets verify the effectiveness of our proposed model, which outperforms the state-of-the-art approaches.",,https://aclanthology.org/2020.coling-main.367,,emotion,No,No
anthology+abstracts,{CDL}: Curriculum Dual Learning for Emotion-Controllable Response Generation,,2020,,,,10.18653/v1/2020.acl-main.52,"Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.",,https://aclanthology.org/2020.acl-main.52,,emotion,No,No
anthology+abstracts,Learning and Evaluating Emotion Lexicons for 91 Languages,,2020,,,,10.18653/v1/2020.acl-main.112,"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at \url{https://github.com/JULIELab/MEmoLon} archived under DOI 10.5281/zenodo.3779901.",,https://aclanthology.org/2020.acl-main.112,,emotion,No,No
anthology+abstracts,Modeling Label Semantics for Predicting Emotional Reactions,,2020,,,,10.18653/v1/2020.acl-main.426,"Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. They ignore information that may be conveyed by the emotion labels themselves. We propose that the semantics of emotion labels can guide a model{'}s attention when representing the input story. Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.",,https://aclanthology.org/2020.acl-main.426,,emotion,Yes,No
anthology+abstracts,An Analysis of Emotion Communication Channels in Fan-Fiction: Towards Emotional Storytelling,,2019,,,,10.18653/v1/W19-3406,"Centrality of emotion for the stories told by humans is underpinned by numerous studies in literature and psychology. The research in automatic storytelling has recently turned towards emotional storytelling, in which characters{'} emotions play an important role in the plot development (Theune et al., 2004; y Perez, 2007; Mendez et al., 2016). However, these studies mainly use emotion to generate propositional statements in the form {``}A feels affection towards B{''} or {``}A confronts B{''}. At the same time, emotional behavior does not boil down to such propositional descriptions, as humans display complex and highly variable patterns in communicating their emotions, both verbally and non-verbally. In this paper, we analyze how emotions are expressed non-verbally in a corpus of fan fiction short stories. Our analysis shows that stories written by humans convey character emotions along various non-verbal channels. We find that some non-verbal channels, such as facial expressions and voice characteristics of the characters, are more strongly associated with joy, while gestures and body postures are more likely to occur with trust. Based on our analysis, we argue that automatic storytelling systems should take variability of emotion into account when generating descriptions of characters{'} emotions.",,https://aclanthology.org/W19-3406,,emotion,No,No
anthology+abstracts,Modeling Word Emotion in Historical Language: Quantity Beats Supposed Stability in Seed Word Selection,,2019,,,,10.18653/v1/W19-2501,"To understand historical texts, we must be aware that language{---}including the emotional connotation attached to words{---}changes over time. In this paper, we aim at estimating the emotion which is associated with a given word in former language stages of English and German. Emotion is represented following the popular Valence-Arousal-Dominance (VAD) annotation scheme. While being more expressive than polarity alone, existing word emotion induction methods are typically not suited for addressing it. To overcome this limitation, we present adaptations of two popular algorithms to VAD. To measure their effectiveness in diachronic settings, we present the first gold standard for historical word emotions, which was created by scholars with proficiency in the respective language stages and covers both English and German. In contrast to claims in previous work, our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful.",,https://aclanthology.org/W19-2501,,emotion,No,No
anthology+abstracts,How do we feel when a robot dies? Emotions expressed on {T}witter before and after hitch{BOT}{'}s destruction,,2019,,,,10.18653/v1/W19-1308,"In 2014, a chatty but immobile robot called hitchBOT set out to hitchhike across Canada. It similarly made its way across Germany and the Netherlands, and had begun a trip across the USA when it was destroyed by vandals. In this work, we analyze the emotions and sentiments associated with words in tweets posted before and after hitchBOT{'}s destruction to answer two questions: Were there any differences in the emotions expressed across the different countries visited by hitchBOT? And how did the public react to the demise of hitchBOT? Our analyses indicate that while there were few cross-cultural differences in sentiment towards hitchBOT, there was a significant negative emotional reaction to its destruction, suggesting that people had formed an emotional connection with hitchBOT and perceived its destruction as morally wrong. We discuss potential implications of anthropomorphism and emotional attachment to robots from the perspective of robot ethics.",,https://aclanthology.org/W19-1308,,emotion,No,No
anthology+abstracts,{NTUA}-{ISL}ab at {S}em{E}val-2019 Task 3: Determining emotions in contextual conversations with deep learning,,2019,,,,10.18653/v1/S19-2047,"Sentiment analysis (SA) in texts is a well-studied Natural Language Processing task, which in nowadays gains popularity due to the explosion of social media, and the subsequent accumulation of huge amounts of related data. However, capturing emotional states and the sentiment polarity of written excerpts requires knowledge on the events triggering them. Towards this goal, we present a computational end-to-end context-aware SA methodology, which was competed in the context of the SemEval-2019 / EmoContext task (Task 3). The proposed system is founded on the combination of two neural architectures, a deep recurrent neural network, structured by an attentive Bidirectional LSTM, and a deep dense network (DNN). The system achieved 0.745 micro f1-score, and ranked 26/165 (top 20{\%}) teams among the official task submissions.",,https://aclanthology.org/S19-2047,,emotion,No,No
anthology+abstracts,Adversarial Attention Modeling for Multi-dimensional Emotion Regression,,2019,,,,10.18653/v1/P19-1045,"In this paper, we propose a neural network-based approach, namely Adversarial Attention Network, to the task of multi-dimensional emotion regression, which automatically rates multiple emotion dimension scores for an input text. Especially, to determine which words are valuable for a particular emotion dimension, an attention layer is trained to weight the words in an input sequence. Furthermore, adversarial training is employed between two attention layers to learn better word weights via a discriminator. In particular, a shared attention layer is incorporated to learn public word weights between two emotion dimensions. Empirical evaluation on the EMOBANK corpus shows that our approach achieves notable improvements in r-values on both EMOBANK Reader{'}s and Writer{'}s multi-dimensional emotion regression tasks in all domains over the state-of-the-art baselines.",,https://aclanthology.org/P19-1045,,emotion,No,No
anthology+abstracts,Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts,,2019,,,,10.18653/v1/P19-1096,"Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach.",,https://aclanthology.org/P19-1096,,emotion,No,No
anthology+abstracts,Generating Responses with a Specific Emotion in Dialog,,2019,,,,10.18653/v1/P19-1359,"It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression.",,https://aclanthology.org/P19-1359,,emotion,No,Yes
anthology+abstracts,A Time Series Analysis of Emotional Loading in Central Bank Statements,,2019,,,,10.18653/v1/D19-5103,"We examine the affective content of central bank press statements using emotion analysis. Our focus is on two major international players, the European Central Bank (ECB) and the US Federal Reserve Bank (Fed), covering a time span from 1998 through 2019. We reveal characteristic patterns in the emotional dimensions of valence, arousal, and dominance and find{---}despite the commonly established attitude that emotional wording in central bank communication should be avoided{---}a correlation between the state of the economy and particularly the dominance dimension in the press releases under scrutiny and, overall, an impact of the president in office.",,https://aclanthology.org/D19-5103,,emotion,No,No
anthology+abstracts,A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis,,2019,,,,10.18653/v1/D19-1563,"Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08{\%} in F-measure.",,https://aclanthology.org/D19-1563,,emotion,No,No
anthology+abstracts,Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis,,2019,,,,10.18653/v1/D19-1566,"In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information, (e.g., textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. We employ a context-aware attention module to exploit the correspondence among the neighboring utterances. We evaluate our proposed approach for five standard multi-modal affect analysis datasets. Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.",,https://aclanthology.org/D19-1566,,emotion,No,No
anthology+abstracts,"Propagation of emotions, arousal and polarity in {W}ord{N}et using Heterogeneous Structured Synset Embeddings",,2019,,,,,"In this paper we present a novel method for emotive propagation in a wordnet based on a large emotive seed. We introduce a sense-level emotive lexicon annotated with polarity, arousal and emotions. The data were annotated as a part of a large study involving over 20,000 participants. A total of 30,000 lexical units in Polish WordNet were described with metadata, each unit received about 50 annotations concerning polarity, arousal and 8 basic emotions, marked on a multilevel scale. We present a preliminary approach to propagating emotive metadata to unlabeled lexical units based on the distribution of manual annotations using logistic regression and description of mixed synset embeddings based on our Heterogeneous Structured Synset Embeddings.",,https://aclanthology.org/2019.gwc-1.43,,emotion,Yes,No
anthology+abstracts,Emotional Neural Language Generation Grounded in Situational Contexts,,2019,,,,,,,https://aclanthology.org/2019.ccnlg-1.3,,emotion,No,No
anthology+abstracts,Exclamative Sentences in Emotion Expressions in {M}andarin {C}hinese: A Corpus-based Approach,,2018,,,,,,,https://aclanthology.org/Y18-1020,,emotion,No,No
anthology+abstracts,"Questions as a Pre-event, Pivot Event and Post-event of Emotions",,2018,,,,,,,https://aclanthology.org/Y18-1037,,emotion,No,No
anthology+abstracts,{UTFPR} at {IEST} 2018: Exploring Character-to-Word Composition for Emotion Analysis,,2018,,,,10.18653/v1/W18-6224,"We introduce the UTFPR system for the Implicit Emotions Shared Task of 2018: A compositional character-to-word recurrent neural network that does not exploit heavy and/or hard-to-obtain resources. We find that our approach can outperform multiple baselines, and offers an elegant and effective solution to the problem of orthographic variance in tweets.",,https://aclanthology.org/W18-6224,,emotion,No,No
anthology+abstracts,{UBC}-{NLP} at {IEST} 2018: Learning Implicit Emotion With an Ensemble of Language Models,,2018,,,,10.18653/v1/W18-6250,"We describe UBC-NLP contribution to IEST-2018, focused at learning implicit emotion in Twitter data. Among the 30 participating teams, our system ranked the 4th (with 69.3{\%} \textit{F}-score). Post competition, we were able to score slightly higher than the 3rd ranking system (reaching 70.7{\%}). Our system is trained on top of a pre-trained language model (LM), fine-tuned on the data provided by the task organizers. Our best results are acquired by an average of an ensemble of language models. We also offer an analysis of system performance and the impact of training data size on the task. For example, we show that training our best model for only one epoch with {\textless} 40{\%} of the data enables better performance than the baseline reported by Klinger et al. (2018) for the task.",,https://aclanthology.org/W18-6250,,emotion,No,No
anthology+abstracts,Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System,,2018,,,,10.18653/v1/W18-5017,"Positive emotion elicitation seeks to improve user{'}s emotional state through dialogue system interaction, where a chat-based scenario is layered with an implicit goal to address user{'}s emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert{'}s action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert{'}s responses and use the resulting labels to train the network. Our experiments and evaluation show that the proposed approach yields lower perplexity and generates a larger variety of responses.",,https://aclanthology.org/W18-5017,,emotion,No,No
anthology+abstracts,An Analysis of the Effect of Emotional Speech Synthesis on Non-Task-Oriented Dialogue System,,2018,,,,10.18653/v1/W18-5044,"This paper explores the effect of emotional speech synthesis on a spoken dialogue system when the dialogue is non-task-oriented. Although the use of emotional speech responses have been shown to be effective in a limited domain, e.g., scenario-based and counseling dialogue, the effect is still not clear in the non-task-oriented dialogue such as voice chatting. For this purpose, we constructed a simple dialogue system with example- and rule-based dialogue management. In the system, two types of emotion labeling with emotion estimation are adopted, i.e., system-driven and user-cooperative emotion labeling. We conducted a dialogue experiment where subjects evaluate the subjective quality of the system and the dialogue from the multiple aspects such as richness of the dialogue and impression of the agent. We then analyze and discuss the results and show the advantage of using appropriate emotions for the expressive speech responses in the non-task-oriented system.",,https://aclanthology.org/W18-5044,,emotion,No,No
anthology+abstracts,"Proceedings of the Second Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media",,2018,,,,10.18653/v1/W18-11,,,https://aclanthology.org/W18-1100,,emotion,No,No
anthology+abstracts,Social and Emotional Correlates of Capitalization on {T}witter,,2018,,,,10.18653/v1/W18-1102,"Social media text is replete with unusual capitalization patterns. We posit that capitalizing a token like THIS performs two expressive functions: it marks a person socially, and marks certain parts of an utterance as more salient than others. Focusing on gender and sentiment, we illustrate using a corpus of tweets that capitalization appears in more negative than positive contexts, and is used more by females compared to males. Yet we find that both genders use capitalization in a similar way when expressing sentiment.",,https://aclanthology.org/W18-1102,,emotion,No,No
anthology+abstracts,{NLPZZX} at {S}em{E}val-2018 Task 1: Using Ensemble Method for Emotion and Sentiment Intensity Determination,,2018,,,,10.18653/v1/S18-1015,"In this paper, we put forward a system that competed at SemEval-2018 Task 1: {``}Affect in Tweets{''}. Our system uses a simple yet effective ensemble method which combines several neural network components. We participate in two subtasks for English tweets: EI-reg and V-reg. For two subtasks, different combinations of neural components are examined. For EI-reg, our system achieves an accuracy of 0.727 in Pearson Correlation Coefficient (all instances) and an accuracy of 0.555 in Pearson Correlation Coefficient (0.5-1). For V-reg, the achieved accuracy scores are respectively 0.835 and 0.670",,https://aclanthology.org/S18-1015,,emotion,No,No
anthology+abstracts,{E}mo{I}ntens Tracker at {S}em{E}val-2018 Task 1: Emotional Intensity Levels in {\#}Tweets,,2018,,,,10.18653/v1/S18-1026,"The „Affect in Tweets{''} task is centered on emotions categorization and evaluation matrix using multi-language tweets (English and Spanish). In this research, SemEval Affect dataset was preprocessed, categorized, and evaluated accordingly (precision, recall, and accuracy). The system described in this paper is based on the implementation of supervised machine learning (Naive Bayes, KNN and SVM), deep learning (NN Tensor Flow model), and decision trees algorithms.",,https://aclanthology.org/S18-1026,,emotion,No,No
anthology+abstracts,{P}lus{E}mo2{V}ec at {S}em{E}val-2018 Task 1: Exploiting emotion knowledge from emoji and {\#}hashtags,,2018,,,,10.18653/v1/S18-1039,This paper describes our system that has been submitted to SemEval-2018 Task 1: Affect in Tweets (AIT) to solve five subtasks. We focus on modeling both sentence and word level representations of emotion inside texts through large distantly labeled corpora with emojis and hashtags. We transfer the emotional knowledge by exploiting neural network models as feature extractors and use these representations for traditional machine learning models such as support vector regression (SVR) and logistic regression to solve the competition tasks. Our system is placed among the Top3 for all subtasks we participated.,,https://aclanthology.org/S18-1039,,emotion,No,No
anthology+abstracts,psy{ML} at {S}em{E}val-2018 Task 1: Transfer Learning for Sentiment and Emotion Analysis,,2018,,,,10.18653/v1/S18-1056,"In this paper, we describe the first attempt to perform transfer learning from sentiment to emotions. Our system employs Long Short-Term Memory (LSTM) networks, including bidirectional LSTM (biLSTM) and LSTM with attention mechanism. We perform transfer learning by first pre-training the LSTM networks on sentiment data before concatenating the penultimate layers of these networks into a single vector as input to new dense layers. For the E-c subtask, we utilize a novel approach to train models for correlated emotion classes. Our system performs 4/48, 3/39, 8/38, 4/37, 4/35 on all English subtasks EI-reg, EI-oc, V-reg, V-oc, E-c of SemEval 2018 Task 1: Affect in Tweets.",,https://aclanthology.org/S18-1056,,emotion,No,No
anthology+abstracts,{M}oji{T}alk: Generating Emotional Responses at Scale,,2018,,,,10.18653/v1/P18-1104,"Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.",,https://aclanthology.org/P18-1104,,emotion,Yes,No
anthology+abstracts,Automatic Dialogue Generation with Expressed Emotions,,2018,,,,10.18653/v1/N18-2008,"Despite myriad efforts in the literature designing neural dialogue generation systems in recent years, very few consider putting restrictions on the response itself. They learn from collections of past responses and generate one based on a given utterance without considering, speech act, desired style or emotion to be expressed. In this research, we address the problem of forcing the dialogue generation to express emotion. We present three models that either concatenate the desired emotion with the source input during the learning, or push the emotion in the decoder. The results, evaluated with an emotion tagger, are encouraging with all three models, but present better outcome and promise with our model that adds the emotion vector in the decoder.",,https://aclanthology.org/N18-2008,,emotion,No,No
anthology+abstracts,Representation Mapping: A Novel Approach to Generate High-Quality Multi-Lingual Emotion Lexicons,,2018,,,,,,,https://aclanthology.org/L18-1028,,emotion,No,No
anthology+abstracts,Understanding Emotions: A Dataset of Tweets to Study Interactions between Affect Categories,,2018,,,,,,,https://aclanthology.org/L18-1030,,emotion,No,No
anthology+abstracts,Bootstrapping Polar-Opposite Emotion Dimensions from Online Reviews,,2018,,,,,,,https://aclanthology.org/L18-1098,,emotion,No,No
anthology+abstracts,A Comparison Of Emotion Annotation Schemes And A New Annotated Data Set,,2018,,,,,,,https://aclanthology.org/L18-1192,,emotion,Yes,No
anthology+abstracts,Dialogue Scenario Collection of Persuasive Dialogue with Emotional Expressions via Crowdsourcing,,2018,,,,,,,https://aclanthology.org/L18-1194,,emotion,No,No
anthology+abstracts,{W}iki{A}rt Emotions: An Annotated Dataset of Emotions Evoked by Art,,2018,,,,,,,https://aclanthology.org/L18-1197,,emotion,Yes,No
anthology+abstracts,Distribution of Emotional Reactions to News Articles in {T}witter,,2018,,,,,,,https://aclanthology.org/L18-1225,,emotion,No,No
anthology+abstracts,{E}motion{L}ines: An Emotion Corpus of Multi-Party Conversations,,2018,,,,,,,https://aclanthology.org/L18-1252,,emotion,No,No
anthology+abstracts,{EMTC}: Multilabel Corpus in Movie Domain for Emotion Analysis in Conversational Text,,2018,,,,,,,https://aclanthology.org/L18-1418,,emotion,No,No
anthology+abstracts,A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation,,2018,,,,10.18653/v1/D18-1071,"Traditional neural language models tend to generate generic replies with poor logic and no emotion. In this paper, a syntactically constrained bidirectional-asynchronous approach for emotional conversation generation (E-SCBA) is proposed to address this issue. In our model, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding. It is much different from most existing methods which generate replies from the first word to the last. Through experiments, the results indicate that our approach not only improves the diversity of replies, but gains a boost on both logic and emotion compared with baselines.",,https://aclanthology.org/D18-1071,,emotion,No,No
anthology+abstracts,A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness,,2018,,,,10.18653/v1/D18-1506,"Emotion cause analysis has been a key topic in natural language processing. Existing methods ignore the contexts around the emotion word which can provide an emotion cause clue. Meanwhile, the clauses in a document play different roles on stimulating a certain emotion, depending on their content relevance. Therefore, we propose a co-attention neural network model for emotion cause analysis with emotional context awareness. The method encodes the clauses with a co-attention based bi-directional long short-term memory into high-level input representations, which are further fed into a convolutional layer for emotion cause analysis. Experimental results show that our approach outperforms the state-of-the-art baseline methods.",,https://aclanthology.org/D18-1506,,emotion,No,No
anthology+abstracts,{J}e{S}em{E}: Interleaving Semantics and Emotions in a Web Service for the Exploration of Language Change Phenomena,,2018,,,,,"We here introduce a substantially extended version of JeSemE, an interactive website for visually exploring computationally derived time-variant information on word meanings and lexical emotions assembled from five large diachronic text corpora. JeSemE is designed for scholars in the (digital) humanities as an alternative to consulting manually compiled, printed dictionaries for such information (if available at all). This tool uniquely combines state-of-the-art distributional semantics with a nuanced model of human emotions, two information streams we deem beneficial for a data-driven interpretation of texts in the humanities.",,https://aclanthology.org/C18-2003,,emotion,No,No
anthology+abstracts,Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level,,2018,,,,,"Emotion Representation Mapping (ERM) has the goal to convert existing emotion ratings from one representation format into another one, e.g., mapping Valence-Arousal-Dominance annotations for words or sentences into Ekman{'}s Basic Emotions and vice versa. ERM can thus not only be considered as an alternative to Word Emotion Induction (WEI) techniques for automatic emotion lexicon construction but may also help mitigate problems that come from the proliferation of emotion representation formats in recent years. We propose a new neural network approach to ERM that not only outperforms the previous state-of-the-art. Equally important, we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least.",,https://aclanthology.org/C18-1245,,emotion,No,No
anthology+abstracts,整合個人化磁振造影深度神經網路之演算法技術 (Joint Modeling of Individual Neural Responses using a Deep Voting Fusion Network for Automatic Emotion Perception Decoding),,2018,International Journal of Computational Linguistics and Chinese Language Processing,23,,,,,https://aclanthology.org/2018.ijclclp-1.3,,emotion,No,No
anthology+abstracts,Are doggies really nicer than dogs? The impact of morphological derivation on emotional valence in {G}erman,,2017,,,,,,,https://aclanthology.org/W17-6922,,emotion,No,No
anthology+abstracts,Seernet at {E}mo{I}nt-2017: Tweet Emotion Intensity Estimator,,2017,,,,10.18653/v1/W17-5228,"The paper describes experiments on estimating emotion intensity in tweets using a generalized regressor system. The system combines various independent feature extractors, trains them on general regressors and finally combines the best performing models to create an ensemble. The proposed system stood 3rd out of 22 systems in leaderboard of WASSA-2017 Shared Task on Emotion Intensity.",,https://aclanthology.org/W17-5228,,emotion,No,No
anthology+abstracts,{DMG}roup at {E}mo{I}nt-2017: Emotion Intensity Using Ensemble Method,,2017,,,,10.18653/v1/W17-5234,"In this paper, we present a novel ensemble learning architecture for emotion intensity analysis, particularly a novel framework of ensemble method. The ensemble method has two stages and each stage includes several single machine learning models. In stage1, we employ both linear and nonlinear regression models to obtain a more diverse emotion intensity representation. In stage2, we use two regression models including linear regression and XGBoost. The result of stage1 serves as the input of stage2, so the two different type models (linear and non-linear) in stage2 can describe the input in two opposite aspects. We also added a method for analyzing and splitting multi-words hashtags and appending them to the emotion intensity corpus before feeding it to our model. Our model achieves 0.571 Pearson-measure for the average of four emotions.",,https://aclanthology.org/W17-5234,,emotion,No,Yes
anthology+abstracts,{LIPN}-{UAM} at {E}mo{I}nt-2017:Combination of Lexicon-based features and Sentence-level Vector Representations for Emotion Intensity Determination,,2017,,,,10.18653/v1/W17-5236,"This paper presents the combined LIPN-UAM participation in the WASSA 2017 Shared Task on Emotion Intensity. In particular, the paper provides some highlights on the Tweetaneuse system that was presented to the shared task. We combined lexicon-based features with sentence-level vector representations to implement a random forest regressor.",,https://aclanthology.org/W17-5236,,emotion,No,No
anthology+abstracts,Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text Understanding in Emotion Annotation,,2017,,,,10.18653/v1/W17-0801,"We here examine how different perspectives of understanding written discourse, like the reader{'}s, the writer{'}s or the text{'}s point of view, affect the quality of emotion annotations. We conducted a series of annotation experiments on two corpora, a popular movie review corpus and a genre- and domain-balanced corpus of standard English. We found statistical evidence that the writer{'}s perspective yields superior annotation quality overall. However, the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from. Our data further suggest that the popular movie review data set suffers from an atypical bimodal distribution which may decrease model performance when used as a training resource.",,https://aclanthology.org/W17-0801,,emotion,Yes,No
anthology+abstracts,{E}mo{B}ank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis,,2017,,,,,"We describe EmoBank, a corpus of 10k English sentences balancing multiple genres, which we annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. On the one hand, we distinguish between writer{'}s and reader{'}s emotions, on the other hand, a subset of the corpus complements dimensional VAD annotations with categorical ones based on Basic Emotions. We find evidence for the supremacy of the reader{'}s perspective in terms of IAA and rating intensity, and achieve close-to-human performance when mapping between dimensional and categorical formats.",,https://aclanthology.org/E17-2092,,emotion,No,No
anthology+abstracts,"Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",,2016,,,,,,,https://aclanthology.org/W16-4300,,emotion,No,No
anthology+abstracts,Feelings from the {P}ast{---}{A}dapting Affective Lexicons for Historical Emotion Analysis,,2016,,,,,"We here describe a novel methodology for measuring affective language in historical text by expanding an affective lexicon and jointly adapting it to prior language stages. We automatically construct a lexicon for word-emotion association of 18th and 19th century German which is then validated against expert ratings. Subsequently, this resource is used to identify distinct emotional patterns and trace long-term emotional trends in different genres of writing spanning several centuries.",,https://aclanthology.org/W16-4008,,emotion,No,No
anthology+abstracts,Classifying Emotions in Customer Support Dialogues in Social Media,,2016,,,,10.18653/v1/W16-3609,,,https://aclanthology.org/W16-3609,,emotion,No,No
anthology+abstracts,Do Enterprises Have Emotions?,,2016,,,,10.18653/v1/W16-0423,,,https://aclanthology.org/W16-0423,,emotion,No,No
anthology+abstracts,Emotions and {NLP}: Future Directions,,2016,,,,10.18653/v1/W16-0430,,,https://aclanthology.org/W16-0430,,emotion,No,No
anthology+abstracts,Metaphor as a Medium for Emotion: An Empirical Study,,2016,,,,10.18653/v1/S16-2003,,,https://aclanthology.org/S16-2003,,emotion,No,No
anthology+abstracts,Inferring Perceived Demographics from User Emotional Tone and User-Environment Emotional Contrast,,2016,,,,10.18653/v1/P16-1148,,,https://aclanthology.org/P16-1148,,emotion,No,No
anthology+abstracts,Mirroring Facial Expressions and Emotions in Dyadic Conversations,,2016,,,,,"This paper presents an investigation of mirroring facial expressions and the emotions which they convey in dyadic naturally occurring first encounters. Mirroring facial expressions are a common phenomenon in face-to-face interactions, and they are due to the mirror neuron system which has been found in both animals and humans. Researchers have proposed that the mirror neuron system is an important component behind many cognitive processes such as action learning and understanding the emotions of others. Preceding studies of the first encounters have shown that overlapping speech and overlapping facial expressions are very frequent. In this study, we want to determine whether the overlapping facial expressions are mirrored or are otherwise correlated in the encounters, and to what extent mirroring facial expressions convey the same emotion. The results of our study show that the majority of smiles and laughs, and one fifth of the occurrences of raised eyebrows are mirrored in the data. Moreover some facial traits in co-occurring expressions co-occur more often than it would be expected by chance. Finally, amusement, and to a lesser extent friendliness, are often emotions shared by both participants, while other emotions indicating individual affective states such as uncertainty and hesitancy are never showed by both participants, but co-occur with complementary emotions such as friendliness and support. Whether these tendencies are specific to this type of conversations or are more common should be investigated further.",,https://aclanthology.org/L16-1075,,emotion,No,No
anthology+abstracts,Comparison of Emotional Understanding in Modality-Controlled Environments using Multimodal Online Emotional Communication Corpus,,2016,,,,,"In online computer-mediated communication, speakers were considered to have experienced difficulties in catching their partner{'}s emotions and in conveying their own emotions. To explain why online emotional communication is so difficult and to investigate how this problem should be solved, multimodal online emotional communication corpus was constructed by recording approximately 100 speakers{'} emotional expressions and reactions in a modality-controlled environment. Speakers communicated over the Internet using video chat, voice chat or text chat; their face-to-face conversations were used for comparison purposes. The corpora incorporated emotional labels by evaluating the speaker{'}s dynamic emotional states and the measurements of the speaker{'}s facial expression, vocal expression and autonomic nervous system activity. For the initial study of this project, which used a large-scale emotional communication corpus, the accuracy of online emotional understanding was assessed to demonstrate the emotional labels evaluated by the speakers and to summarize the speaker{'}s answers on the questionnaire regarding the difference between an online chat and face-to-face conversations in which they actually participated. The results revealed that speakers have difficulty communicating their emotions in online communication environments, regardless of the type of communication modality and that inaccurate emotional understanding occurs more frequently in online computer-mediated communication than in face-to-face communication.",,https://aclanthology.org/L16-1343,,emotion,No,No
anthology+abstracts,Accuracy of Automatic Cross-Corpus Emotion Labeling for Conversational Speech Corpus Commonization,,2016,,,,,"There exists a major incompatibility in emotion labeling framework among emotional speech corpora, that is, category-based and dimension-based. Commonizing these requires inter-corpus emotion labeling according to both frameworks, but doing this by human annotators is too costly for most cases. This paper examines the possibility of automatic cross-corpus emotion labeling. In order to evaluate the effectiveness of the automatic labeling, a comprehensive emotion annotation for two conversational corpora, UUDB and OGVC, was performed. With a state-of-the-art machine learning technique, dimensional and categorical emotion estimation models were trained and tested against the two corpora. For the emotion dimension estimation, the automatic cross-corpus emotion labeling for the different corpus was effective for the dimensions of aroused-sleepy, dominant-submissive and interested-indifferent, showing only slight performance degradation against the result for the same corpus. On the other hand, the performance for the emotion category estimation was not sufficient.",,https://aclanthology.org/L16-1634,,emotion,No,No
anthology+abstracts,"Book Review: Sentiment Analysis: Mining Opinions, Sentiments, and Emotions by Bing {L}iu",,2016,Computational Linguistics,42,,10.1162/COLI_r_00259,,,https://aclanthology.org/J16-3008,,emotion,No,No
anthology+abstracts,Emotion Distribution Learning from Texts,,2016,,,,10.18653/v1/D16-1061,,,https://aclanthology.org/D16-1061,,emotion,No,No
anthology+abstracts,Event-Driven Emotion Cause Extraction with Corpus Construction,,2016,,,,10.18653/v1/D16-1170,,,https://aclanthology.org/D16-1170,,emotion,No,No
anthology+abstracts,{M}ixed{E}motions: Social Semantic Emotion Analysis for Innovative Multilingual Big Data Analytics Markets,,2015,,,,,,,https://aclanthology.org/W15-4929,,emotion,No,No
anthology+abstracts,A Linked Data Model for Multimodal Sentiment and Emotion Analysis,,2015,,,,10.18653/v1/W15-4202,,,https://aclanthology.org/W15-4202,,emotion,No,No
anthology+abstracts,Emotion in Code-switching Texts: Corpus Construction and Analysis,,2015,,,,10.18653/v1/W15-3116,,,https://aclanthology.org/W15-3116,,emotion,No,No
anthology+abstracts,{I}magisaurus: An Interactive Visualizer of Valence and Emotion in the {R}oget{'}s Thesaurus,,2015,,,,10.18653/v1/W15-2912,,,https://aclanthology.org/W15-2912,,emotion,No,No
anthology+abstracts,Emotion and Inner State Adverbials in {R}ussian,,2015,,,,,,,https://aclanthology.org/W15-2106,,emotion,No,No
anthology+abstracts,Invited Talk: Modeling Socio-Emotional Humanoid Agent,,2015,,,,,,,https://aclanthology.org/W15-1802,,emotion,No,No
anthology+abstracts,Embarrassed or Awkward? Ranking Emotion Synonyms for {ESL} Learners{'} Appropriate Wording,,2015,,,,10.3115/v1/W15-0617,,,https://aclanthology.org/W15-0617,,emotion,No,No
anthology+abstracts,And That{'}s A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue,,2015,,,,10.3115/v1/W15-0515,,,https://aclanthology.org/W15-0515,,emotion,No,No
anthology+abstracts,About Emotion Identification in Visual Sentiment Analysis,,2015,,,,,,,https://aclanthology.org/R15-1035,,emotion,No,No
anthology+abstracts,Multi-Lingual Sentiment Analysis of Social Data Based on Emotion-Bearing Patterns,,2014,,,,10.3115/v1/W14-5906,,,https://aclanthology.org/W14-5906,,emotion,No,No
anthology+abstracts,Naturalistic Audio-Visual Emotion Database,,2014,,,,,,,https://aclanthology.org/W14-5131,,emotion,No,No
anthology+abstracts,Discriminating Neutral and Emotional Speech using Neural Networks,,2014,,,,,,,https://aclanthology.org/W14-5132,,emotion,No,No
anthology+abstracts,"{W}ords: Evaluative, Emotional, Colourful, Musical!",,2014,,,,10.3115/v1/W14-2601,,,https://aclanthology.org/W14-2601,,emotion,No,No
anthology+abstracts,Semantic Role Labeling of Emotions in Tweets,,2014,,,,10.3115/v1/W14-2607,,,https://aclanthology.org/W14-2607,,emotion,No,No
anthology+abstracts,Synalp-Empathic: A Valence Shifting Hybrid System for Sentiment Analysis,,2014,,,,10.3115/v1/S14-2106,,,https://aclanthology.org/S14-2106,,empathy,No,No
anthology+abstracts,Generating a Word-Emotion Lexicon from {\#}Emotional Tweets,,2014,,,,10.3115/v1/S14-1002,,,https://aclanthology.org/S14-1002,,emotion,No,No
anthology+abstracts,A Topic Model for Building Fine-grained Domain-specific Emotion Lexicon,,2014,,,,10.3115/v1/P14-2069,,,https://aclanthology.org/P14-2069,,emotion,No,No
anthology+abstracts,Depeche Mood: a Lexicon for Emotion Analysis from Crowd Annotated News,,2014,,,,10.3115/v1/P14-2070,,,https://aclanthology.org/P14-2070,,emotion,No,No
anthology+abstracts,"Toward a unifying model for Opinion, Sentiment and Emotion information extraction",,2014,,,,,"This paper presents a logical formalization of a set 20 semantic categories related to opinion, emotion and sentiment. Our formalization is based on the BDI model (Belief, Desire and Intetion) and constitues a first step toward a unifying model for subjective information extraction. The separability of the subjective classes that we propose was assessed both formally and on two subjective reference corpora.",,http://www.lrec-conf.org/proceedings/lrec2014/pdf/1010_Paper.pdf,,emotion,No,No
anthology+abstracts,{E}milya: Emotional body expression in daily actions database,,2014,,,,,"The studies of bodily expression of emotion have been so far mostly focused on body movement patterns associated with emotional expression. Recently, there is an increasing interest on the expression of emotion in daily actions, called also non-emblematic movements (such as walking or knocking at the door). Previous studies were based on database limited to a small range of movement tasks or emotional states. In this paper, we describe our new database of emotional body expression in daily actions, where 11 actors express 8 emotions in 7 actions. We use motion capture technology to record body movements, but we recorded as well synchronized audio-visual data to enlarge the use of the database for different research purposes. We investigate also the matching between the expressed emotions and the perceived ones through a perceptive study. The first results of this study are discussed in this paper.",,http://www.lrec-conf.org/proceedings/lrec2014/pdf/334_Paper.pdf,,emotion,No,No
anthology+abstracts,Acquiring a Dictionary of Emotion-Provoking Events,,2014,,,,10.3115/v1/E14-4025,,,https://aclanthology.org/E14-4025,,emotion,No,No
anthology+abstracts,"Weighted {K}rippendorff{'}s alpha is a more reliable metrics for multi-coders ordinal annotations: experimental studies on emotion, opinion and coreference annotation",,2014,,,,10.3115/v1/E14-1058,,,https://aclanthology.org/E14-1058,,emotion,No,No
anthology+abstracts,"Learning Emotion Indicators from Tweets: Hashtags, Hashtag Patterns, and Phrases",,2014,,,,10.3115/v1/D14-1127,,,https://aclanthology.org/D14-1127,,emotion,No,No
anthology+abstracts,Joint Emotion Analysis via Multi-task {G}aussian Processes,,2014,,,,10.3115/v1/D14-1190,,,https://aclanthology.org/D14-1190,,emotion,No,No
anthology+abstracts,Identifying Emotional and Informational Support in Online Health Communities,,2014,,,,,,,https://aclanthology.org/C14-1079,,emotion,No,No
anthology+abstracts,Identifying Emotion Labels from Psychiatric Social Texts Using Independent Component Analysis,,2014,,,,,,,https://aclanthology.org/C14-1080,,emotion,No,No
anthology+abstracts,Bilingual analysis of {LOVE} and {HATRED} emotional markers ({SPSS}-based approach),,2013,,,,,,,https://aclanthology.org/W13-4103,,emotion,No,No
anthology+abstracts,Demonstration of the {E}mote{W}izard of {O}z Interface for Empathic Robotic Tutors,,2013,,,,,,,https://aclanthology.org/W13-4058,,empathy,No,No
anthology+abstracts,Recent adventures with emotion-reading technology,,2013,,,,,,,https://aclanthology.org/W13-1601,,emotion,No,No
anthology+abstracts,Bootstrapped Learning of Emotion Hashtags {\#}hashtags4you,,2013,,,,,,,https://aclanthology.org/W13-1602,,emotion,No,No
anthology+abstracts,{SINAI}: Machine Learning and Emotion of the Crowd for Sentiment Analysis in Microblogs,,2013,,,,,,,https://aclanthology.org/S13-2066,,emotion,No,No
anthology+abstracts,Joint Modeling of News Reader{'}s and Comment Writer{'}s Emotions,,2013,,,,,,,https://aclanthology.org/P13-2091,,emotion,No,No
anthology+abstracts,Probabilistic Sense Sentiment Similarity through Hidden Emotions,,2013,,,,,,,https://aclanthology.org/P13-1097,,emotion,No,No
anthology+abstracts,"Emotion Co-referencing - Emotional Expression, Holder, and Topic",,2013,,,,,,,https://aclanthology.org/O13-2004,,emotion,No,No
anthology+abstracts,Causing Emotion in Collocation:An Exploratory Data Analysis,,2013,,,,,,,https://aclanthology.org/O13-1024,,emotion,No,No
anthology+abstracts,Construction of Emotional Lexicon Using Potts Model,,2013,,,,,,,https://aclanthology.org/I13-1078,,emotion,No,No
anthology+abstracts,Extracting Causes of Emotions from Text,,2013,,,,,,,https://aclanthology.org/I13-1121,,emotion,No,No
anthology+abstracts,Emotional Tendency Identification for Micro-blog Topics Based on Multiple Characteristics,,2012,,,,,,,https://aclanthology.org/Y12-1030,,emotion,No,No
anthology+abstracts,Emotion Estimation from Sentence Using Relation between {J}apanese Slangs and Emotion Expressions,,2012,,,,,,,https://aclanthology.org/Y12-1037,,emotion,No,No
anthology+abstracts,{E}motiphons: Emotion Markers in Conversational Speech - Comparison across {I}ndian Languages,,2012,,,,,,,https://aclanthology.org/W12-5308,,emotion,No,No
anthology+abstracts,Prior versus Contextual Emotion of a Word in a Sentence,,2012,,,,,,,https://aclanthology.org/W12-3711,,emotion,No,No
anthology+abstracts,The Role of Emotional Stability in {T}witter Conversations,,2012,,,,,,,https://aclanthology.org/W12-0602,,emotion,No,No
anthology+abstracts,{\#}Emotional Tweets,,2012,,,,,,,https://aclanthology.org/S12-1033,,emotion,No,No
anthology+abstracts,The {I}3{MEDIA} speech database: a trilingual annotated corpus for the analysis and synthesis of emotional speech,,2012,,,,,"In this article the I3Media corpus is presented, a trilingual (Catalan, English, Spanish) speech database of neutral and emotional material collected for analysis and synthesis purposes. The corpus is actually made up of six different subsets of material: a neutral subcorpus, containing emotionless utterances; a dialog' subcorpus, containing typical call center utterances; an emotional' corpus, a set of sentences representative of pure emotional states; a football' subcorpus, including utterances imitating a football broadcasting situation; a SMS' subcorpus, including readings of SMS texts; and a paralinguistic elements' corpus, including recordings of interjections and paralinguistic sounds uttered in isolation. The corpus was read by professional speakers (male, in the case of Spanish and Catalan; female, in the case of the English corpus), carefully selected to meet criteria of language competence, voice quality and acting conditions. It is the result of a collaboration between the Speech Technology Group at Telef{\'o}nica Investigaci{\'o}n y Desarrollo (TID) and the Speech and Language Group at Barcelona Media Centre d'Innovaci{\'o} (BM), as part of the I3Media project.",,http://www.lrec-conf.org/proceedings/lrec2012/pdf/865_Paper.pdf,,emotion,Yes,No
anthology+abstracts,L{'}analyse de l{'}{\'e}motion dans les forums de sant{\'e} (Analysis of Emotion in Health Fora) [in {F}rench],,2012,,,,,,,https://aclanthology.org/F12-3021,,emotion,No,No
anthology+abstracts,Impact du Comportement Social d{'}un Robot sur les Emotions de l{'}Utilisateur : une Exp{\'e}rience Perceptive (Impact of the Social Behaviour of a Robot on the User{'}s Emotions: a Perceptive Experiment) [in {F}rench],,2012,,,,,,,https://aclanthology.org/F12-1036,,emotion,No,No
anthology+abstracts,"{CL}ex: A Lexicon for Exploring Color, Concept and Emotion Associations in Language",,2012,,,,,,,https://aclanthology.org/E12-1031,,emotion,No,No
anthology+abstracts,"Lyrics, Music, and Emotions",,2012,,,,,,,https://aclanthology.org/D12-1054,,emotion,No,No
anthology+abstracts,Exploring Emotional Words for {C}hinese Document Chief Emotion Analysis,,2011,,,,,,,https://aclanthology.org/Y11-1064,,emotion,No,No
anthology+abstracts,Emotion Modeling from Writer/Reader Perspectives Using a Microblog Dataset,,2011,,,,,,,https://aclanthology.org/W11-3703,,emotion,No,No
anthology+abstracts,Analyzing Emotional Statements {--} Roles of General and Physiological Variables,,2011,,,,,,,https://aclanthology.org/W11-3709,,emotion,No,No
anthology+abstracts,Tracking Sentiment in Mail: How Genders Differ on Emotional Axes,,2011,,,,,,,https://aclanthology.org/W11-1709,,emotion,No,No
anthology+abstracts,Developing {J}apanese {W}ord{N}et Affect for Analyzing Emotions,,2011,,,,,,,https://aclanthology.org/W11-1710,,emotion,No,No
anthology+abstracts,{EMOC}ause: An Easy-adaptable Approach to Extract Emotion Cause Contexts,,2011,,,,,,,https://aclanthology.org/W11-1720,,emotion,No,No
anthology+abstracts,From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels and Fairy Tales,,2011,,,,,,,https://aclanthology.org/W11-1514,,emotion,No,No
anthology+abstracts,"Evaluation de la d{\'e}tection des {\'e}motions, des opinions ou des sentiments : dictature de la majorit{\'e} ou respect de la diversit{\'e} d{'}opinions ? (Evaluation of the detection of emotions, opinions or sentiments: majority dictatorship or respect for opinion diversity?)",,2011,,,,,"D{\'e}tection d{'}{\'e}motion, fouille d{'}opinion et analyse des sentiments sont g{\'e}n{\'e}ralement {\'e}valu{\'e}s par comparaison des r{\'e}ponses du syst{\`e}me concern{\'e} par rapport {\`a} celles contenues dans un corpus de r{\'e}f{\'e}rence. Les questions pos{\'e}es dans cet article concernent {\`a} la fois la d{\'e}finition de la r{\'e}f{\'e}rence et la fiabilit{\'e} des m{\'e}triques les plus fr{\'e}quemment utilis{\'e}es pour cette comparaison. Les exp{\'e}rimentations men{\'e}es pour {\'e}valuer le syst{\`e}me de d{\'e}tection d{'}{\'e}motions EmoLogus servent de base de r{\'e}flexion pour ces deux probl{\`e}mes. L{'}analyse des r{\'e}sultats d{'}EmoLogus et la comparaison entre les diff{\'e}rentes m{\'e}triques remettent en cause le choix du vote majoritaire comme r{\'e}f{\'e}rence. Par ailleurs elles montrent {\'e}galement la n{\'e}cessit{\'e} de recourir {\`a} des outils statistiques plus {\'e}volu{\'e}s que ceux g{\'e}n{\'e}ralement utilis{\'e}s pour obtenir des {\'e}valuations fiables de syst{\`e}mes qui travaillent sur des donn{\'e}es intrins{\`e}quement subjectives et incertaines.",,https://aclanthology.org/2011.jeptalnrecital-court.1,,emotion,No,No
anthology+abstracts,Une proc{\'e}dure pour identifier les modifieurs de la valence affective d{'}un mot dans des textes (A procedure to identify modifiers of the word emotional valence in texts),,2011,,,,,"Cette recherche s{'}inscrit dans le champ de la fouille d{'}opinion et, plus particuli{\`e}rement, dans celui de l{'}analyse de la polarit{\'e} d{'}une phrase ou d{'}un syntagme. Dans ce cadre, la prise en compte du contexte linguistique dans lequel apparaissent les mots porteurs de valence est particuli{\`e}rement importante. Nous proposons une m{\'e}thodologie pour extraire automatiquement de corpus de textes de telles expressions linguistiques. Cette approche s{'}appuie sur un corpus de textes, ou d{'}extraits de textes, dont la valence est connue, sur un lexique de valence construit {\`a} partir de ce corpus au moyen d{'}une proc{\'e}dure automatique et sur un analyseur syntaxique. Une {\'e}tude exploratoire, limit{\'e}e {\`a} la seule relation syntaxique associant un adverbe {\`a} un adjectif, laisse entrevoir les potentialit{\'e}s de l{'}approche.",,https://aclanthology.org/2011.jeptalnrecital-court.23,,emotion,No,No
anthology+abstracts,"Identifying Emotional Expressions, Intensities and Sentence Level Emotion Tags Using a Supervised Framework",,2010,,,,,,,https://aclanthology.org/Y10-1013,,emotion,No,No
anthology+abstracts,Finding Emotion Holder from {B}engali Blog {T}exts{---}{A}n Unsupervised Syntactic Approach,,2010,,,,,,,https://aclanthology.org/Y10-1071,,emotion,No,No
anthology+abstracts,Textual Emotion Processing From Event Analysis,,2010,,,,,,,https://aclanthology.org/W10-4102,,emotion,No,No
anthology+abstracts,The Color of Emotions in Texts,,2010,,,,,,,https://aclanthology.org/W10-3405,,emotion,No,No
anthology+abstracts,Labeling Emotion in {B}engali Blog Corpus {--} A Fine Grained Tagging at Sentence Level,,2010,,,,,,,https://aclanthology.org/W10-3207,,emotion,No,No
anthology+abstracts,An Embodied Dialogue System with Personality and Emotions,,2010,,,,,,,https://aclanthology.org/W10-2706,,emotion,No,No
anthology+abstracts,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,,2010,,,,,,,https://aclanthology.org/W10-0200,,emotion,No,No
anthology+abstracts,Emotion Analysis Using Latent Affective Folding and Embedding,,2010,,,,,,,https://aclanthology.org/W10-0201,,emotion,No,No
anthology+abstracts,Emotions Evoked by Common Words and Phrases: Using {M}echanical {T}urk to Create an Emotion Lexicon,,2010,,,,,,,https://aclanthology.org/W10-0204,,emotion,No,No
anthology+abstracts,A Corpus-based Method for Extracting Paraphrases of Emotion Terms,,2010,,,,,,,https://aclanthology.org/W10-0205,,emotion,No,No
anthology+abstracts,"Identifying Emotions, Intentions, and Attitudes in Text Using a Game with a Purpose",,2010,,,,,,,https://aclanthology.org/W10-0209,,emotion,No,No
anthology+abstracts,Emotional Perception of Fairy Tales: Achieving Agreement in Emotion Annotation of Text,,2010,,,,,,,https://aclanthology.org/W10-0212,,emotion,No,No
anthology+abstracts,{N}ews{V}iz: Emotional Visualization of News Stories,,2010,,,,,,,https://aclanthology.org/W10-0215,,emotion,No,No
anthology+abstracts,Discerning Emotions of Bloggers based on Topics {--} a Supervised Coreference Approach in {B}engali,,2010,,,,,,,https://aclanthology.org/O10-2008,,emotion,No,No
anthology+abstracts,Determining Reliability of Subjective and Multi-label Emotion Annotation through Novel Fuzzy Agreement Measure,,2010,,,,,"The paper presents a new fuzzy agreement measure {\$}{\textbackslash}gamma{\_}f{\$} for determining the agreement in multi-label and subjective annotation task. In this annotation framework, one data item may belong to a category or a class with a belief value denoting the degree of confidence of an annotator in assigning the data item to that category. We have provided a notion of disagreement based on the belief values provided by the annotators with respect to a category. The fuzzy agreement measure {\$}{\textbackslash}gamma{\_}f{\$} has been proposed by defining different fuzzy agreement sets based on the distribution of difference of belief values provided by the annotators. The fuzzy agreement has been computed by studying the average agreement over all the data items and annotators. Finally, we elaborate on the computation {\$}{\textbackslash}gamma{\_}f{\$} measure with a case study on emotion text data where a data item (sentence) may belong to more than one emotion category with varying belief values.",,http://www.lrec-conf.org/proceedings/lrec2010/pdf/67_Paper.pdf,,emotion,Yes,No
anthology+abstracts,Developing an Expressive Speech Labeling Tool Incorporating the Temporal Characteristics of Emotion,,2010,,,,,"A lot of research effort has been spent on the development of emotion theories and modeling, however, their suitability and applicability to expressions in human computer interaction has not exhaustively been evaluated. Furthermore, investigations concerning the ability of the annotators to map certain expressions onto the developed emotion models is lacking proof. The proposed annotation tool, which incorporates the standard Geneva Emotional Wheel developed by Klaus Scherer and a novel temporal characteristic description feature, is aiming towards enabling the annotator to label expressions recorded in human computer interaction scenarios on an utterance level. Further, it is respecting key features of realistic and natural emotional expressions, such as their sequentiality, temporal characteristics, their mixed occurrences, and their expressivity or clarity of perception. Additionally, first steps towards evaluating the proposed tool, by analyzing utterance annotations taken from two expressive speech corpora, are undertaken and some future goals including the open source accessibility of the tool are given.",,http://www.lrec-conf.org/proceedings/lrec2010/pdf/101_Paper.pdf,,emotion,No,No
anthology+abstracts,The Demo / Kemo Corpus: A Principled Approach to the Study of Cross-cultural Differences in the Vocal Expression and Perception of Emotion,,2010,,,,,"This paper presents the Demo / Kemo corpus of Dutch and Korean emotional speech. The corpus has been specifically developed for the purpose of cross-linguistic comparison, and is more balanced than any similar corpus available so far: a) it contains expressions by both Dutch and Korean actors as well as judgments by both Dutch and Korean listeners; b) the same elicitation technique and recording procedure was used for recordings of both languages; c) the same nonsense sentence, which was constructed to be permissible in both languages, was used for recordings of both languages; and d) the emotions present in the corpus are balanced in terms of valence, arousal, and dominance. The corpus contains a comparatively large number of emotions (eight) uttered by a large number of speakers (eight Dutch and eight Korean). The counterbalanced nature of the corpus will enable a stricter investigation of language-specific versus universal aspects of emotional expression than was possible so far. Furthermore, given the carefully controlled phonetic content of the expressions, it allows for analysis of the role of specific phonetic features in emotional expression in Dutch and Korean.",,http://www.lrec-conf.org/proceedings/lrec2010/pdf/511_Paper.pdf,,emotion,No,No
anthology+abstracts,Build {C}hinese Emotion Lexicons Using A Graph-based Algorithm and Multiple Resources,,2010,,,,,,,https://aclanthology.org/C10-1136,,emotion,No,No
anthology+abstracts,Are Emotions Enumerable or Decomposable? And its Implications for Emotion Processing,,2009,,,,,,,https://aclanthology.org/Y09-1011,,emotion,No,No
anthology+abstracts,A Cognitive-based Annotation System for Emotion Computing,,2009,,,,,,,https://aclanthology.org/W09-3001,,emotion,No,No
anthology+abstracts,The Modulation of Cooperation and Emotion in Dialogue: The {REC} Corpus,,2009,,,,,,,https://aclanthology.org/P09-3010,,emotion,No,No
anthology+abstracts,Word to Sentence Level Emotion Tagging for {B}engali Blogs,,2009,,,,,,,https://aclanthology.org/P09-2038,,emotion,No,No
anthology+abstracts,Using Emotion to Gain Rapport in a Spoken Dialog System,,2009,,,,,,,https://aclanthology.org/N09-3009,,emotion,No,No
anthology+abstracts,Construction of a Blog Emotion Corpus for {C}hinese Emotional Expression Analysis,,2009,,,,,,,https://aclanthology.org/D09-1150,,emotion,No,No
anthology+abstracts,Generating Story Reviews Using Phrases Expressing Emotion,,2008,,,,,,,https://aclanthology.org/Y08-1030,,emotion,No,No
anthology+abstracts,On the Role of the {NIMITEK} Corpus in Developing an Emotion Adaptive Spoken Dialogue System,,2008,,,,,"This paper reports on the creation of the multimodal NIMITEK corpus of affected behavior in human-machine interaction and its role in the development of the NIMITEK prototype system. The NIMITEK prototype system is a spoken dialogue system for supporting users while they solve problems in a graphics system. The central feature of the system is adaptive dialogue management. The system dynamically defines a dialogue strategy according to the current state of the interaction (including also the emotional state of the user). Particular emphasis is devoted to the level of naturalness of interaction. We discuss that a higher level of naturalness can be achieved by combining a habitable natural language interface and an appropriate dialogue strategy. The role of the NIMITEK multimodal corpus in achieving these requirements is twofold: (1) in developing the model of attentional state on the level of users commands that facilitates processing of flexibly formulated commands, and (2) in defining the dialogue strategy that takes the emotional state of the user into account. Finally, we sketch the implemented prototype system and describe the incorporated dialogue management module. Whereas the prototype system itself is task-specific, the described underlying concepts are intended to be task-independent.",,http://www.lrec-conf.org/proceedings/lrec2008/pdf/149_paper.pdf,,emotion,No,No
anthology+abstracts,Annotating Expressions of Opinion and Emotion in the {I}talian Content Annotation Bank,,2008,,,,,"In this paper we describe the result of manually annotating I-CAB, the Italian Content Annotation Bank, by expressions of private state (EPSs), i.e., expressions that denote the presence of opinions, emotions, and other cognitive states. The aim of this effort was the generation of a standard resource for supporting the development of opinion extraction algorithms for Italian, and of a benchmark for testing such algorithms. To this end we have employed a previously existing annotation language (here dubbed WWC, from the initials of its proponents). We here describe the results of this annotation effort, including the results of a thorough inter-annotator agreement test. We conclude by discussing how WWC can be adapted to the specificities of a Romance language such as Italian.",,http://www.lrec-conf.org/proceedings/lrec2008/pdf/566_paper.pdf,,emotion,No,Yes
anthology+abstracts,Automatic Emotional Degree Labeling for Speakers{'} Anger Utterance during Natural {J}apanese Dialog,,2008,,,,,"This paper describes a method of automatic emotional degree labeling for speakers anger utterances during natural Japanese dialog. First, we explain how to record anger utterance naturally appeared in natural Japanese dialog. Manual emotional degree labeling was conducted in advance to grade the utterances by a 6 Likert scale to obtain a correct anger degree. Then experiments of automatic anger degree estimation were conducted to label an anger degree with each utterance by its acoustic features. Also estimation experiments were conducted with speaker-dependent datasets to find out any influence of individual emotional expression on automatic emotional degree labeling. As a result, almost all the speakers models show higher adjusted R square so that those models are superior to the speaker-independent model in those estimation capabilities. However, a residual between automatic emotional degree and manual emotional degree (0.73) is equivalent to those of speakers models. There still has a possibility to label utterances with the speaker-independent model.",,http://www.lrec-conf.org/proceedings/lrec2008/pdf/575_paper.pdf,,emotion,No,No
anthology+abstracts,Linguistic Interpretation of Emotions for Affect {S}ensing from Text,,2008,,,,,,,https://aclanthology.org/I08-2128,,emotion,No,No
anthology+abstracts,Ranking Reader Emotions Using Pairwise Loss Minimization and Emotional Distribution Regression,,2008,,,,,,,https://aclanthology.org/D08-1015,,emotion,No,No
anthology+abstracts,Comparison of the Methods of Self-Organizing Maps and Multidimensional Scaling in Analysis of {E}stonian Emotion Concepts,,2007,,,,,,,https://aclanthology.org/W07-2417,,emotion,No,No
anthology+abstracts,Building Emotion Lexicon from Weblog Corpora,,2007,,,,,,,https://aclanthology.org/P07-2034,,emotion,No,No
anthology+abstracts,以部落格語料進行情緒趨勢分析 (Emotion Trend Analysis Using Blog Corpora) [In {C}hinese],,2007,,,,,,,https://aclanthology.org/O07-1015,,emotion,No,No
anthology+abstracts,A Model of Compliance and Emotion for Potentially Adversarial Dialogue Agents,,2007,,,,,,,https://aclanthology.org/2007.sigdial-1.6,,emotion,No,No
anthology+abstracts,Designing and Recording an Emotional Speech Database for Corpus Based Synthesis in {B}asque,,2006,,,,,"This paper describes an emotional speech database recorded for standard Basque. The database has been designed with the twofold purpose of being used for corpus based synthesis, and also of allowing the study of prosodic models for the emotions. The database is thus large, to get good corpus based synthesis quality and contains the same texts recorded in the six basic emotions plus the neutral style. The recordings were carried out by two professional dubbing actors, a man and a woman. The paper explains the whole creation process, beginning with the design stage, following with the corpus creation and the recording phases, and finishing with some learned lessons and hints.",,http://www.lrec-conf.org/proceedings/lrec2006/pdf/19_pdf.pdf,,emotion,No,No
anthology+abstracts,Real life emotions in {F}rench and {E}nglish {TV} video clips: an integrated annotation protocol combining continuous and discrete approaches,,2006,,,,,"A major barrier to the development of accurate and realistic models of human emotions is the absence of multi-cultural / multilingual databases of real-life behaviours and of a federative and reliable annotation protocol. QUB and LIMSI teams are working towards the definition of an integrated coding scheme combining their complementary approaches. This multilevel integrated scheme combines the dimensions that appear to be useful for the study of real-life emotions: verbal labels, abstract dimensions and contextual (appraisal based) annotations. This paper describes this integrated coding scheme, a protocol that was set-up for annotating French and English video clips of emotional interviews and the results (e.g. inter-coder agreement measures and subjective evaluation of the scheme).",,http://www.lrec-conf.org/proceedings/lrec2006/pdf/411_pdf.pdf,,emotion,No,No
anthology+abstracts,Manual Annotation and Automatic Image Processing of Multimodal Emotional Behaviours: Validating the Annotation of {TV} Interviews,,2006,,,,,"There has been a lot of psychological researches on emotion and nonverbal communication. Yet, these studies were based mostly on acted basic emotions. This paper explores how manual annotation and image processing can cooperate towards the representation of spontaneous emotional behaviour in low resolution videos from TV. We describe a corpus of TV interviews and the manual annotations that have been defined. We explain the image processing algorithms that have been designed for the automatic estimation of movement quantity. Finally, we explore how image processing can be used for the validation of manual annotations.",,http://www.lrec-conf.org/proceedings/lrec2006/pdf/465_pdf.pdf,,emotion,No,No
anthology+abstracts,Annotation of Emotions in Real-Life Video Interviews: Variability between Coders,,2006,,,,,"Research on emotional real-life data has to tackle the problem of their annotation. The annotation of emotional corpora raises the issue of how different coders perceive the same multimodal emotional behaviour. The long-term goal of this paper is to produce a guideline for the selection of annotators. The LIMSI team is working towards the definition of a coding scheme integrating emotion, context and multimodal annotations. We present the current defined coding scheme for emotion annotation, and the use of soft vectors for representing a mixture of emotions. This paper describes a perceptive test of emotion annotations and the results obtained with 40 different coders on a subset of complex real-life emotional segments selected from the EmoTV Corpus collected at LIMSI. The results of this first study validate previous annotations of emotion mixtures and highlight the difference of annotation between male and female coders.",,http://www.lrec-conf.org/proceedings/lrec2006/pdf/531_pdf.pdf,,emotion,Yes,No
anthology+abstracts,Annotating Emotions in Meetings,,2006,,,,,"We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.",,http://www.lrec-conf.org/proceedings/lrec2006/pdf/626_pdf.pdf,,emotion,No,No
anthology+abstracts,基于現代漢語語法信息詞典的詞語情感評價研究 (Research on Lexical Emotional Evaluation Based on the Grammatical Knowledge-Base of Contemporary {C}hinese) [In {C}hinese],,2005,,,,,,,https://aclanthology.org/O05-5014,,emotion,No,No
anthology+abstracts,Annotating Student Emotional States in Spoken Tutoring Dialogues,,2004,,,,,,,https://aclanthology.org/W04-2326,,emotion,No,No
anthology+abstracts,應用機率式句法結構與隱含式語意索引於情緒語音合成之單元選取 (Unit Selection for Corpus-Based Emotional Speech Synthesis Using {PCFG} and {LSI}) [In {C}hinese],,2004,,,,,,,https://aclanthology.org/O04-1033,,emotion,No,No
anthology+abstracts,The Construction and Testing of a {M}andarin Emotional Speech Database,,2004,,,,,,,https://aclanthology.org/O04-1037,,emotion,No,No
anthology+abstracts,Reusing Language Resources for Speech Applications involving Emotion,,2004,,,,,,,http://www.lrec-conf.org/proceedings/lrec2004/pdf/27.pdf,,emotion,No,No
anthology+abstracts,Designing and Recording an Audiovisual Database of Emotional Speech in {B}asque,,2004,,,,,,,http://www.lrec-conf.org/proceedings/lrec2004/pdf/28.pdf,,emotion,No,No
anthology+abstracts,"Corpus Design, Recording and Phonetic Analysis of {G}reek Emotional Database",,2004,,,,,,,http://www.lrec-conf.org/proceedings/lrec2004/pdf/41.pdf,,emotion,No,No
anthology+abstracts,Annotating emotion in dialogue,,2003,,,,,,,https://aclanthology.org/W03-2127,,emotion,No,No
anthology+abstracts,Towards Emotional Variation in Speech-Based Natural Language Processing,,2002,,,,,,,https://aclanthology.org/W02-2108,,emotion,No,No
anthology+abstracts,Interface Databases: Design and Collection of a Multilingual Emotional Speech Database,,2002,,,,,,,http://www.lrec-conf.org/proceedings/lrec2002/pdf/174.pdf,,emotion,No,No
anthology+abstracts,Objective analysis of emotional speech for {E}nglish and {S}lovenian Interface emotional speech databases,,2002,,,,,,,http://www.lrec-conf.org/proceedings/lrec2002/pdf/175.pdf,,emotion,No,No
anthology+abstracts,Alternation Across Semantic Fields: A Study on {M}andarin Verbs of Emotion,,2000,,,,,,,https://aclanthology.org/O00-2004,,emotion,No,No
anthology+abstracts,Alternation Across Semantic Fields : A Study of {M}andarin Verbs of Emotion,,1999,,,,http://hdl.handle.net/2065/12136,,,https://aclanthology.org/Y99-1005,,emotion,No,No
anthology+abstracts,The Role of Metaphors in Descriptions of Emotions,,1987,,,,,,,https://aclanthology.org/T87-1037,,emotion,No,No
web-of-science,ROSbag-based Multimodal Affective Dataset for Emotional and Cognitive States,"Jo, W; Kannan, SS; Cha, GE; Lee, A; Min, BC; IEEE",2020,,,,10.1109/smc42975.2020.9283320,"This paper introduces a new ROSbag-based multimodal affective dataset for emotional and cognitive states generated using the Robot Operating System (ROS). We utilized images and sounds from the International Affective Pictures System (IAPS) and the International Affective Digitized Sounds (IADS) to stimulate targeted emotions (happiness, sadness, anger, fear, surprise, disgust, and neutral), and a dual N-back game to stimulate different levels of cognitive workload. 30 human subjects participated in the user study; their physiological data were collected using the latest commercial wearable sensors, behavioral data were collected using hardware devices such as cameras, and subjective assessments were carried out through questionnaires. All data were stored in single ROSbag files rather than in conventional Comma-Separated Values (CSV) files. This not only ensures synchronization of signals and videos in a data set, but also allows researchers to easily analyze and verify their algorithms by connecting directly to this dataset through ROS. The generated affective dataset consists of 1,602 ROSbag files, and the size of the dataset is about 787GB. The dataset is made publicly available. We expect that our dataset can be a great resource for many researchers in the fields of affective computing, Human-Computer Interaction (HCI), and Human-Robot Interaction (HRI).",,,,emotion,No,No
web-of-science,"A review of multimodal emotion recognition from datasets, preprocessing, and fusion methods","Pan, B; Hirota, K; Jia, ZY; Dai, YP",2023,,561,,10.1016/j.neucom.2023.126866,"Affective computing is one of the most important research fields in modern human-computer interaction (HCI). The goal of affective computing is to study and develop the theories, methods, and systems that can recognize, explain, process, and simulate human emotions. As a branch of affective computing, emotion recognition aims to enlighten the machine/computer automatically analyzing human emotions, which has received increasing attention from researchers in various fields. Human beings generally observe and understand the emotional states of one person by integrating the perceived information from his/her facial expressions, voice tone, speech content, behavior, or physiological features. To imitate the emotion observation manner of humans, researchers have been devoted to constructing multimodal emotion recognition models by fusing information from two or more modalities. In this paper, we provide a comprehensive review of multimodal emotion recognition from the perspectives of multimodal datasets, data preprocessing, unimodal feature extraction, and multimodal information fusion methods in recent decades. Furthermore, challenges and future research directions of the topic are specified and discussed. The main motivations of this review are to conclude the recent emergence of abundant works on multimodal emotion recognition and to provide potential guidance to researchers in the related field for understanding the pipeline and mainstream approaches to multimodal emotion recognition.",Emotion recognition; Multimodal information fusion; Classifier; Feature learning; AUDIO; SENTIMENT ANALYSIS; FACIAL EXPRESSION RECOGNITION; CLASSIFICATION; EVOLUTIONARY COMPUTATION; FEATURE-SELECTION; FEATURES; NEURAL-NETWORK; PHYSIOLOGICAL SIGNALS; RANDOM FOREST,,,emotion,No,No
web-of-science,EmoReact: A Multimodal Approach and Dataset for Recognizing Emotional Responses in Children,"Nojavanasghari, B; Baltrusaitis, T; Hughes, CE; Morency, LP",2016,,,,10.1145/2993148.2993168,"Automatic emotion recognition plays a central role in the technologies underlying social robots, affect-sensitive human computer interaction design and affect-aware tutors. Although there has been a considerable amount of research on automatic emotion recognition in adults, emotion recognition in children has been understudied. This problem is more challenging as children tend to fidget and move around more than adults, leading to more self-occlusions and non-frontal head poses. Also, the lack of publicly available datasets for children with annotated emotion labels leads most researchers to focus on adults. In this paper, we introduce a newly collected multimodal emotion dataset of children between the ages of four and fourteen years old. The dataset contains 1102 audio-visual clips annotated for 17 different emotional states: six basic emotions, neutral, valence and nine complex emotions including curiosity, uncertainty and frustration. Our experiments compare unimodal and multimodal emotion recognition baseline models to enable future research on this topic. Finally, we present a detailed analysis of the most indicative behavioral cues for emotion recognition in children.",Emotion Recognition; Audio-Visual Sensing; Facial Analysis; Nonverbal Behavior Analysis,,,emotion,Yes,No
web-of-science,"EngageDat-vL: A Multimodal Engagement Dataset Comprising of Emotional, Cognitive, and Behavioral Cues in Virtual Learning Environment","Akre, S; Palandurkar, N; Iyengar, A; Chayande, G; Kumar, P",2023,,14301,,10.1007/978-3-031-45170-6_28,"To assess student engagement in an e-learning environment, this work proposes a novel learning analytics dataset collection that combines emotional, cognitive, and behavioral data. The dataset includes facial expressions recorded by a webcam during e-learning sessions and log data such as play, pause, seek, course views, course material views, lecture views, quiz responses, etc. The procedure for gathering the data and the difficulties encountered are covered in the study. Combining emotional, cognitive, and behavioral cues for engagement detection in e-learning settings is made possible by this dataset, which represents a significant advancement in the field of learning analytics.",Facial emotion recognition; Engagement detection; Learning analytics,,,emotion,No,No
web-of-science,"What multimodal components, tools, dataset and focus of emotion are used in the current research of multimodal emotion: a systematic literature review","Rahmalina, R; Gunawan, W",2024,,10,,10.1080/23311886.2024.2376309,"Emotional engagement is essential in human communication, and the meaning of emotions often entails multimodal relationships. Besides language, multimodality and emotions are semiotic systems that have increasingly attracted the attention of researchers, especially concerning their contribution to the meaning-making process in communication. However, research trends that address how emotions and multimodal components (hereafter multimodal emotions) complement each other in the meaning-making process have not been extensively researched. Hence, this research aims to identify research trends on multimodality and emotions studied from 2018 to 2022. The method used in this research was systematic literature review (SLR) to review, identify, evaluate and interpret all existing research on this topic. Data were obtained from IEEE Xplore, Science Direct and Emerald journals which were then sorted based on the PRISMA method. This analysis acquired comprehensive information on the multimodal components of emotions, data acquisition technologies, datasets and emotional focus and trends. This SLR demonstrated that the research of emotions involving multimodality was conducted with a focus on audio-visual mode with the use of machines namely EEG and LSTM sourced from the IEMOCAP dataset and focused on positive and negative emotions, with anger being the largest focus. Moreover, the findings also showed the interconnectedness of each multimodal component of emotions. Based on these findings, this study suggests that multimodal emotion research should focus on identifying and investigating the meanings generated from emotions to produce good communication. Lack of subject privacy and unmanageable bias in research are other things to consider for future research directions. Communication is a meaning-making endeavor comprised language and various semiotic systems, including multimodality and emotion. No research has yet identified how emotion and multimodal components complement each other. This study aims to identify research trends on multimodality and emotion from journal articles published in 2018-2022 to investigate multimodal components and other data sets used to explore the meaning-making process and to explore how emotions are comprehensively accessed. Using the PRISMA method, the data was analyzed to obtain comprehensive information regarding the multimodal components of emotions, data acquisition technologies, datasets and emotional focus with research trends. This SLR showed that emotion research involving multimodality was conducted with a focus and dominance on audio-visual mode with the use of machines namely EEG and LSTM sourced from the IEMOCAP dataset type. In addition, the dominant emotion focus in this study was positive and negative emotions, with anger being the largest focus.","Multimodality; emotions; RECOGNITION; EXPRESSIONS; FUSION; BIG DATA; DEPRESSION; Guangchao Charles Feng, School of Communication, Hong Kong Baptist University, Hong Kong; Interpersonal Communication; multimodal emotions; Semiotics; Sociolinguistics; systematic literature review; TWITTER",,,emotion,No,No
web-of-science,How emotional cues affect the financing performance in rewarded crowdfunding? - an insight into multimodal data analysis,"Chen, J; Du, MM; Yang, X",2024,,,,10.1007/s10660-024-09841-6,"An increasing number of rewarded crowdfunding platforms recommend that fundraisers post multimodal data to improve data diversity and attract investors' attention. Fundraisers' speech contains both textual modality (text) and acoustic modality (voice) data. This study aims to explore how linguistic style of the speech text and acoustic features of the speech voice influence the crowdfunding campaign performance from the perspective of emotional contagion. An econometric model to investigate the effects of emotional cues from fundraisers' speech on rewarded crowdfunding performance is constructed, and an empirical analysis with 21,996 projects data in Kickstarter is conducted. The findings demonstrate that both acoustic emotional cues (voice pitch, intensity and speech rate) and textual emotional cues (intimate and perceptual language) have significant effect on the financing performance. Among them, voice pitch, speech rate, intimate language and perceptual language have significant positive effect on the financing performance, while voice intensity is negatively related to financing performance. The number of comments positively moderates the relationship between emotional cues in speech and project financing performance. Research also finds that the acoustic and textual emotional features in speech have different effects on the success rate of project financing under different types of projects. This investigation provides both theoretical implications for the literature of crowdfunding and practical implications for fundraisers.",Acoustic feature; EXPRESSION; SPEECH; BACKERS; CAMPAIGN; CONTAGION; Emotional contagion; Funding outcomes; LANGUAGE USE; Linguistic style; PITCH; Rewarded crowdfunding; SUCCESS; VOCAL AFFECT; WORDS,,,emotion,No,No
web-of-science,Design and Efficacy of a Data Lake Architecture for Multimodal Emotion Feature Extraction in Social Media,"Fan, YY; Mi, XF",2024,,2024,,10.1049/2024/6819714,"In the rapidly evolving landscape of social media, the demand for precise sentiment analysis (SA) on multimodal data has become increasingly pivotal. This paper introduces a sophisticated data lake architecture tailored for efficient multimodal emotion feature extraction, addressing the challenges posed by diverse data types. The proposed framework encompasses a robust storage solution and an innovative SA model, multilevel spatial attention fusion (MLSAF), adept at handling text and visual data concurrently. The data lake architecture comprises five layers, facilitating real-time and offline data collection, storage, processing, standardized interface services, and data mining analysis. The MLSAF model, integrated into the data lake architecture, utilizes a novel approach to SA. It employs a text-guided spatial attention mechanism, fusing textual and visual features to discern subtle emotional interplays. The model's end-to-end learning approach and attention modules contribute to its efficacy in capturing nuanced sentiment expressions. Empirical evaluations on established multimodal sentiment datasets, MVSA-Single and MVSA-Multi, validate the proposed methodology's effectiveness. Comparative analyses with state-of-the-art models showcase the superior performance of our approach, with an accuracy improvement of 6% on MVSA-Single and 1.6% on MVSA-Multi. This research significantly contributes to optimizing SA in social media data by offering a versatile and potent framework for data management and analysis. The integration of MLSAF with a scalable data lake architecture presents a strategic innovation poised to navigate the evolving complexities of social media data analytics.",SENTIMENT ANALYSIS,,,emotion,No,No
web-of-science,Model for Determining the Psycho-Emotional State of a Person Based on Multimodal Data Analysis,"Shakhovska, N; Zherebetskyi, O; Lupenko, S",2024,,14,,10.3390/app14051920,"The paper aims to develop an information system for human emotion recognition in streaming data obtained from a PC or smartphone camera, using different methods of modality merging (image, sound and text). The objects of research are the facial expressions, the emotional color of the tone of a conversation and the text transmitted by a person. The paper proposes different neural network structures for emotion recognition based on unimodal flows and models for the margin of the multimodal data. The analysis determined that the best classification accuracy is obtained for systems with data fusion after processing each channel separately and obtaining individual characteristics. The final analysis of the model based on data from a camera and microphone or recording or broadcast of the screen, which were received in the ""live"" mode, gave a clear understanding that the quality of the obtained results is highly dependent on the quality of the data preparation and labeling. This is directly related to the fact that the data on which the neural network is trained is highly qualified. The neural network with combined data on the penultimate layer allows a psycho-emotional state recognition accuracy of 0.90 to be obtained. The spatial distribution of emotion analysis was also analyzed for each data modality. The model with late fusion of multimodal data demonstrated the best recognition accuracy.",multimodal data; convolution neural network; late fusion; multi-modal emotion recognition; emotional state; SECURITY,,,emotion,Yes,Yes
web-of-science,Leveraging Tripartite Tier Convolutional Neural Network for Human Emotion Recognition: A Multimodal Data Approach,"Dharmichand, S; Perumal, S",2023,,40,,10.18280/ts.400619,"In the recent past, significant strides have been made in the field of deep learning and data fusion, enabling computers to comprehend, identify, and analyse human emotions with remarkable precision. However, reliance on external biological features for emotion recognition can be misleading, as individuals may consciously or unconsciously mask their true emotions. Consequently, an objective and reliable approach is sought, one that draws on physiological markers for emotion recognition. This paper introduces a novel model, the Tripartite Tier Convolutional Neural Network (TTCNN), specifically designed to leverage deep learning methods for the extraction and classification of significant features in multimodal emotion recognition. Amongst various physiological features, this study prioritizes eye movement and Electroencephalogram (EEG) data due to their robust potential to reflect emotional states. The multimodal data-based feature extraction facilitated by the TTCNN model yields a comprehensive set of features, enhancing the effectiveness of emotion classification into categories such as disgust, fear, sadness, happiness, and neutrality. This innovative cognitive approach has been evaluated using two established datasets, SEED and DEAP. The performance of the TTCNN model demonstrates its efficacy, achieving an impressive 95.84% classification accuracy on the SEED dataset and 87.01% on the DEAP dataset. These results significantly outperform existing state-of-the-art methods, underscoring the TTCNN model's potential as a robust tool for human emotion recognition. This research contributes to the advancement of computer-aided emotion analysis, presenting a significant step forward in the field and opening up potential applications in diverse areas such as psychology, healthcare, and human-computer interaction.",emotion recognition; multimodal data; electroencephalogram; classification; FACE; cognitive approach; tripartite tier convolutional neural network,,,emotion,No,Yes
web-of-science,A Negative Emotion Recognition System with Internet of Things-Based Multimodal Biosignal Data,"Ham, SM; Lee, HM; Lim, JH; Seo, J",2023,,12,,10.3390/electronics12204321,"Previous studies to recognize negative emotions for mental healthcare have used heavy equipment directly attaching electroencephalogram (EEG) electrodes to the head, and they have proposed binary classification methods to identify negative emotions. To tackle this problem, we propose a negative emotion recognition system to collect multimodal biosignal data such as five EEG signals from an EEG headset and heart rate, galvanic skin response, and skin temperature from a smart band for classifying multiple negative emotions. This consists of an Android Internet of Things (IoT) application, a oneM2M-compliant IoT server, and a machine learning server. The Android IoT application uploads the biosignal data to the IoT server. By using the biosignal data stored in the IoT server, the machine learning server recognizes the negative emotions of disgust, fear, and sadness using a multiclass support vector machine (SVM) model with a radial basis function kernel. The experimental results demonstrate that the multimodal biosignal data approach achieves 93% accuracy. Moreover, when considering only data from the smart band, the system achieved 98% accuracy by optimizing the hyperparameters of the multiclass SVM model. Based on these results, we plan to develop a metaverse system that detects and expresses negative emotions in real time.",emotion recognition; multimodal; support vector machine; Internet of Things; wearable device; biosignal,,,emotion,No,Yes
web-of-science,Exploring multimodal data analysis for emotion recognition in teachers' teaching behavior based on LSTM and MSCNN,"Lu, YY; Chen, ZZ; Zheng, QY; Zhu, YH; Wang, MK",2023,,,,10.1007/s00500-023-08760-2,"The diverse range of emotions exhibited in instructional behavior exerts a profound influence on the effectiveness of teaching and the cognitive state of learners. By leveraging an emotion recognition model, we can analyze the invaluable feedback information derived from teaching behavior data, thereby facilitating the enhancement of pedagogical effectiveness. However, conventional emotion recognition models fall short in capturing the intricate emotional features and subtle nuances inherent in teaching behavior, thereby hindering the accuracy of emotion classification. In light of this, we propose a groundbreaking multimodal emotion recognition model for teaching behavior, founded upon the Long Short-Term Memory (LSTM) and Multi-Scale Convolutional Neural Network (MSCNN). Our approach involves extracting both low-level and high-level local features from text, audio, and image modalities, utilizing LSTM and MSCNN, respectively. Subsequently, a transformer encoder is employed to fuse the extracted features, which are then fed into a fully connected layer for emotion recognition. Experimental results affirm that our proposed model attains an accuracy rate of 84.5% and an F1 score of 84.1% on a self-curated dataset, surpassing other comparative models. These outcomes unequivocally establish the efficacy and superiority of our emotion recognition model.",Transformer; Multimodal emotion recognition; LSTM; MSCNN; Teaching behavior; FACIAL EXPRESSION,,,emotion,No,Yes
web-of-science,Task-specific speech enhancement and data augmentation for improved multimodal emotion recognition under noisy conditions,"Kshirsagar, S; Pendyala, A; Falk, TH",2023,,5,,10.3389/fcomp.2023.1039261,"Automatic emotion recognition (AER) systems are burgeoning and systems based on either audio, video, text, or physiological signals have emerged. Multimodal systems, in turn, have shown to improve overall AER accuracy and to also provide some robustness against artifacts and missing data. Collecting multiple signal modalities, however, can be very intrusive, time consuming, and expensive. Recent advances in deep learning based speech-to-text and natural language processing systems, however, have enabled the development of reliable multimodal systems based on speech and text while only requiring the collection of audio data. Audio data, however, is extremely sensitive to environmental disturbances, such as additive noise, thus faces some challenges when deployed ""in the wild."" To overcome this issue, speech enhancement algorithms have been deployed at the input signal level to improve testing accuracy in noisy conditions. Speech enhancement algorithms can come in different flavors and can be optimized for different tasks (e.g., for human perception vs. machine performance). Data augmentation, in turn, has also been deployed at the model level during training time to improve accuracy in noisy testing conditions. In this paper, we explore the combination of task-specific speech enhancement and data augmentation as a strategy to improve overall multimodal emotion recognition in noisy conditions. We show that AER accuracy under noisy conditions can be improved to levels close to those seen in clean conditions. When compared against a system without speech enhancement or data augmentation, an increase in AER accuracy of 40% was seen in a cross-corpus test, thus showing promising results for ""in the wild"" AER.",multimodal emotion recognition; data augmentation; CLASSIFICATION; BERT based text features; context-awareness; modulation spectrum features; speech enhancement,,,emotion,No,Yes
web-of-science,A pilot case study for developing a software for human emotion recognition using multimodal data,"Akiyama, T; Matsumoto, K; Osaka, K; Tanioka, R; Yasuhara, Y; Ito, H; Soriano, G; Blaquera, AP; Kai, Y; Tanioka, T; IEEE",2023,,,,10.1109/SII55687.2023.10039248,"In developing software to analyze the emotions of patients with schizophrenia using multimodal data, a pilot case study was conducted in order to examine its accuracy in a healthy subject. This study shows a low agreement and reliability between the MTCNN and the subjective evaluation of the three examiners based on the result of the ICC and Cronbach's alpha coefficient. However, it can be revealed that the Multi-Task Cascaded Convolutional Networks (MTCNN) facial expression recognition and the Heart Rate Variability (HRV) analysis showed consistent results. Subject who experienced anticipated feelings of happiness when the conversation was focused in the subject's favorite food, which showed an increased in HFnu, indicating increased parasympathetic activity. It was considered that the subject felt that the conversation with the robot was lively and empathetic. Findings suggest that MTCNN can be used in combination with HRV analysis in determining the facial expression of an individual. However, further research should be done involving additional subjects in order to ascertain the validity and reliability of the MTCNN.",,,,emotion,No,Yes
web-of-science,The 5thWorkshop on Modeling Socio-Emotional and Cognitive Processes from Multimodal Data in the Wild (MSECP-Wild),"Dudzik, B; Hrkalovic, TM; Küster, D; St-Onge, D; Putze, F; Devillers, L; ACM",2023,,,,10.1145/3577190.3616883,"The ability to automatically infer relevant aspects of human users' thoughts and feelings is crucial for technologies to intelligently adapt their behaviors in complex interactions. Research on multimodal analysis has demonstrated the potential of technology to provide such estimates for a broad range of internal states and processes. However, constructing robust approaches for deployment in real-world applications remains an open problem. The MSECP-Wild workshop series is a multidisciplinary forum to present and discuss research addressing this challenge. Submissions to this 5(th) iteration span eforts relevant to multimodal data collection, modeling, and applications. In addition, our workshop program builds on discussions emerging in previous iterations, highlighting ethical considerations when building and deploying technology modeling internal states in the wild. For this purpose, we host a range of relevant keynote speakers and interactive activities.",Afective Computing; Human-centered AI; Multimodal Modeling,,,emotion,No,No
web-of-science,Multimodal music emotion recognition method based on multi data fusion,"Zeng, FG",2023,,14,,10.1504/IJART.2023.133662,"In order to overcome the problems of low recognition accuracy and long recognition time in traditional multimodal music emotion recognition methods, a multimodal music emotion recognition method based on multiple data fusion is proposed. The multi-modal music emotion is decomposed by the non-negative matrix decomposition method to obtain the multi-modal data of audio and lyrics, and extract the audio modal emotional features and text modal emotional features respectively. After the multi-modal data of the two modal emotional features are weighted and fused through the linear prediction residual, the normalised multi-modal data is used as the training sample and input into the classification model based on support vector machine, so as to identify multimodal music emotion. The experimental results show that the proposed method takes the shortest time for multimodal music emotion recognition and improves the recognition accuracy.",emotional recognition; multi data fusion; multimodal music; non-negative matrix decomposition method; support vector machines; SVMs,,,emotion,No,Yes
web-of-science,Consumer shopping emotion and interest database: a unique database with a multimodal emotion recognition method for retail service robots to infer consumer shopping intentions better than humans,"Chen, XY; Sun, SQ; Zhang, ZQ; Ma, ZR; Wu, XD; Li, HB; Chen, TR; Zhang, KJ",2022,,31,,10.1117/1.JEI.31.6.061807,"Empowering retail service robots with empathy is one of the current research hotspots in the field of artificial intelligence. Identifying consumer emotions, understanding the changes in shopping interests, and developing appropriate sales strategies is a challenging task for retail service robots. We investigate the feasibility of using computer vision methods for empowering robots with empathy by examining the correlation between consumer emotion and levels of shopping interest. To this end, we construct the first video database of consumer sentiment changes in a business context and propose a deep learning method that uses multimodal information to infer consumers' shopping intentions, and conduct preliminary experimental validation on this database. The experimental results show that the proposed method is 7% and 10% more accurate than manual assessment (n = 40) in identifying consumer emotions and predicting consumer shopping interest levels, respectively. Thus, the proposed method is valid and effective. We anticipate that the results of this study will have considerable implications for human-computer interaction research in service robots. (c) 2022 SPIE and IS&T",deep learning; emotion recognition; image processing; affective computing; computer vision; database; EXPRESSIONS,,,emotion,No,No
web-of-science,Multimodal information fusion method in emotion recognition in the background of artificial intelligence,"Dai, Z; Fei, HX; Lian, CY",2024,,7,,10.1002/itl2.520,"Recent advances in Semantic IoT data integration have highlighted the importance of multimodal fusion in emotion recognition systems. Human emotions, formed through innate learning and communication, are often revealed through speech and facial expressions. In response, this study proposes a hidden Markov model-based multimodal fusion emotion detection system, combining speech recognition with facial expressions to enhance emotion recognition rates. The integration of such emotion recognition systems with Semantic IoT data can offer unprecedented insights into human behavior and sentiment analysis, contributing to the advancement of data integration techniques in the context of the Internet of Things. Experimental findings indicate that in single-modal emotion detection, speech recognition achieves a 76% accuracy rate, while facial expression recognition achieves 78%. However, when state information fusion is applied, the recognition rate increases to 95%, surpassing the national average by 19% and 17% for speech and facial expressions, respectively. This demonstrates the effectiveness of multimodal fusion in emotion recognition, leading to higher recognition rates and reduced workload compared to single-modal approaches.",emotion recognition; multimodal information fusion; internet of things; unimodal information fusion,,,emotion,No,Yes
web-of-science,Construction and Application of English-Chinese Multimodal Emotional Corpus Based on Artificial Intelligence,"Zhou, W; Gao, B",2023,,,,10.1080/10447318.2023.2169526,"Multimodal corpus is a new type of multimedia teaching aid tool that was born and widely used in the current social development and educational reform process. It is mainly based on computer technology and network technology, and uses various multimedia materials as corpus to establish a more comprehensive English database. Corpus is increasingly used in English-Chinese multimodal teaching. Combining the content of the corpus with modern information technology will help to better understand the meaning, usage and collocation of English and Chinese multimodality, and then improve the initiative of autonomous learning. Furthermore, English-Chinese multimodality is a fixed or semi-fixed programmed language, which can be directly extracted from memory according to the context during use, which can effectively improve learners' writing efficiency and overcome the negative impact of native language transfer. Facing the task of sentiment analysis, this article has taken the text semantic representation technology based on representation learning as the main method, and carries out a series of researches from the definition of implicit emotion, the characteristics of language expression and the recognition method. Finally, in the language system of modern Chinese, it is found that there are many sentence patterns that are very similar to the middle verb structure in language expression, but the efficiency is still improved by 3.6%. Compared with traditional methods, the English-Chinese multimodal sentiment corpus method based on artificial intelligence can better reflect the application value of language, which also helps to feel the real context, change the way of thinking, and make the way of thinking more suitable for the language awareness of native speakers, thereby improving the vividness of language expression.",,,,emotion,No,No
web-of-science,Annotation Efficiency in Multimodal Emotion Recognition with Deep Learning,"Zhu, LL; Spachos, P; IEEE",2022,,,,10.1109/GLOBECOM48099.2022.10000909,"In the fast pace of life, emotion recognition systems are essential to help monitor mental health and well-being. The continuous development of the Internet of Things (IoT) and Human-Computer Interaction (HCI) improve the availability and accessibility to devices that can capture the facial expressions of a user, while wearable devices can also capture physiological signals and use them for emotion recognition. Meanwhile, machine learning and deep learning methods can provide emotion prediction models. However, the training of the models relies heavily on massive amounts of labeled data. The accuracy of data labels affects the success of the overall system. Research targeting emotion recognition uses the participants' self-reports as labels. However, participants often fail to give accurate self-reports, thus affecting the accuracy of the analysis. In this study, we examine the performance of the self-reports and external annotations for emotion recognition based on visual and physiological signals. Specifically, we use video data, as well as the Electrodermal Activity (EDA), Electroencephalogram (EEG), and Electrocardiogram (ECG) signals collected from wearable devices. We use two machine learning and three deep learning methods to process the signals and train the classifiers. The results show that the classifiers trained with external annotations offer better emotion recognition accuracy than self-reports. Also, the classifiers trained on facial expression offer better emotion prediction accuracy than the physiological signals, and the Deep Convolutional Network model shows the best results.",deep learning; emotion recognition; Affective computing; electroencephalogram (EEG); physiological signals; FACIAL EXPRESSION; electrocardiogram (ECG); electrodermal activity (EDA),,,emotion,Yes,Yes
web-of-science,Multimodal Architecture for Emotion in Robots using Deep Learning,"Ghayoumi, M; Bansal, AK; IEEE",2016,,,,,"These days, some robots have emotional state (expression and recognition) to make Human-Robot Interaction (HRI) and Robot-Robot Interaction (RRI) better. In this article we analyze what it means for a robot to have emotion and distinguishing emotional state for communication from an emotional state as a mechanism for the organization of its behavior with humans and robots by convolutional neural network (CNN). We discuss these relations and explain why it can be more effective by CNN for having better emotion in the robots. Here, we present a multimodal system for Emotions in Robots by CNN.",convolutional neural network; deep learning; emotion recognition; multimodal systems; emotion expression; robot,,,emotion,No,No
web-of-science,Deep-Learning-Based Multimodal Emotion Classification for Music Videos,"Pandeya, YR; Bhattarai, B; Lee, J",2021,,21,,10.3390/s21144927,"Music videos contain a great deal of visual and acoustic information. Each information source within a music video influences the emotions conveyed through the audio and video, suggesting that only a multimodal approach is capable of achieving efficient affective computing. This paper presents an affective computing system that relies on music, video, and facial expression cues, making it useful for emotional analysis. We applied the audio-video information exchange and boosting methods to regularize the training process and reduced the computational costs by using a separable convolution strategy. In sum, our empirical findings are as follows: (1) Multimodal representations efficiently capture all acoustic and visual emotional clues included in each music video, (2) the computational cost of each neural network is significantly reduced by factorizing the standard 2D/3D convolution into separate channels and spatiotemporal interactions, and (3) information-sharing methods incorporated into multimodal representations are helpful in guiding individual information flow and boosting overall performance. We tested our findings across several unimodal and multimodal networks against various evaluation metrics and visual analyzers. Our best classifier attained 74% accuracy, an f1-score of 0.73, and an area under the curve score of 0.926.",EXPRESSION; RECOGNITION; MODEL; channel and filter separable convolution; end-to-end emotion classification; NETWORK; unimodal and multimodal,,,emotion,No,Yes
web-of-science,Multimodal machine learning approach for emotion recognition using physiological signals,"Ramadan, MA; Salem, NM; Mahmoud, LN; Sadek, I",2024,,96,,10.1016/j.bspc.2024.106553,"This study explores a novel approach to emotion recognition through machine learning, addressing the limitations of previous methods. While deep learning has shown promise in this field, it often requires significant computational resources and time. In response, we propose a multimodal approach utilizing Feature-level Fusion (FLF) and Decision-level Fusion (DLF) to enhance performance while reducing complexity. The study focuses on integrating electroencephalogram (EEG), electromyography (EMG), and electrooculogram (EOG) signals. Signal preprocessing involves extracting statistical features, power spectral density (PSD), and incremental entropy analysis. Recursive Feature Elimination (RFE) is employed as a feature selector, facilitating the fusion of different signal features. Three fusion strategies are explored: EEG with EOG, EEG with EMG, and a combination of EEG with EOG and EMG. For classification, the Bagging Classifier and K-Nearest Neighbors Algorithm are chosen. Results demonstrate promising accuracy rates, with 95.7% for arousal, 96.41% for valence in subject-dependent classification, 93.68% for arousal, and 93.23% for valence in subject-independent classification. This approach presents a viable alternative to deep learning methods, offering improved performance with reduced computational burden.",Machine learning; Emotion recognition; EEG; Multimodality; EMG; EOG,,,emotion,No,Yes
web-of-science,Chinese Multimodal Emotion Recognition in Deep and Traditional Machine Learning Approaches,"Miao, HT; Zhang, YF; Li, WP; Zhang, HR; Wang, DL; Feng, S; IEEE",2018,,,,,"in this paper, we propose our system in the MEC 2017 Chinese multimodal emotion recognition challenge which is based on the Chinese Natural Audio-Visual Emotion Database (CHEAVD). We extract various features in different modalities such as acoustic signals and facial expressions, and employ traditional machine learning and deep learning ways to recognize human emotions in the videos. For audio, we use two acoustic feature sets, IS09 and large emotion feature set, combined with traditional classifiers and DBN, respectively. For video, we take LBP-TOP feature as the input of SVM and also utilize CNN for extracting face expression features to train RNN to learn the temporal information. A decision-level average fusion method is applied to make prediction for both unimodal and multimodal emotion recognition. Experiment results on the challenge database show our effectiveness.",CNN; multimodal emotion recognition; SVM; RNN; random forest; DBN; decision-level fusion; REPTree,,,emotion,No,No
web-of-science,Multimodal Information-Based Broad and Deep Learning Model for Emotion Understanding,"Li, M; Chen, LEF; Wu, M; Pedrycz, W; Hirota, K",2021,,,,,"Multimodal information-based broad and deep learning model (MIBDL) for emotion understanding is proposed, in which facial expression and body gesture are used to achieve emotional states recognition for emotion understanding. It aims to understand coexistence multimodal information in human-robot interaction by using different processing methods of deep network and broad network, which obtains the features of depth and width dimensions. Moreover, random mapping in the initial broad learning network could cause information loss and its shallow layer network is difficult to cope with complex tasks. To address this problem, we use principal component analysis to generate the nodes of the broad learning, and the stacked broad learning network is adapted to make it easier for the existing broad learning networks to cope with complex tasks by creating deep variations of the existing network. To verify the effectiveness of the proposal, experiments completed on benchmark database of spontaneous emotion expressions are developed, and experimental results show that the proposal outperforms the state-of-the-art methods. According to the simulation experiments on the FABO database, by using the proposed method, the multimodal recognition rate is 17,54%, 1.24%, and 0.23% higher than those of the temporal normalized motion and appearance features(TN), the multi-channel CNN (MCCNN), and the hierarchical classification fusion strategy (HCFS), respectively.",facial expression; Convolution neural network; RECOGNITION; SYSTEM; body gesture; broad learning; emotion understanding,,,emotion,No,No
web-of-science,A Cross-Culture Study on Multimodal Emotion Recognition Using Deep Learning,"Gan, L; Liu, W; Luo, Y; Wu, X; Lu, BL",2019,,1142,,10.1007/978-3-030-36808-1_73,"In this paper, we aim to investigate the similarities and differences of multimodal signals between Chinese and French on three emotions recognition task using deep learning. We use videos including positive, neutral and negative emotions as stimuli material. Both Chinese and French subjects wear electrode caps and eye tracking glass while doing experiments to collect electroencephalography (EEG) and eye movement data. To deal with the problem of lacking data for training deep neural networks, conditional Wasserstein generative adversarial network is adopted to generate EEG and eye movement data. The EEG and eye movement features are fused by using Deep Canonical Correlation Analysis to analyze the relationship between EEG and eye movement data. Our experimental results show that French has higher classification accuracy on beta frequency band while Chinese performs better on gamma frequency band. In addition, EEG signals and eye movement data of French participants have complementary characteristics in discriminating positive and negative emotions.",Emotion recognition; Deep learning; EEG; Eye movement; Cross-culture; FACIAL EXPRESSIONS; Chinese; French,,,emotion,No,Yes
web-of-science,Puzzling Out Emotions: A Deep-Learning Approach to Multimodal Sentiment Analysis,"Shrivastava, V; Richhariya, V; Richhariya, V; IEEE",2018,,,,,"Emotions steer both active and passive semantics of human interactions. Precise analysis of these emotions is indispensable to ensure a meaningful communication. Humans, in general, express their emotions in various forms. In order to encompass multiple dimensions of these expressions, this paper proposes a triple-layer (facial, verbal, and vocal) sentiment analysis system based on an application of deep-learning concepts. As such, in our experiment, first we separately examined facial expressions, verbal sentiments and vocal characteristics of a speaker and then mapped the individual results to perform a complete multimodal sentiment analysis. As a part of our two-stage facial expression analysis algorithm, we trained three multi-layer perceptrons using backpropagation technique to recognize a number of action units in human faces and seven single layer perceptrons each to identify one of seven basic human emotions (happiness, sadness, surprise, anger, fear, contempt or disgust, and neutral) expressed by the action units. In our vocal analysis module, we extracted important features (such as, jittering, shimmering, etc.) from sampled audio signals using standard formulae and used those features in a Bayesian Classifier to determine the type of sentiment (positive, negative, or neutral) in the voice. In the final segment of our experiment, we trained seven one dimensional convolutional neural networks to analyze verbal sentiments using the results of vocal analysis module as a bias. We were able to obtain results with as high as 91.80% (training) and 88% (testing) accuracies in our vocal and verbal analysis module; whereas, our facial expression analysis module provided results with 93.71% (training) and 92% (testing) accuracies.",sentiment analysis; convolutional neural networks; facial expression analysis; EXTRACTION; FACES; multi-layer perceptrons; vocal and verbal analysis,,,emotion,No,Yes
web-of-science,Audio-Video Based Multimodal Emotion Recognition Using SVMs and Deep Learning,"Sun, B; Xu, QH; He, J; Yu, LJ; Li, LD; Wei, QL",2016,,663,,10.1007/978-981-10-3005-5_51,"In this paper, we explored a multi-feature based classification framework for the Multimodal Emotion Recognition Challenge, which is part of the Chinese Conference on Pattern Recognition (CCPR 2016). The task of the challenge is to recognize one of eight facial emotions in short video segments extracted from Chinese films, TV plays and talk shows. In our framework, both traditional methods and Deep Convolutional Neural Network (DCNN) methods are used to extract various features. With different features, different classifiers are trained to predict video emotion labels. Moreover, a decision-level fusion method is explored to aggregate these different prediction results. According to the results on the competition database, our method shows better effectiveness on Chinese facial emotion.",Emotion recognition; Deep learning; Deep convolutional neural network; Decision-level fusion; Spatio-temporal information,,,emotion,Yes,No
web-of-science,Multimodal Affective Communication Analysis: Fusing Speech Emotion and Text Sentiment Using Machine Learning,"Faria, DR; Weinberg, AI; Ayrosa, PP",2024,,14,,10.3390/app14156631,"Affective communication, encompassing verbal and non-verbal cues, is crucial for understanding human interactions. This study introduces a novel framework for enhancing emotional understanding by fusing speech emotion recognition (SER) and sentiment analysis (SA). We leverage diverse features and both classical and deep learning models, including Gaussian naive Bayes (GNB), support vector machines (SVMs), random forests (RFs), multilayer perceptron (MLP), and a 1D convolutional neural network (1D-CNN), to accurately discern and categorize emotions in speech. We further extract text sentiment from speech-to-text conversion, analyzing it using pre-trained models like bidirectional encoder representations from transformers (BERT), generative pre-trained transformer 2 (GPT-2), and logistic regression (LR). To improve individual model performance for both SER and SA, we employ an extended dynamic Bayesian mixture model (DBMM) ensemble classifier. Our most significant contribution is the development of a novel two-layered DBMM (2L-DBMM) for multimodal fusion. This model effectively integrates speech emotion and text sentiment, enabling the classification of more nuanced, second-level emotional states. Evaluating our framework on the EmoUERJ (Portuguese) and ESD (English) datasets, the extended DBMM achieves accuracy rates of 96% and 98% for SER, 85% and 95% for SA, and 96% and 98% for combined emotion classification using the 2L-DBMM, respectively. Our findings demonstrate the superior performance of the extended DBMM for individual modalities compared to individual classifiers and the 2L-DBMM for merging different modalities, highlighting the value of ensemble methods and multimodal fusion in affective communication analysis. The results underscore the potential of our approach in enhancing emotional understanding with broad applications in fields like mental health assessment, human-robot interaction, and cross-cultural communication.",deep learning; machine learning; sentiment analysis; data fusion; multimodality; speech emotion recognition; affective communication; dynamic Bayesian mixture model,,,emotion,No,Yes
web-of-science,Multimodal emotion classification using machine learning in immersive and non-immersive virtual reality,"Lima, R; Chirico, A; Varandas, R; Gamboa, H; Gaggioli, A; Badia, SB",2024,,28,,10.1007/s10055-024-00989-y,"Affective computing has been widely used to detect and recognize emotional states. The main goal of this study was to detect emotional states using machine learning algorithms automatically. The experimental procedure involved eliciting emotional states using film clips in an immersive and non-immersive virtual reality setup. The participants' physiological signals were recorded and analyzed to train machine learning models to recognize users' emotional states. Furthermore, two subjective ratings emotional scales were provided to rate each emotional film clip. Results showed no significant differences between presenting the stimuli in the two degrees of immersion. Regarding emotion classification, it emerged that for both physiological signals and subjective ratings, user-dependent models have a better performance when compared to user-independent models. We obtained an average accuracy of 69.29 +/- 11.41% and 71.00 +/- 7.95% for the subjective ratings and physiological signals, respectively. On the other hand, using user-independent models, the accuracy we obtained was 54.0 +/- 17.2% and 24.9 +/- 4.0%, respectively. We interpreted these data as the result of high inter-subject variability among participants, suggesting the need for user-dependent classification models. In future works, we intend to develop new classification algorithms and transfer them to real-time implementation. This will make it possible to adapt to a virtual reality environment in real-time, according to the user's emotional state.",Emotions; Machine learning; Physiological signals; Affective computing; Virtual reality; Wearables; RECOGNITION; DATABASE; APPRAISAL; STRESS; VALENCE,,,emotion,No,Yes
web-of-science,Recognition of emotions using multimodal physiological signals and an ensemble deep learning model,"Yin, Z; Zhao, MY; Wang, YX; Yang, JD; Zhang, JH",2017,,140,,10.1016/j.cmpb.2016.12.005,"Background and Objective: Using deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions. Methods: In this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states. Results: DEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%. Conclusions: The superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances. (C) 2016 Elsevier Ireland Ltd. All rights reserved.",Emotion recognition; Physiological signals; Deep learning; EEG; Affective computing; Ensemble learning; BCI; CLASSIFICATION; FUSION; VECTOR,,,emotion,No,Yes
web-of-science,Deep learning based multimodal emotion recognition using model-level fusion of audio-visual modalities,"Middya, AI; Nag, B; Roy, S",2022,,244,,10.1016/j.knosys.2022.108580,"Emotion identification based on multimodal data (e.g., audio, video, text, etc.) is one of the most demanding and important research fields, with various uses. In this context, this research work has conducted a rigorous exploration of model-level fusion to find out the optimal multimodal model for emotion recognition using audio and video modalities. More specifically, separate novel feature extractor networks for audio and video data are proposed. After that, an optimal multimodal emotion recognition model is created by fusing audio and video features at the model level. The performances of the proposed models are assessed based on two benchmark multimodal datasets namely Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and Surrey Audio-Visual Expressed Emotion (SAVEE) using various performance metrics. The proposed models achieve high predictive accuracies of 99% and 86% on the SAVEE and RAVDESS datasets, respectively. The effectiveness of the models are also verified by comparing their performances with the existing emotion recognition models. Some case studies are also conducted to explore the model's ability to capture the variability of emotional states of the speakers in publicly available real-world audio-visual media.(c) 2022 Elsevier B.V. All rights reserved.",Deep learning; Multimodal emotion recognition; Classification; Audio features; Video features,,,emotion,No,Yes
web-of-science,Emotion Intensity Detection in Online Media: An Attention Mechanism Based Multimodal Deep Learning Approach,"Chai, YC",2024,,31,,10.17559/TV-20230628001154,"With the increasing influence of online public opinion, mining opinions and trend analysis from massive data of online media is important for understanding user sentiment, managing brand reputation, analyzing public opinion and optimizing marketing strategies. By combining data from multiple perceptual modalities, more comprehensive and accurate sentiment analysis results can be obtained. However, using multimodal data for sentiment analysis may face challenges such as data fusion, modal imbalance and inter -modal correlation. To overcome these challenges, the paper introduces an attention mechanism to multimodal sentiment analysis by constructing text, image, and audio feature extractors and using a custom cross -modal attention layer to compute the attention weights between different modalities, and finally fusing the attention -weighted features for sentiment classification. Through the cross -modal attention mechanism, the model can automatically learn the correlation between different modalities, dynamically adjust the modal weights, and selectively fuse features from different modalities, thus improving the accuracy and expressiveness of sentiment analysis.",attention mechanism; multimodal; emotion detection; NETWORK; online media,,,emotion,No,Yes
web-of-science,Emotion Recognition and Classification of Film Reviews Based on Deep Learning and Multimodal Fusion,"Na, RS; Sun, N",2022,,2022,,10.1155/2022/2024352,"In terms of cross-cultural exchanges, the film is not only an important embodiment of a country's cultural soft power but also the most direct and favorable way of communication. The advent of the all-around well-off era has propelled people's demand for spiritual, cultural, and entertainment which promotes the vigorous development of the film culture industry. The expansion and development of China's film market, domestic films, are a reflection and extension of China's culture and ideas. It plays an extremely important role in enhancing cultural self-confidence and cultural output. In order to better grasp the emotional tendency of the audience, understand the viewing needs, and put forward suggestions on domestic film production, it is very necessary to analyze the emotion of film reviews and dig deep into semantics. Since the evaluation of film works considers many factors that are complex and changeable, the choice of model plays a significant role in the process of emotion analysis. The deep learning model represented by a deep neural network has high tolerance to sentence noise, has strong information discrimination, and features self-learning ability. It also has great advantages in emotion classification tasks. This study conducts an in-depth study and research on the traditional emotion analysis methods and finally puts forward an effective emotion analysis framework that combines the traditional emotion analysis method and deep learning network. This framework enhances the text vectorization representation and emotion classification model by performing emotion analysis. The effectiveness is verified by corresponding experiments which justify the superiority of the approach.",INTELLIGENCE; METAANALYSIS; PERFORMANCE; FREEDOM TECHNIQUES; OUTCOMES,,,emotion,No,Yes
web-of-science,Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns,"Jaiswal, M; Assoc Advancement Artificial Intelligence",2020,,34,,,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. These predicted emotions are used in variety of downstream applications: (a) generating more human like dialogues, (b) predicting mental health issues, and (c) hate speech detection and intervention. To enable this, data are transmitted from users' devices and stored on central servers. These data are then processed further, either annotated or used as inputs for training a model for a specific task. Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary. My work focuses on two major issues that are faced while training emotion recognition algorithms: (a) privacy of the generated representations and, (b) explaining and ensuring that the predictions are robust to various situations. Tackling these issues would lead to emotion based algorithms that are deployable and helpful at a larger scale, thus enabling more human like experience when interacting with AI.",,,,emotion,Yes,No
web-of-science,Multimodal Real-Time patient emotion recognition system using facial expressions and brain EEG signals based on Machine learning and Log-Sync methods,"Mutawa, AM; Hassouneh, A",2024,,91,,10.1016/j.bspc.2023.105942,"Human Machine Interface (HMI) depends on emotion detection, especially for hospitalized patients. The emergence of the fourth industrial revolution (4IR) has heightened the interest in emotional intelligence in human-computer interaction (HCI). This work employs electroencephalography (EEG), an optical flow algorithm, and machine learning to create a multimodal intelligent real-time emotion recognition system. The objective is to assist hospitalized patients, disabled (deaf, mute, and bedridden) individuals, and autistic youngsters in expressing their authentic feelings. We fed our multimodal feature fusion vector to a classifier with long short-term memory (LSTM). We distinguished six fundamental emotions: anger, disgust, fear, sadness, joy, and surprise. The fusion feature vector was created utilizing the patient's geometric facial characteristics and EEG inputs. Utilizing 14 EEG inputs, we used four-band relative power channels, namely alpha (8-13 Hz), beta (13-30 Hz), gamma (30-49 Hz), and theta (4-8 Hz). We achieved a maximum recognition rate of 90.25 percent using just facial landmarks and 87.25 percent using only EEG data. When both facial and EEG streams were examined, we achieved 99.3 percent accuracy in a multimodal method.",Multi-modal; Emotion recognition; Face recognition; EEG; Neural network; Hospitalized patients; Log-sync,,,emotion,No,Yes
